[{"ring":"assess","blip_id":202304063,"quadrant":"tools","volume_date":"2023-04","description":"Knowledge management is critical for tech workers, as we need to be constantly learning and staying up to date with the latest technology developments. Recently, tools such as Obsidian and Logseq have emerged in the category of note-taking tools that support linking notes to form a knowledge graph, while storing them in plain markdown files in a local directory, thus letting users own their data. These tools help users organize and link their notes in a flexible, nonlinear way.\n\nObsidian has a rich repository of community plugins. Some that have caught our attention, in particular, are Canvas, akin to a local version of Miro or Mural, and Dataview, which effectively treats your notes as a database and provides a query language for filtering, sorting and extracting data from your markdown notes.","blip_selector":"obsidian","name":"Obsidian","display_name":"Obsidian","url":"/radar/tools/obsidian","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1133,"quadrant":"techniques","volume_date":"2023-04","description":"We keep getting good feedback from teams applying product management to internal platforms. One key feature to remember, though: It's not just about team structure or renaming existing platform teams; it’s also about applying product-centric working practices within the team. Specifically, we've received feedback that teams face challenges with this technique unless they have a product-centric mindset. This likely means additional roles, such as a product manager, alongside changes to other areas, such as requirements gathering and the measurement of success. Working this way means establishing empathy with internal consumers (the development teams) and collaborating with them on the design. Platform product managers create roadmaps and ensure the platform delivers value to the business and enhances the developer experience. We continue to see this technique as key to building internal platforms to roll out new digital solutions quickly and efficiently. | More and more companies are building internal platforms to roll out new digital solutions quickly and efficiently. Companies that succeed with this strategy are applying product management to internal platforms. This means establishing empathy with internal consumers (the development teams) and collaborating with them on the design. Platform product managers create roadmaps and ensure the platform delivers value to the business and enhances the developer experience. Unfortunately, we're also seeing less successful approaches, where teams create a platform in the void, based on unverified assumptions and without internal customers. These platforms, often despite aggressive internal tactics, end up being underutilized and a drain on the organization's delivery capability. As usual, good product management is all about building products that consumers love. | We've seen a steep increase in interest in the topic of digital platforms over the past 12 months. Companies looking to roll out new digital solutions quickly and efficiently are building internal platforms, which offer teams self-service access to the business APIs, tools, knowledge and support necessary to build and operate their own solutions. We find that these platforms are most effective when they're given the same respect as an external product offering. Applying product management to internal platforms means establishing empathy with internal consumers (read: developers) and collaborating with them on the design. Platform product managers establish roadmaps and ensure the platform delivers value to the business and enhances the developer experience. Some owners even create a brand identity for the internal platform and use that to market the benefits to their colleagues. Platform product managers look after the quality of the platform, gather usage metrics, and continuously improve it over time. Treating the platform as a product helps to create a thriving ecosystem and avoids the pitfall of building yet another stagnant, underutilized service-oriented architecture. | We've seen a steep increase in interest in the topic of digital platforms over the past 12 months. Companies looking to roll out new digital solutions quickly and efficiently are building internal platforms, which offer teams self-service access to the business APIs, tools, knowledge and support necessary to build and operate their own solutions. We find that these platforms are most effective when they're given the same respect as an external product offering. Applying product management to internal platforms means establishing empathy with internal consumers (read: developers) and collaborating with them on the design. Platform product managers establish roadmaps and ensure the platform delivers value to the business and enhances the developer experience. Some owners even create a brand identity for the internal platform and use that to market the benefits to their colleagues. Platform product managers look after the quality of the platform, gather usage metrics, and continuously improve it over time. Treating the platform as a product helps to create a thriving ecosystem and avoids the pitfall of building yet another stagnant, underutilized service-oriented architecture.","blip_selector":"applying-product-management-to-internal-platforms","name":"Applying product management to internal platforms","display_name":"Applying product management to internal platforms","url":"/radar/techniques/applying-product-management-to-internal-platforms","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304037,"quadrant":"tools","volume_date":"2023-04","description":"Within any organization, API producers and consumers need to stay in sync about the schemas that will be used for communication among them. Especially as the number of APIs and related producers and consumers grow in the organization, what may start with simply passing around schemas among teams will start to hit scaling challenges. Faced with this issue, some of our teams have turned to Apicurio Registry, an open-source, centralized registry for various types of schemas and API artifacts, including OpenAPI specifications and Protobuf and Avro schemas. Apicurio Registry allows users to interact with it through a UI as well as a REST API and a Maven plugin. It also has the option to enforce schema evolution restrictions, such as backward compatibility. Moreover, when it comes to working with Kafka clients, Apicurio Registry is compatible with Confluent Schema Registry. While our teams have found Confluent Schema Registry's documentation more helpful, Apicurio Registry meets their needs for a source of truth for various schemas.","blip_selector":"apicurio-registry","name":"Apicurio Registry","display_name":"Apicurio Registry","url":"/radar/tools/apicurio-registry","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304031,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"When working on the WebVR experimental API, it became clear that it would make more sense to have a combined API for VR and AR. Rather than changing the WebVR API significantly, a new specification was created: WebXR. At its core is the WebXR Device API which provides key capabilities for writing VR and AR applications in a web browser. The API is extensive, and at the time of writing it isn’t fully supported by all browsers. Our teams have used WebXR on several occasions, and we see the benefits described by the Immersive Web Working Group. For prototypes, we especially like that the experience is available immediately in a web browser. The development team doesn't have to go through an app-store process, and users can play with the experience without having to install an app. Given the status of the API and the fact it’s hidden behind a feature toggle in some browsers, we haven’t seen it used beyond proofs of concept and prototypes.","blip_selector":"webxr-device-api","name":"WebXR Device API","display_name":"WebXR Device API","url":"/radar/languages-and-frameworks/webxr-device-api","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304039,"quadrant":"platforms","volume_date":"2023-04","description":"Faced with the challenge of exploring large configuration spaces, where it may take a significant amount of time to evaluate a given configuration, teams can turn to adaptive experimentation, a machine-guided, iterative process to find optimal solutions in a resource-efficient manner. Ax is a platform for managing and automating adaptive experiments, including machine learning experiments, A/B tests and simulations. Currently, it supports two optimization strategies: Bayesian optimization using BoTorch, which is built on top of PyTorch, and contextual bandits. Facebook, when releasing Ax and BoTorch, described use cases like increasing the efficiency of back-end infrastructure, tuning ranking models and optimizing hyperparameter search for a machine learning platform. We've had good experiences using Ax for a variety of use cases, and while tools for hyperparameter tuning exist, we're unaware of a platform that provides functionality in a scope similar to Ax.","blip_selector":"ax","name":"Ax","display_name":"Ax","url":"/radar/platforms/ax","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304093,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Many people speak more than one language and use different languages in different contexts. Devices and platforms that can run apps generally ask the user to select one language for the system and then make the apps follow suit. For mobile phones, in particular, users may prefer certain apps to run in a language other than the system's language; Apple introduced a per-app language setting in iOS a while ago. Android app developers, however, had to implement a custom solution within their apps if they wanted to provide this option — until now. Android 13 introduced a new system setting, per-app language preferences, and a public API, making it easier for developers to offer this feature. For backward compatibility, equivalent APIs are available in AndroidX via AppCompatDelegate. We encourage developers to replace their custom solutions and instead use this feature in their apps.","blip_selector":"per-app-language-preferences","name":"Per-app language preferences","display_name":"Per-app language preferences","url":"/radar/languages-and-frameworks/per-app-language-preferences","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304057,"quadrant":"tools","volume_date":"2023-04","description":"Measuring energy consumption is an important step for teams to reduce the carbon footprint of their software. Cloud Carbon Footprint (CCF) estimates energy based on billing and usage data retrieved from cloud APIs. Kepler — short for Kubernetes-based Efficient Power Level Exporter — goes one step further: it uses software counters via RAPL, ACPI and nvml to measure power consumption by hardware resources and employs an eBPF-based approach to attribute power consumption to processes, containers and Kubernetes pods. Power usage is then converted to energy estimates using a custom ML model and data from the SPEC Power benchmark. Finally, pod-level energy consumption reporting is made available as Prometheus metrics. In cases where Kubernetes is running on virtual machines, for example when not using bare metal instances, Kepler uses cgroups to estimate energy consumption. We've had significant experience with CCF and can attest to its usefulness, but we're intrigued by the Kepler project’s approach.","blip_selector":"kepler","name":"Kepler","display_name":"Kepler","url":"/radar/tools/kepler","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010098,"quadrant":"techniques","volume_date":"2023-04","description":"When we first included it in the Radar three years ago, verifiable credentials (VC) was an intriguing standard with some promising potential applications, but it wasn't widely known or understood outside the community of enthusiasts. This was particularly true when it came to the credential-granting institutions, such as state governments, who would be responsible for implementing the standards. Three years and one pandemic later, the demand for cryptographically secure, privacy-respecting and machine-verifiable electronic credentials has grown and, as a result, governments are starting to wake up to VC's potential. The W3C standard puts credential holders at the center, which is similar to our experience when using physical credentials: users can put their verifiable credentials in their own digital wallets and show them to anyone at any time without the permission of the credentials' issuer. This decentralized approach also helps users to better manage and selectively disclose their own information which greatly improves data privacy protection.\n\nSeveral of our teams have engaged in projects involving verifiable credentials technology in the past six months. Not surprisingly, the scenarios vary across countries and government departments. Our team has explored different combinations of decentralized identifiers, verifiable credentials and verifiable presentation on multiple projects. This is a developing field, and now that we've had more experience, we want to keep track of it in the Radar. | When we first included it in the Radar two years ago, verifiable credentials (VC) was an intriguing standard with some promising potential applications, but it wasn't widely known or understood outside the community of enthusiasts. This was particularly true when it came to the credential-granting institutions, such as state governments, who would be responsible for implementing the standards. Two years and one pandemic later, the demand for cryptographically secure, privacy-respecting and machine-verifiable electronic credentials has grown and, as a result, governments are starting to wake up to VC's potential. We're now starting to see VC crop up in our work for public-sector clients. The W3C standard puts credential holders at the center, which is similar to our experience when using physical credentials: users can put their verifiable credentials in their own digital wallets and show them to anyone at any time without the permission of the credentials' issuer. This decentralized approach also enables users to better manage and selectively disclose their own information which greatly improves data privacy protection. For example, powered by zero-knowledge proof technology, you can construct a verifiable credential to prove that you're an adult without revealing your birthday. It’s important to note that although many VC-based decentralized identity solutions rely on blockchain technology, blockchain is not a prerequisite for all VC implementations. | Credentials are everywhere in our lives and include passports, driver’s licenses and academic certificates. However, most digital credentials today are simple data records from information systems that are easy to modify and forge and often expose unnecessary information. In recent years, we've seen the continuous maturity of Verifiable Credentials solve this issue. The W3C standard defines it in a way that is cryptographically secure, privacy respecting and machine verifiable. The model puts credential holders at the center, which is similar to our experience when using physical credentials: users can put their verifiable credentials in their own digital wallets and show them to anyone at any time without the permission of the credentials’ issuer. This decentralized approach also enables users to better manage their own information and selectively disclose certain information and greatly improves data privacy protection. For example, powered by zero-knowledge proof technology, you can construct a verifiable credential to prove that you are an adult without revealing your birthday. The community has developed many use cases around verifiable credentials. We've implemented our own COVID health certification with reference to the COVID-19 Credentials Initiative (CCI). Although verifiable credentials don't rely on blockchain technology or decentralized identity, this technique often works with DID in practice and uses blockchain as a verifiable data registry. Many decentralized identity frameworks are also embedded with verifiable credentials.","blip_selector":"verifiable-credentials","name":"Verifiable credentials","display_name":"Verifiable credentials","url":"/radar/techniques/verifiable-credentials","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304105,"quadrant":"platforms","volume_date":"2023-04","description":"Strapi is an open-source, NodeJS-based, headless content management system (CMS) similar to Contentful. It has been around for a while, and we've used it successfully in a few projects. Strapi provides both REST and GraphQL APIs, has comprehensive documentation, features an easy-to-use data model API and supports customizing both UI and logic.","blip_selector":"strapi","name":"Strapi","display_name":"Strapi","url":"/radar/platforms/strapi","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304004,"quadrant":"platforms","volume_date":"2023-04","description":"Arm compute instances in the cloud have become increasingly popular over the past few years due to their cost and energy efficiency compared to traditional x86-based instances. Many cloud providers now offer Arm-based instances, including AWS, Azure and GCP. The cost benefits of running Arm in the cloud can be particularly beneficial for businesses that run large workloads or need to scale. Based on our experiences we recommend Arm compute instances for all workloads unless there are architecture-specific dependencies. The tooling to support multiple architectures, like multi-arch docker images, also simplify build and deploy workflows.","blip_selector":"arm-in-the-cloud","name":"Arm in the cloud","display_name":"Arm in the cloud","url":"/radar/platforms/arm-in-the-cloud","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304084,"quadrant":"platforms","volume_date":"2023-04","description":"Immuta is a data security platform that allows you to secure access to your data, automatically discover sensitive data and audit how data is being used in an organization. In the past, we've talked about the importance of automation, engineering practices and treating security policy as code when we think about security concerns. Data security is no different. Our teams have been exploring Immuta to manage data policies as code to allow for fine-grained access control which is beyond what role-based access control (RBAC) can offer. Version-controlled policies can be tested and then provisioned as part of a CI/CD pipeline. In a decentralized data ecosystem, like one facilitated by data mesh, having domain-specific roles can lead to role or group proliferation in the identity system. Immuta’s attribute-based access control (ABAC) capability reduces the access grant to a mathematical equation of matching an \"attribute\" on the user to a \"tag\" on the data source. This platform is still new but certainly worth highlighting for data security needs.","blip_selector":"immuta","name":"Immuta","display_name":"Immuta","url":"/radar/platforms/immuta","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304028,"quadrant":"tools","volume_date":"2023-04","description":"While GitHub Actions runners cover a wide range of the most common runtimes, sometimes you need something that is more specific to your particular use case, such as a less common language runtime or a particular hardware configuration. That's when you need a self-hosted runner. Philips's self-hosted GitHub runner is a Terraform module that lets you spin up custom runners on AWS EC2 spot instances. The module also creates a set of Lambdas to make up for the fact that you lose some of GitHub Actions' lifecycle management when you self-host runners. They do the heavy lifting for scaling runners up and down as needed. That helps manage costs and allows you to make runners ephemeral, which helps improve repeatability and security. When you do need to self-host runners, you might miss a lot of things when building custom runners from scratch. Look for tools like this one instead.","blip_selector":"philips-s-self-hosted-github-runner","name":"Philips's self-hosted GitHub runner","display_name":"Philips's self-hosted GitHub runner","url":"/radar/tools/philips-s-self-hosted-github-runner","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304052,"quadrant":"tools","volume_date":"2023-04","description":"Giskard is an open-source tool designed to help organizations build more robust and ethical AI models by providing quality assurance capabilities with a focus on explainability and fairness. It facilitates cooperation between technical and nontechnical stakeholders, allowing them to evaluate models collaboratively and establish acceptance criteria based on bias avoidance and other essential quality metrics. Giskard ensures model outcomes are better aligned with business objectives and helps to solve quality issues before production deployment.","blip_selector":"giskard","name":"Giskard","display_name":"Giskard","url":"/radar/tools/giskard","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304048,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"The Rust language has been gaining popularity in recent years for its safety, performance and concurrency features. However, certified Rust toolchains have been missing for applications in safety-critical markets like automotive. This gap is being addressed by Ferrocene, a Rust compiler toolchain. Ferrocene promises to be ISO26262 functional safety standard compliant for the electronic systems in road vehicles; an effort to qualify the language and toolchain for use in such domains is already underway. We're excited by its progress and the availability of such safety-compliant tools will certainly speed up the adoption of Rust in the automotive industry.","blip_selector":"ferrocene","name":"Ferrocene","display_name":"Ferrocene","url":"/radar/languages-and-frameworks/ferrocene","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304030,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"SolidJS is a declarative JavaScript library for creating user interfaces. In the last year, we've seen an increase in SolidJS's visibility and popularity among developers, particularly those interested in creating richer user interactions. SolidJS compiles its templates to real DOM nodes (instead of using vDOM) and updates them with fine-grained reactions which reduces unnecessary DOM updates and results in faster performance and a better user experience. It has a simple API and great support for TypeScript, which can help catch errors during development. Another benefit of SolidJS is its small bundle size, which is ideal for building fast and lightweight web applications and benefits a mobile-first approach. SolidJS is a relatively new framework, so it doesn't have as large of a community or ecosystem as other frameworks. However, judging by the growing number of useful libraries and tools, it seems to be growing in popularity. Its reactive update system, functional component model and templating system make SolidJS an attractive choice to assess, and we're seeing interest from several teams and communities.","blip_selector":"solidjs","name":"SolidJS","display_name":"SolidJS","url":"/radar/languages-and-frameworks/solidjs","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304053,"quadrant":"tools","volume_date":"2023-04","description":"GitHub Copilot is an AI coding assistant, created by a collaboration between Microsoft and OpenAI. It uses machine learning models to generate suggestions based on the context of what a developer is working on. It features strong IDE integration and uses an existing codebase and editor context to create suggestions. Despite being billed as \"your AI pair programmer\" we would not call what it does \"pairing\" — we'd probably describe it as a kind of supercharged, context-sensitive Stack Overflow. When it correctly predicts what a developer is trying to do, it can be a powerful tool for getting stuff done. Like all LLM-based AIs, though, it has a tendency to hallucinate the use of plausible but nonexistent APIs and may introduce bugs through slightly faulty algorithms. We've had success generating code at the line, block and method level, as well as creating tests or infrastructure configurations. Interestingly, it works best when you use good naming practices, so it encourages more readable code.\n\nAI tool capabilities are advancing rapidly, and we think it's sensible for organizations to try them. Some sales pitches for Copilot have claimed very high efficiency gains, but we remain skeptical: after all, writing code isn't the only thing that developers spend time on, and it's notoriously difficult to measure developer productivity in the first place. That said, Copilot is a fairly inexpensive tool; if it offers any productivity gain at all, it's probably worth it. Copilot X — in preview as of this writing — offers additional functionality and integration within a software creation workflow. Copilot has a \"for business\" offering, which provides more clarity around intellectual property issues as well as the ability to manage tool features centrally across an organization. We think these features are critical for enterprise adoption.","blip_selector":"github-copilot","name":"GitHub Copilot","display_name":"GitHub Copilot","url":"/radar/tools/github-copilot","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304094,"quadrant":"techniques","volume_date":"2023-04","description":"Prompt engineering refers to the process of designing and refining prompts for generative AI models to obtain high-quality responses from the model. This involves carefully crafting prompts that are specific, clear and relevant to the desired task or application in order to elicit useful outputs from the model. Prompt engineering aims to enhance large language model (LLM) capabilities in tasks like question answering and arithmetic reasoning or in domain-specific contexts. For software creation, you might use prompt engineering to get an LLM to write a story, an API or a test suite based on a brief conversation with a stakeholder or some notes. Developing effective prompting techniques is becoming a valuable skill in working with AI systems. There is debate over whether prompt engineering is an art or science, and potential security risks, such as “prompt injection attacks,” should be considered.","blip_selector":"prompt-engineering","name":"Prompt engineering","display_name":"Prompt engineering","url":"/radar/techniques/prompt-engineering","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304019,"quadrant":"tools","volume_date":"2023-04","description":"Mend SCA (software composition analysis), previously Whitesource, helps detect open-source software dependencies by identifying if they are up to date, contain security flaws or have licensing requirements. Our teams have had good experience with integrating Mend SCA in their paths to production. Right from IDE integration, raising an automatic PR based on an identified issue to integrating into the CI/CD pipeline, this tool offers a great developer experience. Other popular SCA tools, such as Snyk, are comparable and also worth exploring for your security needs.","blip_selector":"mend-sca","name":"Mend SCA","display_name":"Mend SCA","url":"/radar/tools/mend-sca","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304041,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"dbt-expectations is an extension package for dbt inspired by Great Expectations. Data quality is an important tenet of data governance, so when it comes to automated data governance, it's important to craft built-in controls that flag anomalies or quality issues in data pipelines. Just as unit tests run in a build pipeline, dbt-expectations makes assertions during the execution of a data pipeline. In the dbt world, you can run Great Expectations–style data quality tests on your warehouse directly within dbt. Our teams have been exploring this, and it made sense to highlight it.","blip_selector":"dbt-expectations","name":"dbt-expectations","display_name":"dbt-expectations","url":"/radar/languages-and-frameworks/dbt-expectations","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304038,"quadrant":"platforms","volume_date":"2023-04","description":"Autoware is an open-source autonomous driving software stack built on ROS (Robot Operating System) which can be used to develop and deploy advanced driver assist systems (ADAS) for a wide range of vehicles like cars and trucks. It provides a set of tools and algorithms for various aspects of autonomous driving, such as perception, decision-making and control. It also has a planning and control module that generates a trajectory for the vehicle based on its environment and objectives. It encourages open innovations in autonomous driving technology. We're building prototypes using Autoware to validate new product ideas and find it helpful.","blip_selector":"autoware","name":"Autoware","display_name":"Autoware","url":"/radar/platforms/autoware","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304066,"quadrant":"platforms","volume_date":"2023-04","description":"Spin is an open-source platform for building and running microservices in WebAssembly (WASM). In previous editions of the Radar, we talked about WebAssembly in the context of browsers, but we’re now witnessing the penetration on the server side due to its capabilities for fine-grained sandboxing, cross-language interoperability and hot reloading. With Spin CLI, you can quickly create and distribute WebAssembly microservices in Rust, TypeScript, Python and TinyGo. We're excited about Spin, and we recommend you carefully assess it as it moves out of early preview.","blip_selector":"spin","name":"Spin","display_name":"Spin","url":"/radar/platforms/spin","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1254,"quadrant":"platforms","volume_date":"2023-04","description":"Headless content management systems have become a common component of digital platforms. Contentful is still our default choice in this space, but new entrants like Strapi have impressed us too. We particularly like Contentful's API-first approach and implementation of CMS as code. It supports powerful content modeling primitives as code and content model evolution scripts, which allow it to be treated like other data store schemas and enable evolutionary database design practices to be applied to CMS development. Recently, Contentful has released an app framework to write apps that make it easier to adapt Contentful to individual business processes and to integrate with other services. Apps can be built by and for a specific organization but a marketplace for apps is emerging, too. | Headless content management systems (CMSes) are becoming a common component of digital platforms. Contentful is a modern headless CMS that our teams have successfully integrated into their development workflows. We particularly like its API-first approach and implementation of CMS as code. It supports powerful content modeling primitives as code and content model evolution scripts, which allow it to be treated like other data store schemas and enable evolutionary database design practices to be applied to CMS development. Its robustness and a stream of new features, including a sandbox environment, have impressed our teams further and made Contentful our default choice in this space. | Headless content management systems (CMSes) are becoming a common component of digital platforms. Contentful is a modern headless CMS that our teams have successfully integrated into their development workflows. We particularly like its API-first approach and implementing CMS as code. It supports powerful content modeling primitives as code and content model evolution scripts, which allow treating it as other data store schemas and applying evolutionary database design practices to CMS development. Other notable features that we've liked include inclusion of two CDNs to deliver media assets and JSON documents, good support for localization and the ability—albeit with some effort—to integrate with Auth0. | Headless Content Management Systems (CMSes) are becoming a common component of digital platforms. Contentful is a modern headless CMS that our teams have successfully integrated into their development workflows. We particularly like its API-first approach and implementing CMS as Code. It supports powerful content modelling primitives as code and content model evolution scripts, which allow treating it as other data store schemas and applying evolutionary database design practices to CMS development. Other notable features that we’ve liked include inclusion of two CDNs by default to deliver media assets and JSON documents, good support for localization, and the ability — albeit with some effort — to integrate with Auth0.","blip_selector":"contentful","name":"Contentful","display_name":"Contentful","url":"/radar/platforms/contentful","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304021,"quadrant":"tools","volume_date":"2023-04","description":"Design tokens are a useful mechanism for defining standard elements in design systems. But, keeping those design elements consistent across media such as mobile apps or web frameworks is an increasingly formidable task. Design token translation tools simplify this problem by organizing and automating transformation from the token description (in YAML or JSON) into the code that actually controls rendering in a given medium such as CSS, React components or HTML. Style Dictionary is an open-source example that is widely used and integrates well into automated build pipelines, but there are also commercial alternatives such as Specify.","blip_selector":"design-token-translation-tools","name":"Design token translation tools","display_name":"Design token translation tools","url":"/radar/tools/design-token-translation-tools","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304111,"quadrant":"tools","volume_date":"2023-04","description":"ChatGPT is an interesting tool that has the potential to be useful for various aspects of the software creation process. As a large language model (LLM) that has \"read\" billions of web pages, ChatGPT can provide additional perspectives and assist with different tasks, from generating ideas and requirements to creating code and tests. Its ability to work across multiple parts of the software lifecycle makes it a versatile tool that might improve efficiency and reduce errors in the development process. GPT4, the LLM that powers ChatGPT, now also has the ability to integrate with external tools such as a knowledge management repository, sandboxed coding environment or web search. For now, we think that ChatGPT is best used as an input to a process, such as helping with a first draft of a story or the boilerplate for a coding task, rather than a tool that produces \"fully baked\" results.\n\nThere are concerns around intellectual property and data privacy with these AI tools, including some unresolved legal questions, so we recommend organizations seek advice from their legal teams before use. Some of our clients have already begun experimenting with ChatGPT across various stages of the software lifecycle, and we encourage others to explore the tool and assess its potential benefits. We expect that, like GitHub Copilot, a \"for business\" offering will soon be available which may ease intellectual property concerns.","blip_selector":"chatgpt","name":"ChatGPT","display_name":"ChatGPT","url":"/radar/tools/chatgpt","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304026,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Galacean Engine is a web- and mobile-first interactive engine, designed to provide a seamless way to render component-based architecture and animation in a mobile-friendly manner. With its focus on lightweight and high-performance rendering, it has become an increasingly popular choice for developers creating engaging mobile games. It's a TypeScript-based engine that developers report outperforms alternatives.","blip_selector":"galacean-engine","name":"Galacean Engine","display_name":"Galacean Engine","url":"/radar/languages-and-frameworks/galacean-engine","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203031,"quadrant":"tools","volume_date":"2023-04","description":"Typesense is an open-source, typo-tolerant search engine optimized for low-latency and high-performance search experiences. If you're building a latency-sensitive search application with a search index size that can fit in memory, Typesense is a powerful alternative. Our teams use Typesense in high availability multi-node clusters to distribute workload and ensure critical search infrastructure is resilient. They had a good experience with Typesense in production, which is why we've moved it to Trial. | Typesense is a fast, typo-tolerant text search engine. For use cases with large volumes of data, Elasticsearch might still be a good option as it provides a horizontally scalable disk-based search solution. However, if you're building a latency-sensitive search application with a search index size that can fit in memory, Typesense is a powerful alternative and another option to evaluate alongside tools such as Meilisearch.","blip_selector":"typesense","name":"Typesense","display_name":"Typesense","url":"/radar/tools/typesense","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304047,"quadrant":"tools","volume_date":"2023-04","description":"Evidently is an open-source Python tool designed to help build monitoring for machine learning models to guarantee their quality and stable production operations. It can be used at various stages of a model lifecycle: as a dashboard to review the model in a notebook, as part of a pipeline or as a monitoring service after deployment. With a particular focus on model drift detection, Evidently also offers features such as model quality, data quality inspection and target drift detection. In addition, it has many built-in metrics, associated visualizations and tests which are easily combined into a report, dashboard or a test-driven pipeline.","blip_selector":"evidently","name":"Evidently","display_name":"Evidently","url":"/radar/tools/evidently","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304008,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Jetpack DataStore is a new data storage solution to store data asynchronously, consistently and transactionally. It has two implementations: Preferences DataStore for untyped key-value pairs and Proto DataStore for complex data types using Protobufs. By default it is used with Kotlin coroutines and Flow but additional support for RXJava 2 and 3 is available. The documentation recommends you consider migrating to DataStore if you're currently using SharedPreferences, and we agree with that recommendation.","blip_selector":"jetpack-datastore","name":"Jetpack DataStore","display_name":"Jetpack DataStore","url":"/radar/languages-and-frameworks/jetpack-datastore","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202304013,"quadrant":"techniques","volume_date":"2023-04","description":"The options for CI/CD infrastructure as a service have become so manifold and mature that the cases in which it's worth managing your entire CI infrastructure yourself are becoming very rare. Using managed services like GitHub Actions, Azure DevOps or Gitlab CI/CD comes with all the common advantages (and trade-offs) of managed cloud services. You don't have to spend time, effort and hardware costs on maintenance and operations of this often complex infrastructure. Teams can take advantage of elasticity and self-service, whereas provisioning more of the right agents or getting a new plugin or feature are often a bottleneck in companies that host CI themselves. Even the use cases that require to run build and verification on your own hardware can now mostly be covered with self-hosted runners (we've written about some for GitHub Actions, actions-runner-controller and the Philips's self-hosted GitHub runner). Note, however, that you won’t get out-of-the-box security just because you are using a managed services; while mature services provide all the security features you need, you'll still need to use them to implement zero trust security for your CI/CD infrastructure.","blip_selector":"ci-cd-infrastructure-as-a-service","name":"CI/CD infrastructure as a service","display_name":"CI/CD infrastructure as a service","url":"/radar/techniques/ci-cd-infrastructure-as-a-service","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304032,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":".NET 7 Native AOT is a big step forward in a long line of approaches to deploying .NET applications natively. It does away with IL and JIT at runtime entirely. Introduced in .NET 7, this improvement is particularly significant for running .NET applications in serverless functions. This new deployment option eliminates the cold start issue, which has been a persistent problem for .NET on serverless platforms like AWS Lambda and Azure Functions. With Native AOT, you can generate a smaller deployable binary than previous methods, resulting in faster cold start times. AWS has officially embraced Native AOT, supporting it with their Amazon Lambda Tools. This new deployment option brings .NET 7 on par with TypeScript/JavaScript in terms of cold start times, making it a viable option for organizations with a largely .NET-oriented infrastructure.","blip_selector":"net-7-native-aot","name":".NET 7 Native AOT","display_name":".NET 7 Native AOT","url":"/radar/languages-and-frameworks/net-7-native-aot","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304035,"quadrant":"platforms","volume_date":"2023-04","description":"The \"end of passwords\" might be near, finally. Shepherded by the FIDO alliance and backed by Apple, Google and Microsoft, passkeys are nearing mainstream usability. When setting up a new login with passkeys, a key pair is generated: the website receives the public key and the user keeps the private key. Handling login uses asymmetric cryptography. The user proves that they're in possession of the private key, but, unlike passwords, it’s never sent to the website. On users' devices, access to passkeys is protected using biometrics or a PIN.\n\nPasskeys can be stored and synced within the Big Tech ecosystems, using Apple's iCloud Keychain, Google Password Manager or Windows Hello. In most cases this works only with recent OS and browser versions. Notably, storing passkeys in Windows Hello is not supported on Windows 10. Fortunately, though, the Client to Authenticator Protocol (CTAP) makes it possible for passkeys to be kept on a different device other than the one that creates the key or needs it for login. For example, a user creates a passkey for a website on Windows 10 and stores it on an iPhone by scanning a QR code. Because the key is synced via iCloud the user can log in to the website from, say, their MacBook. Passkeys can be stored on hardware security keys, too, and support for native apps has arrived on iOS and Android.\n\nDespite some usability issues — for example, Bluetooth needs to work because device proximity is checked when a QR code is scanned — passkeys are worth considering. We suggest you experiment with them on passkeys.io to get a feeling for their usability.","blip_selector":"passkeys","name":"Passkeys","display_name":"Passkeys","url":"/radar/platforms/passkeys","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304001,"quadrant":"techniques","volume_date":"2023-04","description":"One of the many places in the software delivery process to consider accessibility requirements early on is during web component testing. Testing framework plugins like chai-a11y-axe provide assertions in their API to check for the basics. But in addition to using what testing frameworks have to offer, accessibility-aware component test design further helps to provide all the semantic elements needed by screen readers and other assistive technologies.\n\nFirstly, instead of using test ids or classes to find and select the elements you want to validate, use a principle of identifying elements by ARIA roles or other semantic attributes that are used by assistive technologies. Some testing libraries, like Testing Library, even recommend this in their documentation. Secondly, do not just test for click interactions; also consider users who cannot use a mouse or see the screen, and consider adding additional tests for the keyboard and other interactions.","blip_selector":"accessibility-aware-component-test-design","name":"Accessibility-aware component test design","display_name":"Accessibility-aware component test design","url":"/radar/techniques/accessibility-aware-component-test-design","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304022,"quadrant":"tools","volume_date":"2023-04","description":"Helmfile is an open-source command line tool and a declarative specification for managing and installing a collection of Helm charts. You can use it to help with version control of the Helm values files, the charts used and other overrides. It enables CI/CD workflows with Helm charts and helps create reproducible environments. We've used Helmfile to manage complex deployments with multiple dozens of Helm charts and found it simplifies the deployment workflow.","blip_selector":"helmfile","name":"Helmfile","display_name":"Helmfile","url":"/radar/tools/helmfile","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304104,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Stencil is a library that enables developers to build reusable Web Components using well-established tools such as TypeScript, JSX and JSDoc. According to our teams' experiences, Stencil is a very good choice for building platform-agnostic design systems. For the few browsers that don't support modern browser features, Stencil ensures compatibility by polyfilling unsupported features and APIs on demand.","blip_selector":"stencil","name":"Stencil","display_name":"Stencil","url":"/radar/languages-and-frameworks/stencil","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210008,"quadrant":"tools","volume_date":"2023-04","description":"Soda Core is an open-source data quality and observability tool. Our teams have used it to validate data as it arrives in a system, before and after transformations, and set up automated monitoring checks for anomalies. We're happy with SodaCL, the DSL for writing data checks in Soda Core, as it helps team members other than data engineers write quality checks. Overall, our experience using Soda Core to find and resolve data issues at scale has been positive. | Soda Core is an open-source data quality and observability tool. We talked about Great Expectations previously in the Radar, and Soda Core is an alternative with a key difference — you express the data validations in a DSL called SodaCL (previously called Soda SQL) as opposed to Python functions. Once the validations are written, it can be executed as part of a data pipeline or scheduled to run programmatically. As we become increasingly data-driven, it's critical to maintain data quality, and we encourage you to assess Soda Core.","blip_selector":"soda-core","name":"Soda Core","display_name":"Soda Core","url":"/radar/tools/soda-core","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304051,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"In data engineering we're seeing a bewildering choice of tools and technologies. For less experienced engineers especially, it can make sense to work with an abstraction layer to get into the tools, to focus on the task at hand without having to learn several technology-specific APIs and to have the option of switching underlying technologies without too much effort. Fugue is such an abstraction layer. It provides a unified interface for distributed computing, which makes it possible to run Python, pandas and SQL code on Spark, Dask, Ray and DuckDB with minimal rewrites. However, if your team has already decided on a set of technologies, and if they're familiar with their APIs and deep into tweaking and tuning their backend systems, such an abstraction layer provides less value in our experience.","blip_selector":"fugue","name":"Fugue","display_name":"Fugue","url":"/radar/languages-and-frameworks/fugue","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210007,"quadrant":"tools","volume_date":"2023-04","description":"Kubeshark is an API traffic viewer for Kubernetes. Until November 2022, it was known as Mizu. Unlike other tools, Kubeshark does not require instrumentation or code changes. It runs as a DaemonSet to inject a container at the node level in your Kubernetes cluster and performs tcpdump-like operations. We find it useful as a debugging tool, as it can observe all API communications across multiple protocols (REST, gRPC, Kafka, AMQP and Redis) in real time. | Mizu is an API traffic viewer for Kubernetes. Unlike other tools, Mizu does not require instrumentation or code changes. It runs as a DaemonSet to inject a container at the node level in your Kubernetes cluster and performs tcpdump-like operations. We find it useful as a debugging tool, as it can observe all API communications across multiple protocols (REST, gRPC, Kafka, AMQP and Redis) in real time.","blip_selector":"kubeshark","name":"Kubeshark","display_name":"Kubeshark","url":"/radar/tools/kubeshark","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304077,"quadrant":"platforms","volume_date":"2023-04","description":"Cozo is an embeddable relational database that uses Datalog for querying. We're intrigued with its support for time-travel queries and modeling graph data in relational schema. We quite like that it delegates data storage to existing popular engines — including SQLite, RocksDB, Sled and TiKV. Although Cozo is still in its early stages of development, we find it's worth assessing.","blip_selector":"cozo","name":"Cozo","display_name":"Cozo","url":"/radar/platforms/cozo","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304064,"quadrant":"techniques","volume_date":"2023-04","description":"Large language models (LLMs) generally require significant GPU infrastructure to operate. We're now starting to see ports, like llama.cpp, that make it possible to run LLMs on different hardware — including Raspberry Pis, laptops and commodity servers. As such, self-hosted LLMs are now a reality, with open-source examples including GPT-J, GPT-JT and LLaMA. This approach has several benefits, offering better control in fine-tuning for a specific use case, improved security and privacy as well as offline access. However, you should carefully assess the capability within the organization and the cost of running such LLMs before making the decision to self-host.","blip_selector":"self-hosted-llms","name":"Self-hosted LLMs","display_name":"Self-hosted LLMs","url":"/radar/techniques/self-hosted-llms","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304017,"quadrant":"tools","volume_date":"2023-04","description":"It's becoming increasingly easy for developers to catch accessibility issues early in the development process. While tools like axe-core scan code for accessibility issues in your pipelines, the axe Linter VSCode extension helps find them even before that, while writing code. The vast majority of accessibility issues fall into categories that could be prevented by automated testing and using live feedback linters like this.","blip_selector":"axe-linter","name":"axe Linter","display_name":"axe Linter","url":"/radar/tools/axe-linter","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202304007,"quadrant":"platforms","volume_date":"2023-04","description":"Denodo is a data virtualization tool that aims to make it easier to expose and secure transformed, consumer-friendly data (from multiple underlying data sources and through a variety of interfaces) from one platform. Data transformations within Denodo can be defined by creating virtual databases and views using a SQL-like language called VQL which are executed when a user queries the virtual database. Underneath, Denodo can delegate queries on the virtual databases against one or multiple underlying databases.\n\nAlthough Denodo makes it easy to start exposing consumer-friendly data, performance degrades as layers of views and virtual databases are built on top of each other and queries with multiple joins start hitting multiple underlying databases. These problems are solvable, but they require fairly deep knowledge of the product's behavior and performance tuning options. Because of these drawbacks and given its limited support for unit testing, we recommend that you do not use Denodo as a primary data transformation tool and use tools like Spark or SQL (with dbt) for your data transformations instead.","blip_selector":"denodo-as-primary-data-transformation-tool","name":"Denodo as primary data transformation tool","display_name":"Denodo as primary data transformation tool","url":"/radar/platforms/denodo-as-primary-data-transformation-tool","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304102,"quadrant":"tools","volume_date":"2023-04","description":"DataFusion is a part of the data community's exploration of Rust's performance, memory safety and concurrency features applied to data processing. It shares similarities with Polars, namely a familiar DataFrame API in Rust (with Python bindings), the use of Apache Arrow under the hood and SQL support. Even though it's primarily designed for single-process execution, distributed processing support is in the works within Ballista. We think the Rust libraries for data processing are an evolving space worth following and exploring, and DataFusion is a part of it.","blip_selector":"datafusion","name":"DataFusion","display_name":"DataFusion","url":"/radar/tools/datafusion","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304097,"quadrant":"techniques","volume_date":"2023-04","description":"When deploying infrastructure as code, we've noticed that a lot of time can be spent diagnosing and repairing production issues that result from systems being unable to communicate with one another. Because the network topology between them can be complex, the entire route may not be traversable even if individual ports and endpoints have been configured correctly. Infrastructure testing practices usually include verifying the right ports are open or closed or that an endpoint can be accessed, but we've only recently begun doing reachability analysis when testing infrastructure. The analysis generally involves more than simple yes/no determinations. For example, a tool might traverse and report on multiple routes through transit gateways. This technique is supported by tools across all the major cloud providers. Azure has a service called Network Watcher that can be scripted in automated tests and GCP supports Connectivity Tests. Now, in AWS, you can test reachability across accounts in the same organization.","blip_selector":"reachability-analysis-when-testing-infrastructure","name":"Reachability analysis when testing infrastructure","display_name":"Reachability analysis when testing infrastructure","url":"/radar/techniques/reachability-analysis-when-testing-infrastructure","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202104005,"quadrant":"platforms","volume_date":"2023-04","description":"GitHub Actions has become a default starting point for many teams that need to get CI or CD up and running quickly in a greenfield environment. Among other things, it can take on more complex workflows and call other actions in composite actions. Although the ecosystem in GitHub Marketplace continues to grow, we still urge caution in giving third-party GitHub Actions access to your build pipeline. We recommend following GitHub's advice on security hardening to avoid sharing secrets in insecure ways. However, the convenience of creating your build workflow directly in GitHub next to your source code combined with the ability to run GitHub Actions locally, using open-source tools such as act, is a compelling option that has streamlined the setup and onboarding of our teams. | GitHub Actions has grown considerably last year. It has proven that it can take on more complex workflows and call other actions in composite actions among other things. It still has some shortcomings, though, such as its inability to re-trigger a single job of a workflow. Although the ecosystem in the GitHub Marketplace has its obvious advantages, giving third-party GitHub Actions access to your build pipeline risks sharing secrets in insecure ways (we recommend following GitHub's advice on security hardening). However, the convenience of creating your build workflow directly in GitHub next to your source code combined with the ability to run GitHub Actions locally using open-source tools such as act is a compelling option that has facilitated setup and onboarding of our teams. | Despite our cautionary advice when we last blipped it, we've seen continued enthusiasm for GitHub Actions. What we said before still holds true: GitHub Actions is not yet a full-fledged CI/CD replacement for complex workflows. It cannot, for example, re-trigger a single job of a workflow, call other actions inside a composite action or support a shared library. Furthermore, while the ecosystem in the GitHub Marketplace offers obvious advantages, giving third-party GitHub Actions access to your build pipeline risks sharing secrets in insecure ways (we recommend following GitHub's advice on security hardening). Despite those concerns, the convenience of creating your build workflow directly in GitHub next to your source code is a compelling option for some teams, and act helps you run GitHub Actions locally. As always, we recommend a clear-eyed assessment of the trade-offs, but some of our teams are happy with the simplicity of GitHub Actions. | CI servers and build tools are some of the oldest and most widely used in our kit. They run the gamut from simple cloud-hosted services to complex, code-defined pipeline servers that support fleets of build machines. Given our experience and the wide range of options already available, we were initially skeptical when GitHub Actions were introduced as another mechanism to manage the build and integration workflow. But the opportunity for developers to start small and easily customize behavior means that GitHub Actions are moving toward the default category for smaller projects. It's hard to argue with the convenience of having the build tool integrated directly into the source code repository. An enthusiastic community has emerged around this feature and that means a wide range of user-contributed tools and workflows are available to get started. Tools vendors are also getting on board via the GitHub Marketplace. However, we still recommend you proceed with caution. Although code and Git history can be exported into alternative hosts, a development workflow based on GitHub Actions can't. Also, use your best judgment to determine when a project is large or complex enough to warrant an independently supported pipeline tool. But for getting up and running quickly on smaller projects, it's worth considering GitHub Actions and the ecosystem that is growing around them.","blip_selector":"github-actions","name":"GitHub Actions","display_name":"GitHub Actions","url":"/radar/platforms/github-actions","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304074,"quadrant":"techniques","volume_date":"2023-04","description":"If not properly secured, the infrastructure and tools that run our build and delivery pipelines can become a big liability. Pipelines need access to critical data and systems like source code, credentials and secrets to build and deploy software. This makes these systems very inviting to malicious actors. We therefore highly recommend applying zero trust security for CI/CD pipelines and infrastructure — trusting them as little as necessary. This encompasses a number of techniques: If available, authenticate your pipelines with your cloud provider via federated identity mechanisms like OIDC, instead of giving them direct access to secrets. Implement the principle of least privilege by minimizing the access of individual user or runner accounts, rather than employing \"god user accounts\" with unlimited access. Use your runners in an ephemeral way instead of reusing them, to reduce the risk of exposing secrets from previous jobs or running jobs on compromised runners. Keep the software in your agents and runners up to date. Monitor the integrity, confidentiality and availability of your CI/CD systems the same way you would monitor your production software.\n\nWe're seeing teams forget about these types of practices particularly when they’re used to working with a self-managed CI/CD infrastructure in internal network zones. While all of these practices are important in your internal networks, they become even more crucial when using a managed service, as that extends the attack surface and blast radius even more.","blip_selector":"zero-trust-security-for-ci-cd","name":"Zero trust security for CI/CD","display_name":"Zero trust security for CI/CD","url":"/radar/techniques/zero-trust-security-for-ci-cd","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202203041,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Our teams now view Gradle Kotlin DSL as the default for starting new projects using Gradle, preferring it over Groovy. Teams already using Groovy should consider migration. Kotlin provides better support for refactoring and simpler editing in IDEs, and our teams report that it produces code that is easier to read and maintain. Given some IDEs now support migration, it should be relatively quick to experiment with replacing existing Groovy. In some situations Kotlin might be slower than Groovy; however, for many projects, this is unlikely to impact the team. | Previously, we blipped about the Android Gradle plugin Kotlin DSL, or Gradle Kotlin DSL, which added support for Kotlin Script as an alternative to Groovy for Android projects using Gradle build scripts. The goal of replacing Groovy with Kotlin is to provide better support for refactoring and simpler editing in IDEs and, ultimately, to produce code that is easier to read and maintain. For teams already using Kotlin, it also means working on the build in a familiar language. We now suggest trialing Kotlin DSL as an alternative language to Groovy for Gradle projects in general, especially if you have large or complex Gradle build scripts. Many IDEs now include support for the migration of existing projects. Some caveats remain, and we suggest checking the documentation for the most up-to-date details, including the prerequisites. We had a team with an at least seven-year-old, 450-line build script migrate successfully within a few days. | Android Gradle plugin Kotlin DSL added support for Kotlin Script as an alternative to Groovy for Gradle build scripts. The goal of replacing Groovy with Kotlin is to provide better support for refactoring and simpler editing in IDEs as well as ultimately to produce code that is easier to read and maintain. For teams already using Kotlin it also means working on the build in a familiar language. We had a team with an at least seven-year-old 450-line build script migrate within a few days. If you have large or complex gradle build scripts, then it's worth assessing whether Kotlin Script will produce better outcomes for your teams.","blip_selector":"gradle-kotlin-dsl","name":"Gradle Kotlin DSL","display_name":"Gradle Kotlin DSL","url":"/radar/languages-and-frameworks/gradle-kotlin-dsl","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202005061,"quadrant":"tools","volume_date":"2023-04","description":"DVC continues to be our tool of choice for managing experiments in data science projects. The fact that it's Git-based makes it a known turf for developers to bring engineering practices to the data science ecosystem. DVC's opinionated view of a model checkpoint carefully encapsulates a training data set, a test data set, model hyperparameters and the code. By making reproducibility a first-class concern, it allows the team to time travel across various versions of the model. Our teams have successfully used DVC in production to enable continuous delivery for ML (CD4ML); it can be plugged in with any type of storage (including AWS S3, Google Cloud Storage, MinIO and Google Drive). However, with data sets getting bigger, file system–based snapshotting could become particularly expensive. When the underlying data is changing rapidly, DVC on top of a good versioned storage allows tracking model drifts over a period of time. Our teams have effectively used DVC on top of data storage formats like Delta Lake which optimizes versioning (COW). A majority of our data science teams set up DVC as a day zero task while they bootstrap a project; for this reason we're happy to move it to Adopt. | In 2018 we mentioned DVC in conjunction with the versioning data for reproducible analytics. Since then it has become a favorite tool for managing experiments in machine learning (ML) projects. Since it's based on Git, DVC is a familiar environment for software developers to bring their engineering practices to ML practice. Because it versions the code that processes data along with the data itself and tracks stages in a pipeline, it helps bring order to the modeling activities without interrupting the analysts’ flow.","blip_selector":"dvc","name":"DVC","display_name":"DVC","url":"/radar/tools/dvc","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201911055,"quadrant":"techniques","volume_date":"2023-04","description":"While serverless architectures can be extremely useful for solving some problems, they do come with a certain level of complexity, especially when they involve nontrivial execution and data flows across multiple interdependent Lambdas — this can sometimes result in a Lambda pinball architecture. Our teams have reported that maintaining and testing Lambda pinball architectures can be very challenging: understanding the infrastructure, deployment, diagnosis and debugging can become difficult. At a code level, simple mapping between domain concepts and the multiple Lambdas involved is practically impossible, making any changes and additions challenging. Although we believe serverless is the right fit for some problems and domains, it's not a \"silver bullet\" for every problem, which is why you should try to avoid Lambda pinball. One pattern that can help is to draw a distinction between public and published interfaces and apply domain boundaries with published interfaces between them. | We've been building serverless architectures on our projects for a couple of years now, and we've noticed that it's quite easy to fall into the trap of building a distributed monolith. Lambda pinball architectures characteristically lose sight of important domain logic in the tangled web of lambdas, buckets and queues as requests bounce around increasingly complex graphs of cloud services. Typically they're hard to test as units, and the application needs must be tested as an integrated whole. One pattern we can use to avoid these pinball architectures is to draw a distinction between public and published interfaces and apply good old domain boundaries with published interfaces between them.","blip_selector":"lambda-pinball","name":"Lambda pinball","display_name":"Lambda pinball","url":"/radar/techniques/lambda-pinball","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304009,"quadrant":"techniques","volume_date":"2023-04","description":"Lakehouse architecture is an architectural style that combines the scalability of data lakes with the reliability and performance of data warehouses. It enables organizations to store and analyze large volumes of diverse data in a single platform as opposed to having them in separate lake and warehouse tiers, using the same familiar SQL-based tools and techniques. While the term is often associated with vendors like Databricks, open alternatives such as Delta Lake, Apache Iceberg and Apache Hudi are worth considering. Lakehouse architecture can complement data mesh implementations. Autonomous data product teams can choose to leverage a Lakehouse within their data products.","blip_selector":"lakehouse-architecture","name":"Lakehouse architecture","display_name":"Lakehouse architecture","url":"/radar/techniques/lakehouse-architecture","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104011,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"At the heart of many approaches to machine learning lies the creation of a model from a set of training data. Once a model is created, it can be used over and over again. However, the world isn't stationary, and often the model needs to change as new data becomes available. Simply re-running the model creation step can be slow and costly. Incremental learning addresses this issue, making it possible to learn from streams of data incrementally to react to change faster. As a bonus, the compute and memory requirements are lower and predictable. Our practical experience with River continues to be positive. Vowpal Wabbit, which can be an alternative, has a much steeper learning curve, and the Scikit-like API offered by River makes River more accessible to data scientists. | At the heart of many approaches to machine learning lies the creation of a model from a set of training data. Once a model is created, it can be used over and over again. However, the world isn't stationary, and often the model needs to change as new data becomes available. Simply re-running the model creation step can be slow and costly. Incremental learning addresses this issue, making it possible to learn from streams of data incrementally to react to change faster. As a bonus the compute and memory requirements are lower and predictable. In our implementations we've had good experience with the River framework, but so far we've added checks, sometimes manual, after updates to the model.","blip_selector":"river","name":"River","display_name":"River","url":"/radar/languages-and-frameworks/river","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304090,"quadrant":"platforms","volume_date":"2023-04","description":"Neon is an open-source alternative to AWS Aurora PostgreSQL. Cloud-native analytical databases have embraced the technique of separating storage from compute nodes to elastically scale on demand. However, it's difficult to do the same in a transactional database. Neon achieves this with its new multi-tenant storage engine for PostgreSQL. With minimal changes to the mainstream PostgreSQL code, Neon leverages AWS S3 for long-term data storage and elastically scales the processing up or down (including scale-to-zero) for compute. This architecture has several benefits — including cheap and fast clones, copy-on-write and branching. We're quite excited to see new innovations on top of PostgreSQL. Our teams are evaluating Neon, and we recommend you assess it as well.","blip_selector":"neon","name":"Neon","display_name":"Neon","url":"/radar/platforms/neon","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304067,"quadrant":"tools","volume_date":"2023-04","description":"Steampipe is an open-source tool that lets you instantly query cloud services like AWS, Azure and GCP with SQL. With 100+ plugins and built-in support for creating dashboards, Steampipe makes it trivial to connect live cloud configuration data with internal or external data sets and create security or compliance dashboards. We've enjoyed working with Steampipe and created several such dashboards with AWS cloud configurations.","blip_selector":"steampipe","name":"Steampipe","display_name":"Steampipe","url":"/radar/tools/steampipe","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304036,"quadrant":"platforms","volume_date":"2023-04","description":"Apache Hudi is an open-source data lake platform that brings ACID transactional guarantees to the data lake. Our teams have had a great experience using Hudi in a high-volume, high-throughput scenario with real-time inserts and upserts. We particularly like the flexibility Hudi offers for customizing the compaction algorithm which helps in dealing with \"small files\" problems. Apache Hudi falls in the same category as Delta Lake and Apache Iceberg. They all support similar features, but each differs in the underlying implementations and detailed feature lists.","blip_selector":"apache-hudi","name":"Apache Hudi","display_name":"Apache Hudi","url":"/radar/platforms/apache-hudi","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304027,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"In previous Radars, we've featured data validation and testing platforms like Great Expectations that can be used to validate assumptions and test the quality of incoming data used for training or classification. Sometimes, though, all you need is a simple code library to implement tests and quality checks directly in pipelines. pandera is a Python library for testing and validating data across a wide range of frame types such as pandas, Dask or PySpark. pandera can implement simple assertions about fields or hypothesis tests based on statistical models. The wide range of supported frame libraries means tests can be written once and then applied to a variety of underlying data formats. pandera can also be used to generate synthetic data to test ML models.","blip_selector":"pandera","name":"pandera","display_name":"pandera","url":"/radar/languages-and-frameworks/pandera","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202304010,"quadrant":"techniques","volume_date":"2023-04","description":"As remote work continues to increase, so does the adoption of chat collaboration platforms and ChatOps. These platforms often offer webhooks as a simple way to automate sending messages and notifications, but we're noticing a concerning trend: the casual management of webhooks — where they’re treated as configuration rather than a secret or credential. This can lead to phishing attacks and compromised internal spaces.\n\nWebhooks are credentials that offer privileged access to an internal space and may contain API keys that can be easily extracted and utilized directly. Not treating them as secrets opens up the possibility of successful phishing attacks. Webhooks in Git repos can easily be extracted and used to send fraudulent payloads, which the user may not have any way to authenticate. To mitigate this threat, teams handling webhooks need to shift their culture and treat webhooks as sensitive credentials. Software developers building integrations with ChatOps platforms must also be mindful of this risk and ensure that webhooks are handled with proper security measures.","blip_selector":"casual-management-of-webhooks","name":"Casual management of webhooks","display_name":"Casual management of webhooks","url":"/radar/techniques/casual-management-of-webhooks","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304005,"quadrant":"platforms","volume_date":"2023-04","description":"Dapr, short for Distributed Application Runtime, helps developers to build resilient, stateless and stateful microservices that run in the cloud. Some people may confuse it with a service mesh, because it uses a sidecar architecture that runs as a separate process alongside the application. Dapr is more application oriented and focuses on encapsulating the fault tolerance and connectivity required for building distributed applications. For example, Dapr provides multiple building blocks, from service invocation and message pub/sub to distributed lock, all of which are common patterns in distributed communication. One of our teams evaluated Dapr on a recent project; given their positive experience, they’re looking forward to bringing it to other projects in the future.","blip_selector":"dapr","name":"Dapr","display_name":"Dapr","url":"/radar/platforms/dapr","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304062,"quadrant":"platforms","volume_date":"2023-04","description":"Modal is a platform as a service (PaaS) that offers on-demand compute without the need for your own infrastructure. Modal lets you deploy machine learning models, massively parallel compute jobs, task queues and web apps. It provides a container abstraction that makes the switch from local to cloud deployment seamless, with hot reload both locally and in the cloud. It even removes deployments automatically, avoiding the need for manual clean-up, but can also make them persistent.\n\nModal is written by the same team that developed the first recommendation engine for Spotify. It takes care of the AI/ML stack end-to-end and can provide on-demand GPU resources, which is useful if you have particularly intensive computational needs. Whether you're working on your laptop or in the cloud, Modal just works, providing an easy and efficient way to run and deploy your projects.","blip_selector":"modal","name":"Modal","display_name":"Modal","url":"/radar/platforms/modal","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304033,"quadrant":"techniques","volume_date":"2023-04","description":"The earlier accessibility is considered in software delivery, the easier and cheaper it is to ensure what's built works for as many people as possible. Tools that help communicate accessibility annotations in designs help teams consider important elements like document structure, semantic HTML and alternative texts from the beginning of their work. This enables them to ensure user interfaces meet global accessibility standards and address common failures that are actually fairly easy to avoid. Figma offers a range of accessibility notation plugins: The A11y Annotation Kit, Twitter's Accessibility Annotation Library and the Axe toolset's Axe for Designers.","blip_selector":"accessibility-annotations-in-designs","name":"Accessibility annotations in designs","display_name":"Accessibility annotations in designs","url":"/radar/techniques/accessibility-annotations-in-designs","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202304012,"quadrant":"techniques","volume_date":"2023-04","description":"While the practice of creating excess capacity in the delivery process is well-known in the product management community, we still see far too many teams planning for full utilization of team members. Reserving some capacity during sprint planning generally leads to better predictability and better quality; it promotes team resilience to unexpected events like illnesses, production issues, unexpected product requests and tech debt, while also allowing productive activities like team building and ideation that can lead to product innovation. Running at less than full utilization means teams can be more thoughtful about the robustness of the resulting software and pay closer attention to the right observability signals. Our experience is that a fully utilized team leads to a collapse in throughput as well, just as a fully utilized highway creates slow and demoralizing traffic. For example, when one of our teams had unpredictable support issues, they saw a 25% increase in throughput and a 50% decrease in cycle time volatility by planning feature velocity based on only two of the three developer pairs' capacities.","blip_selector":"planning-for-full-utilization","name":"Planning for full utilization","display_name":"Planning for full utilization","url":"/radar/techniques/planning-for-full-utilization","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304003,"quadrant":"tools","volume_date":"2023-04","description":"As more organizations adopt cloud computing, many are starting to integrate multiple cloud providers simultaneously to maximize flexibility and minimize vendor lock-in. However, managing keys and access controls across multiple cloud providers can be a significant challenge, leading to increased complexity and security risks. Akeyless is a centralized, cloud-based platform that provides unified secrets management with a range of advantages for managing secrets and sensitive data. It integrates seamlessly with different providers, simplifying the management of secrets and access controls to monitor and control who has access to sensitive data; with encryption, access controls, multi-factor authentication and other security mechanisms it ensures only authorized users are able to access sensitive data. Additionally, it provides an intuitive interface for administration and monitoring, providing a less complex and more scalable developer and administration experience.","blip_selector":"akeyless","name":"Akeyless","display_name":"Akeyless","url":"/radar/tools/akeyless","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304089,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"nanoGPT is a framework for training and fine-tuning medium-sized generative pretrained transformers (GPT). The author, Andrej Karpathy, references Attention is All You Need and OpenAI's GPT-3 papers to build a GPT from scratch using PyTorch. With all the hype around generative AI, we want to highlight nanoGPT for its simplicity and focus on clearly articulating the building blocks of the GPT architecture.","blip_selector":"nanogpt","name":"nanoGPT","display_name":"nanoGPT","url":"/radar/languages-and-frameworks/nanogpt","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304018,"quadrant":"tools","volume_date":"2023-04","description":"Creating exactly the minimum viable AWS IAM policies we want, according to the principle of least privilege, can be a long journey of trial and error. iamlive can shorten that journey considerably. It monitors the AWS CLI calls made from a machine and determines the policies needed to execute those calls. The tool generates a policy document with statements, actions, principals and resources that can be used as a good starting point. We've found it particularly useful to create policies needed in CI/CD pipelines that provision infrastructure, reducing the usual back and forth after a Terraform run fails because the IAM role's policy is insufficient.","blip_selector":"iamlive","name":"iamlive","display_name":"iamlive","url":"/radar/tools/iamlive","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202304044,"quadrant":"techniques","volume_date":"2023-04","description":"Starter kits and templates are widely used in software projects to speed up initial setup, but they can pull in many unnecessary dependencies for a particular project. It's important to practice dependency pruning — periodically taking a hard look at these dependencies and pruning any that are not used. This helps reduce build and deploy times and decrease the project's attack surface by removing potential vulnerabilities. Although this isn't a new technique, given the increasing frequency of attacks on software supply chains, we advocate for renewed attention to it.","blip_selector":"dependency-pruning","name":"Dependency pruning","display_name":"Dependency pruning","url":"/radar/techniques/dependency-pruning","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304042,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"dbt-unit-testing is a dbt package that allows writing unit tests for a model and its logic by mocking its dependencies. This brings the engineering rigor of fast development feedback to the data ecosystem. Our teams use this package with Snowflake to practice test-driven development (TDD), although it was only feasible for simple transformations. The library certainly has rough edges when it comes to debugging failed test runs, but the ability to write unit tests on transformers as we develop the model provided a neat developer experience.","blip_selector":"dbt-unit-testing","name":"dbt-unit-testing","display_name":"dbt-unit-testing","url":"/radar/languages-and-frameworks/dbt-unit-testing","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304043,"quadrant":"tools","volume_date":"2023-04","description":"As machine learning finds its way into the mainstream, practices are maturing around automatically testing models, validating training data and observing model performance in production. Increasingly, these automated checks are being incorporated into continuous delivery pipelines or run against production models to detect drift and model performance. A number of tools with similar or overlapping capabilities have emerged to handle various steps in this process (Giskard and Evidently are also covered in this volume). Deepchecks is another of these tools that’s available as an open-source Python library and can be invoked from pipeline code through an extensive set of APIs. One unique feature is its ability to handle either tabular or image data with a module for language data currently in alpha release. At the moment, no single tool can handle the variety of tests and guardrails across the entire ML pipeline. We recommend assessing Deepchecks for your particular application niche.","blip_selector":"deepchecks","name":"Deepchecks","display_name":"Deepchecks","url":"/radar/tools/deepchecks","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1172,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"PyTorch continues to be our choice of machine learning (ML) framework. Most of our teams prefer PyTorch over TensorFlow. PyTorch exposes the inner workings of ML that TensorFlow hides, making it easier to debug. With dynamic computational graphs, model optimization is much easier compared to any other ML framework. The extensive availability of State-of-the-Art (SOTA) models and the ease of implementing research papers make PyTorch stand out. When it comes to graph ML, PyTorch Geometric is a more mature ecosystem and our teams have had great experiences with it. PyTorch has also gradually bridged gaps when it comes to model deployment and scaling; our teams have used TorchServe to serve pretrained models successfully in production, for example. With many teams defaulting to PyTorch for their end-to-end deep-learning needs, we happily recommend adopting PyTorch. | Our teams have continued to use and appreciate the PyTorch machine learning framework, and several teams prefer PyTorch over TensorFlow. PyTorch exposes the inner workings of ML that TensorFlow hides, making it easier to debug, and contains constructs that programmers are familiar with such as loops and actions. Recent releases have improved performance of PyTorch, and we've been using it successfully in production projects. | PyTorch is a complete rewrite of the Torch machine learning framework from Lua to Python. Although quite new and immature compared to Tensorflow, programmers find PyTorch much easier to work with. Because of its object-orientation and native Python implementation, models can be expressed more clearly and succinctly and debugged during execution. Although many of these frameworks have emerged recently, PyTorch has the backing of Facebook and broad range of partner organisations, including NVIDIA, which should ensure continuing support for CUDA architectures. ThoughtWorks teams find PyTorch useful for experimenting and developing models but still rely on TensorFlow’s performance for production-scale training and classification. | PyTorch is a complete rewrite of the Torch machine learning framework from Lua to Python. Although quite new and immature compared to Tensorflow, programmers find PyTorch much easier to work with. Because of its object-orientation and native Python implementation, models can be expressed more clearly and succinctly and debugged during execution. Although many of these frameworks have emerged recently, PyTorch has the backing of Facebook and broad range of partner organisations, including NVIDIA, which should ensure continuing support for CUDA architectures. ThoughtWorks teams find PyTorch useful for experimenting and developing models but still rely on TensorFlow’s performance for production-scale training and classification.","blip_selector":"pytorch","name":"PyTorch","display_name":"PyTorch","url":"/radar/languages-and-frameworks/pytorch","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304072,"quadrant":"platforms","volume_date":"2023-04","description":"TypeDB is a knowledge graph database, designed to work with intricate data relationships which makes it easier to query and analyze large data sets. TypeDB's TypeQL query language has a SQL-like syntax which eases the learning curve for schema definition, querying and exploration. TypeDB comes with a variety of tools that make it easier to work with the database, including a command-line interface and a graphical user interface, TypeDB Studio, which provides some features for working with TypeDB, such as managing schemas, querying data, visualizing relationships or even collaborating with others. There is a good deal of documentation available and an active community for support. Our teams used it to build knowledge graphs of taxonomic concepts across different databases and took advantage of its strong inference capabilities by adding new inference rules increasing efficiency and reducing workload. With its intuitive developer experience and supportive community, TypeDB is a good candidate to consider for any team looking to build data solutions that depend on complex data relationships, including natural language data, recommendation engines and knowledge graphs.","blip_selector":"typedb","name":"TypeDB","display_name":"TypeDB","url":"/radar/platforms/typedb","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010028,"quadrant":"platforms","volume_date":"2023-04","description":"DuckDB is an embedded, columnar database for data science and analytical workloads. Data analysts usually load the data locally in tools like pandas or data.table to quickly analyze patterns and form hypotheses before scaling the solution in the server. However, we're now using DuckDB for such use cases, because it unlocks the potential to do larger than memory analysis. DuckDB supports range joins, vectorized execution and multiversion concurrency control (MVCC) for large transactions, and our teams are quite happy with it. | DuckDB is an embedded, columnar database for data science and analytical workloads. Analysts spend significant time cleaning and visualizing data locally before scaling it to servers. Although databases have been around for decades, most of them are designed for client-server use cases and therefore not suitable for local interactive queries. To work around this limitation analysts usually end up using in-memory data-processing tools such as Pandas or data.table. Although these tools are effective, they do limit the scope of analysis to the volume of data that can fit in memory. We feel DuckDB neatly fills this gap in tooling with an embedded columnar engine that is optimized for analytics on local, larger-than-memory data sets.","blip_selector":"duckdb","name":"DuckDB","display_name":"DuckDB","url":"/radar/platforms/duckdb","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110033,"quadrant":"tools","volume_date":"2023-04","description":"Our advice when it comes to secrets management has always been to decouple it from source code. However, teams are often presented with a tradeoff between full automation (in the spirit of infrastructure as code) versus a few manual steps (using tools like vaults) for managing, seeding and rotating seed secrets. For instance, our teams use SOPS to manage seed credentials for bootstrapping infrastructure. In some situations, however, it's impossible to remove secrets from legacy code repositories. For such needs, we found Mozilla SOPS to be a good choice for encrypting secrets in text files. SOPS integrates with cloud-managed keystores such as AWS and GCP Key Management Service (KMS) or Azure Key Vault as sources of encryption keys. It also works cross-platform and supports PGP keys. | Plaintext secrets checked into source control (usually Github) are one of the most pervasive security mistakes developers make. For this reason we thought it useful to feature Mozilla Sops, a tool for encrypting secrets in text files that our developers find useful in situations where it is impossible to remove secrets from legacy code repositories. We've mentioned many tools of this type before (Blackbox, git-crypt), but Sops has several features that set it apart. For example, Sops integrates with cloud-managed keystores such as AWS and GCP Key Management Service (KMS) or Azure Key Vault as sources of encryption keys. It also works cross-platform and supports PGP keys. This enables fine-grained access control to secrets on a file-by-file basis. Sops leaves the identifying key in plain text so that secrets can still be located and diffed by git. We're always supportive of anything that makes it easier for developers to be secure; however, remember that you don't have to keep secrets in source control to begin with. See Decoupling secret management from source code in our November 2017 issue.","blip_selector":"mozilla-sops","name":"Mozilla SOPS","display_name":"Mozilla SOPS","url":"/radar/tools/mozilla-sops","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304056,"quadrant":"techniques","volume_date":"2023-04","description":"It can be a bit daunting to make a web application compliant with assistive technologies when you yourself never use them, and you feel like you don't yet know anything about directives like the Web Content Accessibility Guidelines (WCAG). Intelligent guided accessibility tests are one category of tools that help test if you've done the right thing without needing to be an expert on accessibility. These tools are browser extensions that scan your website, summarize how assistive technology would interpret it and then ask you a set of questions to confirm whether the structure and elements you created are as intended. We've used axe DevTools, Accessibility Insights for Web or the ARC Toolkit on some of our projects.","blip_selector":"intelligent-guided-accessibility-tests","name":"Intelligent guided accessibility tests","display_name":"Intelligent guided accessibility tests","url":"/radar/techniques/intelligent-guided-accessibility-tests","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304060,"quadrant":"platforms","volume_date":"2023-04","description":"Matter is an open standard for smart home technology, launched by Amazon, Apple, Google, Comcast and the Zigbee Alliance (now Connectivity Standards Alliance, or CSA). It enables devices to work with any Matter-certified ecosystem, thus reducing fragmentation and promoting interoperability among devices and IoT platforms from different providers. Its focus on standardization at the application level, support for Wi-Fi and Thread as communication mediums and backing from major tech companies set it apart from other protocols like Zigbee. Although the number of Matter-enabled devices is still relatively low, its growing importance in the IoT space makes it worth assessing for those looking to build smart home and IoT solutions.","blip_selector":"matter","name":"Matter","display_name":"Matter","url":"/radar/platforms/matter","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1338,"quadrant":"techniques","volume_date":"2023-04","description":"Automatically estimating, tracking and predicting cloud infrastructure run cost is crucial for today's organizations. The cloud providers' savvy pricing models, combined with the proliferation of pricing parameters and the dynamic nature of today's architecture, can lead to surprisingly expensive run costs. Even though this technique has been in Adopt since 2019, we want to highlight the importance of considering run cost as an architecture fitness function, especially today, due to accelerated cloud adoption and the growing attention to FinOps practices. Many commercial platforms provide tools that can consolidate and clarify cloud costs for business leaders. Some of them are designed to show cloud run costs to finance organizations or originating business units. \n\nHowever, cloud consumption decisions are usually made at the engineering level, where systems are designed. It's important that the engineers making design decisions have some way of predicting the cost impact of their architectural decisions. Some teams automate this prediction early in the development lifecycle. Tools like Infracost help teams predict cost impact when thinking about possible changes to infrastructure as code. This computation can be automated and woven into the CD pipeline. Note that cost will be impacted by architectural decisions combined with actual usage levels; to do this properly, you need good projections of expected usage levels. Early and frequent feedback on run cost can prevent it from soaring. When the predicted cost deviates from what was expected or acceptable, the team can discuss whether it's time to evolve the architecture. | Automating the estimation, tracking and projection of cloud infrastructure's run cost is necessary for today's organizations. The cloud providers' savvy pricing models, combined with the proliferation of pricing parameters and the dynamic nature of today's architecture, can lead to surprisingly expensive run costs. For example, the price of serverless based on API calls, event streaming solutions based on traffic or data processing clusters based on running jobs, all have a dynamic nature that changes over time as the architecture evolves. When our teams manage infrastructure on the cloud, implementing run cost as architecture fitness function is one of their early activities. This means that our teams can observe the cost of running services against the value delivered; when they see deviations from what was expected or acceptable, they'll discuss whether it's time to evolve the architecture. The observation and calculation of the run cost is implemented as an automated function. | Automating the estimation, tracking and projection of cloud infrastructure's run cost is necessary for today's organizations. The cloud providers' savvy pricing models, combined with proliferation of pricing parameters and the dynamic nature of today's architecture, can lead to surprisingly expensive run cost. For example, the price of serverless based on API calls, event streaming solutions based on traffic or data processing clusters based on running jobs, all have a dynamic nature that changes over time as the architecture evolves. When our teams manage infrastructure on the cloud, implementing run cost as architecture fitness function is one of their early activities. This means that our teams can observe the cost of running services against the value delivered; when they see deviations from what was expected or acceptable, they'll discuss whether it's time to evolve the architecture. The observation and calculation of the run cost is implemented as an automated function. | We still see teams who aren't tracking the cost of running their applications as closely as they should as their software architecture or usage evolves. This is particularly true when they're using serverless, which developers assume will provide lower costs since you're not paying for unused server cycles. However, the major cloud providers are pretty savvy at setting their pricing models, and heavily used serverless functions, although very useful for rapid iteration, can get expensive quickly when compared with dedicated cloud (or on-premise) servers. We advise teams to frame a system's run cost as architecture fitness function , which means: track the cost of running your services against the value delivered; when you see deviations from what was expected or acceptable, have a discussion about whether it's time to evolve your architecture.","blip_selector":"run-cost-as-architecture-fitness-function","name":"Run cost as architecture fitness function","display_name":"Run cost as architecture fitness function","url":"/radar/techniques/run-cost-as-architecture-fitness-function","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304071,"quadrant":"tools","volume_date":"2023-04","description":"TruffleHog is an open-source SAST (static application security testing) tool for detecting secrets in various sources. While GitHub and GitLab repositories are the most popular use cases, it can also be used to scan cloud storage buckets like S3 and GCS, local files and directories and CircleCI logs. Developers can set up TruffleHog as a pre-commit hook or scan the history of existing repositories in an entire GitHub organization to detect secrets. The tool supports detecting custom regex patterns, which have been found to be quite useful even in its current alpha stage. TruffleHog also has an enterprise version, but our devs have found the open-source version easy to set up and sufficient for the most common use cases. The tool has a very active community who regularly adds features.","blip_selector":"trufflehog","name":"TruffleHog","display_name":"TruffleHog","url":"/radar/tools/trufflehog","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304024,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":".NET MAUI is a new cross-platform framework for creating native mobile and desktop apps with C# and XAML. It allows the creation of apps that can run on Android, iOS, macOS and Windows from a single shared codebase. However, as a new technology, the ecosystem around MAUI is not as developed as React Native or other cross-system platforms, and it only supports C#. Additionally, MAUI may face challenges encountered by organizations using Xamarin in the past, including poor cross-platform tooling, mobile integration problems, developer availability and an immature ecosystem.\n\nWhile Microsoft announced their commitment to MAUI as an open-source, mobile-first framework for mobile development, its success has yet to be proven. If you’re already using Xamarin, you may want to assess MAUI as a potential upgrade; however, if C# or Xamarin isn't part of your tool set yet, you may want to approach MAUI with some caution until the technology is more widely adopted and proven in the market.","blip_selector":"net-maui","name":".NET MAUI","display_name":".NET MAUI","url":"/radar/languages-and-frameworks/net-maui","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304045,"quadrant":"techniques","volume_date":"2023-04","description":"We've featured large language models (LLMs) like BERT and ERNIE in the Radar before; domain-specific LLMs, however, are an emerging trend. Fine-tuning general-purpose LLMs with domain-specific data can tailor them for various tasks, including information retrieval, customer support augmentation and content creation. This practice has shown promising results in industries like legal and finance, as demonstrated by OpenNyAI for legal document analysis. With more organizations experimenting with LLMs and new models like GPT4 being released, we can expect more domain-specific use cases in the near future.\n\nHowever, there are challenges and pitfalls to consider. First, LLMs can be confidently wrong, so it's essential to build mechanisms into your process to ensure the accuracy of results. Second, third-party LLMs may retain and re-share your data, posing a risk to proprietary and confidential information. Organizations should carefully review the terms of use and trustworthiness of providers or consider training and running LLMs on an infrastructure they control. As with any new technology, businesses must tread carefully, understanding the implications and risks associated with LLM adoption.","blip_selector":"domain-specific-llms","name":"Domain-specific LLMs","display_name":"Domain-specific LLMs","url":"/radar/techniques/domain-specific-llms","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304023,"quadrant":"tools","volume_date":"2023-04","description":"Kubernetes External Secrets Operator allows external secret providers to be integrated with Kubernetes. It reads from the external provider API and injects the result into a Kubernetes Secret. The operator works with a large variety of secret management tools, including some we've featured in previous editions of the Radar. Our teams have found it simplified the use of secrets when working with Kubernetes by allowing the use of a single store across a whole project.","blip_selector":"kubernetes-external-secrets-operator","name":"Kubernetes External Secrets Operator","display_name":"Kubernetes External Secrets Operator","url":"/radar/tools/kubernetes-external-secrets-operator","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304099,"quadrant":"platforms","volume_date":"2023-04","description":"RudderStack is a customer data platform (CDP) that makes it easy to store data in a data warehouse or data lake. This approach, increasingly known as Headless CDP, separates the CDP's features from its user interface and emphasizes configurability through APIs and the data warehouse/lake as primary storage. As expected from a product in this category, RudderStack has a rich repository of integrations with third-party products (both as source and sink) and the ability to ingest custom events. RudderStack has both a commercial offering and a self-hosted OSS version with functionality limitations.","blip_selector":"rudderstack","name":"RudderStack","display_name":"RudderStack","url":"/radar/platforms/rudderstack","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304040,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"When adding camera capabilities to Android apps, developers had to look out for pitfalls. The recently introduced Jetpack CameraViewfinder API significantly improves the developer experience in this area. Internally it uses either a TextureView or SurfaceView to display the camera feed and applies transformations to correctly display the viewfinder, fixing aspect ratio, scale and rotation where necessary. Optimized layouts for foldable devices are provided, too. While not a major feature, we highlight it here to ensure teams are aware of its existence.","blip_selector":"jetpack-cameraviewfinder","name":"Jetpack CameraViewfinder","display_name":"Jetpack CameraViewfinder","url":"/radar/languages-and-frameworks/jetpack-cameraviewfinder","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304011,"quadrant":"techniques","volume_date":"2023-04","description":"Team knowledge management is a familiar concept with teams using tools such as wikis to store information and onboard new team members. Some of our teams now prefer to use Logseq as a team knowledge base. An open-source knowledge-management system, Logseq is powered by a graph database, helps users organize thoughts, notes and ideas and can be adapted for team use with Git-based storage. Logseq allows teams to build a democratic and accessible knowledge base, providing each member with a personalized learning journey and facilitating efficient onboarding. However, as with any knowledge management tool, teams will need to apply good curation and management of their knowledge base to avoid information overload or disorganization.\n\nWhile similar functionality is available in tools like Obsidian, the key difference lies in Logseq's focus on consumption, with paragraph-based linking enabling team members to quickly find the relevant context without having to read an entire article.","blip_selector":"logseq-as-team-knowledge-base","name":"Logseq as team knowledge base","display_name":"Logseq as team knowledge base","url":"/radar/techniques/logseq-as-team-knowledge-base","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911060,"quadrant":"tools","volume_date":"2023-04","description":"Kubeflow is a Kubernetes-native machine learning (ML) platform that simplifies build, train and deploy lifecycles of models to diverse infrastructure. We've extensively used Pipelines to encode ML workflows for several models across experimentation, training and serving use cases. Besides Pipelines, Kubeflow ships with multiple components, among which we find hyperparameter tuning with Katib and multi-tenancy to be quite useful. | Kubeflow is interesting for two reasons. First, it is an innovative use of Kubernetes Operators which we've spotlighted in our April 2019 edition of the Radar. Second, it provides a way to encode and version machine-learning workflows so that they can be more easily ported from one execution environment to another. Kubeflow consists of several components, including Jupyter notebooks, data pipelines, and control tools. Several of these components are packaged as Kubernetes operators to draw on Kubernetes's ability to react to events generated by pods implementing various stages of the workflow. By packaging the individual programs and data as containers, entire workflows can be ported from one environment to another. This can be useful when moving a useful but computationally challenging workflow developed in the cloud to a custom supercomputer or tensor processing unit cluster.","blip_selector":"kubeflow","name":"Kubeflow","display_name":"Kubeflow","url":"/radar/tools/kubeflow","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304029,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"One of the challenges of creating a rich, interactive browser-based experience is in minimizing the time from first request to full user interactivity. When starting up, the application may need to download large amounts of JavaScript to the browser or execute a lengthy process to restore application state on the server. Qwik is a new front-end framework that serializes application state so it can be rendered on the server without rehydrating and replaying application logic. This is achieved through resumability, which involves pausing execution on the server to resume it on the client. Like other newer front-end frameworks, such as Astro or Svelte, Qwik also speeds up initial page load times by minimizing the amount of JavaScript to load. In Qwik's case, the initial application download is primarily HTML, with most JavaScript loaded dynamically on demand from a local cache, if possible.","blip_selector":"qwik","name":"Qwik","display_name":"Qwik","url":"/radar/languages-and-frameworks/qwik","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304110,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"One of the topics that seems to perennially draw interest in our discussions is the issue of monorepos. Some places have embraced them for the whole organization, while others have applied the concept in certain narrow applications such as mobile applications or combined UI/BFF development. Regardless of whether or where monorepos are appropriate, the industry seems to be revisiting tools that can effectively manage large codebases and build them efficiently into deployable units. Turborepo is a relatively new tool in this category that offers an alternative to Nx or Lerna for large JavaScript or TypeScript codebases. One of the challenges with large repos is executing builds quickly enough that they don't interrupt developer flow or reduce efficiency. Turborepo is written in Rust which makes it highly performant; it also builds incrementally and caches intermediate steps to speed things up further. However, it does require changes to the developer workflow that take time to learn and is probably best suited to large codebases with multiple independent builds where a different approach is warranted. We've found that the documentation is sparse, leading some teams to stick with more established tools for now. However, it's worth assessing and seeing if Turborepo and its newer companion, Turbopack (currently in beta), continue to evolve.","blip_selector":"turborepo","name":"Turborepo","display_name":"Turborepo","url":"/radar/languages-and-frameworks/turborepo","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304054,"quadrant":"tools","volume_date":"2023-04","description":"Gitleaks is an open-source SAST (static application security testing) command line tool for detecting and preventing hardcoded secrets like passwords, API keys and tokens in Git repositories. It can be used as a Git pre-commit hook or in the CI/CD pipeline. Our teams found Gitleaks to be more sensitive than some of the other secret-scanning tools. Gitleaks utilizes regular expressions and entropy string coding to detect secrets. In our experience, the flexibility to supply custom regex along with entropy coding allowed the teams to better categorize secrets based on their needs. For example, instead of categorizing all API keys as \"generic-api-key,\" it allowed categorization as specific \"cloud provider key.\"","blip_selector":"gitleaks","name":"Gitleaks","display_name":"Gitleaks","url":"/radar/tools/gitleaks","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304006,"quadrant":"techniques","volume_date":"2023-04","description":"One of the big challenges in developing APIs is capturing and communicating their business value. APIs are, by their nature, technical artifacts. Whereas developers can easily comprehend JSON payloads, OpenAPI (Swagger) specs and Postman demos, business stakeholders tend to respond better to demos they can interact with. The value of the product is more clearly articulated when you can see and touch it, which is why we sometimes find it worthwhile to invest in demo frontends for API-only products. When a custom graphical UI is built alongside an API product, stakeholders can see analogies to paper forms or reports that might be more familiar to them. As the interaction model and richness of the demo UI evolves, it allows them to make more informed decisions about the direction the API product should take. Working on the UI has the added benefit of increasing developers' empathy for business users. This isn't a new technique — we've been doing this successfully when necessary as long as API products have been around. However, because this technique isn't widely known, we thought it worthwhile calling attention to it.","blip_selector":"demo-frontends-for-api-only-products","name":"Demo frontends for API-only products","display_name":"Demo frontends for API-only products","url":"/radar/techniques/demo-frontends-for-api-only-products","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304085,"quadrant":"tools","volume_date":"2023-04","description":"The Kotlin ecosystem continues to evolve, and our teams report positive experiences with Ktlint, a simple and easy-to-configure linter and formatter for Kotlin code. We like opinionated and automated code formatting as it lets developers focus more on what the code does rather than how it looks; this tool enables development teams to maintain consistency and readability in their codebases efficiently, reducing the likelihood of messy merges due to formatting issues. Ktlint can be easily configured to run in pre-commit hooks, targeting only the files with changes and resulting in faster integration processes.","blip_selector":"ktlint","name":"Ktlint","display_name":"Ktlint","url":"/radar/tools/ktlint","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210050,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Synthetic Data Vault (SDV) is a synthetic data generation ecosystem of libraries that can learn the distribution of a data set to generate synthetic data with the same format and statistical properties as the source. In the past, we talked about the downsides of using production data in test environments. However, the nuances of data distribution in production can hardly be replicated manually, resulting in defects and surprises. We've had good experiences using SDV to generate large data for performance testing. SDV fares well with modeling a single table. However, data generation time increases considerably as the number of tables with foreign key constraints increases. Nonetheless, SDV offers great promise for local performance testing. It's a good tool for synthetic data generation and worth considering for your testing needs. | Synthetic Data Vault (SDV) is a synthetic data generation ecosystem of libraries that can learn the distribution of a data set to generate synthetic data with the same format and statistical properties as the source. In the past, we talked about the downsides of using production data in test environments. However, the nuances of data distribution in production can hardly be replicated manually, resulting in defects and surprises. We believe SDV and similar tools can address this gap by generating production-like data for single-table, complex multi-table and multivariate timeseries data. Although SDV isn't new, we quite like it and decided to highlight it.","blip_selector":"synthetic-data-vault","name":"Synthetic Data Vault","display_name":"Synthetic Data Vault","url":"/radar/languages-and-frameworks/synthetic-data-vault","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304112,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Vitest is a unit testing framework for JavaScript. Up to now, many teams have relied on Jest, but Jest doesn't play well with Vite, a modern front-end build tool. Using Jest and Vite together forced teams to create two pipelines — one for build and development and one for unit testing — which required tedious configuration of the pipelines with duplicate settings. These problems are solved with Vitest. It is designed specifically for Vite and uses Vite as a bundler. As an additional feature, Vitest has Jest-compatible APIs which makes it possible to use Vitest as a drop-in replacement for Jest in various build setups. However, using Vite and Vitest together provides a better developer experience, and although Vitest is fast, in our experience, it isn't necessarily faster than using Jest.","blip_selector":"vitest","name":"Vitest","display_name":"Vitest","url":"/radar/languages-and-frameworks/vitest","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304100,"quadrant":"tools","volume_date":"2023-04","description":"Ruff is a new linter for Python. For us, the question is not whether to use a linter or not but which linter to use, and there are several choices for Python. Ruff stands out for two reasons: its out-of-box experience and its speed. It has over 500 rules built in and readily replaces Flake8, including many of that linter's plug-ins. The claims by the team behind Ruff about its performance are borne out by our experience. It really is at least an order of magnitude faster than other linters, which is a huge benefit because it helps reduce build times on large codebases.","blip_selector":"ruff","name":"Ruff","display_name":"Ruff","url":"/radar/tools/ruff","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110099,"quadrant":"tools","volume_date":"2023-04","description":"Vite, a front-end build tool, has continued to mature and grow in popularity since we featured it in the Assess ring in the previous Radar. It is rapidly becoming the default choice among our teams when starting a new front-end project. Vite provides a set of defaults for building, bundling and managing dependencies in applications that depend on ES modules in the browser. Because it takes advantage of the native speed of esbuild and the Rollup bundler, Vite significantly improves the front-end developer experience. Moreover, when used with React, Vite offers an attractive alternative to the stalwart but nearly defunct Create React App. Vite relies on ES modules, and unlike most older tools, it doesn't provide shimming or polyfills, which means you need a different strategy for older browsers that don't support ES modules. In cases where older browsers had to be supported, some of our teams import polyfills at the module level so that Vite can be used consistently across environments. | Fast feedback is crucial for a good developer experience. Nothing breaks the flow of development more than having to wait a minute or two before getting feedback on the last code changes. Unfortunately, with applications growing in size and complexity, the popular build tools for front-end pipelines are often not fast enough anymore. Previously, we featured esbuild, which offers a significant performance improvement, because it's implemented in a compile-to-native language rather than JavaScript. Vite, which is built on top of esbuild, delivers significant improvements over other tools. It consists of two major parts: a dev server that provides rich feature enhancements over native ES modules, such as extremely fast Hot Module Replacement (HMR), and a build command that bundles your code with Rollup. Vite relies on ES modules, and unlike most older tools, it doesn't provide shimming or polyfills, which means it's not compatible with older browsers that don't support ES modules. In cases where older browsers had to be supported, some of our teams used Vite during development and other tools for production builds.","blip_selector":"vite","name":"Vite","display_name":"Vite","url":"/radar/tools/vite","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304091,"quadrant":"platforms","volume_date":"2023-04","description":"OpenLineage is an open standard for lineage metadata collection for data pipelines, designed to instrument jobs as they're running. It defines a generic model of run, job and data set entities using consistent naming conventions. The core lineage model is extensible by defining specific facets to enrich those entities. OpenLineage solves the interoperability problem between producers and consumers of lineage data who otherwise would need to know how to speak to each other in various ways. Although there is a risk of it being another \"standard in the middle,\" being a Linux Foundation AI & Data Foundation project increases its chances of gaining widespread adoption. OpenLineage currently supports data collection for multiple platforms, such as Spark, Airflow and dbt, although users need to configure its listeners. Support for OpenLineage data consumers is more limited at this time.","blip_selector":"openlineage","name":"OpenLineage","display_name":"OpenLineage","url":"/radar/platforms/openlineage","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304096,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Quarto is an open-source scientific and technical publishing system. With it, we can build computational notebooks that allow you to write documents in markdown, embed code and emit that code’s output into the final document. It can be used to create reproducible and customizable data analysis reports, which can be easily shared in a variety of formats. Our data science teams used Quarto to share data analysis reports containing visualizations (plots) and tables. They liked being able to use R and Python to generate these dynamic reports and then export them as HTML to share with stakeholders. If you're looking to share your research and analysis within or outside of your organization, we recommend evaluating Quarto.","blip_selector":"quarto","name":"Quarto","display_name":"Quarto","url":"/radar/languages-and-frameworks/quarto","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304055,"quadrant":"tools","volume_date":"2023-04","description":"Defects are cheaper to fix when they're caught early. That's why we always try to get the fastest possible feedback to developers in the form of static analysis, unit tests or end-to-end tests run in the local environment. Accessibility is no exception to this and that’s why we've featured tools such as Lighthouse, axe-core and axe Linter in the past. When it comes to automatically testing web pages that are already deployed in production, one of our teams chose instead to go with IBM Equal Access Accessibility Checker in a head-to-head comparison. Although we're still in the process of assessing the results, we can say that it offers an efficient way to test pages once they've been deployed. We emphasize that this should be used to augment, not replace, early automated testing by the developer. The tool is distributed under a Creative Commons license and is free to use under those restrictions.","blip_selector":"ibm-equal-access-accessibility-checker","name":"IBM Equal Access Accessibility Checker","display_name":"IBM Equal Access Accessibility Checker","url":"/radar/tools/ibm-equal-access-accessibility-checker","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304002,"quadrant":"techniques","volume_date":"2023-04","description":"Like many in the software industry, we've been exploring the rapidly evolving AI tools that can support us in writing code. We see many people feed ChatGPT with an implementation, and then ask it to generate tests for that implementation. However, because we're big believers in TDD, and we don't always want to feed an external model with our potentially sensitive implementation code, one of our experiments in this space is a technique we call AI-aided test-first development. In this approach, we get ChatGPT to generate tests for us, and then a developer implements the functionality. Specifically, we first describe the tech stack and the design patterns we're using in a prompt \"fragment\" that is reusable across multiple use cases. Then we describe the specific feature we want to implement, including the acceptance criteria. Based on all that, we ask ChatGPT to generate an implementation plan for that feature in our architectural style and tech stack. Once we sanity check that implementation plan, we ask it to generate tests for our acceptance criteria.\n\nThis approach has worked surprisingly well for us: It required the team to come up with a concise description of their architectural style and helped junior developers and new team members code features aligned with the team’s existing style. The main drawback of this approach is that even though we don't give the model our source code, we still feed it potentially sensitive information such as our tech stack and feature descriptions. Teams should ensure they're working with their legal advisors to avoid any intellectual property issues, at least until a \"for business\" version of these AI tools becomes available.","blip_selector":"ai-aided-test-first-development","name":"AI-aided test-first development","display_name":"AI-aided test-first development","url":"/radar/techniques/ai-aided-test-first-development","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304109,"quadrant":"techniques","volume_date":"2023-04","description":"Tracking technical debt is a perennial topic in software delivery organizations. What is technical debt and what is not? How do you prioritize it? And most importantly, how do you express the value of paying it off to your internal stakeholders? Following the Agile Manifesto’s manner of reasoning — \"while there is value in the item on the right, we value the item on the left more\" — we like the idea of tracking health over debt. The folks at REA in Australia share a good example of what such health tracking can look like. They track system ratings in the categories of development, operations and architecture.\n\nFocusing on health instead of debt is a more constructive framing. It connects a team to the ultimate value of reducing debt and helps them prioritize it. Every piece of tackled technical debt should ideally be connectable to one of the agreed expectations. Teams should treat the health rating the same as other service-level objectives (SLOs) and prioritize improvements whenever they drop out of the \"green zone\" for a given category.","blip_selector":"tracking-health-over-debt","name":"Tracking health over debt","display_name":"Tracking health over debt","url":"/radar/techniques/tracking-health-over-debt","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010004,"quadrant":"techniques","volume_date":"2023-04","description":"We've always been advocates of writing less code. Simplicity is one of the core values underlying our sensible defaults for software development. For example, we try not to anticipate needs and only introduce code that satisfies immediate business requirements and nothing else. One way to achieve this is to create engineering platforms that make this possible on an organizational basis.\n\nThis is also the stated aim of many low-code platforms surging in popularity right now. Platforms like Mendix or Microsoft Power Apps can expose common business processes for reuse and simplify the problems of getting new functionality deployed and in the hands of users. These platforms have made great strides in recent years with testability and support for good engineering practices. They're particularly useful for simple tasks or event-triggered apps. However, asking them to adapt to a nearly infinite range of business requirements brings complexity. Although developers might be writing less (or zero) code, they must also become experts in an all-encompassing commercial platform. We would advise businesses to consider if they need all the functionality these products bring or if they're better off pursuing bounded low-code platforms, either by developing their own platform as an internal product or by carefully constraining the use of commercial low-code products to those simple tasks at which they excel. | One of the most nuanced decisions facing companies at the moment is the adoption of low-code or no-code platforms, that is, platforms that solve very specific problems in very limited domains. Many vendors are pushing aggressively into this space. The problems we see with these platforms typically relate to an inability to apply good engineering practices such as versioning. Testing too is typically really hard. However, we noticed some interesting new entrants to the market — including Amazon Honeycode, which makes it easy to create simple task or event management apps, and Parabola for IFTTT-like cloud workflows — which is why we're once again including bounded low-code platforms in this volume. Nevertheless, we remain deeply skeptical about their wider applicability since these tools, like Japanese Knotweed, have a knack of escaping their bounds and tangling everything together. That's why we still strongly advise caution in their adoption. | One of the most nuanced decisions facing companies at the moment is the adoption of low-code or no-code platforms, that is, platforms that solve very specific problems in very limited domains. Many vendors are pushing aggressively into this space. The problems we see with these platforms typically relate to an inability to apply good engineering practices such as versioning. Testing too is typically really hard. However, we noticed some interesting new entrants to the market — including Amazon Honeycode, which makes it easy to create simple task or event management apps, and Parabola for IFTTT-like cloud workflows — which is why we're including bounded low-code platforms in this volume. Nevertheless, we remain deeply skeptical about their wider applicability since these tools, like Japanese Knotweed, have a knack of escaping their bounds and tangling everything together. That's why we still strongly advise caution in their adoption.","blip_selector":"bounded-low-code-platforms","name":"Bounded low-code platforms","display_name":"Bounded low-code platforms","url":"/radar/techniques/bounded-low-code-platforms","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304050,"quadrant":"tools","volume_date":"2023-04","description":"FOSSA is an open-source compliance tool that helps developers and teams determine which open-source components their code relies on and which licenses these components are released under. This information is essential for ensuring compliance with various open-source licenses and maintaining the Software Bill of Materials. FOSSA integrates with dependency management tools of various tech stacks to identify which open-source components are used in a project. It also highlights any license issues based on the organization’s policies and generates reports of the same. Some key features of FOSSA include its ability to integrate with development workflows, such as the CI, and to perform real-time compliance monitoring. Many of our clients and teams have found FOSSA to be a valuable and effective tool.","blip_selector":"fossa","name":"FOSSA","display_name":"FOSSA","url":"/radar/tools/fossa","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202010039,"quadrant":"platforms","volume_date":"2023-04","description":"K3s remains our default Kubernetes distribution for edge computing needs and resource-constrained environments. It's a lightweight, fully compliant Kubernetes but with reduced operational overhead. It uses sqlite3 as the default storage backend instead of etcd. It has a reduced memory footprint because it runs all relevant components in a single process. We've used K3s in environments like industrial control systems and point-of-sale machines, and we're quite happy with our decision. With the K3s runtime containerd now supporting wasm, K3s can run and manage WebAssembly workloads directly, further reducing the runtime overhead. | K3s is a lightweight Kubernetes distribution built for IoT and edge computing. You get the benefits of a fully compliant Kubernetes but with reduced operational overhead. Its enhancements include lightweight storage backends (sqlite3 as default instead of etcd), a single binary package with minimal OS dependencies and reduced memory footprint, all of which make K3s suitable for resource-constrained environments. We've used K3s in point-of-sale machines, and we're quite happy with our decision. | K3s is a lightweight Kubernetes distribution built for IoT and edge computing. It's packaged as a single binary and has minimal to no OS dependencies, making it really easy to operate and use. It uses sqlite3 as the default storage backend instead of etcd. It has a reduced memory footprint because it runs all relevant components in a single process. It also achieves a smaller binary by stripping out third-party storage drivers and cloud providers that are not relevant for the K3s use cases. For environments with constrained resources, K3s is a pretty good choice and worth considering.","blip_selector":"k3s","name":"K3s","display_name":"K3s","url":"/radar/platforms/k3s","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304046,"quadrant":"tools","volume_date":"2023-04","description":"Enterprises now often use event streaming as the source of truth and as an information-sharing mechanism in microservices architectures. This creates the need to standardize event types and share those standards across the enterprise. Event schema registries are commonly deployed but the existing offerings tend to be specialized to a single broker such as Apache Kafka or Azure Event Hub. They also fall short of conveying rich documentation about event types that goes beyond simple schema definitions. EventCatalog is an open-source project that provides something we often see businesses building for themselves: a widely accessible repository of documentation for events and schemas. These describe the role the events play in the business, where they belong in a business domain model and which services subscribe and publish them. If you're looking for a way to publish event documentation to your organization, this tool might save you the trouble of building it yourself.","blip_selector":"eventcatalog","name":"EventCatalog","display_name":"EventCatalog","url":"/radar/tools/eventcatalog","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304049,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Flutter for embedded makes it relatively easy to create and maintain a modern UI similar to mobile apps but for embedded systems like human-machine interface (HMI) in cars, refrigerators and other consumer appliances. This is made possible with Flutter now supporting custom embedders, which allows portability to different platforms. The apps are written in the Dart programming language using the Flutter SDK and ecosystem. We've been building prototypes with it —  our developers love the dev experience and our customers like the agility, speed and modern user experience that it brings.","blip_selector":"flutter-for-embedded","name":"Flutter for embedded","display_name":"Flutter for embedded","url":"/radar/languages-and-frameworks/flutter-for-embedded","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304070,"quadrant":"tools","volume_date":"2023-04","description":"More and more teams are using the Kubernetes Operators pattern to manage their Kubernetes clusters. We used to recommend Crossplane for this, and now we have an alternative tool, Terraform Cloud Operator for Kubernetes. This tool integrates Terraform Cloud and Kubernetes by extending the Kubernetes control plane to enable lifecycle management of cloud and on-premise infrastructures through Kubernetes manifests. Our team uses it to provision resources from Kubernetes namespaces and RoleBindings to cloud database instances and other SaaS resources. We quite like it because it leverages the Terraform module, which is a more familiar abstraction layer for us to operate cloud resources.","blip_selector":"terraform-cloud-operator","name":"Terraform Cloud Operator","display_name":"Terraform Cloud Operator","url":"/radar/tools/terraform-cloud-operator","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304078,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"We've used Directus as a headless content management system (CMS). Although we have options when it comes to headless CMS products, we needed a self-hosted solution with rich digital asset management and content authoring workflows. In this evaluation we find Directus to be a good fit for our needs; we quite like its event-driven data processing and automation via flows.","blip_selector":"directus","name":"Directus","display_name":"Directus","url":"/radar/languages-and-frameworks/directus","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104088,"quadrant":"platforms","volume_date":"2023-04","description":"Any software system needs to properly represent the given domain in which it is employed and should always be informed by key aims and goals. Machine learning (ML) projects are no different. Feature engineering is a crucial aspect of engineering and designing ML software systems. Feature Store is a related architectural concept to facilitate the identification, discovery and monitoring of the features pertinent to the given domain or business problem. Implementing this concept involves a combination of architectural design, data engineering and infrastructure management to create a scalable, efficient and reliable ML system. From a tooling perspective, you can find open-source and fully managed platforms, but they're only one part of this concept. In the end-to-end design of ML systems, implementing a feature store enables the following capabilities: the ability to (1) define the right features; (2) enhance reusability and make features consistently available regardless of the type of model, which also includes setting up the feature engineering pipelines that curate data as described in the feature store; (3) enable feature discovery and (4) enable feature serving. Our teams leverage feature stores in production to reap these benefits for end-to-end ML systems. | Feature Store is an ML-specific data platform that addresses some of the key challenges we face today in feature engineering with three fundamental capabilities: (1) it uses managed data pipelines to remove struggles with pipelines as new data arrives; (2) catalogs and stores feature data to promote discoverability and collaboration of features across models; and (3) consistently serves feature data during training and interference. \n\nSince Uber revealed their Michelangelo platform, many organizations and startups have built their own versions of a feature store; examples include Hopsworks, Feast and Tecton. We see potential in Feature Store and recommend you carefully assess it.","blip_selector":"feature-store","name":"Feature Store","display_name":"Feature Store","url":"/radar/platforms/feature-store","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304020,"quadrant":"tools","volume_date":"2023-04","description":"Devbox provides an approachable interface for creating reproducible, per-project development environments leveraging the Nix package manager. Our teams use it to eliminate version and configuration mismatches in their development environments, and they like it for its ease of use. Devbox supports shell hooks, custom scripts and devcontainer.json generation for integration with VSCode.","blip_selector":"devbox","name":"Devbox","display_name":"Devbox","url":"/radar/tools/devbox","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304061,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"mljar-supervised is an AutoML Python package that assists with understanding and explaining tabular data. Our data science teams are excited about it and use it to automate exploratory data analysis. It abstracts the common way to preprocess the data, construct the machine learning (ML) models and perform hyper-parameters tuning to find the best model. Explainability and transparency are important tenets, and that's where mljar-supervised shines. It allows you to see exactly how the ML pipeline is constructed with a detailed markdown report for each ML model. It’s definitely an interesting AutoML package that’s worth assessing for your ML needs.","blip_selector":"mljar-supervised","name":"mljar-supervised","display_name":"mljar-supervised","url":"/radar/languages-and-frameworks/mljar-supervised","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202304025,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"Mikro ORM is an object-relational mapping (ORM) framework that has an interesting TypeScript-centric approach. By leveraging TypeScript throughout the entire stack, it offers a consistent development experience from browser to backend, making it easier for developers to write and maintain code. Notably, Mikro ORM's performance is excellent, enabling rapid query execution and minimizing latency. While Mikro ORM offers appealing features, it’s essential to keep in mind the general caveats associated with object-relational mappers. ORM frameworks are often complex and offer only a leaky abstraction over a relational data store, and so using one is always a balance of trade-offs.","blip_selector":"mikro-orm","name":"Mikro ORM","display_name":"Mikro ORM","url":"/radar/languages-and-frameworks/mikro-orm","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304092,"quadrant":"tools","volume_date":"2023-04","description":"We've already assessed Ory Hydra as a self-hosted OAuth2 solution, and the feedback from the team has been good. This time, we turn to Ory Kratos, an API-first identity and user management system that’s developer friendly and easy to customize. It already provides common functions we want to achieve in an identity management system, including self-service login and registration, multi-factor authentication (MFA/2FA), account verification and account recovery. Like Hydra, Kratos is headless and requires developers to build the UI themselves, which gives the team more flexibility. Developers can also customize identity schema to fit different business contexts. Kratos has no external dependencies other than the database, and it's easy to deploy and scale in different cloud environments. If you need to build a user management system, we recommend you give Kratos a try.","blip_selector":"ory-kratos","name":"Ory Kratos","display_name":"Ory Kratos","url":"/radar/tools/ory-kratos","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202304059,"quadrant":"languages-and-frameworks","volume_date":"2023-04","description":"LangChain is a framework for building applications with large language models (LLMs). These models have triggered a race to incorporate generative AI in several use cases. However, using these LLMs in isolation may not be enough — you have to combine them with your differentiated assets to build an impactful product. LangChain fills this niche with some neat features, including prompt management, chaining, data augmented generation and a rich set of agents to determine which actions to take and in what order. We expect more tooling and frameworks to evolve with LLMs, and we recommend assessing LangChain.","blip_selector":"langchain","name":"LangChain","display_name":"LangChain","url":"/radar/languages-and-frameworks/langchain","isCurrentEdition":true,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210015,"quadrant":"tools","volume_date":"2022-10","description":"As data work becomes more common, we continue to see tools that try to enhance the SQL language; Kusto Query Language (KQL) is one of them. KQL was created by Azure, and it brings modularity, encapsulation, composability, reusability, extensibility and dynamism to relational querying. Our teams quite like its interactivity: you can pipe a query to the render operator and see a chart instantly. You can also combine these charts into dashboards and get insights from logs to execs in minutes. Although the KQL language is currently limited to the Azure Data Explorer, we anticipate the move to enhance SQL to achieve better data operability will not stop.","blip_selector":"kusto-query-language","name":"Kusto Query Language","display_name":"Kusto Query Language","url":"/radar/tools/kusto-query-language","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202210108,"quadrant":"techniques","volume_date":"2022-10","description":"The term \"cloud native\" was originally used to describe architectures with characteristics that took maximum advantage of public cloud hosting. Examples include distributed architectures composed of many small, stateless and collaborating processes, and systems with high levels of automation for building, testing and deploying applications. However, we've noticed a growing trend toward superficial cloud native designs that simply use a lot of a cloud vendor's proprietary services and stop there without revisiting the fundamentally monolithic, brittle or toil-intensive nature of the application. It’s important to remember that serverless functions by themselves don't make an application more resilient or easier to maintain and that cloud native is really a matter of design rather than a set of implementation choices.","blip_selector":"superficial-cloud-native","name":"Superficial cloud native","display_name":"Superficial cloud native","url":"/radar/techniques/superficial-cloud-native","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210030,"quadrant":"techniques","volume_date":"2022-10","description":"Sustainability is a topic that demands the attention of enterprises. In the software development space its importance has increased, and we're now seeing different ways to approach this topic. Looking at the carbon footprint of building software, for example, we recommend assessing carbon efficiency as an architectural characteristic. An architecture that takes into consideration carbon efficiency is one where design and infrastructure choices have been made in order to to minimize energy consumption and therefore carbon emissions. The measurement tooling and advice in this space is maturing, making it feasible for teams to consider carbon efficiency alongside other factors such as performance, scalability, financial cost and security. Like almost everything in software architecture, this should be considered a trade-off; our advice is to think about this as one additional characteristic in a whole set of relevant quality attributes that are driven and prioritized by organizational goals and not left to a small cadre of experts to ponder in a siloed manner.","blip_selector":"carbon-efficiency-as-an-architectural-characteristic","name":"Carbon efficiency as an architectural characteristic","display_name":"Carbon efficiency as an architectural characteristic","url":"/radar/techniques/carbon-efficiency-as-an-architectural-characteristic","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210054,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"As smart devices continue to embed themselves in our lives, we are starting to see new use cases emerge that span across multiple devices. The classic example is a text we start reading on a phone but prefer to finish on a tablet. Other examples include plotting a cycling route on a laptop and then transferring the data to a bike computer for easier navigation or using a mobile phone as a webcam. Such use cases require very specific kinds of features, like the discovery of nearby devices, secure communication and multi-device sessions. Apple started introducing such features a while ago to its own SDKs, and now Google has released the first preview of its Cross device SDK. Although the preview has several limitations — for example, only phones and tablets are supported and only two devices at a time — the technology is exciting and can be utilized as it is rolled out over time.","blip_selector":"cross-device-sdk","name":"Cross device SDK","display_name":"Cross device SDK","url":"/radar/languages-and-frameworks/cross-device-sdk","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210022,"quadrant":"tools","volume_date":"2022-10","description":"When implementing robust, secure and reliable disaster recovery, it’s necessary to ensure that backups can't be deleted or altered before their expiry, either maliciously or accidentally. Previously, with AWS Backup, these policies and guarantees had to be implemented by hand. Recently, AWS has added the Vault Lock feature to ensure backups are immutable and untamperable. AWS Backup Vault Lock enforces retention and deletion policies and prevents even those with administrator privileges from altering or deleting backup files. This has proved to be a valuable addition and fills a previously empty space.","blip_selector":"aws-backup-vault-lock","name":"AWS Backup Vault Lock","display_name":"AWS Backup Vault Lock","url":"/radar/tools/aws-backup-vault-lock","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202210080,"quadrant":"techniques","volume_date":"2022-10","description":"The term \"remote team setup\" does not just describe one setup; it encompasses multiple patterns and flavors. And many teams have been changing patterns recently. They're coming out of the \"everybody always remote\" mode that was forced on them by a pandemic and moving into a pattern of (often rotating) satellite workers, where part of the team is co-located and part of the team is remote. We see many of them failing to properly consider what this means for their ways of working. Satellite workers without \"remote native\" ways of working is a slip back into privileging co-located practices. In a setup with satellite workers, it's important to still use \"remote native\" processes and approaches by default. For example, if the co-located part of the team joins a meeting together, they should still all be on their individual laptops to participate in digital collaboration or meeting chat. Teams need to be aware of the risk of excluding their satellite workers and creating silos and feelings of exclusion. If you know that you'll always have at least one satellite team member, the default ways of working should assume remoteness.","blip_selector":"satellite-workers-without-remote-native","name":"Satellite workers without \"remote native\"","display_name":"Satellite workers without \"remote native\"","url":"/radar/techniques/satellite-workers-without-remote-native","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210029,"quadrant":"techniques","volume_date":"2022-10","description":"The accidental publication of secrets seems to be a perennial issue with tools such as Talisman popping up to help with the problem. Before now, GitHub Enterprise Cloud users with an Advanced Security License could enable security scanning on their accounts, and any secrets (API keys, access tokens, credentials, etc.) that were accidentally committed and pushed would trigger an alert. GitHub push protection takes this one step further, and brings it one step earlier in the development workflow, by blocking changes from being pushed at all if secrets are detected. This needs to be configured for the organization and applies, of course, only to license holders, but additional protection from publishing secrets is to be welcomed.","blip_selector":"github-push-protection","name":"GitHub push protection","display_name":"GitHub push protection","url":"/radar/techniques/github-push-protection","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911049,"quadrant":"techniques","volume_date":"2022-10","description":"Since we last talked about BERT (Bidirectional Encoder Representations from Transformers) in the Radar, our teams have successfully used it in a few natural language processing (NLP) projects. In one of our engagements, we observed significant improvements when we switched from the default BERT tokenizer to a domain-trained word-piece tokenizer for queries that contain nouns like brand names or dimensions. Although NLP has several new transformer models, BERT is well understood with good documentation and a vibrant community, and we continue to find it effective in an enterprise NLP context. | BERT stands for Bidirectional Encoder Representations from Transformers; it's a new method of pretraining language representations which was published by researchers at Google in October 2018. BERT has significantly altered the natural language processing (NLP) landscape by obtaining state-of-the-art results on a wide array of NLP tasks. Based on Transformer architecture, it learns from both the left and right side of a token's context during training. Google has also released pretrained general-purpose BERT models that have been trained on a large corpus of unlabelled text including Wikipedia. Developers can use and fine-tune these pre-trained models on their task-specific data and achieve great results. We talked about transfer learning for NLP in our April 2019 edition of the Radar; BERT and its successors continue to make transfer learning for NLP a very exciting field with significant reduction in effort for users dealing with text classification.","blip_selector":"bert","name":"BERT","display_name":"BERT","url":"/radar/techniques/bert","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210032,"quadrant":"techniques","volume_date":"2022-10","description":"During our discussions for this edition of the Radar, several tools and applications for synthetic data generation came up. As the tools mature, we've found that using synthetic data for testing models is a powerful and broadly useful technique. Although not intended as a substitute for real data in validating the discrimination power of machine-learning models, synthetic data can be used in a variety of situations. For example, it can be used to guard against catastrophic model failure in response to rarely occurring events or to test data pipelines without exposing personally identifiable information. Synthetic data is also useful for exploring edge cases that lack real data or for identifying model bias. Some helpful tools for generating data include Faker or Synth, which generate data that conforms to desired statistical properties, and tools like Synthetic Data Vault that can generate data that mimics the properties of an input data set.","blip_selector":"synthetic-data-for-testing-models","name":"Synthetic data for testing models","display_name":"Synthetic data for testing models","url":"/radar/techniques/synthetic-data-for-testing-models","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203042,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"For those who prefer a more natural language than JSON for infrastructure code, Azure Bicep is a domain-specific language (DSL) that uses a declarative syntax and supports reusable parameterized templates for modular resource definitions. A Visual Studio Code extension provides instant type safety, intellisense and syntax checking, while the compiler allows bidirectional transpilation to and from Azure Resource Manager (ARM) templates. Bicep's resource-oriented DSL and native integration with the Azure ecosystem make it a compelling choice for Azure infrastructure development. | For those who prefer a more natural language than JSON for infrastructure code, Azure Bicep is a domain-specific language (DSL) that uses a declarative syntax. It supports reusable parameterized templates for modular resource definitions. A Visual Studio Code extension provides instant type-safety, intellisense and syntax checking, and the compiler allows bidirectional transpilation to and from ARM templates. Bicep's resource-oriented DSL and native integration with the Azure ecosystem make it a compelling choice for Azure infrastructure development.","blip_selector":"azure-bicep","name":"Azure Bicep","display_name":"Azure Bicep","url":"/radar/languages-and-frameworks/azure-bicep","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210045,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Cypress Component Testing provides a testable component workbench to quickly build and test UI components. You can write component visual regression tests with the same API that you write end-to-end (E2E) UI tests. Although still in beta, component testing will be the most important feature in Cypress 10.","blip_selector":"cypress-component-testing","name":"Cypress Component Testing","display_name":"Cypress Component Testing","url":"/radar/languages-and-frameworks/cypress-component-testing","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210049,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Million is a new virtual DOM JavaScript library. Similar to Svelte, it leverages the compiler, Vite, to create small JavaScript bundles with exceptional rendering performance. The Million library ships as a single NPM package with several modules — including router, jsx-runtime and a module for React compatibility to create single-page applications. Although React popularized the virtual DOM a decade ago, it's fascinating to see new innovations in this space.","blip_selector":"million","name":"Million","display_name":"Million","url":"/radar/languages-and-frameworks/million","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210072,"quadrant":"platforms","volume_date":"2022-10","description":"In previous Radars, we've written about TinyML — the practice of running trained models on small devices with onboard sensors to make decisions or extract features without a roundtrip to the cloud. Edge Impulse has made the process of collecting sensor data and then training and deploying a model as simple as possible. Edge Impulse is an end-to-end hosted platform for developing models optimized to run on small edge devices such as microcontrollers. The platform guides the developer through the entire pipeline, including the task of collecting and labeling training data. They've made it easy to get started using your mobile phone for both data collection and running the classifier while the model training and refining happens in the more powerful, cloud-hosted environment. The resulting recognition algorithms can also be optimized, compiled and uploaded to a wide range of microcontroller architectures. Although Edge Impulse is a commercial venture, the platform is free for developers and makes the entire process fun and engaging even for those who are new to machine learning. The low barrier of entry to creating a working application means that we'll be seeing more edge devices with smart decisioning built in.","blip_selector":"edge-impulse","name":"Edge Impulse","display_name":"Edge Impulse","url":"/radar/platforms/edge-impulse","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202104127,"quadrant":"techniques","volume_date":"2022-10","description":"Team interaction is a key concept when redesigning an organization for business agility and speed. These interactions will be reflected in the software being built (see Conway's Law) and indicate how effectively teams can autonomously deliver value to their customers. Our advice is to be intentional about how teams are designed and how they interact. Because we believe that organizational design and team interactions evolve over time, we think it's particularly important to measure and keep track of the team cognitive load, which indicates how easy or difficult teams find building, testing and maintaining their services. We've been using a template to assess team cognitive load that is based on ideas by the authors of the Team Topologies book.\n\nWe continue to be impressed by the positive impact of applying this book's concepts when communicating to clients and redesigning organizations. The authors recommend a simple but powerful approach to organizational design, identifying just four types of teams and three modes of interaction; this helps reduce ambiguity within the organization and provides a common vocabulary for teams, stakeholders and leadership to describe and design a team's work. To implement an org design change, we design the ideal to-be team topologies structure, apply any technical/staffing constraints (i.e., not enough employees) and then end up with the final to-be structure. That allows us to better advise clients and anticipate whether we're indeed improving cognitive load by comparing the as-is/to-be team structures. | A system's architecture mimics an organizational structure and its communication. It's not big news that we should be intentional about how teams interact — see, for instance, the Inverse Conway Maneuver. Team interaction is one of the variables for how fast and how easily teams can deliver value to their customers. We were happy to find a way to measure these interactions; we used the Team Topologies author's assessment which gives you an understanding of how easy or difficult the teams find it to build, test and maintain their services. By measuring team cognitive load, we could better advise our clients on how to change their teams' structure and evolve their interactions. | A system's architecture mimics organizational structure and its communication. It's not big news that we should be intentional about how teams interact — see, for instance, the Inverse Conway Maneuver. Team interaction is one of the variables for how fast and how easily teams can deliver value to their customers. We were happy to find a way to measure these interactions; we used the Team Topologies author's assessment which gives you an understanding of how easy or difficult the teams find it to build, test and maintain their services. By measuring team cognitive load, we could better advise our clients on how to change their teams' structure and evolve their interactions. | A system's architecture mimics organizational structure and its communication. It's not big news that we should be intentional about how teams interact — see, for instance, the Inverse Conway Maneuver. Team interaction is one of the variables for how fast and how easily teams can deliver value to their customers. We were happy to find a way to measure these interactions; we used the Team Topologies author's assessment which gives you an understanding of how easy or difficult the teams find it to build, test and maintain their services. By measuring team cognitive load, we could better advise our clients on how to change their teams' structure and evolve their interactions.","blip_selector":"team-cognitive-load","name":"Team cognitive load","display_name":"Team cognitive load","url":"/radar/techniques/team-cognitive-load","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202210014,"quadrant":"tools","volume_date":"2022-10","description":"We previously called out production data in test environments and now want to highlight another common practice that needs to be approached with care or even stopped entirely: online services for formatting or parsing code. There are many useful sites for formatting or parsing formats such as JSON and YAML, as well as sites that assess code tutorials or produce online code metrics. Great care is needed when using these. Pasting a block of JavaScript, JSON or similar into an unknown website can easily create security and privacy issues and might unknowingly export personal data into a different jurisdiction. These sites should never be used with production data and should be approached with caution in all other circumstances.","blip_selector":"online-services-for-formatting-or-parsing-code","name":"Online services for formatting or parsing code","display_name":"Online services for formatting or parsing code","url":"/radar/tools/online-services-for-formatting-or-parsing-code","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210027,"quadrant":"techniques","volume_date":"2022-10","description":"Since Google first popularized service-level indicators (SLIs) and service-level objectives (SLOs) as part of their site reliability engineering (SRE) practice, observability tools like Datadog, Honeycomb and Dynatrace started incorporating SLO monitoring into their toolchains. OpenSLO is an emerging standard that allows defining SLIs and SLOs as code, using a declarative, vendor-neutral specification language based on the YAML format used by Kubernetes. While the standard is still quite new, we're seeing some encouraging momentum, as with Sumo Logic's contribution of the slogen tool to generate monitoring and dashboards. We're excited by the promise of versioning SLI and SLO definitions in code and updating observability tooling as part of the CI/CD pipeline of the service being deployed.","blip_selector":"slis-and-slos-as-code","name":"SLIs and SLOs as code","display_name":"SLIs and SLOs as code","url":"/radar/techniques/slis-and-slos-as-code","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210025,"quadrant":"tools","volume_date":"2022-10","description":"On remote teams, we sorely lack having a dedicated build monitor in the room; unfortunately, newer continuous integration (CI) tools lack support for the old CCTray format. The result is that broken builds aren't always picked up as quickly as we'd like. To solve this problem, many of our teams have started using xbar for build monitoring. With xbar, one can execute a script to poll build status, displaying it on the menu bar. It can be further scripted to track other team metrics such as pending credential expiries or how far the production release lags behind the user acceptance testing (UAT) release. Of course, xbar is more general purpose, but it solves an immediate and emergent problem caused by remote working. Rumps, among other tools, can solve the same problem.","blip_selector":"xbar-for-build-monitoring","name":"xbar for build monitoring","display_name":"xbar for build monitoring","url":"/radar/tools/xbar-for-build-monitoring","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210006,"quadrant":"tools","volume_date":"2022-10","description":"Databricks Overwatch is a Databricks Labs project that enables teams to analyze various operational metrics of Databricks workloads around cost, governance and performance with support to run what-if experiments. It's essentially a set of data pipelines that populate tables in Databricks, which can then be analyzed using tools like notebooks. Overwatch is very much a power tool; however, it's still in its early stages and it may take some effort to set it up — our use of it required Databricks solution architects to help set it up and populate a price reference table for cost calculations — but we expect adoption to get easier over time. The level of analysis made possible by Overwatch is deeper than what is allowed by cloud providers' cost analysis tools. For example, we were able to analyze the cost of job failures — recognizing that failing fast saves money compared to jobs that only fail near the final step — and break down the cost by various groupings (workspace, cluster, job, notebook, team). We also appreciated the improved operational visibility, as we could easily audit access controls around cluster configurations and analyze operational metrics like finding the longest running notebook or largest read/write volume. Overwatch can analyze historical data, but its real-time mode allows for alerting which helps you to add appropriate controls to your Databricks workloads.","blip_selector":"databricks-overwatch","name":"Databricks Overwatch","display_name":"Databricks Overwatch","url":"/radar/tools/databricks-overwatch","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005098,"quadrant":"tools","volume_date":"2022-10","description":"Apache Superset is a great business intelligence (BI) tool for data exploration and visualization to work with large data lake and data warehouse setups. It supports several data sources — including AWS Redshift, BigQuery, Azure MS SQL, Snowflake and ClickHouse. Moreover, you don't have to be a data engineer to use it; it's meant to benefit all engineers exploring data in their everyday work. For demanding use cases, we found it easy to scale Superset by deploying it in a Kubernetes cluster. Since we last talked about it in the Radar, Superset has graduated as an Apache product, and we've seen great success in several projects. | Apache Superset is a great business intelligence (BI) tool for data exploration and visualization to work with large data lake and data warehouse setups. It works, for example, with Presto, Amazon Athena and Amazon Redshift and can be nicely integrated with enterprise authentication. Moreover, you don't have to be a data engineer to use it; it’s meant to benefit all engineers exploring data in their everyday work. It's worth pointing out that Apache Superset is currently undergoing incubation at the Apache Software Foundation (ASF), meaning it's not yet fully endorsed by ASF.","blip_selector":"apache-superset","name":"Apache Superset","display_name":"Apache Superset","url":"/radar/tools/apache-superset","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202010076,"quadrant":"tools","volume_date":"2022-10","description":"Great Expectations has become a sensible default for our teams in the data quality space, which is why we recommend adopting it — not only for the lack of better alternatives but also because our teams have reported great results in several client projects. Great Expectations is a framework that allows you to craft built-in controls that flag anomalies or quality issues in data pipelines. Just as unit tests run in a build pipeline, Great Expectations makes assertions during the execution of a data pipeline. We like its simplicity and ease of use — the rules stored in JSON can be modified by our data domain experts without necessarily needing data engineering skills. | We wrote about Great Expectations in the previous edition of the Radar. We continue to like it and have moved it to Trial in this edition. Great Expectations is a framework that enables you to craft built-in controls that flag anomalies or quality issues in data pipelines. Just as unit tests run in a build pipeline, Great Expectations makes assertions during execution of a data pipeline. We like its simplicity and ease of use — the rules stored in JSON can be modified by our data domain experts without necessarily needing data engineering skills. | With the rise of CD4ML, operational aspects of data engineering and data science have received more attention. Automated data governance is one aspect of this development. Great Expectations is a framework that enables you to craft built-in controls that flag anomalies or quality issues in data pipelines. Just as unit tests run in a build pipeline, Great Expectations makes assertions during execution of a data pipeline. This is useful not only for implementing a sort of Andon for data pipelines but also for ensuring that model-based algorithms remain within the operating range determined by their training data. Automated controls like these can help distribute and democratize data access and custodianship. Great Expectations also ships with a profiler tool to help understand the qualities of a particular data set and to set appropriate limits.","blip_selector":"great-expectations","name":"Great Expectations","display_name":"Great Expectations","url":"/radar/tools/great-expectations","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210076,"quadrant":"platforms","volume_date":"2022-10","description":"Monte Carlo is a data observability platform. Using machine learning models, it infers and learns about data, identifying issues and notifying users when they arise. It allows our teams to maintain data quality across ETL pipelines, data lakes, data warehouses and business intelligence (BI) reports. With features such as monitoring dashboards as code, a central data catalog and field-level lineage, our teams find Monte Carlo to be an invaluable tool for overall data governance.","blip_selector":"monte-carlo","name":"Monte Carlo","display_name":"Monte Carlo","url":"/radar/platforms/monte-carlo","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210013,"quadrant":"tools","volume_date":"2022-10","description":"One of the fundamental capabilities of Kubernetes is its ability to automatically launch new pods when additional capacity is needed and shut them down when loads decrease. This horizontal autoscaling is a useful feature, but it can only work if the nodes needed to host the pods already exist. While Cluster Autoscaler can do some rudimentary cluster expansion triggered by pod failures, it has limited flexibility; Karpenter, however, is an open-source Kubernetes Operator autoscaler with more smarts built in: it analyzes the current workloads and the pod scheduling constraints to automatically select an appropriate instance type and then start or stop it as needed. Karpenter is an operator in the spirit of tools like Crossplane that can provision cloud resources outside the cluster. Karpenter is an attractive companion to the autoscaling services cloud vendors provide natively with their managed Kubernetes clusters. For example, AWS now supports Karpenter as a first-class alternative in their EKS Cluster Autoscaler service.","blip_selector":"karpenter","name":"Karpenter","display_name":"Karpenter","url":"/radar/tools/karpenter","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210070,"quadrant":"platforms","volume_date":"2022-10","description":"Gradient is a platform for building, deploying and running machine-learning applications, very similar to Google's Colab. Notebooks can be created from templates, helping you to get started with PyTorch or TensorFlow or with applications like Stable Diffusion. In our experience, Gradient is well-suited for GPU-intensive models, and we like that the web-based environment is persistent.","blip_selector":"gradient","name":"Gradient","display_name":"Gradient","url":"/radar/platforms/gradient","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210067,"quadrant":"platforms","volume_date":"2022-10","description":"IAM Roles Anywhere is a new service from AWS that lets you obtain temporary security credentials in IAM for workloads such as servers, containers and applications that run outside of AWS. We find it particularly useful in hybrid cloud setups where workloads are split across AWS and non-AWS resources. Instead of creating long-lived credentials, with IAM Roles Anywhere, you can now create short-lived credentials to access AWS resources using X.509 certificates. We believe this approach streamlines the access pattern across the hybrid cloud and recommend you check it out.","blip_selector":"iam-roles-anywhere","name":"IAM Roles Anywhere","display_name":"IAM Roles Anywhere","url":"/radar/platforms/iam-roles-anywhere","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202203006,"quadrant":"techniques","volume_date":"2022-10","description":"The prevalence of teams choosing a single-page application (SPA) when they need a website continues. We remain concerned that people aren't properly recognizing SPAs as an architectural style to begin with; instead they're immediately jumping into framework selection. SPAs incur complexity that simply doesn't exist with traditional server-based websites: issues such as search engine optimization, browser history management, web analytics and first page load time all need to be addressed. Proper analysis and consideration of the trade-offs is required to determine if that complexity is warranted for business or user experience reasons. Too often teams are skipping that trade-off analysis, blindly accepting the complexity of SPAs by default even when business needs don't justify it. We still see some developers who aren't aware of an alternative approach because they've spent their entire career in a framework like React. We believe that many websites will benefit from the simplicity of server-side logic, and we're encouraged by techniques like Hotwire that help close the gap on user experience. | We generally avoid putting blips in Hold when we consider that advice too obvious, including blindly following an architectural style without paying attention to trade-offs. However, the sheer prevalence of teams choosing a single-page application (SPA) by default when they need a website has us concerned that people aren't even recognizing SPAs as an architectural style to begin with, instead immediately jumping into framework selection. SPAs incur complexity that simply doesn't exist with traditional server-based websites: search engine optimization, browser history management, web analytics, first page load time, etc. That complexity is often warranted for user experience reasons, and tooling continues to evolve to make those concerns easier to address (although the churn in the React community around state management hints at how hard it can be to get a generally applicable solution). Too often, though, we don't see teams making that trade-off analysis, blindly accepting the complexity of SPAs by default even when the business needs don't justify it. Indeed, we've started to notice that many newer developers aren't even aware of an alternative approach, as they've spent their entire career in a framework like React. We believe that many websites will benefit from the simplicity of server-side logic, and we're encouraged by techniques like Hotwire that help close the gap on user experience.","blip_selector":"spa-by-default","name":"SPA by default","display_name":"SPA by default","url":"/radar/techniques/spa-by-default","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202110089,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"React Query is often described as the missing data-fetching library for React. Fetching, caching, synchronizing and updating server state is a common requirement in many React applications, and although the requirements are well understood, getting the implementation right is notoriously difficult. React Query provides a straightforward solution using hooks. It works hand-in-hand with existing async data-fetching libraries like axios, Fetch and GraphQL since they are built on promises. As an application developer, you simply pass a function that resolves your data and leave everything else to the framework. We like that it works out of the box but still offers a lot of configuration when needed. The developer tools, unfortunately not yet available for React Native, also help developers new to the framework understand how it works. For React Native, you can use a third-party developer tools plugin utilizing Flipper. In our experience, version 3 of React Query brought the stability needed to be used in production with our clients. | React Query is often described as the missing data-fetching library for React. Fetching, caching, synchronizing and updating server state is a common requirement in many React applications, and although the requirements are well-understood, getting the implementation right is notoriously difficult. React Query provides a straightforward solution using hooks. As an application developer you simply pass a function that resolves your data and leave everything else to the framework. We like that it works out-of-the-box but still offers a lot of configuration when needed. The developer tools, unfortunately not yet available for React Native, do help with understanding of how the framework works, which benefits developers new to it. In our experience, version 3 of the framework brought the stability needed to be used in production with our clients.","blip_selector":"react-query","name":"React Query","display_name":"React Query","url":"/radar/languages-and-frameworks/react-query","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210017,"quadrant":"tools","volume_date":"2022-10","description":"Styra Declarative Authorization Service (DAS) is a governance and automation tool for managing Open Policy Agent (OPA) at scale. Built by the creators of OPA, the tool allows us to deploy policies across \"systems,\" including Kubernetes clusters, infrastructure code repositories, namespaces and more. Most importantly, it allows for real-time analysis of decisions made by an OPA agent, along with replayability for debugging and investigating what-if scenarios for policy changes. It also comes with an audit log that can help security teams with historical reporting.","blip_selector":"styra-declarative-authorization-service","name":"Styra Declarative Authorization Service","display_name":"Styra Declarative Authorization Service","url":"/radar/tools/styra-declarative-authorization-service","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202203007,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"When introduced in 2014, Swift didn't come with a package manager. Later, Swift Package Manager was created as an official Apple open-source project, and this solution has continued to develop and mature. Our teams rely increasingly on SwiftPM because most packages can be included through it and the processes for both creators and consumers of packages have been streamlined. In the previous Radar, we recommended trialing, but we now believe it makes sense to select it as the default when starting new projects. For existing projects using tools like CocoaPods or Carthage, it might be worth a quick experiment to gauge the level of effort to migrate and to check whether all dependencies are available. | Some programming languages, especially newer ones, have a package and dependency management solution built in. When it was introduced in 2014, Swift didn't come with a package manager, and so the macOS and iOS developer community simply kept using CocoaPods and Carthage, the third-party solutions that had been created for Objective-C. A couple of years later Swift Package Manager (SwiftPM) was started as an official Apple open-source project, and it then took another few  years before Apple added support for it to Xcode. Even at that point, though, many development teams continued to use CocoaPods and Carthage, mostly because many packages were simply not available via SwiftPM. Now that most packages can be included via SwiftPM and processes have been further streamlined for both creators and consumers of packages, our teams are increasingly relying on SwiftPM.","blip_selector":"swift-package-manager","name":"Swift Package Manager","display_name":"Swift Package Manager","url":"/radar/languages-and-frameworks/swift-package-manager","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210009,"quadrant":"tools","volume_date":"2022-10","description":"Unfortunately, a big part of the world still runs on spreadsheets and will continue to do so. They're the ultimate tool to let anyone build those small custom tools tailored to their exact needs. However, when you want to enhance them with a level of logic that requires \"real\" code, the low-code nature of spreadsheets can then become a constraint. If you're with a company that, like Thoughtworks, uses Google's G-Suite, Clasp enables you to apply at least some Continuous Delivery practices to Apps Script code. You can write the code outside of the Apps Script project, which creates options for testing, source control and build pipelines; it even lets you use TypeScript. Clasp has been around for a while, and you shouldn’t expect a programming environment with all of the usual comforts, but it can greatly improve the experience of using Apps Script.","blip_selector":"clasp","name":"Clasp","display_name":"Clasp","url":"/radar/tools/clasp","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911023,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"In the past, we've cautioned about Node overload, and we're still cautious about the reasons to choose it. However, in scenarios where Node.js is required to build back-end applications, our teams are reporting that NestJS is a suitable option to enable developers to create testable, scalable, loosely coupled and easily maintainable applications in enterprises. NestJS is a TypeScript-first framework that makes the development of Node.js applications safer and less error-prone. NestJS is opinionated and comes with SOLID principles and an Angular-inspired architecture out of the box. | The growth in popularity of Node.js and trends such as Node overload have led to the application of Node.js for developing business applications. We often see problems, such as scalability and maintainability, with large JavaScript-based applications. NestJS is a TypeScript-first framework that makes the development of Node.js applications safer and less error prone. NestJS is opinionated and comes with SOLID principles and an Angular-inspired architecture out of the box. When building Node.js microservices, NestJS is one of the frameworks that our teams commonly use to empower developers to create testable, scalable, loosely coupled and easily maintainable applications. | NestJS is a server-side Node.js framework written in TypeScript. By integrating the rich ecology of the Node.js community, NestJS provides an out-of-the-box application architecture. The mental model to develop NestJS is similar to the server-side version of Angular or the TypeScript version of Spring Boot, so the learning curve for developers is low. NestJS supports protocols such as GraphQL, Websocket and ORM libraries.","blip_selector":"nestjs","name":"NestJS","display_name":"NestJS","url":"/radar/languages-and-frameworks/nestjs","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202010078,"quadrant":"tools","volume_date":"2022-10","description":"Since we first mentioned it in the Radar, k6 has become a go-to tool for performance testing. We continue to be fans of how easy it is to write JavaScript code for tests, but k6 also has a low-code test builder to make playing with the tool even easier. The documentation shows how easy it is to add performance testing to a pipeline across multiple CI/CD tools. Our teams find it easy to integrate visualization tools like Grafana and New Relic, which help them tune both infrastructure and applications. The developer friendliness and ecosystem make k6 a compelling option for investigating a system's behavior under heavy load. | We've had a bit more experience performance testing with k6 since we first covered it in the Radar, and with good results. Our teams have enjoyed the focus on the developer experience and flexibility of the tool. Although it's easy to get started with k6 all on its own, it really shines with its ease of integration into a developer ecosystem. For example, using the Datadog adapter, one team was quickly able to visualize performance in a distributed system and identify significant concerns before releasing the system to production. Another team, with the commercial version of k6, was able to use the Azure pipelines marketplace extension to wire performance tests into their CD pipeline and get Azure DevOps reporting with little effort. Since k6 supports thresholds that allow for automated testing assertions out of the box, it's relatively easy to add a stage to your pipeline that detects performance degradation of new changes, adding a powerful feedback mechanism for developers. | We're quite excited by k6, a relatively new tool in the performance testing ecosystem with a heavy focus on developer experience. The k6 command line runner executes scripts written in JavaScript and allows you to configure the execution time and the number of virtual users. The CLI has several advanced features that let you see the current statistics before the test has finished executing, scale the number of virtual users beyond what was originally defined and even pause and resume a running test. The command line output provides a set of customizable metrics with transformers that let you visualize the results in Datadog and other observability tools. Adding checks to your scripts is an easy way to integrate performance testing into your CI/CD pipeline. For accelerated performance testing, check out the commercial version, k6 Cloud, which provides cloud scaling and additional visualizations.","blip_selector":"k6","name":"k6","display_name":"k6","url":"/radar/tools/k6","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210043,"quadrant":"platforms","volume_date":"2022-10","description":"Modern observability relies on collecting and aggregating an exhaustive set of granular metrics to fully understand, predict and analyze system behavior. But when applied to a cloud native system composed of many redundant and cooperating processes and hosts, the cardinality (or number of unique time series) becomes unwieldy because it grows exponentially with each additional service, container, node, cluster, etc. When dealing with high-cardinality data, we've found that VictoriaMetrics performs well. VictoriaMetrics is particularly useful for operating Kubernetes-hosted microservice architectures, and the VictoriaMetrics operator makes it easy for teams to implement their own monitoring in a self-service way. We also like its componentized architecture and ability to continue collecting metrics even when the central server is unavailable. Although our team has been happy with VictoriaMetrics, this is a rapidly evolving area, and we'd recommend keeping an eye on other high-performance, Prometheus-compatible time series databases such as Cortex or Thanos.","blip_selector":"victoriametrics","name":"VictoriaMetrics","display_name":"VictoriaMetrics","url":"/radar/platforms/victoriametrics","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210028,"quadrant":"techniques","volume_date":"2022-10","description":"Metrics store, sometimes referred to as headless business intelligence (BI), is a layer that decouples metrics definitions from their usage in reports and visualizations. Traditionally, metrics are defined inside the context of BI tools, but this approach leads to duplication and inconsistencies as different teams use them in different contexts. By decoupling the definition in the metrics store, we get clear and consistent reuse across BI reports, visualizations and even embedded analytics. This technique is not new; for example, Airbnb introduced Minerva a year ago. However, we're now seeing considerable traction in the data and analytics ecosystem with more tools supporting metrics stores out of the box.","blip_selector":"metrics-store","name":"Metrics store","display_name":"Metrics store","url":"/radar/techniques/metrics-store","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210082,"quadrant":"techniques","volume_date":"2022-10","description":"Visual regression testing is a useful and powerful tool to have in your toolbox, but it has a significant cost given it's done for the entire page. With the rise of component-based frameworks such as React and Vue, we've also seen the rise of component visual regression testing. This technique strikes a good balance between value and cost to ensure that no undesired visuals have been added to the application. In our experience, component visual regression testing presents fewer false positives and promotes a good architectural style. By using it with tools such as Vite and the webpack feature Hot Module Replacement (HMR), it could be seen as a paradigm shift for applying test-driven development to front-end development.","blip_selector":"component-visual-regression-testing","name":"Component visual regression testing","display_name":"Component visual regression testing","url":"/radar/techniques/component-visual-regression-testing","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210071,"quadrant":"platforms","volume_date":"2022-10","description":"Bun is a new JavaScript runtime, similar to Node.js or Deno. Unlike Node.js or Deno, however, Bun is built using WebKit's JavaScriptCore instead of Chrome's V8 engine. Designed as a drop-in replacement for Node.js, Bun is a single binary (written in Zig) that acts as a bundler, transpiler and package manager for JavaScript and TypeScript applications. Bun is currently in beta, so expect bugs or compatibility issues with a few Node.js libraries. However, it’s been built from the ground up with several optimizations, including fast startup and improved server-side rendering, and we believe it’s worthwhile to assess.","blip_selector":"bun","name":"Bun","display_name":"Bun","url":"/radar/platforms/bun","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210057,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"It's hard to believe, but in 2022, the developer community continues to pump out interesting new frameworks for building web applications. Astro is a recent, open-source, multi-page application framework that renders HTML on the server and minimizes the amount of JavaScript sent over the wire. Astro seems particularly well-suited to content-oriented websites that pull from many different sources. We like the fact that although Astro encourages sending only HTML, it still supports — when appropriate — select active components written in the front-end JavaScript framework of your choice. It does this through its island architecture. Islands are regions of interactivity within a single page where the necessary JavaScript is downloaded only when needed. Astro is relatively new but seems to support a growing ecosystem of developers and code. It's one to watch as it develops.","blip_selector":"astro","name":"Astro","display_name":"Astro","url":"/radar/languages-and-frameworks/astro","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202010103,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Our teams developing in TypeScript are finding io-ts invaluable, especially when interacting with APIs that ultimately result in the creation of objects with specific types. When working with TypeScript, getting data into the bounds of the type system (i.e., from the aforementioned APIs) can lead to run-time errors that can be hard to find and debug. io-ts bridges the gap between compile-time type checking and run-time consumption of external data by providing encode and decode functions. Given the experiences of our teams and the elegance of its approach, we think io-ts is worth adopting. | We've really enjoyed using TypeScript for a while now and love the safety that the strong typing provides. However, getting data into the bounds of the type system — from, for example, a call to a back-end service — can lead to run-time errors. One library that helps solve this problem is io-ts. It bridges the gap between compile-time type-checking and run-time consumption of external data by providing encode and decode functions. It can also be used as a custom type guard. As we gain more experience with io-ts in our work, our initially positive impressions are confirmed, and we still like the elegance of its approach. | We've been really enjoying using TypeScript for a while now and love the safety that the strong typing provides. However, getting data into the bounds of the type system, from say a call to a back-end service, can lead to run-time errors. One library that helps solve this problem is io-ts. It bridges the gap between compile-time type-checking and run-time consumption of external data by providing encode and decode functions. It can also be used as a custom type guard. According to our teams, it's an elegant solution to a rascal of a problem.","blip_selector":"io-ts","name":"io-ts","display_name":"io-ts","url":"/radar/languages-and-frameworks/io-ts","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203002,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Android had several media APIs: Jetpack Media, also known as MediaCompat, Jetpack Media2 and ExoPlayer. Unfortunately, these libraries were developed independently, with different goals but overlapping functionality. Android developers not only had to choose which library to use, they also had to contend with writing adaptors or other connecting code when features from multiple APIs were needed. Jetpack Media3 is an API that takes common areas of functionality from the existing APIs — including UI, playback and media session handling — and combines them into a merged and refined API. The player interface from ExoPlayer has also been updated, enhanced and streamlined to act as the common player interface for Media3. After an early access phase, Media3 is now in beta. Although its first release is forthcoming, we've already had positive experiences using it in apps. | Android today has several media APIs: Jetpack Media, also known as MediaCompat, Jetpack Media2 and ExoPlayer. Unfortunately, these libraries were developed independently, with different goals but overlapping functionality. Android developers not only had to choose which library to use, they also had to contend with writing adaptors or other connecting code when features from multiple APIs were needed. Jetpack Media3 is an effort, currently in early access, to create a new API that takes common areas of functionality from the existing APIs — including UI, playback and media session handling — combining them into a merged and refined API. The player interface from ExoPlayer has also been updated, enhanced and streamlined to act as the common player interface for Media3.","blip_selector":"jetpack-media3","name":"Jetpack Media3","display_name":"Jetpack Media3","url":"/radar/languages-and-frameworks/jetpack-media3","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203036,"quadrant":"tools","volume_date":"2022-10","description":"We continue to hear enthusiastic reports about Excalidraw from our teams, but our previous caveat about security remains in place. Excalidraw is a simple yet powerful online drawing tool. Sometimes teams just need a quick picture instead of a formal diagram; for remote teams, Excalidraw provides a quick way to create and share diagrams. Our teams also like the \"lo-fi\" look of the diagrams it can produce, which is reminiscent of the whiteboard diagrams they would have produced when co-located. Regarding security, at the time of writing, anyone who has the link can see your diagrams; note, though, that the paid version of Excalidraw provides further authentication and options to run a server locally do exist. | Excalidraw is a simple but powerful online drawing tool that our teams enjoy using. Sometimes teams just need a quick picture instead of a formal diagram, for remote teams Excalidraw provides a quick way to create and share diagrams. Our teams also like the \"lo-fi\" look of the diagrams it can produce, which is reminiscent of the whiteboard diagrams they would have produced when co-located. One caveat: you need to pay attention to the default security — at the time of writing, anyone who has the link can see the diagram. A paid-for version provides further authentication.","blip_selector":"excalidraw","name":"Excalidraw","display_name":"Excalidraw","url":"/radar/tools/excalidraw","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210059,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"We're hearing that our Kotlin-based teams are seeking alternatives to Java frameworks such as GSON when handling JSON. Although it's been around for some time, Moshi has now emerged as a preferred framework for many of these teams. It's easy to migrate from GSON and Moshi provides native support for Kotlin non-nullable types and default parameters. Moshi makes working with JSON faster and easier. If you're currently using a Java framework from within Kotlin to handle JSON, we recommend giving Moshi a try.","blip_selector":"moshi","name":"Moshi","display_name":"Moshi","url":"/radar/languages-and-frameworks/moshi","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210031,"quadrant":"techniques","volume_date":"2022-10","description":"In a centralized application, the data on the server is the single source of truth — any modification to the data must go through the server. Local data is subordinate to the server version. This seems like a natural and inevitable choice to enable collaboration among multiple users of the software. Local-first application, or local-first software, is a set of principles that enables both collaboration and local data ownership. It prioritizes the use of local storage and local networks over servers in remote data centers or the cloud. Techniques like conflict-free replicated data types (CRDTs) and peer-to-peer (P2P) networks have the potential to be a foundational technology for realizing local-first software.","blip_selector":"local-first-application","name":"Local-first application","display_name":"Local-first application","url":"/radar/techniques/local-first-application","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005111,"quadrant":"platforms","volume_date":"2022-10","description":"For several years now, the Linux kernel has included the extended Berkeley Packet Filter (eBPF), a virtual machine that provides the ability to attach filters to particular sockets. But eBPF goes far beyond packet filtering and allows custom scripts to be triggered at various points within the kernel with very little overhead. By allowing you to run sandboxed programs within the operating system kernel, application developers can run eBPF programs to add additional capabilities to the operating system at run time. Some of our projects require troubleshooting and profiling at the system call level, and our teams found that tools like bcc and bpftrace have made their jobs easier. Observability and network infrastructure also benefit from eBPF — for example, the Cilium project can implement traffic load balancing and observability without sidecar overhead in Kubernetes, and Hubble provides further security and traffic observability on top of it. The Falco project uses eBPF for security monitoring, and the Katran project uses eBPF to build more efficient L4 load balancing. The eBPF community is growing rapidly, and we're seeing more and more synergy with the field of observability. | For several years now, the Linux kernel has included the extended Berkeley Packet Filter (eBPF), a virtual machine that provides the ability to attach filters to particular sockets. But eBPF goes far beyond packet filtering and allows custom scripts to be triggered at various points within the kernel with very little overhead. Although this technology isn't new, it's now coming into its own with the increasing use of microservices deployed as orchestrated containers. Kubernetes and service mesh technology such as Istio are commonly used, and they employ sidecars to implement control functionality. With new tools — Bumblebee in particular makes building, running and distributing eBPF programs much easier — eBPF can be seen as an alternative to the traditional sidecar. A maintainer of Cilium, a tool in this space, has even proclaimed the demise of the sidecar. An approach based on eBPF reduces some overhead in performance and operation that comes with sidecars, but it doesn't support common features such as SSL termination. | For several years now, the Linux kernel has included the extended Berkeley Packet Filter (eBPF) virtual machine and provided the ability to attach eBPF filters to particular sockets. But extended BPF goes far beyond packet filtering and allows custom scripts to be triggered at various points within the kernel with very little overhead. Although this technology isn't new, it's now coming into its own with the increasing use of microservices deployed as orchestrated containers. Service-to-service communications can be complex in these systems, making it difficult to correlate latency or performance issues back to an API call. We're now seeing tools released with prewritten eBPF scripts for collecting and visualizing packet traffic or reporting on CPU utilization. With the rise of Kubernetes, we’re seeing a new generation of security enforcement and instrumentation based on eBPF scripts that help tame the complexity of a large microservices deployment.","blip_selector":"ebpf","name":"eBPF","display_name":"eBPF","url":"/radar/platforms/ebpf","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203063,"quadrant":"techniques","volume_date":"2022-10","description":"As software continues to grow in complexity, the threat vector of software dependencies becomes increasingly challenging to guard against. Supply chain Levels for Software Artifacts, or SLSA (pronounced \"salsa\"), is a consortium-curated set of guidance for organizations to protect against supply chain attacks, evolved from internal guidance Google has been using for years. We appreciate that SLSA doesn't promise a \"silver bullet,\" tools-only approach to securing the supply chain, but it does provide a checklist of concrete threats and practices along a maturity model. The threat model is easy to follow with real-world examples of attacks, and the requirements provide guidance to help organizations prioritize actions based on levels of increasing robustness to improve their supply chain security posture. Since we first mentioned it in the Radar, SLSA has added more detail around software attestations with examples to track concerns like build provenance. Our teams have found SLSA to strike a nice balance between implementation guidance and higher-level awareness around supply chain threats. | As software continues to grow in complexity, the threat vector of software dependencies becomes increasingly challenging to guard against. The recent Log4J vulnerability showed how difficult it can be to even know those dependencies — many companies who didn't use Log4J directly were unknowingly vulnerable simply because other software in their ecosystem relied on it. Supply chain Levels for Software Artifacts, or SLSA (pronounced \"salsa\"), is a consortium-curated set of guidance for organizations to protect against supply chain attacks, evolved from internal guidance Google has been using for years. We appreciate that SLSA doesn't promise a \"silver bullet,\" tools-only approach to securing the supply chain but instead provides a checklist of concrete threats and practices along a maturity model. The threat model is easy to follow with real-world examples of attacks, and the requirements provide guidance to help organizations prioritize actions based on levels of increasing robustness to improve their supply chain security posture. We think SLSA provides applicable advice and look forward to more organizations learning from it.","blip_selector":"slsa","name":"SLSA","display_name":"SLSA","url":"/radar/techniques/slsa","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210026,"quadrant":"tools","volume_date":"2022-10","description":"Multi-team account management is a challenge in AWS, especially in setup and governance; AWS Control Tower is an attempt to address this challenge. Our team has reported good results using it to manage accounts and access control for multiple teams in the organization through a single, centralized place.","blip_selector":"aws-control-tower","name":"AWS Control Tower","display_name":"AWS Control Tower","url":"/radar/tools/aws-control-tower","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210075,"quadrant":"platforms","volume_date":"2022-10","description":"Many of our teams have successfully used AWS Database Migration Service (DMS) to migrate data to and from AWS. In one of our Digital Transformation engagements, we achieved nearly zero downtime cut-over to the new system as we migrated data from Microsoft SQL Server to an AWS Relational Database Service (RDS) PostgreSQL instance. Such transformations involve many moving parts that require planning and coordination across multidisciplinary teams, but for data migration we're quite happy with DMS. It automatically manages the deployment, management and monitoring of all required resources. Over the years DMS has matured to support several source and target databases, and we continue to like it.","blip_selector":"aws-database-migration-service","name":"AWS Database Migration Service","display_name":"AWS Database Migration Service","url":"/radar/platforms/aws-database-migration-service","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210078,"quadrant":"platforms","volume_date":"2022-10","description":"In previous editions, we’ve recommended assessing bounded low-code platforms as a method for applying low-code solutions to specific use cases in very limited domains. We’ve seen some traction in this space, specifically with Retool, a low-code platform that our teams use to build solutions for internal users, predominantly to query and visualize data. It allows them to produce non-business-critical read-only solutions faster. The main reported benefits of Retool are its UI components and its ability to be integrated quickly and easily with common data sources.","blip_selector":"retool","name":"Retool","display_name":"Retool","url":"/radar/platforms/retool","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210040,"quadrant":"platforms","volume_date":"2022-10","description":"Undoubtedly, data discoverability has become a very important focal point for companies since it is an enabler for data to be shared and used efficiently by different people. We’ve included platforms such as DataHub and Collibra in previous editions of the Radar. However, our teams are constantly assessing options in this space and have recently shown interest in OpenMetadata, a platform dedicated to metadata management by using open standards. Our teams like this open-source platform because it improves the development experience due to its simple architecture, easy deployment with a focus on automation and strong focus on data discoverability.","blip_selector":"openmetadata","name":"OpenMetadata","display_name":"OpenMetadata","url":"/radar/platforms/openmetadata","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202210047,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Conflict-free replicated data type (CRDT) algorithms are proven to be able to automatically distribute and merge changes among peers without conflicts. But in practice, even for small enough data, these algorithms usually require a significant amount of memory to trace all the changes made by different peers, thus making them impractical. Yjs is a carefully optimized CRDT implementation that keeps memory consumption at a reasonable level for large data sets and millions of modifications. It also provides bindings for popular text editors, which greatly reduce the cost of building collaborative tools.","blip_selector":"yjs","name":"Yjs","display_name":"Yjs","url":"/radar/languages-and-frameworks/yjs","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210063,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"When looking at reducing the carbon footprint of an application — the carbon dioxide emissions caused indirectly by running the software — attention is usually directed at making the software more efficient. The thinking is clear: more efficient software needs less electricity and fewer servers, reducing the emissions from electricity generation and manufacturing of the servers. An additional strategy is to make the application carbon aware. This is because the same workload does not always have the same carbon footprint. For example, when run in a data center in a cooler climate, less power for air conditioning is needed; or, when run at a time when more renewable energy is available (more sunshine, stronger winds), less electricity from carbon-based  sources is required. With the Carbon Aware SDK, software engineers can query data sources to discover less carbon-intensive options for a given workload and then move it to a different location or run it at a different time. This makes sense for large workloads that are neither time nor latency sensitive, such as training a machine-learning model. Although the SDK and available data sources are not very comprehensive yet, we believe it's time to start looking at how we can make our systems carbon aware.","blip_selector":"carbon-aware-sdk","name":"Carbon Aware SDK","display_name":"Carbon Aware SDK","url":"/radar/languages-and-frameworks/carbon-aware-sdk","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210055,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"BentoML is a python-first framework for serving machine-learning models in production at scale. The models it provides are agnostic of their environment; all model artifacts, source code and dependencies are encapsulated in a self-contained format called Bento. It's like having your model \"as a service.\" Think of BentoML as the Docker for ML models: It generates VM images with pre-programmed APIs ready for deployment and includes features that make it easy to test these images. BentoML can help speed up the initial development effort by easing the start of projects which is why we included it in Assess.","blip_selector":"bentoml","name":"BentoML","display_name":"BentoML","url":"/radar/languages-and-frameworks/bentoml","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210005,"quadrant":"tools","volume_date":"2022-10","description":"Teller is an open-source universal secret manager for developers that ensures the correct environment variables are set when starting an application. However, it's not a vault itself — it's a CLI tool that connects to a variety of sources, ranging from cloud secrets providers to third-party solutions like HashiCorp Vault to local environment files. Teller has additional functionality to scan for vault-kept secrets in your code, to redact secrets from logs, to detect drift between secrets providers and to sync between them. Given the sensitivity of accessing secrets, we can't emphasize enough the need to secure the supply chain for open-source dependencies, but we appreciate how easy the CLI is to use in local development environments, CI/CD pipelines and deployment automation.","blip_selector":"teller","name":"Teller","display_name":"Teller","url":"/radar/tools/teller","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202210084,"quadrant":"techniques","volume_date":"2022-10","description":"Although path-to-production mapping has been a near-universal practice at Thoughtworks since codifying Continuous Delivery, we often come across organizations unfamiliar with the practice. The activity is most often done in a workshop with a cross-functional group of people —  that includes everyone involved in designing, developing, releasing and operating the software — around a shared whiteboard (or virtual equivalent). First, the steps in the process are listed in order, from the developer workstation all the way to production. Then, a facilitated session is used to capture further information and pain points. The most common technique we see is based on value-stream mapping, although plenty of process map variants are equally valuable. The activity is often eye-opening for many of the participants, as they identify delays, risks and inconsistencies and continue to use the visual representation for the continuous improvement of the build and deploy process. We consider this technique so foundational that we were surprised to discover we hadn't blipped it before.","blip_selector":"path-to-production-mapping","name":"Path-to-production mapping","display_name":"Path-to-production mapping","url":"/radar/techniques/path-to-production-mapping","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210033,"quadrant":"techniques","volume_date":"2022-10","description":"Observability practices have shifted the conversation from monitoring for well-understood problems to helping troubleshoot unknown problems in distributed systems. We've seen success taking that perspective outside of the traditional production environment by applying observability for CI/CD pipelines to help optimize testing and deployment bottlenecks. Complex pipelines create developer friction when they run too slow or suffer from nondeterminism, reducing important feedback loops and hindering developer effectiveness. Additionally, their role as critical deployment infrastructure creates stress points during periods of rapid deployments, as happened to several organizations responding to the recent log4shell vulnerability. The concept of traces translates nicely to pipelines: instead of capturing the cascade of service calls, child spans capture information about each stage of the build. The same waterfall charts used to analyze a call flow in a distributed architecture can also be effective in helping us to identify bottlenecks in pipelines, even complex ones with fan-in and fan-out. This enables far more focused optimization efforts. While the technique should work with any tracing tool, Honeycomb supports a tool called buildevents that helps capture pipeline trace information. An alternative approach of capturing information already exposed by CI/CD platforms, taken by the open-source buildviz (built and maintained by a Thoughtworker), allows for a similar investigation without changing the step configurations themselves.","blip_selector":"observability-for-ci-cd-pipelines","name":"Observability for CI/CD pipelines","display_name":"Observability for CI/CD pipelines","url":"/radar/techniques/observability-for-ci-cd-pipelines","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203029,"quadrant":"techniques","volume_date":"2022-10","description":"Server-driven UI continues to be a hot topic of discussion in mobile circles because it offers the potential for developers to take advantage of faster change cycles without falling foul of an app store's policies around revalidation of the mobile app itself. Server-driven UI separates the rendering into a generic container in the mobile app while the structure and data for each view is provided by the server. This means that changes that once required a round trip to an app store can now be accomplished via simple changes to the responses the server sends. While some very large mobile app teams have had great success with this technique, it also requires a substantial investment in building and maintaining a complex proprietary framework. Such an investment requires a compelling business case. Until the case is made, it might be best to proceed with caution; indeed, we've experienced some horrendous, overly configurable messes that didn't actually deliver on the promised benefits. But with the backing of behemoths such as Airbnb and Lyft, we may very well see some useful frameworks emerge that help tame the complexity. Watch this space. | When putting together a new volume of the Radar, we're often overcome by a sense of déjà vu, and the technique of server-driven UI sparks a particularly strong case with the advent of frameworks that allow mobile developers to take advantage of faster change cycles while not falling foul of an app store's policies around revalidation of the mobile app itself. We've blipped about this before from the perspective of enabling mobile development to scale across teams. Server-driven UI separates the rendering into a generic container in the mobile app while the structure and data for each view is provided by the server. This means that changes that once required a round trip to an app store can now be accomplished via simple changes to the responses the server sends. Note, we're not recommending this approach for all UI development, indeed we've experienced some horrendous, overly configurable messes, but with the backing of behemoths such as AirBnB and Lyft, we suspect it's not only us at Thoughtworks getting tired of everything being done client side. Watch this space.","blip_selector":"server-driven-ui","name":"Server-driven UI","display_name":"Server-driven UI","url":"/radar/techniques/server-driven-ui","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":871,"quadrant":"techniques","volume_date":"2022-10","description":"We continue to recommend that teams carry out threat modeling — a set of techniques to help you identify and classify potential threats during the development process — but we want to emphasize that this is not a one-off activity only done at the start of projects; teams need to avoid the security sandwich. This is because throughout the lifetime of any software, new threats will emerge and existing ones will continue to evolve thanks to external events and ongoing changes to requirements and architecture. This means that threat modeling needs to be repeated periodically — the frequency of repetition will depend on the circumstances and will need to consider factors such as the cost of running the exercise and the potential risk to the business. When used in conjunction with other techniques, such as establishing cross-functional security requirements to address common risks in the project's technologies and using automated security scanners, threat modeling can be a powerful asset. | With the number of high-profile security breaches in the past months, software development teams no longer need convincing that they must place an emphasis on writing secure software and dealing with their users' data in a responsible way. The teams face a steep learning curve, though, and the vast number of potential threats—ranging from organized crime and government spying to teenagers who attack systems \"for the lulz\"—can be overwhelming. Threat modeling provides a set of techniques that help you identify and classify potential threats early in the development process. It is important to understand that it is only part of a strategy to stay ahead of threats. When used in conjunction with techniques such as establishing cross-functional security requirements to address common risks in the technologies a project uses and using automated security scanners, threat modeling can be a powerful asset. | With the number of high-profile security breaches in the past months, software development teams no longer need convincing that they must place an emphasis on writing secure software and dealing with their users’ data in a responsible way. The teams face a steep learning curve, though, and the vast number of potential threats - ranging from organized crime and government spying to teenagers who attack systems 'for the lulz' can be overwhelming. Threat modeling provides a set of techniques, mostly from a defensive perspective, that help you understand and classify potential threats. Turned into 'evil-user stories', threat models can give a team a manageable and effective approach to making their systems more secure. | At this point the vast majority of development teams are aware of the importance of writing secure software and dealing with their users’ data in a responsible way. They do face a steep learning curve and a vast number of potential threats, ranging from organized crime and government spying to teenagers who attack systems 'for the lulz'. Threat modelingis a set of techniques, mostly from a defensive perspective, that help understand and classify potential threats. When turned into 'evil user stories' this can give a team a manageable and effective approach to making their systems more secure.","blip_selector":"threat-modeling","name":"Threat modeling","display_name":"Threat modeling","url":"/radar/techniques/threat-modeling","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210066,"quadrant":"platforms","volume_date":"2022-10","description":"Dragonfly is a new in-memory data store with compatible Redis and Memcached APIs. It leverages the new Linux-specific io_uring API for I/O and implements novel algorithms and data structures on top of a multithreaded, shared-nothing architecture. Because of these clever choices in implementation, Dragonfly achieves impressive results in performance. Although Redis continues to be our default choice for in-memory data store solutions, we do think Dragonfly is an interesting choice to assess.","blip_selector":"dragonfly","name":"Dragonfly","display_name":"Dragonfly","url":"/radar/platforms/dragonfly","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210052,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"OpenAI's DALL·E caught everyone's attention with its ability to create images from text prompts. Now, Stable Diffusion offers the same capability but, critically, it's open source. Anyone with access to a powerful graphics card can experiment with the model, and anyone with sufficient compute resources can recreate the model themselves. The results are astounding but also raise significant questions. For example, the model is trained on image-text pairs obtained via a broad scrape of the internet and therefore will reflect societal biases, which means it could possibly produce content that is illegal, upsetting, or at the very least undesirable. Stable Diffusion now includes an AI-based safety classifier; however, given its open-source nature, people can disable the classifier. Finally, artists have noted that with the right prompts the model is adept at mimicking their artistic style. This raises questions about the ethical and legal implications of an AI capable of imitating an artist.","blip_selector":"stable-diffusion","name":"Stable Diffusion","display_name":"Stable Diffusion","url":"/radar/languages-and-frameworks/stable-diffusion","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210051,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Soketi is an open-source WebSockets server. If your application is compatible with the Pusher protocol, you can plug Soketi in directly as it fully implements the Pusher Protocol v7. We find the beta support for Cloudflare Workers particularly interesting because it opens the door to using WebSockets at the network edge.","blip_selector":"soketi","name":"Soketi","display_name":"Soketi","url":"/radar/languages-and-frameworks/soketi","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911053,"quadrant":"techniques","volume_date":"2022-10","description":"We're now seeing client projects that use federated machine learning (ML). Traditionally, ML model training has required data to be placed in a centralized location where the relevant training algorithm can be run. From a privacy point of view, this is problematic, especially when the training data contains sensitive or personally identifiable information; users might be reluctant to share data or local data protection legislation may prevent us from moving data to a central location. Federated ML is a decentralized technique for training on a large and diverse set of data that allows the data to remain remote — for example, on a user's device. Network bandwidth and the computational limitations of devices still present significant technical challenges, but we like the way federated ML leaves users in control of their own personal information. | Model training generally requires collecting data from its source and transporting it to a centralized location where the model training algorithm runs. This becomes particularly problematic when the training data consists of personally identifiable information. We're encouraged by the emergence of federated learning as a privacy-preserving method for training on a large diverse set of data relating to individuals. Federated learning techniques allow the data to remain on the users' device, under their control, yet contribute to an aggregate corpus of training data. In one such technique, each user device updates a model independently; then the model parameters, rather than the data itself, are combined into a centralized view. Network bandwidth and device computational limitations present some significant technical challenges, but we like the way federated learning leaves users in control of their own personal information.","blip_selector":"federated-machine-learning","name":"Federated machine learning","display_name":"Federated machine learning","url":"/radar/techniques/federated-machine-learning","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210012,"quadrant":"tools","volume_date":"2022-10","description":"We're always looking for ways to remove small frictions from pair programming, which is why we're excited by git-together, a tool written in Rust that simplifies git commit attribution during pairing. By aliasing git-together as git, the tool allows you to add extensions to git config that capture committer information, aliasing each committer by their initials. Changing pairs (or switching to soloing or mob programming) requires you to run git with, followed by the initials of the pair (for example: git with bb cc), allowing you to resume your regular git workflow afterward. Every time you commit, git-together will rotate through the pair as the official author that git stores, and it will automatically add any other authors to the bottom of the commit message. The configuration can be checked in with the repo, allowing git-together to work automatically after cloning a repo.","blip_selector":"git-together","name":"git-together","display_name":"git-together","url":"/radar/tools/git-together","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210068,"quadrant":"platforms","volume_date":"2022-10","description":"OrioleDB is a new storage engine for PostgreSQL. Our teams use PostgreSQL a lot, but its storage engine was originally designed for hard drives. Although there are several options to tune for modern hardware, it can be difficult and cumbersome to achieve optimal results. OrioleDB addresses these challenges by implementing a cloud-native storage engine with explicit support for solid-state drives (SSDs) and nonvolatile random-access memory (NVRAM). To try the new engine, first install the enhancement patches to the current table access methods and then install OrioleDB as a PostgreSQL extension. We believe OrioleDB has great potential to address several long-pending issues in PostgreSQL, and we encourage you to carefully assess it.","blip_selector":"orioledb","name":"OrioleDB","display_name":"OrioleDB","url":"/radar/platforms/orioledb","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210034,"quadrant":"techniques","volume_date":"2022-10","description":"We've been writing about developer platforms and how to build them in almost every edition of the Radar since 2017. In the meantime, the Team Topologies book has also done a great job of describing the ideal of a platform that supports developers with \"self-service APIs, tools, services and knowledge.\" However, we often see teams shooting for too much of that platform vision too fast. Instead, building an incremental developer platform is key.\n\nTeam Topologies recommends to always strive for what they call the \"Thinnest Viable Platform\" necessary at any given stage, where the first version could even be just a set of documentation on a wiki. The next increment could increase the service level by providing templates or allowing teams to create pull requests. Further increments could then introduce self-service APIs, but only if valuable. In short, even though we've cautioned against fully ticket-driven platform operating models, going from zero to self-service is the other extreme. Pace yourself, treat your platform as a product and build it up incrementally.","blip_selector":"incremental-developer-platform","name":"Incremental developer platform","display_name":"Incremental developer platform","url":"/radar/techniques/incremental-developer-platform","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110076,"quadrant":"techniques","volume_date":"2022-10","description":"With continued pressure to keep systems secure and no reduction in the general threat landscape, a machine-readable Software Bill of Materials (SBOM) may help teams stay on top of security problems in the libraries that they rely on. Since the original Executive Order was published, the industry has gained clarity and understanding of what an SBOM is and how to create one; the National Institute of Standards and Technology (NIST), for example, now has more specific advice on how to comply with the order. We've had production experience using SBOMs on projects ranging from small companies to large multinationals and even government departments, and we're convinced they provide a benefit. More organizations and governments should consider requiring SBOMs for the software they use. The technique will be strengthened by the new tools that continue to emerge, such as the Firebase Android BOM that automatically aligns an application's library dependencies to those listed in the BOM. | With continued pressure to keep systems secure and no reduction in the general threat landscape, a machine-readable Software Bill of Materials (SBOM) may help teams stay on top of security problems in the libraries that they rely on. The recent Log4Shell zero-day remote exploit was critical and widespread, and if teams had had an SBOM ready, it could have been scanned for and fixed quickly. We've now had production experience using SBOMs on projects ranging from small companies to large multinationals and even government departments, and we're convinced they provide a benefit. Tools such as Syft make it easy to use an SBOM for vulnerability detection. | In May 2021, the U.S. White House published its Executive Order on Improving the Nation's Cybersecurity. The document puts forward several technical mandates that relate to items we've featured in past Radars, such as zero trust architecture and automated compliance scanning using security policy as code. Much of the document is devoted to improving the security of the software supply chain. One item in particular that caught our attention was the requirement that government software should contain a machine-readable Software Bill of Materials (SBOM), defined as \"a formal record containing the details and supply chain relationships of various components used in building software.\" In other words, it should detail not just the components shipped but also the tools and frameworks used to deliver the software. This order has the potential to usher in a new era of transparency and openness in software development. This will undoubtedly have an impact on those of us who produce software for a living. Many, if not all software products produced today contain open-source components or employ them in the build process. Often, the consumer has no way of knowing which version of which package might have an impact on the security of their product. Instead they must rely on the security alerts and patches provided by the retail vendor. This executive order will ensure that an explicit description of all components is made available to consumers, empowering them to implement their own security controls. And since the SBOM is machine-readable, those controls can be automated. We sense that this move also represents a shift toward embracing open-source software and practically addressing both the security risks and benefits that it provides.","blip_selector":"software-bill-of-materials","name":"Software Bill of Materials","display_name":"Software Bill of Materials","url":"/radar/techniques/software-bill-of-materials","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210019,"quadrant":"tools","volume_date":"2022-10","description":"Most of today's CI/CD pipeline tools and platforms are built on containers as runtimes. Many of our teams are using Kaniko to build container images from within those container-based pipelines. This comes as part of a trend away from Docker as the de facto standard for container runtimes. With Kaniko, you can build your images without using a Docker daemon. This helps avoid the security issue of Docker's \"privileged\" mode, which would be necessary for any \"Docker-in-Docker\" activity. Moreover, you don't have to assume that your pipeline has access to a Docker daemon in the first place, which cannot be taken for granted anymore and often requires extra configuration.","blip_selector":"kaniko","name":"Kaniko","display_name":"Kaniko","url":"/radar/tools/kaniko","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005013,"quadrant":"techniques","volume_date":"2022-10","description":"Since introducing them in the Radar in 2016, we've seen widespread adoption of micro frontends for web UIs. Recently, however, we've seen projects extend this architectural style to include micro frontends for mobile apps as well. When an app becomes sufficiently large and complex, it becomes necessary to distribute the development over multiple teams. This presents a number of challenges around team autonomy, repository structures and integration frameworks. In the past we've mentioned Atlas and BeeHive, but these frameworks failed to gain traction and are no longer in active development. More recent approaches include Tuist or the Swift Package Manager for integrating the work of multiple teams into a single app. But in our experience, teams often end up implementing their own framework for integration. While we definitely see a need for modularity in scaling up mobile development teams, the case for micro frontends is less certain. This is because while micro frontends imply a direct correspondence between teams and pages or components, this structure could end up blurring responsibilities for business domain contexts, thereby increasing team cognitive load. Our advice is to follow the basics of good, clean application design, embrace modularity when scaling up to multiple teams and adopt a micro frontend architecture only when the modules and the business domain are strongly aligned. | Since introducing them in the Radar in 2016, we've seen widespread adoption of micro frontends for web UIs. Recently, however, we've seen projects extend this architectural style to include micro frontends for mobile applications as well. When the application becomes sufficiently large and complex, it becomes necessary to distribute the development over multiple teams. This presents the challenge of maintaining team autonomy while integrating their work into a single app. Some teams write their own frameworks to enable this development style, and in the past we've mentioned Atlas and Beehive as possible ways to simplify the problem of integrating multiteam app development. More recently, we've seen teams using React Native to accomplish the same thing. Each React Native micro frontend is kept in its own repository where it can be built, tested and deployed separately. The team responsible for the overall application can then aggregate those micro frontends built by different teams into a single released app. | Since introducing it in the Radar in 2016, we've seen widespread adoption of micro frontends for web UIs. Recently, however, we've seen projects extend this architectural style to include micro frontends for mobile applications as well. When the application becomes sufficiently large and complex, it becomes necessary to distribute the development over multiple teams. This presents the challenge of maintaining team autonomy while integrating their work into a single app. Although we've seen teams writing their own frameworks to enable this development style, existing modularization frameworks such as Atlas and Beehive can also simplify the problem of integrating multiteam app development.","blip_selector":"micro-frontends-for-mobile","name":"Micro frontends for mobile","display_name":"Micro frontends for mobile","url":"/radar/techniques/micro-frontends-for-mobile","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210060,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"As Storybook grew in popularity, it became more and more of a behemoth. If all you really care about is isolating and testing your React UI components, then Ladle is the alternative. Ladle supports most of the Storybook API (MDX files are not supported yet) and can be used as a drop-in replacement. It is lightweight and has better integration with Vite. It also provides simple and clean APIs that can be easily integrated with other testing frameworks.","blip_selector":"ladle","name":"Ladle","display_name":"Ladle","url":"/radar/languages-and-frameworks/ladle","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203070,"quadrant":"techniques","volume_date":"2022-10","description":"We continue to be excited by the TinyML technique and the ability to create machine learning (ML) models designed to run on low-powered and mobile devices. Until recently, executing an ML model was seen as computationally expensive and, in some cases, required special-purpose hardware. While creating the models still broadly sits within this classification, they can now be created in a way that allows them to be run on small, low-cost and low-power consumption devices. If you've been considering using ML but thought it unrealistic because of compute or network constraints, then this technique is worth assessing. | Until recently, executing a machine-learning (ML) model was seen as computationally expensive and in some cases required special-purpose hardware. While creating the models still broadly sits within this classification, they can be created in a way that allows them to be run on small, low-cost and low-power consumption devices. This technique, called TinyML, has opened up the possibility of running ML models in situations many might assume infeasible. For example, on battery-powered devices, or in disconnected environments with limited or patchy connectivity, the model can be run locally without prohibitive cost. If you've been considering using ML but thought it unrealistic because of compute or network constraints, then this technique is worth assessing.","blip_selector":"tinyml","name":"TinyML","display_name":"TinyML","url":"/radar/techniques/tinyml","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203050,"quadrant":"techniques","volume_date":"2022-10","description":"How do you approach writing good code? How do you judge if you've written good code? As software developers, we're always looking for catchy rules, principles and patterns that we can use to share a language and values with each other when it comes to writing simple, easy-to-change code.\n\nDaniel Terhorst-North has recently made a new attempt at creating such a checklist for good code. He argues that instead of sticking to a set of rules like SOLID, using a set of properties to aim for is more generally applicable. He came up with what he calls the CUPID properties to describe what we should strive for to achieve \"joyful\" code: Code should be composable, follow the Unix philosophy and be predictable, idiomatic and domain based. | How do you approach writing good code? How do you judge if you've written good code? As software developers, we're always looking for catchy rules, principles and patterns that we can use to share a language and values with each other when it comes to writing simple, easy-to-change code.\n\nDaniel Terhorst-North has recently made a new attempt at creating such a checklist for good code. He argues that instead of sticking to a set of rules like SOLID, using a set of properties to aim for is more generally applicable. He came up with what he calls the CUPID properties to describe what we should strive for to achieve \"joyful\" code: Code should be composable, follow the Unix philosophy and be predictable, idiomatic and domain based.","blip_selector":"cupid","name":"CUPID","display_name":"CUPID","url":"/radar/techniques/cupid","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104055,"quadrant":"tools","volume_date":"2022-10","description":"Spectral is a JSON/YAML linter with an emphasis on OpenAPI and AsyncAPI specifications. It ships with a comprehensive set of out-of-the-box rules for these specs that can save developers headaches when designing and implementing APIs or event-driven collaboration. These rules check for proper API parameter specifications or the existence of a license statement in the spec, among other things. The CLI makes it easy to incorporate Spectral into both local development and CI/CD pipelines, and the JavaScript API supports more advanced use cases. The GitHub site links to publicly available real-world rule sets from companies like Adidas, giving teams a head start on adopting their own linting rules. | One of the patterns we've seen repeat itself in this publication is that static error- and style-checking tools emerge quickly after a new language gains popularity. These tools are generically known as linters — after the classic and beloved Unix utility lint, which statically analyzes C code. We like these tools because they catch errors early, before code even gets compiled. The latest instance of this pattern is Spectral, a linter for YAML and JSON. Although Spectral is a generic tool for these formats, its main target is OpenAPI (the evolution of Swagger) and AsyncAPI. Spectral ships with a comprehensive set of out-of-the-box rules for these specs that can save developers headaches when designing and implementing APIs or event-driven collaboration. These rules check for proper API parameter specifications or the existence of a license statement in the spec, among other things. While this tool is a welcome addition to the API development workflow, it does raise the question of whether a non-executable specification should be so complex as to require an error-checking technique designed for programming languages. Perhaps developers should be writing code instead of specs?","blip_selector":"spectral","name":"Spectral","display_name":"Spectral","url":"/radar/tools/spectral","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210064,"quadrant":"platforms","volume_date":"2022-10","description":"Keptn is a control plane for delivery and operations that relies on CloudEvents for instrumentation. Like one of the techniques we mentioned in observability for CI/CD pipelines, Keptn visualizes its orchestration as traces. The declarative definition of the delivery pipeline aims to separate SRE intentions from the underlying implementation, relying on other observability, pipeline and deployment tooling to respond to the appropriate events. We're particularly excited by the idea of adding service-level objective (SLO) verifications as architectural fitness functions to CI/CD pipelines: Keptn lets you define service-level indicators (SLIs) as key-value pairs, with the value representing the query to your observability infrastructure. It will then evaluate the result against the defined SLOs as a quality gate. Keptn takes the same approach to automated operations, allowing a declarative definition that specifies the intent of scaling a ReplicaSet in response to a degradation of average response time, for example. Created by Dynatrace, Keptn also integrates with Prometheus and Datadog.","blip_selector":"keptn","name":"Keptn","display_name":"Keptn","url":"/radar/platforms/keptn","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210053,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"There is certainly no shortage of frameworks to build web applications in JavaScript/TypeScript. We've featured many of them in the Radar, but what sets Aleph.js apart in this crowded field is that it's built to run on Deno, the new server-side run time created by the original developer of Node. This puts Aleph.js on a modern foundation that addresses several shortcomings and problems with Node. Aleph.js is still new — it’s approaching the 1.0 release at the time of writing — but it already offers a solid developer experience, including hot module replacement. With Deno now way past its 1.0 release, this is a modern choice for projects that can take the risk.","blip_selector":"aleph-js","name":"Aleph.js","display_name":"Aleph.js","url":"/radar/languages-and-frameworks/aleph-js","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210023,"quadrant":"tools","volume_date":"2022-10","description":"We've had success with Clumio Protect for backing up AWS data, particularly S3. A commercial SaaS solution, Clumio Protect can also back up a range of other AWS services and stores the data offline where it is not accessible through the internet. Our teams responsible for handling data protection and recovery at massive scale found that Clumio Protect is easy to set up and maintain and far outperforms the native AWS Backup service when S3 buckets are particularly big.","blip_selector":"clumio-protect","name":"Clumio Protect","display_name":"Clumio Protect","url":"/radar/tools/clumio-protect","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911021,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Kotest (previously KotlinTest) is a stand-alone testing tool for the Kotlin ecosystem that is widely used among our teams across various Kotlin implementations — native, JVM or JavaScript. Its key advantages are that it offers a variety of testing styles in order to structure test suites and that it comes with a comprehensive set of matchers, which allow for expressive tests in an elegant internal DSL. In addition to its support for property-based testing, our teams like the solid IntelliJ plugin and the support community. Many of our developers consider it their first choice and recommend those who are still using JUnit in Kotlin consider switching over to Kotest. | Kotest (previously KotlinTest) is a stand-alone testing tool for the Kotlin ecosystem that is continuing to gain traction within our teams across various Kotlin implementations — native, JVM or JavaScript. Key advantages are that it offers a variety of testing styles in order to structure the test suites and that it comes with a comprehensive set of matchers, which allow for expressive tests in an elegant internal DSL. In addition to its support for property-based testing — a technique we've highlighted previously in the Radar — our teams like the solid IntelliJ plugin and the growing community of support. | KotlinTest is a stand-alone testing tool for the Kotlin ecosystem that our teams have come to like. It allows property-based testing, a technique we've highlighted in the Radar before. Key advantages are that it offers a variety of testing styles in order to structure the test suites and that it comes with a comprehensive set of matchers, which allow for expressive tests in an elegant internal DSL.","blip_selector":"kotest","name":"Kotest","display_name":"Kotest","url":"/radar/languages-and-frameworks/kotest","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210018,"quadrant":"tools","volume_date":"2022-10","description":"We like spreading the word about linting tools that actually help you find issues rather than just shortcut style disputes in the team. Hadolint is one of those tools — it helps find common issues in Dockerfiles. We find it to be fast, accurate and with good documentation. It explains both how to fix an issue and why it's an issue in the first place, thus nudging Dockerfile authors toward good practices. Incidentally, Hadolint is built on top of ShellCheck, which we recommend in its own right for checking your shell scripts.","blip_selector":"hadolint","name":"Hadolint","display_name":"Hadolint","url":"/radar/tools/hadolint","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210044,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"JobRunr is a library for background job processing in Java and an alternative to the Quartz scheduler. Our teams have enjoyed using JobRunr's built-in dashboard, which is easy to use and allows the monitoring and scheduling of background tasks. JobRunr is open source and free for commercial use; for features such as job migration and recovery, however, you need to get a paid license.","blip_selector":"jobrunr","name":"JobRunr","display_name":"JobRunr","url":"/radar/languages-and-frameworks/jobrunr","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210073,"quadrant":"platforms","volume_date":"2022-10","description":"DataOps.live is a data platform that automates environments in Snowflake. Inspired by DevOps practices, DataOps.live lets you treat the data platform like any other web platform by embracing continuous integration and continuous delivery (CI/CD), automated testing, observability and code management. You can roll back changes immediately without impacting the data or recover from complete failures and rebuild a fresh Snowflake tenant in minutes or hours instead of days. Our teams had good experiences with DataOps.live, because it allowed us to iterate quickly when building data products on top of Snowflake.","blip_selector":"dataops-live","name":"DataOps.live","display_name":"DataOps.live","url":"/radar/platforms/dataops-live","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203020,"quadrant":"tools","volume_date":"2022-10","description":"We continue to see organizations move to the cloud without properly understanding how they will track ongoing spend. We previously blipped run cost as architecture fitness function, and Infracost is a tool that aims to make these cloud cost trade-offs visible in Terraform pull requests. It's open-source software and available for macOS, Linux, Windows and Docker and supports pricing for AWS, GCP and Microsoft Azure out of the box. It also provides a public API that can be queried for current cost data. We remain excited by its potential, especially when it comes to gaining better cost visibility in the IDE. | One often-cited advantage of moving to the cloud is transparency around infrastructure spend. In our experience, this is often not the case. Teams don't always think about the decisions they make around infrastructure in terms of financial cost which is why we previously blipped about run cost as architecture fitness function. We're intrigued by the release of a new tool called Infracost which aims to make cost trade-offs visible in Terraform pull requests. It's open-source software and available for macOS, Linux, Windows and Docker and supports pricing for AWS, GCP and Microsoft Azure out of the box. It also provides a public API that can be queried for current cost data. Our teams are excited by its potential, especially when it comes to gaining better cost visibility in the IDE.","blip_selector":"infracost","name":"Infracost","display_name":"Infracost","url":"/radar/tools/infracost","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202210107,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"We're seeing some interest in the Carbon programming language. That doesn't come as a surprise: it has Google's backing and is presented as a natural successor to C++. In our opinion C++ can't be replaced fast enough as software engineers have shown, over the past decades, that writing safe and error-free C++ code is extremely difficult and time-consuming. While Carbon is an interesting concept with its focus on migration from C++, without a working compiler, it's clearly a long way from being usable and there are other modern programming languages that are good choices if you want to migrate from C++. It's too early to tell whether Carbon will become the natural successor to C++, but, from today's perspective, we recommend that teams look at Rust and Go rather than postponing a migration because they're waiting for Carbon to arrive.","blip_selector":"carbon","name":"Carbon","display_name":"Carbon","url":"/radar/languages-and-frameworks/carbon","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1327,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Since we last mentioned Camunda, we've seen many of our teams and clients use the platform, making it one of our preferred workflow engines in cases where a workflow engine is a good fit for the domain. Camunda offers workflow and decision engines that can be integrated as a library in your Java code. This makes it easy to test, version and refactor workflows, alleviating some of the downsides of other more low-code workflow engines. We've even seen Camunda used in environments with high performance requirements. Teams also like how easy it is to integrate with Spring Boot and its nice user interface. | We tend to be quite skeptical of business process model and notation (BPMN) tools in general as they're often associated with low-code environments and their downsides. Although the OSS BPMN framework Camunda provides some of this whizziness, it also offers workflow and decision engines that can be directly integrated as a library in your Java code. This makes it easy to test, version and refactor workflows. Camunda also integrates with Spring and Spring Boot, among other frameworks, making it a solid choice.","blip_selector":"camunda","name":"Camunda","display_name":"Camunda","url":"/radar/languages-and-frameworks/camunda","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203015,"quadrant":"platforms","volume_date":"2022-10","description":"Colima is becoming a popular open alternative to Docker Desktop. It provisions the Docker container run time in a Lima VM, configures the Docker CLI on macOS and handles port-forwarding and volume mounts. Colima uses containerd as its run time, which is also the run time on most managed Kubernetes services — improving the important dev-prod parity. With Colima you can easily use and test the latest features of containerd, such as lazy loading for container images. We've been having good results with Colima in our projects. When in the Kubernetes space, we also use nerdctl, a Docker-compatible CLI for containerd. Since Kubernetes has deprecated Docker as container run time and most managed-services (EKS, GKE, etc) are following its lead, more people will be looking to containerd native tools, hence the importance of tools like nerdctl. In our opinion, Colima is realizing its strong potential and becoming a go-to option as an alternative to Docker Desktop. | Colima is becoming a popular open alternative to Docker for Desktop. It provisions the Docker container runtime in a Lima VM, configures the Docker CLI on macOS and handles port-forwarding and volume mounts. Colima uses containerd as runtime, which is also the runtime on most managed Kubernetes services (thus improved dev-prod parity). With Colima you can easily use and test the latest features of containerd, such as lazy loading for container images. With its good performance, we're watching Colima as a strong potential for the open-source choice alternative to Docker for Desktop.","blip_selector":"colima","name":"Colima","display_name":"Colima","url":"/radar/platforms/colima","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210056,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Cloudscape is an open-source design system that not only has a rich set of components but also 35 interaction and content representation patterns. In addition, it uses design tokens for theming and provides element wrappers for all components, which greatly simplifies unit testing. This makes it stand out from other design systems out there.","blip_selector":"cloudscape","name":"Cloudscape","display_name":"Cloudscape","url":"/radar/languages-and-frameworks/cloudscape","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210004,"quadrant":"tools","volume_date":"2022-10","description":"Harness Cloud Cost Management is a commercial tool that works across all three of the major cloud providers and their managed Kubernetes clusters to help visualize and manage cloud costs. The product calculates a cost efficiency score by looking at idle resources as well as resources not allocated to any workload and uses historical trends to help optimize resource allocation. The dashboards highlight cost spikes and allow a user to register unexpected anomalies, which are then fed into their reinforcement learning algorithm around anomaly detection. Cloud Cost Management can recommend adjustments to limits for memory and CPU usage, with options to optimize for either cost or performance. \"Perspectives\" allows you to group costs based on organizationally defined filters (which could correspond to business units, teams or products) and automate report distribution to bring visibility into cloud spend. We believe Cloud Cost Management offers a compelling feature set to help organizations mature their FinOps practices.","blip_selector":"harness-cloud-cost-management","name":"Harness Cloud Cost Management","display_name":"Harness Cloud Cost Management","url":"/radar/tools/harness-cloud-cost-management","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210074,"quadrant":"platforms","volume_date":"2022-10","description":"Starting with Databricks 9.1 LTS (Long Term Support), a new run time became available called Databricks Photon, an alternative that was rewritten from the ground up in C++. Several of our teams have now used Photon in production and have been pleased with the performance improvements and corresponding cost savings. Actual improvements and changes in costs will depend upon multiple factors such as data set size and transaction types. We recommend trialing against a realistic workload to gather data for a comparison before making any decision on Photon's use.","blip_selector":"databricks-photon","name":"Databricks Photon","display_name":"Databricks Photon","url":"/radar/platforms/databricks-photon","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210077,"quadrant":"platforms","volume_date":"2022-10","description":"Seldon Core is an open-source platform to package, deploy, monitor and manage machine learning models in Kubernetes clusters. With out-of-the-box support for several machine-learning frameworks, you can easily containerize your models using prepackaged inference servers, custom inference servers or language wrappers. With distributed tracing through Jaeger and model explainability via Alibi, Seldon Core addresses several last-mile delivery challenges with machine learning deployments, and our data teams like it.","blip_selector":"seldon-core","name":"Seldon Core","display_name":"Seldon Core","url":"/radar/platforms/seldon-core","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210042,"quadrant":"platforms","volume_date":"2022-10","description":"Feast is an open-source Feature Store for machine learning. It has several useful properties, including generating point-in-time correct feature sets — so error-prone future feature values do not leak to models during training — and supporting both streaming and batch data sources. However, it currently only supports timestamped structured data and therefore may not be suitable if you work with unstructured data in your models. We've successfully used Feast at a significant scale as an offline store during model training and as an online store during prediction.","blip_selector":"feast","name":"Feast","display_name":"Feast","url":"/radar/platforms/feast","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210041,"quadrant":"platforms","volume_date":"2022-10","description":"Databricks Unity Catalog is a data governance solution for assets such as files, tables or machine learning models in a lakehouse. Although you'll find several platforms in the enterprise data governance space, if you're already using other Databricks solutions, you should certainly assess Unity Catalog. We want to highlight that while these governance platforms usually implement a centralized solution for better consistency across workspaces and workloads, the responsibility to govern should be federated by enabling individual teams to govern their own assets.","blip_selector":"databricks-unity-catalog","name":"Databricks Unity Catalog","display_name":"Databricks Unity Catalog","url":"/radar/platforms/databricks-unity-catalog","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210010,"quadrant":"tools","volume_date":"2022-10","description":"Data Vault 2.0 is a data modeling methodology and design pattern intended to improve the flexibility of data warehouses compared to other popular modeling approaches. Data Vault 2.0 can be applied to any data store such as Snowflake or Databricks. When implementing Data Vault warehouses, we've found the dbtvault package for dbt to be a helpful tool. dbtvault provides a set of jinja templates that generate and execute the ETL scripts necessary to populate a Data Vault warehouse. Although dbtvault has some rough edges — it lacks support for enforcing implied uniqueness or performing incremental loads — overall, it fills a niche and requires minimal configuration to get started.","blip_selector":"dbtvault","name":"dbtvault","display_name":"dbtvault","url":"/radar/tools/dbtvault","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010061,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Among web component frameworks, Svelte stands out by moving reactivity out of the browser and into the compiler. Instead of optimizing DOM updates by using a virtual DOM and browser optimization tricks, Svelte compiles your code into vanilla framework-less JavaScript code that surgically updates the DOM directly. In addition to the run-time performance benefits, this also allows Svelte to optimize the amount of code the browser has to download without sacrificing features for developers; moreover, it's proven to be performant and battery-friendly for mobile web applications as less code has to execute in the browser itself. Performance benefits aside, our teams have appreciated its friendly learning curve and the maintenance benefits that come from writing less code. Svelte itself is only the component framework, but SvelteKit adds features to build full web applications. | We continue to see new front-end JavaScript frameworks, and Svelte stands out as a promising new component framework. Unlike other frameworks that leverage the virtual DOM, Svelte compiles your code into vanilla framework-less JavaScript code that surgically updates the DOM directly. However, it's only a component framework; if you're planning to build feature-rich applications, consider assessing Sapper together with Svelte.","blip_selector":"svelte","name":"Svelte","display_name":"Svelte","url":"/radar/languages-and-frameworks/svelte","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911004,"quadrant":"platforms","volume_date":"2022-10","description":"Teleport is a tool for zero trust network access to infrastructure. Traditional setups require complex policies or jump servers to restrict access to critical resources. Teleport, however, simplifies this with a unified access plane and with fine-grained authorization controls that replace jump servers, VPNs or shared credentials. Implemented as a single binary with out-of-the-box support for several protocols (including SSH, RDP, Kubernetes API, MySQL, MongoDB and PostgreSQL wire protocols), Teleport makes it easy to set up and manage secured access across Linux, Windows or Kubernetes environments. Since we first mentioned it in the Radar, a few teams have used Teleport and our overall positive experience prompted us to highlight it. | Teleport is a security gateway for remotely accessing cloud native infrastructures. One of Teleport's interesting features is its ability to double as a Certificate Authority (CA) for your infrastructure. You can issue short-lived certificates and build richer role-based access control (RBAC) for your Kubernetes infrastructure (or for just SSH). With increased focus on infrastructure security it's important to keep track of changes. However, not all events require the same level of auditing. With Teleport you can stick with logging for most of the events but go the extra mile by recording the user screen for more privileged root sessions.","blip_selector":"teleport","name":"Teleport","display_name":"Teleport","url":"/radar/platforms/teleport","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202010066,"quadrant":"platforms","volume_date":"2022-10","description":"In an increasingly digital world, improving developer effectiveness in large organizations is often a core concern of senior leaders. We've seen enough value with developer portals in general and Backstage in particular that we're happy to recommend it in Adopt. Backstage is an open-source developer portal platform created by Spotify that improves discovery of software assets across the organization. It uses Markdown TechDocs that live alongside the code for each service, which nicely balances the needs of centralized discovery with the need for distributed ownership of assets. Backstage supports software templates to accelerate new development and a plugin architecture that allows for extensibility and adaptability into an organization's infrastructure ecosystem. Backstage Service Catalog uses YAML files to track ownership and metadata for all the software in an organization's ecosystem; it even lets you track third-party SaaS software, which usually requires tracking ownership. | As the focus on improving the developer experience and efficiency increases across organizations, we're seeing Backstage rise in popularity, alongside the adoption of developer portals. These organizations are looking to support and streamline their development environments. As the number of tools and technologies increases, some form of standardization is becoming increasingly important for consistency so that developers can focus on innovation and product development instead of getting bogged down with reinventing the wheel. Backstage is an open-source developer portal platform created by Spotify. It's based on software templates, unifying infrastructure tooling and consistent and centralized technical documentation. The plugin architecture allows for extensibility and adaptability into an organization's infrastructure ecosystem. We'll be watching the new Backstage Service Catalog, currently in alpha, which keeps track of ownership and metadata for all the software in an organization's ecosystem. | We continue to see interest in and use of Backstage grow, alongside the adoption of developer portals, as organizations look to support and streamline their development environments. As the number of tools and technologies increases, some form of standardization is becoming increasingly important for consistency so that developers are able to focus on innovation and product development instead of getting bogged down with reinventing the wheel. Backstage is an open-source developer portal platform created by Spotify, it's based upon software templates, unifying infrastructure tooling and consistent and centralized technical documentation. The plugin architecture allows for extensibility and adaptability into an organization’s infrastructure ecosystem. | Organizations are looking to support and streamline development environments through developer portals or platforms. As the number of tools and technologies increases, some form of standardization is becoming increasingly important for consistency so that developers are able to focus on innovation and product development instead of getting bogged down with reinventing the wheel. A centralized developer portal can offer easy discoverability of services and best practices. Backstage is an open-source platform for creating developer portals by Spotify. It is based upon software templates, unifying infrastructure tooling and consistent and centralized technical documentation. Its plugin architecture allows for extensibility and adaptability into an organization’s infrastructure ecosystem.","blip_selector":"backstage","name":"Backstage","display_name":"Backstage","url":"/radar/platforms/backstage","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104084,"quadrant":"platforms","volume_date":"2022-10","description":"Since we first mentioned data discoverability in the Radar, LinkedIn has evolved WhereHows to DataHub, the next generation platform that addresses data discoverability via an extensible metadata system. Instead of crawling and pulling metadata, DataHub adopts a push-based model where individual components of the data ecosystem publish metadata via an API or a stream to the central platform. This push-based integration shifts ownership from the central entity to individual teams, making them accountable for their metadata. As a result, we've used DataHub successfully as an organization-wide metadata repository and entry point for multiple autonomously maintained data products. When taking this approach, be sure to keep it lightweight and avoid the slippery slope leading to centralized control over a shared resource. | Since we first mentioned data discoverability in the Radar, LinkedIn has evolved WhereHows to DataHub, the next generation platform that addresses data discoverability via an extensible metadata system. Instead of crawling and pulling metadata, DataHub adopts a push-based model where individual components of the data ecosystem publish metadata via an API or a stream to the central platform. This push-based integration shifts the ownership from the central entity to individual teams making them accountable for their metadata. As more and more companies are trying to become data driven, having a system that helps with data discovery and understanding data quality and lineage is critical, and we recommend you assess DataHub in that capacity.","blip_selector":"datahub","name":"DataHub","display_name":"DataHub","url":"/radar/platforms/datahub","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210024,"quadrant":"tools","volume_date":"2022-10","description":"We've been talking about tailored service templates ever since we first identified microservices as a thing. If an organization sets out to create a collection of small services that can be developed, built, deployed and operated independently but consistently, it makes sense to give teams a solid starting point that aligns to the standard. However, one of the enduring problems with that approach is that as the template evolves over time in response to changing technical and business requirements, projects based on older versions of the template fall out of date. Retrofitting template improvements into an established project becomes a major pain. Cruft attempts to address this problem by providing tools to identify and patch differences between a local project and the current head of a master template repository. It combines the Cookiecutter templating engine with git hashes to identify and apply changes to the templates. Think of it as a package manager for a project boilerplate. Keeping templates up-to-date is a notoriously difficult and long-standing problem, so to us the solution Cruft provides sounds almost too good to be true. Based on early feedback from our team, however, Cruft actually works and makes life easier for service builders and maintainers. We're anxious to see how it performs over the long term, but for now it's worth taking a look at this potentially useful tool.","blip_selector":"cruft","name":"Cruft","display_name":"Cruft","url":"/radar/tools/cruft","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210048,"quadrant":"languages-and-frameworks","volume_date":"2022-10","description":"Connect is a family of libraries for building browser- and gRPC-compatible HTTP APIs. Similar to gRPC, you write Protocol Buffer schema and implement the application logic, and Connect generates code to handle marshaling, routing, compression and content type negotiation. However, Connect tries to improve on gRPC in several ways. This includes native support for gRPC-Web without a translating proxy; interoperability with third-party routers or middleware, because connect-go is built on top of net/http (unlike grpc-go); and fully generated type-safe clients with the ergonomics of hand-crafted code. We mostly prefer REST and are not a big fan of the RPC approach to building APIs. That said, Connect does seem to address some of our concerns with RPCs, and we encourage you to assess it.","blip_selector":"connect","name":"Connect","display_name":"Connect","url":"/radar/languages-and-frameworks/connect","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210037,"quadrant":"platforms","volume_date":"2022-10","description":"GCP Vertex AI is a unified artificial intelligence platform that allows teams to build, deploy and scale machine-learning (ML) models. Vertex AI includes pretrained models, which can be used directly, fine-tuned or combined with AutoML, as well as infrastructure such as feature stores and pipelines for ML models. We like Vertex AI's integrated capabilities, which help to make it feel like a coherent AI platform.","blip_selector":"gcp-vertex-ai","name":"GCP Vertex AI","display_name":"GCP Vertex AI","url":"/radar/platforms/gcp-vertex-ai","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210083,"quadrant":"techniques","volume_date":"2022-10","description":"Using test email accounts or entire test SMTP (Single Mail Transfer Protocol) servers remains a common software testing practice. However, using a real server carries the risk that test emails will be sent to real people and often complicates automated integration testing. We've seen success using a fake SMTP server to test mail sending, which records a request to send an email without actually sending it. Multiple open-source tools exist in this space, including fake-smtp-server, which renders emails in a web UI for visual testing, and mountebank, which exposes the sent emails through a REST API for integration testing. We recommend exploring this technique to reduce risk and improve testing efficiency.","blip_selector":"fake-smtp-server-to-test-mail-sending","name":"Fake SMTP server to test mail-sending","display_name":"Fake SMTP server to test mail-sending","url":"/radar/techniques/fake-smtp-server-to-test-mail-sending","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202210081,"quadrant":"techniques","volume_date":"2022-10","description":"When faced with the challenge of using a design system consistently across many form factors and platforms, the team at Salesforce came up with the concept of design tokens. The tokens store values, such as colors and fonts, in one central place. This makes it possible to separate options from decisions, and it significantly improves collaboration between teams. Design tokens are not new, but with the introduction of tools like Tailwind CSS and Style Dictionary, we see design tokens being used more often.","blip_selector":"design-tokens","name":"Design tokens","display_name":"Design tokens","url":"/radar/techniques/design-tokens","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202210002,"quadrant":"tools","volume_date":"2022-10","description":"Xcode Cloud is a CI/CD tool that is built into Xcode and used to build, test and deploy Apple apps. It provides an integrated experience with familiar tools for Apple developers like Xcode, App Store Connect and TestFlight. Based on our team's experience, it does a good job of simplifying the pipeline configuration and provisioning profiles and certificates. This tool is quite fresh and most of our mobile development teams are still using the more mature Bitrise. Still, we think it's worth assessing and tracking its progress.","blip_selector":"xcode-cloud","name":"Xcode Cloud","display_name":"Xcode Cloud","url":"/radar/tools/xcode-cloud","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911008,"quadrant":"platforms","volume_date":"2022-10","description":"Delta Lake is an open-source storage layer, implemented by Databricks, that attempts to bring ACID transactions to big data processing. In our Databricks-enabled data lake or data mesh projects, our teams prefer using Delta Lake storage over the direct use of file storage types such as AWS S3 or ADLS. Until recently, Delta Lake has been a closed proprietary product from Databricks, but it's now open source and accessible to non-Databricks platforms. However, our recommendation of Delta Lake as a default choice currently extends only to Databricks projects that use Parquet file formats. Delta Lake facilitates concurrent data read/write use cases where file-level transactionality is required. We find Delta Lake's seamless integration with Apache Spark batch and micro-batch APIs very helpful, particularly features such as time travel (accessing data at a particular point in time or commit reversion) as well as schema evolution support on write. | Delta Lake is an open-source storage layer, implemented by Databricks, that attempts to bring ACID transactions to big data processing. In our Databricks-enabled data lake or data mesh projects, our teams continue to prefer using Delta Lake storage over the direct use of file storage types such S3 or ADLS. Of course this is limited to projects that use storage platforms that support Delta Lake when using Parquet file formats. Delta Lake facilitates concurrent data read/write use cases where file-level transactionality is required. We find Delta Lake's seamless integration with Apache Spark batch and micro-batch APIs greatly helpful, particularly features such as time travel — accessing data at a particular point in time or commit reversion — as well as schema evolution support on write; though there are some limitations on these features. | Delta Lake is an open-source storage layer by Databricks that attempts to bring transactions to big data processing. One of the problems we often encounter when using Apache Spark is the lack of ACID transactions. Delta Lake integrates with the Spark API and addresses this problem by its use of a transaction log and versioned Parquet files. With its serializable isolation, it allows concurrent readers and writers to operate on Parquet files. Other welcome features include schema enforcement on write and versioning, which allows us to query and revert to older versions of data if necessary. We've started to use it in some of our projects and quite like it.","blip_selector":"delta-lake","name":"Delta Lake","display_name":"Delta Lake","url":"/radar/platforms/delta-lake","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201911058,"quadrant":"techniques","volume_date":"2022-03","description":"For organizations using Azure as their primary cloud provider, Azure Data Factory is currently the default for orchestrating data-processing pipelines. It supports data ingestion, copying data from and to different storage types on prem or on Azure and executing transformation logic. Although we've had adequate experience with Azure Data Factory for simple migrations of data stores from on prem to the cloud, we discourage the use of Azure Data Factory for orchestration of complex data-processing pipelines and workflows. We've had some success with Azure Data Factory when it's used primarily to move data between systems. For more complex data pipelines, it still has its challenges, including poor debuggability and error reporting; limited observability as Azure Data Factory logging capabilities don't integrate with other products such as Azure Data Lake Storage or Databricks, making it difficult to get an end-to-end observability in place; and availability of data source-triggering mechanisms only to certain regions. At this time, we encourage using other open-source orchestration tools (e.g., Airflow) for complex data pipelines and limiting Azure Data Factory for data copying or snapshotting. Our teams continue to use Data Factory to move and extract data, but for larger operations we recommend other, more well-rounded workflow tools. | Azure Data Factory (ADF) is currently Azure's default product for orchestrating data-processing pipelines. It supports data ingestion, copying data from and to different storage types on prem or on Azure and executing transformation logic. While we've had a reasonable experience with ADF for simple migrations of data stores from on prem to cloud, we discourage the use of Azure Data Factory for orchestration of complex data-processing pipelines. Our experience has been challenging due to several factors, including limited coverage of capabilities that can be implemented through coding first, as it appears that ADF is prioritizing enabling low-code platform capabilities first; poor debuggability and error reporting; limited observability as ADF logging capabilities don't integrate with other products such as Azure Data Lake Storage or Databricks, making it difficult to get an end-to-end observability in place; and availability of data source-triggering mechanisms only to certain regions. At this time, we encourage using other open-source orchestration tools (e.g., Airflow) for complex data pipelines and limit ADF for data copying or snapshotting. We're hoping that ADF will address these concerns to support for more complex data-processing workflows and prioritize access to capabilities through code first.","blip_selector":"azure-data-factory-for-orchestration","name":"Azure Data Factory for orchestration","display_name":"Azure Data Factory for orchestration","url":"/radar/techniques/azure-data-factory-for-orchestration","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1336,"quadrant":"platforms","volume_date":"2022-03","description":"Google Cloud Dataflow is a cloud-based data-processing service for both batch and real-time data-streaming applications. Our teams are using Dataflow to create processing pipelines for integrating, preparing and analyzing large data sets, with Apache Beam's unified programming model on top to ease manageability. We first featured Dataflow in 2018, and its stability, performance and rich feature set make us confident to move it to Trial in this edition of the Radar. | Google Cloud Dataflow is useful in traditional ETL scenarios for reading data from a source, transforming it and then storing it to a sink, with configurations and scaling being managed by dataflow. Dataflow supports Java, Python and Scala and provides wrappers for connections to various types of data sources. However, the current version won’t let you add additional libraries, which may make it unsuitable for certain data manipulations. You also can’t change the dataflow DAG dynamically. Hence, if your ETL has conditional execution flows based on parameters, you may not be able to use dataflow without workarounds.","blip_selector":"google-cloud-dataflow","name":"Google Cloud Dataflow","display_name":"Google Cloud Dataflow","url":"/radar/platforms/google-cloud-dataflow","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203048,"quadrant":"platforms","volume_date":"2022-03","description":"When Cloudflare Workers was released, we highlighted it as an early function as a service (FaaS) for edge computing with an interesting implementation. The release of Cloudflare Pages last April didn't feel as noteworthy, because Pages is just one in a class of many Git-backed site-hosting solutions. It did have continuous previews, a useful feature not found in most alternatives. Now, though, Cloudflare has more tightly integrated Workers and Pages, creating a fully integrated Jamstack solution running on the CDN. The inclusion of a key-value store and a strongly consistent coordination primitive further enhance the attractiveness of the new version of Cloudflare Pages.","blip_selector":"cloudflare-pages","name":"Cloudflare Pages","display_name":"Cloudflare Pages","url":"/radar/platforms/cloudflare-pages","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110074,"quadrant":"tools","volume_date":"2022-03","description":"We've used Pact for contract testing long enough to see some of the complexity that comes with scale. Some of our teams have successfully used Pactflow to reduce that friction. Pactflow runs both as software as a service and as an on-prem deployment with the same features as the SaaS offering, and it adds improved usability, security and auditing on top of the open-source Pact Broker offering. We've been pleased with our use so far and are happy to see continued effort to remove some of the overhead of managing contract testing at scale. | For organizations with larger and more complex API ecosystems, especially those who are already using Pact, we think it's worth assessing whether Pactflow could be useful. Pactflow manages the workflow and continuous deployment of tests written in Pact, lowering the barrier to consumer-driven contract testing. The complexity of coordination between multiple producers and various disparate consumers can become prohibitive. We've seen some teams invest significant effort in hand-crafting solutions to this problem and think it's worth assessing whether Pactflow can look after this for you.","blip_selector":"pactflow","name":"Pactflow","display_name":"Pactflow","url":"/radar/tools/pactflow","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203038,"quadrant":"tools","volume_date":"2022-03","description":"Web Test Runner is a package within the Modern Web project, which provides several high-quality tools for modern web development with support for web standards like ES Modules. Web Test Runner is a test runner for web applications. One of its advantages compared to existing test runners is that it runs tests in the browser (which could be headless). It supports multiple browser launchers — including Puppeteer, Playwright, and Selenium — and uses Mocha by default for the test framework. The tests run pretty fast, and we like that we can open a browser window with devtools when debugging. Web Test Runner internally uses Web Dev Server which allows us to leverage its great plugin API for adding customized plugins for our test suite. Modern Web tools look like a very promising developer toolchain, and we're already using it in a few projects.","blip_selector":"web-test-runner","name":"Web Test Runner","display_name":"Web Test Runner","url":"/radar/tools/web-test-runner","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203057,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"Android 12 brought significant changes to app widgets that have improved the user and developer experience. For writing regular Android apps, we've expressed our preference for Jetpack Compose as a modern way of building native user interfaces. Now, with Jetpack Glance, which is built on top of the Compose runtime, developers can use similar declarative Kotlin APIs for writing widgets. Recently, Glance has been extended to support Tiles for Wear OS.","blip_selector":"jetpack-glance","name":"Jetpack Glance","display_name":"Jetpack Glance","url":"/radar/languages-and-frameworks/jetpack-glance","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203023,"quadrant":"tools","volume_date":"2022-03","description":"Metaflow is a user-friendly Python library and back-end service that helps data scientists and engineers build and manage production-ready data processing, ML training and inference workflows. Metaflow provides Python APIs that structure the code as a directed graph of steps. Each step can be decorated with flexible configurations such as the required compute and storage resources. Code and data artifacts for each step's run (aka task) are stored and can be retrieved either for future runs or the next steps in the flow, enabling you to recover from errors, repeat runs and track  versions of models and their dependencies across multiple runs.\n\nThe value proposition of Metaflow is the simplicity of its idiomatic Python library: it fully integrates with the build and run-time infrastructure to enable running data engineering and science tasks in local and scaled production environments. At the time of writing, Metaflow is heavily integrated with AWS services such as S3 for its data store service and step functions for orchestration. Metaflow supports R in addition to Python. Its core features are open sourced.\n\nIf you're building and deploying your production ML and data-processing pipelines on AWS, Metaflow is a lightweight full-stack alternative framework to more complex platforms such as MLflow.","blip_selector":"metaflow","name":"Metaflow","display_name":"Metaflow","url":"/radar/tools/metaflow","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203056,"quadrant":"tools","volume_date":"2022-03","description":"In our previous Radar, we placed modern Unix commands in Assess. One of the commands featured in that collection of tools was jq, effectively a sed for JSON. jc performs a related task: it takes the output of common Unix commands and parses the output into JSON. The two commands together provide a bridge between the Unix CLI world and the raft of libraries and tools that operate on JSON. When writing simple scripts, for example, for software deployment or gathering troubleshooting information, having the myriad of different Unix command output formats mapped into well-defined JSON can save a lot of time and effort. As with jq, you need to make sure the command is available. It can be installed from many of the well-known package repositories.","blip_selector":"jc","name":"jc","display_name":"jc","url":"/radar/tools/jc","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1329,"quadrant":"platforms","volume_date":"2022-03","description":"As the Azure DevOps ecosystem keeps growing, our teams are using it more with success. These services contain a set of managed services, including hosted Git repos, build and deployment pipelines, automated testing tooling, backlog management tooling and artifact repository. We've seen our teams gaining experience in using this platform with good results, which means Azure DevOps is maturing. We particularly like its flexibility; it allows you to use the services you want even if they're from different providers. For instance, you could use an external Git repository while still using the Azure DevOps pipeline services. Our teams are especially excited about Azure DevOps Pipelines. As the ecosystem matures, we're seeing an uptick in onboarding teams that are already on the Azure stack as it easily integrates with the rest of the Microsoft world. | Azure DevOps services contain a set of managed services, including hosted Git repos, CI/CD pipelines, automated testing tooling, backlog management tooling and artifact repository. We've seen our teams getting more experience in using this platform with good results, which means Azure DevOps is maturing. We particularly like its flexibility; it allows you to use the services you want even if they're from different providers. For instance, you could use an external Git repository while still using the Azure DevOps pipeline services. Our teams are especially excited about Azure DevOps Pipelines. Nevertheless, all the services offer a good developer experience that helps our teams deliver value. | Azure DevOps services include a set of managed services such as hosted Git repos, CI/CD pipelines, automated testing tooling, backlog management tooling and artifact repository. Azure DevOps Pipelines have been maturing over time. We particularly like its ability to define Pipelines as code and its ecosystem of extensions on the Azure DevOps marketplace. At the time of writing, our teams are still running into a few immature features, including lack of an effective UI for pipeline visualization and navigation and the inability to trigger a pipeline from artifacts or other pipelines. | Azure DevOps services include a set of managed services such as hosted Git repos, CI and CD pipelines and artifact repository. Azure DevOps services have replaced Visual Studio Team Services. We've had a good experience in starting projects quickly with Azure DevOps services—managing, building and releasing applications to Azure. We've also run into a few challenges—such as lack of full support for CI and CD pipeline as code, slow build agent startup time, separation of build and release into different pipelines—and experienced a few downtimes. We're hoping that Azure DevOps services improve over time to provide a good developer experience when hosting applications on Azure, with a frictionless experience integrating with other Azure services.","blip_selector":"azure-devops","name":"Azure DevOps","display_name":"Azure DevOps","url":"/radar/platforms/azure-devops","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203010,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"Zig is a new language that shares many attributes with C but with stronger typing, easier memory allocation, support for namespacing and a host of other features. Its syntax, however, is reminiscent of JavaScript rather than C, which some may hold against it. Zig's aim is to provide a very simple language with straightforward compilation that minimizes side-effects and delivers predictable, easy-to-trace execution. Zig also provides simplified access to LLVM's cross-compilation capability. Some of our developers have found this feature so viable, they're using Zig as a cross-compiler even though they aren't writing Zig code. Zig is a novel language and worth looking into  for applications where C is being considered or already in use as well as for low-level systems applications that require explicit memory manipulation.","blip_selector":"zig","name":"Zig","display_name":"Zig","url":"/radar/languages-and-frameworks/zig","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203024,"quadrant":"tools","volume_date":"2022-03","description":"Micrometer is a platform-agnostic library for metrics instrumentation on the JVM that supports Graphite, New Relic, CloudWatch and many other integrations. We've found that Micrometer has benefited both library authors and teams: library authors can include metrics instrumentation code in their libraries without needing to support each and every metrics system that their users are using;  and teams can support many different metrics on back-end registries which enables organizations to collect metrics in a consistent way.","blip_selector":"micrometer","name":"Micrometer","display_name":"Micrometer","url":"/radar/tools/micrometer","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203019,"quadrant":"tools","volume_date":"2022-03","description":"Securing the software supply chain has become a commonplace concern among delivery teams, a concern that is reflected by the growing number of new tools in this space. Grype is a new lightweight vulnerability scanning tool for Docker and OCI images. It can be installed as a binary, can scan images before they're pushed to a registry, and it doesn't require a Docker daemon to run on your build agents. Grype comes from the same team that is behind Syft, which generates SBOMs in various formats from container images. Grype can consume the SBOM output of Syft to scan for vulnerabilities.","blip_selector":"grype","name":"Grype","display_name":"Grype","url":"/radar/tools/grype","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203026,"quadrant":"techniques","volume_date":"2022-03","description":"In an organization that practices the \"you build it, you run it\" principle, a definition of production readiness (DPR) is a useful technique to support teams in assessing and preparing the operational readiness of new services. Implemented as a checklist or a template, a DPR gives teams guidance on what to think about and consider before they bring a new service into production. While DPRs do not define specific service-level objectives (SLOs) to fulfill (those would be hard to define one-size-fits-all), they remind teams what categories of SLOs to think of, what organizational standards to comply with and what documentation is required. DPRs provide a source of input that teams turn into respective product-specific requirements around, for example, observability and reliability, to feed into their product backlogs.\n\nDPRs are closely related to Google's concept of a production readiness review (PRR). In organizations that are too small to have a dedicated site reliability engineering team, or who are concerned that a review board process could negatively impact a team's flow to go live, having a DPR can at least provide some guidance and document the agreed-upon criteria for the organization. For highly critical new services, extra scrutiny on fulfilling the DPR can be added via a PRR when needed.","blip_selector":"definition-of-production-readiness","name":"Definition of production readiness","display_name":"Definition of production readiness","url":"/radar/techniques/definition-of-production-readiness","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203017,"quadrant":"platforms","volume_date":"2022-03","description":"Embeddinghub is a vector database for machine-learning embeddings, and quite similar to Milvus. However, with out-of-the-box support for approximate nearest neighbor operations, partitioning, versioning and access control, we recommend you assess Embeddinghub for your embedding vector use cases.","blip_selector":"embeddinghub","name":"Embeddinghub","display_name":"Embeddinghub","url":"/radar/platforms/embeddinghub","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203084,"quadrant":"tools","volume_date":"2022-03","description":"skopeo is a command line utility that performs various operations on container images and image repositories. It doesn't require a user to be root to do most of its operations nor does it require a daemon to be running. It's a useful part of a CI pipeline; we've used it to copy images from one registry to another as we promote the images. It's better than doing a pull and a push as we don't need to store the images locally. It's not a new tool, but it's useful enough and underutilized that we felt it's worth calling it out.","blip_selector":"skopeo","name":"skopeo","display_name":"skopeo","url":"/radar/tools/skopeo","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110014,"quadrant":"tools","volume_date":"2022-03","description":"Conftest is a tool for writing tests against structured configuration data. It relies on the Rego language from Open Policy Agent to write tests for Kubernetes configurations, Tekton pipeline definitions or even Terraform plans. We've had great experiences with Conftest — and its shallow learning curve. With fast feedback from tests, our teams iterate quickly and safely on configuration changes to Kubernetes. | Conftest is a tool for writing tests against structured configuration data. It relies on the Rego language from Open Policy Agent to write tests for Kubernetes configurations, Tekton pipeline definitions or even Terraform plans. Configurations are a critical part of the infrastructure, and we encourage you to assess Conftest to verify assumptions and get quick feedback.","blip_selector":"conftest","name":"Conftest","display_name":"Conftest","url":"/radar/tools/conftest","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203077,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"Flutter is increasingly popular for building cross-platform mobile apps, and Unity is great for building AR/VR experiences. A key piece in the puzzle for integrating Unity and Flutter is the Flutter-Unity widget, which allows embedding Unity apps inside Flutter widgets. One of the key capabilities the widget offers is bi-directional communication between Flutter and Unity. We've found its performance to be pretty good as well, and we're looking forward to leveraging Unity in more Flutter apps.","blip_selector":"flutter-unity-widget","name":"Flutter-Unity widget","display_name":"Flutter-Unity widget","url":"/radar/languages-and-frameworks/flutter-unity-widget","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202110036,"quadrant":"techniques","volume_date":"2022-03","description":"We continue to perceive production data in test environments as an area for concern. Firstly, many examples of this have resulted in reputational damage, for example, where an incorrect alert has been sent from a test system to an entire client population. Secondly, the level of security, specifically around protection of private data, tends to be less for test systems. There is little point in having elaborate controls around access to production data if that data is copied to a test database that can be accessed by every developer and QA. Although you can obfuscate the data, this tends to be applied only to specific fields, for example, credit card numbers. Finally, copying production data to test systems can break privacy laws, for example, where test systems are hosted or accessed from a different country or region. This last scenario is especially problematic with complex cloud deployments. Fake data is a safer approach, and tools exist to help in its creation. We do recognize there are reasons for specific elements of production data to be copied, for example, in the reproduction of bugs or for training of specific ML models. Here our advice is to proceed with caution. | We continue to perceive production data in test environments as an area for concern. Firstly, many examples of this have resulted in reputational damage, for example, where an incorrect alert has been sent from a test system to an entire client population. Secondly, the level of security, specifically around protection of private data, tends to be less for test systems. There is little point in having elaborate controls around access to production data if that data is copied to a test database that can be accessed by every developer and QA. Although you can obfuscate the data, this tends to be applied only to specific fields, for example, credit card numbers. Finally, copying production data to test systems can break privacy laws, for example, where test systems are hosted or accessed from a different country or region. This last scenario is especially problematic with complex cloud deployments. Fake data is a safer approach, and tools exist to help in its creation. We do recognize there are reasons for specific elements of production data to be copied, for example, in the reproduction of bugs or for training of specific ML models. Here our advice is to proceed with caution.","blip_selector":"production-data-in-test-environments","name":"Production data in test environments","display_name":"Production data in test environments","url":"/radar/techniques/production-data-in-test-environments","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1250,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"WebAssembly (WASM) is the W3C standard that provides capabilities of executing code in the browser. Supported by all major browsers and backward compatible, it's a binary compilation format designed to run in the browser at near native speeds. It opens up the range of languages you can use to write front-end functionality, with early focus on C, C++ and Rust, and it's also an LLVM compilation target. When run in the sandbox, it can interact with JavaScript and shares the same permissions and security model. Portability and security are key capabilities that will enable most platforms, including mobile and IoT. | WebAssembly is a big step forward in the capabilities of the browser as a code execution environment. Supported by all major browsers and backward compatible, it's a binary compilation format designed to run in the browser at near native speeds. It opens up the range of languages you can use to write front-end functionality, with early focus on C, C++ and Rust, and it's also an LLVM compilation target. When run in the sandbox, it can interact with JavaScript and shares the same permissions and security model. When used with Firefox's new streaming compiler, it also results in faster page initialization. Although it's still early days, this W3C standard is definitely one to start exploring. | WebAssembly is a big step forward in the capabilities of the browser as a code execution environment. Supported by all major browsers and backward compatible, it's a binary compilation format designed to run in the browser at near native speeds. It opens up the range of languages you can use to write front-end functionality, with early focus on C, C++ and Rust, and it's also an LLVM compilation target. When run in the sandbox, it can interact with JavaScript and shares the same permissions and security model. When used with Firefox’s new streaming compiler, it also results in faster page initialization. Although it's still early days, this W3C standard is definitely one to start exploring.","blip_selector":"webassembly","name":"WebAssembly","display_name":"WebAssembly","url":"/radar/languages-and-frameworks/webassembly","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203039,"quadrant":"tools","volume_date":"2022-03","description":"When working on multiple JavaScript codebases at the same time, it's often necessary to use different versions of Node and other JavaScript tools. On developer machines, these tools are usually installed in the user account or the machine itself, which means a solution is needed to switch between multiple installations. For Node itself there's nvm, but we want to highlight Volta as an alternative that we're seeing in use with our teams. Volta has several advantages over using nvm: it can manage other JavaScript tools such as Yarn; it also has the notion of pinning a version of the toolchain on a project basis, which means that developers can simply use the tools in a given code directory without having to worry about manually switching between tool versions — Volta simply uses shims in the path to select the pinned version. Written in Rust, Volta is fast and ships as a single binary without dependencies.","blip_selector":"volta","name":"Volta","display_name":"Volta","url":"/radar/tools/volta","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203068,"quadrant":"tools","volume_date":"2022-03","description":"Organizations that have adopted infrastructure as code and self-service infrastructure platforms are looking for ways to give teams a maximum of autonomy while still enforcing good security practices and organizational policies. We've highlighted tfsec before and are moving it into the Adopt category in this Radar. For teams working on GCP, Terraform Validator could be an option when creating a policy library, a set of constraints that are checked against Terraform configurations.","blip_selector":"terraform-validator","name":"Terraform Validator","display_name":"Terraform Validator","url":"/radar/tools/terraform-validator","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110010,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"Vowpal Wabbit is a general-purpose machine-learning library. Originally created at Yahoo! Research over a decade ago, Vowpal Wabbit continues to implement new algorithms in reinforcement learning. We want to highlight Vowpal Wabbit 9.0, a major release after six years, and encourage you to plan the migration as it has several usability improvements, new reductions and bug fixes. | Vowpal Wabbit is a general-purpose machine-learning library. Even though it was originally created at Yahoo! Research over a decade ago, we still want to mention it to highlight that it continues to be the place where many of the newest machine-learning techniques get added first. If you're interested in machine learning, you may want to keep an eye on the innovations in Vowpal Wabbit. Note also that Microsoft has shown a deeper interest in Vowpal Wabbit in recent years, employing a main contributor and integrating it into their Azure offerings, for example in their machine-learning designer and in Personalizer.","blip_selector":"vowpal-wabbit","name":"Vowpal Wabbit","display_name":"Vowpal Wabbit","url":"/radar/languages-and-frameworks/vowpal-wabbit","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1147,"quadrant":"platforms","volume_date":"2022-03","description":"Many of our teams choose CircleCI for their continuous integration needs, and they appreciate its ability to run complex pipelines efficiently. The CircleCI developers continue to add new features with CircleCI, now in version 3.0. Orbs and executors were called out by our teams as being particularly useful. Orbs are reusable snippets of code that automate repeated processes, speed up project setup and make it easy to integrate with third-party tools. The wide variety of executor types provides flexibility to set up jobs in Docker, Linux, macOS or Windows VMs. | CircleCI is a continuous integration engine offered as SaaS and on premise. CircleCI has been the go-to SaaS CI tool for many of our development teams, who needed a low-friction and easy-to-setup build and deployment pipeline. CircleCI version 2.0 supports workflows of build jobs, with fan-in and fan-out flows and manual gates, as well as mobile development. It allows developers to run the pipelines locally and easily integrates with Slack and other notification and alerting systems. We recommend you take a closer look at the security practices of CircleCI, just as you would with any other SaaS product that hosts your company’s assets. | CircleCI is a continuous integration engine offered as SaaS and on premise. CircleCI has been the go-to SaaS CI tool for many of our development teams, who needed a low-friction and easy-to-setup build and deployment pipeline. CircleCI version 2.0 supports workflows of build jobs, with fan-in and fan-out flows and manual gates, as well as mobile development. It allows developers to run the pipelines locally and easily integrates with Slack and other notification and alerting systems. We recommend you take a closer look at the security practices of CircleCI, just as you would with any other SaaS product that hosts your company’s assets.","blip_selector":"circleci","name":"CircleCI","display_name":"CircleCI","url":"/radar/platforms/circleci","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203066,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"Developing apps for iOS has become more streamlined over time, and SwiftUI moving into Adopt is a sign of that. Going beyond the general nature of SwiftUI and other common frameworks, The Composable Architecture (TCA) is both a library and an architectural style for building apps. It was designed over the course of a series of videos, and the authors state that they had composition, testing and ergonomics in mind, building on a foundation of ideas from The Elm Architecture and Redux. As expected, the narrow scope and opinionatedness is both a strength and a weakness of TCA. We feel that teams who don't have a lot of expertise in writing iOS apps, which are often teams who may be looking after multiple related codebases with different tech stacks, stand to benefit the most from using an opinionated framework like TCA, and we like the opinions expressed in TCA.","blip_selector":"the-composable-architecture","name":"The Composable Architecture","display_name":"The Composable Architecture","url":"/radar/languages-and-frameworks/the-composable-architecture","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203085,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"SpiceDB is a database system, inspired by Google's Zanzibar, for managing application permissions. With SpiceDB, you create a schema to model the permissions requirements and use the client library to apply the schema to one of the supported databases, insert data and query to efficiently answer questions like \"Does this user have access to this resource?\" or even the inverse \"What are all the resources this user has access to?\" We usually advocate separating the authorization policies from code, but SpiceDB takes it a step further by separating data from the policy and storing it as a graph to efficiently answer authorization queries. Because of this separation, you have to ensure that the changes in your application's primary data store are reflected in SpiceDB. Among other Zanzibar-inspired implementations, we find SpiceDB to be an interesting framework to assess for your authorization needs.","blip_selector":"spicedb","name":"SpiceDB","display_name":"SpiceDB","url":"/radar/languages-and-frameworks/spicedb","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203044,"quadrant":"platforms","volume_date":"2022-03","description":"Azure Pipeline templates allow you to remove duplication in your Azure Pipeline definition through two mechanisms. With \"includes\" templates, you can reference a template such that it will expand inline like a parameterized C++ macro, allowing a simple way of factoring out common configuration across stages, jobs and steps. With \"extends\" templates, you can define an outer shell with common pipeline configuration, and with the required template approval, you can fail the build if the pipeline doesn't extend certain templates, preventing malicious attacks against the pipeline configuration itself. Along with CircleCI Orbs and the newer GitHub Actions Reusable Workflows, Azure Pipeline templates are part of the trend of creating modularity in pipeline design across multiple platforms, and several of our teams have been happy using them.","blip_selector":"azure-pipeline-templates","name":"Azure Pipeline templates","display_name":"Azure Pipeline templates","url":"/radar/platforms/azure-pipeline-templates","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203012,"quadrant":"platforms","volume_date":"2022-03","description":"Apache Iceberg is an open table format for very large analytic data sets. Iceberg supports modern analytical data operations such as record-level insert, update, delete, time-travel queries, ACID transactions, hidden partitioning and full schema evolution. It supports multiple underlying file storage formats such as Apache Parquet, Apache ORC and Apache Avro. Many data-processing engines support Apache Iceberg, including SQL engines such as Dremio and Trino as well as (structured) streaming engines such as Apache Spark and Apache Flink.\n\nApache Iceberg falls in the same category as Delta Lake and Apache Hudi. They all more or less support similar features, but each differs in the underlying implementations and detailed feature lists. Iceberg is an independent format and is not native to any specific processing engine, hence it's supported by an increasing number of platforms, including AWS Athena and Snowflake. For the same reason, Apache Iceberg, unlike native formats such as Delta Lake, may not benefit from optimizations when used with Spark.","blip_selector":"apache-iceberg","name":"Apache Iceberg","display_name":"Apache Iceberg","url":"/radar/platforms/apache-iceberg","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911051,"quadrant":"techniques","volume_date":"2022-03","description":"Data mesh is a decentralized organizational and technical approach in sharing, accessing and managing data for analytics and ML. Its objective is to create a sociotechnical approach that scales out getting value from data as the organization's complexity grows and as the use cases for data proliferate and the sources of data diversify. Essentially, it creates a responsible data-sharing model that is in step with organizational growth and continuous change. In our experience, interest in the application of data mesh has grown tremendously. The approach has inspired many organizations to embrace its adoption and technology providers to repurpose their existing technologies for a mesh deployment. Despite the great interest and growing experience in data mesh, its implementations face high cost of integration. Moreover, its adoption remains limited to sections of larger organizations and technology vendors are distracting the organizations from the hard socio aspects of data mesh — decentralized data ownership and a federated governance operating model.\n\nThese ideas are explored in Data Mesh, Delivering Data-Driven Value at Scale, which guides practitioners, architects, technical leaders and decision makers on their journeys from traditional big data architecture to data mesh. It provides a complete introduction to data mesh principles and its constituents; it covers how to design a data mesh architecture, guide and execute a data mesh strategy and navigate organizational design to a decentralized data ownership model. The goal of the book is to create a new framework for deeper conversations and lead to the next phase in maturity of data mesh. | Increasingly, we see a mismatch between what data-driven organizations want to achieve and what the current data architectures and organizational structures allow. Organizations want to embed data-driven decision-making, machine learning and analytics into many aspects of their products and services and how they operate internally; essentially they want to augment every aspect of their operational landscape with data-driven intelligence. Yet, we still have a ways to go before we can embed analytical data, access to it and how it is managed into the business domains and operations. Today, every aspect of managing analytical data is externalized outside of the operational business domains to the data team and to the data management monoliths: data lakes and data warehouses. Data mesh is a decentralized sociotechnical approach to remove the dichotomy of analytical data and business operation. Its objective is to embed sharing and using analytical data into each operational business domain and close the gap between the operational and analytical planes. It's founded on four principles: domain data ownership, data as a product, self-serve data platform and computational federated governance.\n\nOur teams have been implementing the data mesh architecture; they've created new architectural abstractions such as the data product quantum to encapsulate the code, data and policy as an autonomous unit of analytical data sharing embedded into operational domains; and they've built self-serve data platform capabilities to manage the lifecycle of data product quanta in a declarative manner as described in Data Mesh. Despite our technical advances, we're still experiencing friction using the existing technologies in a data mesh topology, not to mention the resistance of business domains to embrace sharing and using data as a first-class responsibility in some organizations. | Data mesh marks a welcome architectural and organizational paradigm shift in how we manage big analytical data. The paradigm is founded on four principles: (1) domain-oriented decentralization of data ownership and architecture; (2) domain-oriented data served as a product; (3) self-serve data infrastructure as a platform to enable autonomous, domain-oriented data teams; and (4) federated governance to enable ecosystems and interoperability. Although the principles are intuitive and attempt to address many of the known challenges of previous centralized analytical data management, they transcend the available analytical data technologies. After building data mesh for multiple clients on top of the existing tooling, we learned two things: (a) there is a large gap in open-source or commercial tooling to accelerate implementation of data mesh (for example, implementation of a universal access model to time-based polyglot data which we currently custom build for our clients) and (b) despite the gap, it's feasible to use the existing technologies as the basic building blocks.\n\nNaturally, technology fit is a major component of implementing your organization's data strategy based on data mesh. Success, however, demands an organizational restructure to separate the data platform team, create the role of data product owner for each domain and introduce the incentive structures necessary for domains to own and share their analytical data as products. | Data mesh is an architectural and organizational paradigm that challenges the age-old assumption that we must centralize big analytical data to use it, have data all in one place or be managed by a centralized data team to deliver value. Data mesh claims that for big data to fuel innovation, its ownership must be federated among domain data owners who are accountable for providing their data as products (with the support of a self-serve data platform to abstract the technical complexity involved in serving data products);  it must also adopt a new form of federated governance through automation to enable interoperability of domain-oriented data products. Decentralization, along with interoperability and focus on the experience of data consumers, are key to the democratization of innovation using data.\n\nIf your organization has a large number of domains with numerous systems and teams generating data or a diverse set of data-driven use cases and access patterns, we suggest you assess data mesh. Implementation of data mesh requires investment in building a self-serve data platform and embracing an organizational change for domains to take on the long-term ownership of their data products, as well as an incentive structure that rewards domains serving and utilizing data as a product. | Data mesh is an architectural paradigm that unlocks analytical data at scale; rapidly unlocking access to an ever-growing number of distributed domain data sets, for a proliferation of consumption scenarios such as machine learning, analytics or data intensive applications across the organization. Data mesh addresses the common failure modes of the traditional centralized data lake or data platform architecture, with a shift from the centralized paradigm of a lake, or its predecessor, the data warehouse. Data mesh shifts to a paradigm that draws from modern distributed architecture: considering domains as the first-class concern, applying platform thinking to create a self-serve data infrastructure, treating data as a product and implementing open standardization to enable an ecosystem of interoperable distributed data products.","blip_selector":"data-mesh","name":"Data mesh","display_name":"Data mesh","url":"/radar/techniques/data-mesh","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203016,"quadrant":"techniques","volume_date":"2022-03","description":"Writing good documentation is an overlooked aspect of software development that is often left to the last minute and done in a haphazard way. Some of our teams have found documentation quadrants a handy way to ensure the right artifacts are being produced. This technique classifies artifacts along two axes: The first axis relates to the nature of the information, practical or theoretical; the second axis describes the context in which the artifact is used, studying or working. This defines four quadrants in which artifacts such as tutorials, how-to guides or reference pages can be placed and understood. This classification system not only ensures that critical artifacts aren't overlooked but also guides the presentation of the content. We've found this particularly useful for creating onboarding documentation that brings developers up to speed quickly when they join a new team.","blip_selector":"documentation-quadrants","name":"Documentation quadrants","display_name":"Documentation quadrants","url":"/radar/techniques/documentation-quadrants","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203011,"quadrant":"tools","volume_date":"2022-03","description":"AKHQ is a GUI for Apache Kafka that lets you manage topics, topics data, consumer groups and more. Some of our teams have found AKHQ to be an effective tool to watch the real-time status of a Kafka cluster. You can, for example, browse the topics on a cluster. For each topic, you can visualize the name, the number of messages stored, the disk size used, the time of the last record, the number of partitions, the replication factor with the in-sync quantity and the consumer group. With options for Avro and Protobuf deserialization, AKHQ can help you understand the flow of data in your Kafka environment.","blip_selector":"akhq","name":"AKHQ","display_name":"AKHQ","url":"/radar/tools/akhq","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110016,"quadrant":"tools","volume_date":"2022-03","description":"Cloud Carbon Footprint (CCF) is an open-source tool that uses cloud APIs to provide visualizations of estimated carbon emissions based on usage across AWS, GCP and Azure. The Thoughtworks team has successfully used the tool with several organizations, including energy technology companies, retailers, digital service providers and companies that use AI. Cloud platform providers realize that it's important to help their customers understand the carbon impact of using their services, so they've begun to build similar functionality themselves. Because CCF is cloud agnostic, it allows users to view energy usage and carbon emissions for multiple cloud providers in one place, while translating carbon footprints into real-world impact such as flights or trees planted.\n\nIn recent releases, CCF has begun to include Google Cloud and AWS-sourced optimization recommendations alongside potential energy and CO2 savings, as well as to support more cloud instance types such as GPU instances. Given the traction the tool has received and the continued addition of new features, we feel confident moving it to Trial. | Stakeholders increasingly expect businesses to account for the environmental externalities of their decisions, as evidenced by the rise of environmental, social and corporate governance (ESG) investing and employee activism around climate change. Migrating to the cloud offers the potential for more efficient energy usage — the cloud providers have much more scale to justify investment in green energy sources and R&D — but the downside of software abstractions for cloud users is that those abstractions also hide the energy impact as the actual data centers are hidden from view and financed by another company. Cloud Carbon Footprint, a new open-source tool, takes advantage of cloud APIs to provide visualizations of estimated carbon emissions based on usage across AWS, GCP and Azure. It uses heuristics like Etsy's Cloud Jewels to estimate energy usage and public data sources to convert energy usage into emissions based on the carbon intensity of the cloud region's underlying energy grid (GCP publishes this data already). The tool's dashboards act as information radiators, allowing decision makers to modify setups to cut costs and emissions at the same time. The linkage of cloud regions to carbon intensity of the underlying grid provides a nudge to switch dirty workloads to regions with greener energy sources.","blip_selector":"cloud-carbon-footprint","name":"Cloud Carbon Footprint","display_name":"Cloud Carbon Footprint","url":"/radar/tools/cloud-carbon-footprint","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203005,"quadrant":"platforms","volume_date":"2022-03","description":"We've seen increased interest in GitHub Actions since we first blipped it two Radars ago. With the release of reusable workflows, GitHub continues to evolve the product in a way that addresses some of its early shortcomings. Reusable workflows in Github Actions bring modularity to pipeline design, allowing parameterized reuse even across repositories (as long as the workflow repository is public). They support explicit passing of confidential values as secrets and can pass outputs to the calling job. With a few lines of YAML, GitHub Actions now gives you the type of flexibility you see with CircleCI Orbs or Azure Pipeline Templates, but without having to leave GitHub as a platform.","blip_selector":"reusable-workflows-in-github-actions","name":"Reusable workflows in Github Actions","display_name":"Reusable workflows in Github Actions","url":"/radar/platforms/reusable-workflows-in-github-actions","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203098,"quadrant":"techniques","volume_date":"2022-03","description":"Tactical forking is a technique that can assist with restructuring or migrating from monolithic codebases to microservices. Specifically, this technique offers one possible alternative to the more common approach of fully modularizing the codebase first, which in many circumstances can take a very long time or be very challenging to achieve. With tactical forking a team can create a new fork of the codebase and use that to address and extract one particular concern or area while deleting the unnecessary code. Use of this technique would likely be just one part of a longer-term plan for the overall monolith.","blip_selector":"tactical-forking","name":"Tactical forking","display_name":"Tactical forking","url":"/radar/techniques/tactical-forking","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1158,"quadrant":"tools","volume_date":"2022-03","description":"Lighthouse is a tool written by Google to assess web applications and web pages, collecting performance metrics and insights on good development practices. We've long advocated for performance testing as a first-class citizen, and the additions to Lighthouse that we mentioned five years ago certainly helped with that. Our thinking around architectural fitness functions created strong motivation for tools such as Lighthouse to be run in build pipelines. With the introduction of Lighthouse CI, it has become easier than ever to include Lighthouse in pipelines managed by various tools. | Lighthouse is a tool written by Google to assess web applications for adherence to Progressive Web App standards. This year's Lighthouse 2.0 release adds performance metrics and accessibility checks to the basic toolset. This added functionality has now been incorporated into the standard Chrome developer tools under the audit tab. Lighthouse 2.0 is yet another beneficiary of Chrome's headless mode. This provides an alternative to Pa11y and similar tools for running accessibility checks in a continuous integration pipeline, since the tool can be run from the command line or standalone as a Node.js application.","blip_selector":"lighthouse","name":"Lighthouse","display_name":"Lighthouse","url":"/radar/tools/lighthouse","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203018,"quadrant":"tools","volume_date":"2022-03","description":"GoReleaser is a tool that automates the process of building and releasing a Go project for different architectures via multiple repositories and channels, a common need for Go projects targeting different platforms. You run the tool either from your local machine or via CI, with the tool available via several CI services thus minimizing set-up and maintenance. GoReleaser takes care of build, packaging, publishing and announcement of each release and supports different combinations of package format, package repository and source control. Although it's been around for a few years, we're surprised that more teams are not using it. If you're regularly releasing a Go codebase, this tool is worth assessing.","blip_selector":"goreleaser","name":"GoReleaser","display_name":"GoReleaser","url":"/radar/tools/goreleaser","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911027,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"We've had enough experience with Testcontainers that we think it's a useful default option for creating a reliable environment for running tests. It's a library, ported to multiple languages, that Dockerizes common test dependencies — including various types of databases, queuing technologies, cloud services and UI testing dependencies like web browsers — with the ability to run custom Dockerfiles when needed. It works well with test frameworks like JUnit, is flexible enough to let users manage the container lifecycle and advanced networking and quickly sets up an integrated test environment. Our teams have consistently found this library of programmable, lightweight and disposable containers to make functional tests more reliable. | Creating reliable environments for running automated tests is a perennial problem, particularly as the number of components that modern systems depend on keeps increasing. Testcontainers is a Java library that helps mitigate this challenge by managing dockerized dependencies for your tests. This is particularly useful for spinning up repeatable database instances or similar infrastructure, but it can also be used in web browsers for UI testing. Our teams have found this library to be helpful for making integration tests more reliable with these programmable, lightweight and disposable containers.","blip_selector":"testcontainers","name":"Testcontainers","display_name":"Testcontainers","url":"/radar/languages-and-frameworks/testcontainers","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203065,"quadrant":"tools","volume_date":"2022-03","description":"While linting is an ancient practice in the software world, it's had slower adoption in the data world. SQLFluff is a cross-dialect SQL linter written in Python that ships with a simple command line interface (CLI), making it easy to incorporate into a CI/CD pipeline. If you're comfortable with the default conventions, then SQLFluff works without any additional configuration after installing it and will enforce a strongly opinionated set of formatting standards; setting your own conventions involves adding a configuration dotfile. The CLI can automatically fix certain classes of violations that involve formatting concerns like whitespace or uppercasing of keywords. SQLFluff is still new, but we're excited to see SQL getting some attention in the linting world.","blip_selector":"sqlfluff","name":"SQLFluff","display_name":"SQLFluff","url":"/radar/tools/sqlfluff","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203008,"quadrant":"techniques","volume_date":"2022-03","description":"The need to respond quickly to customer insights has driven increasing adoption of event-driven architectures and stream processing. Frameworks such as Spark, Flink or Kafka Streams offer a paradigm where simple event consumers and producers can cooperate in complex networks to deliver real-time insights. But this programming style takes time and effort to master and when implemented as single-point applications, it lacks interoperability. Making stream processing work universally on a large scale can require a significant engineering investment. Now, a new crop of tools is emerging that offers the benefits of stream processing to a wider, established group of developers who are comfortable using SQL to implement analytics. Standardizing on SQL as the universal streaming language lowers the barrier for implementing streaming data applications. Tools like ksqlDB and Materialize help transform these separate applications into unified platforms. Taken together, a collection of SQL-based streaming applications across an enterprise might constitute a streaming data warehouse.","blip_selector":"the-streaming-data-warehouse","name":"The streaming data warehouse","display_name":"The streaming data warehouse","url":"/radar/techniques/the-streaming-data-warehouse","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911018,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"When Apple introduced SwiftUI a few years ago, it was a big step forward for implementing user interfaces on all kinds of devices made by Apple. From the beginning, we liked the declarative, code-centric approach and the reactive programming model provided by Combine. We did notice, though, that writing a lot of view tests, which you still need with a model—view—viewmodel (MVVM) pattern, was not really sensible with the XCUITest automation framework provided by Apple. This gap has been closed by ViewInspector. A final hurdle was the minimum OS version required. At the time of release, only the very latest versions of iOS and macOS could run applications written with SwiftUI, but because of Apple’s regular cadence of updates, SwiftUI apps can now run on practically all versions of macOS and iOS that receive security updates. | Apple has taken a big step forward with their new SwiftUI framework for implementing user interfaces on the macOS and iOS platforms. We like that SwiftUI moves beyond the somewhat kludgy relationship between Interface Builder and Xcode and adopts a coherent, declarative and code-centric approach. You can now view your code and the resulting visual interface side by side in Xcode 11, making for a much better developer experience. The SwiftUI framework also draws inspiration from the React.js world that has dominated web development in recent years. Immutable values in view models and an asynchronous update mechanism make for a unified reactive programming model. This gives developers an entirely native alternative to similar reactive frameworks such as React Native or Flutter. SwiftUI definitely represents the future of Apple UI development, and although new, it has shown its benefits. We've been having great experience with it — and its shallow learning curve. It's worth noting that you should know your customer's use case before jumping into using SwiftUI, given that it doesn't support iOS 12 or below. | Apple has taken a big step forward with their new SwiftUI framework for implementing user interfaces on macOS and iOS platforms. We like that SwiftUI moves beyond the somewhat kludgy relationship between Interface Builder and XCode and adopts a coherent, declarative and code-centric approach. You can now view your code and the resulting visual interface side by side in XCode 11, making for a much better developer experience. The SwiftUI framework also draws inspiration from the React.js world that has dominated web development in recent years. Immutable values in view models and an asynchronous update mechanism make for a unified reactive programming model. This gives developers an entirely native alternative to similar reactive frameworks such as React Native or Flutter. Although SwiftUI definitely represents the future of Apple UI development, it is quite new and it will take time to smooth out the rough edges. We look forward to improved documentation and a community of developers who can establish a set of practices for testing and other engineering concerns.","blip_selector":"swiftui","name":"SwiftUI","display_name":"SwiftUI","url":"/radar/languages-and-frameworks/swiftui","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110008,"quadrant":"techniques","volume_date":"2022-03","description":"We're continuing to see increasing use of the Kubernetes Operator pattern for purposes other than managing applications deployed on the cluster. Using the Operator pattern for nonclustered resources takes advantage of custom resource definitions and the event-driven scheduling mechanism implemented in the Kubernetes control plane to manage activities that are related to yet outside of the cluster. This technique builds on the idea of Kube-managed cloud services and extends it to other activities, such as continuous deployment or reacting to changes in external repositories. One advantage of this technique over a purpose-built tool is that it opens up a wide range of tools that either come with Kubernetes or are part of the wider ecosystem. You can use commands such as diff, dry-run or apply to interact with the operator's custom resources. Kube's scheduling mechanism makes development easier by eliminating the need to orchestrate activities in the proper order. Open-source tools such as Crossplane, Flux and Argo CD take advantage of this technique, and we expect to see more of these emerge over time. Although these tools have their use cases, we're also starting to see the inevitable misuse and overuse of this technique and need to repeat some old advice: Just because you can do something with a tool doesn't mean you should. Be sure to rule out simpler, conventional approaches before creating a custom resource definition and taking on the complexity that comes with this approach. | We're seeing increasing use of the Kubernetes Operator pattern for purposes other than managing applications deployed on the cluster. Using the operator pattern for nonclustered resources takes advantage of custom resource definitions and the event-driven scheduling mechanism implemented in the Kubernetes control plane to manage activities that are related to yet outside of the cluster. This technique builds on the idea of Kube-managed cloud services and extends it to other activities, such as continuous deployment or reacting to changes in external repositories. One advantage of this technique over a purpose-built tool is that it opens up a wide range of tools that either come with Kubernetes or are part of the wider ecosystem. You can use commands such as diff, dry-run or apply to interact with the operator's custom resources. Kube's scheduling mechanism makes development easier by eliminating the need to orchestrate activities in the proper order. Open-source tools such as Crossplane, Flux and ArgoCD take advantage of this technique and we expect to see more of these emerge over time.","blip_selector":"operator-pattern-for-nonclustered-resources","name":"Operator pattern for nonclustered resources","display_name":"Operator pattern for nonclustered resources","url":"/radar/techniques/operator-pattern-for-nonclustered-resources","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202110040,"quadrant":"techniques","volume_date":"2022-03","description":"A single team remote wall is a simple technique to reintroduce the team wall virtually. We recommend that distributed teams adopt this approach; one of the things we hear from teams who moved to remote working is that they miss having the physical team wall. This was a single place where all the various story cards, tasks, status and progress could be displayed, acting as an information radiator and hub for the team. The wall acted as an integration point with the actual data being stored in different systems. As teams have become remote, they've had to revert to looking into the individual source systems and getting an \"at a glance\" view of a project has become very difficult. While there might be some overhead in keeping this up-to-date, we feel the benefits to the team are worth it. For some teams, updating the physical wall formed part of the daily \"ceremonies\" the team did together, and the same can be done with a remote wall. | With the increased use of remote distributed teams, one of the things we hear people have missed having is the physical team wall. This is a single place where all the various story cards, tasks, status and progress can be displayed, acting as an information radiator and hub for the team. Often the wall was an integration point with the actual data being stored in different systems. As teams have become remote, they've had to revert to looking into the individual source systems and getting an \"at a glance\" view of a project has become very difficult. A single team remote wall is a simple technique to reintroduce the team wall virtually. While there might be some overhead in keeping this up-to-date, we feel the benefits to the team are worth it. For some teams, updating the physical wall formed part of the daily \"ceremonies\" the team did together, and the same can be done with a remote wall.","blip_selector":"single-team-remote-wall","name":"Single team remote wall","display_name":"Single team remote wall","url":"/radar/techniques/single-team-remote-wall","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203054,"quadrant":"techniques","volume_date":"2022-03","description":"We recommend organizations assess inclusive design as a way of making sure accessibility is treated as a first-class requirement. All too often requirements around accessibility and inclusivity are ignored until just before, if not just after, the release of software. The cheapest and simplest way to accommodate these requirements, while also providing early feedback to teams, is to incorporate them fully into the development process. In the past, we've highlighted techniques that perform a \"shift-left\" for security and cross-functional requirements; one perspective on this technique is that it achieves the same goal for accessibility.","blip_selector":"inclusive-design","name":"Inclusive design","display_name":"Inclusive design","url":"/radar/techniques/inclusive-design","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203040,"quadrant":"platforms","volume_date":"2022-03","description":"actions-runner-controller is a Kubernetes controller that operates self-hosted runners for GitHub Actions on your Kubernetes cluster. With this tool you create a runner resource on Kubernetes, and it will run and operate the self-hosted runner. Self-hosted runners are helpful in scenarios where the job that your GitHub Actions runs needs to access resources that are either not accessible to GitHub cloud runners or have specific operating system and environmental requirements that are different from what GitHub provides. In those cases where you have a Kubernetes cluster, you can run your self-hosted runners as a Kubernetes pod, with the ability to scale up or down hooking into GitHub webhook events. actions-controller-runner is lightweight and scalable.","blip_selector":"actions-runner-controller","name":"actions-runner-controller","display_name":"actions-runner-controller","url":"/radar/platforms/actions-runner-controller","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110077,"quadrant":"tools","volume_date":"2022-03","description":"In our previous Radar, we featured two tools that search and replace code using an abstract syntax tree (AST) representation, Comby and Sourcegraph. Although they share some similarities, they also differ in several ways. Sourcegraph is a commercial tool (with a 10-user free tier). It's particularly suited for searching, navigating or cross-referencing in large codebases, with an emphasis on an interactive developer experience. In contrast, Comby is a lightweight open-source command-line tool for automating repetitive tasks. Because Sourcegraph is a hosted service, it also has the ability to continuously monitor code bases and send alerts when a match occurs. Now that we've gained more experience with Sourcegraph, we decided to move it into the Trial ring to reflect our positive experience — which doesn't mean that Sourcegraph is better than Comby. Each tool focuses on a different niche. | Another abstract syntax tree–based code search tool that received our attention is Sourcegraph. In contrast to Comby, which is open source, Sourcegraph is a commercial tool (with a 10-user free tier). Sourcegraph is particularly suited for searching, navigating or cross-referencing in large codebases. The cloud-hosted version can be accessed through Sourcegraph's website and is designed to search publicly available open-source repositories. Whereas Comby is a lightweight command-line tool for automating repetitive tasks, Sourcegraph's emphasis is on interactive developer tools for understanding and navigating large code bases. Unlike Comby's sed-like interface, Sourcegraph's automated code rewriting capability is driven from a UI, allowing users to review changes before they're made. Because Sourcegraph is a hosted service, it also has the ability to continuously monitor code bases and send alerts when a match occurs.","blip_selector":"sourcegraph","name":"Sourcegraph","display_name":"Sourcegraph","url":"/radar/tools/sourcegraph","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203060,"quadrant":"techniques","volume_date":"2022-03","description":"Service mesh is usually implemented as a reverse-proxy process, aka sidecar, deployed alongside each service instance. Although these sidecars are lightweight processes, the overall cost and operational complexity of adopting service mesh increases with every new instance of the service requiring another sidecar. However, with the advancements in eBPF, we're observing a new service mesh without sidecar approach where the functionalities of the mesh are safely pushed down to the OS kernel, thereby enabling services in the same node to communicate transparently via sockets without the need of additional proxies. You can try this with Cilium service mesh and simplify the deployment from one proxy-per-service to one proxy-per-node. We're intrigued by the capabilities of eBPF and find this evolution of service mesh to be important to assess.","blip_selector":"service-mesh-without-sidecar","name":"Service mesh without sidecar","display_name":"Service mesh without sidecar","url":"/radar/techniques/service-mesh-without-sidecar","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203013,"quadrant":"platforms","volume_date":"2022-03","description":"Blueboat is a multitenant platform for serverless web applications. It leverages the popular V8 JavaScript engine and implements commonly used web application libraries natively in Rust for security and performance. You can think of Blueboat as an alternative to CloudFlare Workers or Deno Deploy but with an important distinction — you have to operate and manage the underlying infrastructure. That said, we recommend you carefully assess Blueboat for your on-prem serverless needs.","blip_selector":"blueboat","name":"Blueboat","display_name":"Blueboat","url":"/radar/platforms/blueboat","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203059,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"While many tools support multipackage development in the node.js world, npm 7 adds direct support with the addition of npm workspaces. Managing related packages together facilitates development, allowing you, for example, to store multiple related libraries in a single repo. With npm workspaces, once you add a configuration in a top-level package.json file to refer to one or more nested package.json files, commands like npm install work across multiple packages, symlinking the dependent source packages into the root node_modules directory. Other npm commands are also now workspace aware, allowing you, for example, to execute npm run and npm test commands across multiple packages with a single command. Having that flexibility out of the box decreases the need for some teams to reach for another package manager.","blip_selector":"npm-workspaces","name":"npm workspaces","display_name":"npm workspaces","url":"/radar/languages-and-frameworks/npm-workspaces","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110044,"quadrant":"tools","volume_date":"2022-03","description":"cert-manager is a tool to manage your X.509 certificates within your Kubernetes cluster. It models certificates and issuers as first-class resource types and provides certificates as a service securely to developers and applications working within the Kubernetes cluster. The obvious choice when using the Kubernetes default ingress controller, it's also recommended for others and much preferred over hand-rolling your own certificate management. Several of our teams have been using cert-manager extensively, and we've also found that its usability has much improved in the past few months. | cert-manager is a tool to manage your X.509 certificates within your Kubernetes cluster. It models certificates and issuers as first-class resource types and provides certificates as a service securely to developers and applications working within the Kubernetes cluster. With built-in support for Let's Encrypt, HashiCorp Vault and Venafi, cert-manager is an interesting tool to assess for certificate management.","blip_selector":"cert-manager","name":"cert-manager","display_name":"cert-manager","url":"/radar/tools/cert-manager","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203046,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"When building an app with React Native you sometimes find yourself having to create your own modules. For example, we've encountered this need when building a UI component library for a React Native app. Creating such a module project isn't straightforward, and our teams report success using Bob to automate this task. Bob provides a CLI to create the scaffolding for different targets. The scaffolding is not limited to core functionality but, optionally, can include example code, linters, build pipeline configuration and other features.","blip_selector":"bob","name":"Bob","display_name":"Bob","url":"/radar/languages-and-frameworks/bob","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203061,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"Executing a scheduled task once and only once in a cluster of distributed processors is a relatively common requirement. For example, the situation might arise when ingesting a batch of data, sending a notification or performing some regular cleanup activity. But this is a notoriously difficult problem. How does a group of processes cooperate reliably over laggy and less reliable networks? Some kind of locking mechanism is required to coordinate actions across the cluster. Fortunately, a variety of distributed stores can implement a lock. Systems like ZooKeeper and Consul as well as databases such as DynamoDB or Couchbase have the necessary underlying mechanisms to manage consensus across the cluster. ShedLock is a small library for taking advantage of these providers in your own Java code, if you're looking to implement your own scheduled tasks. It provides an API for acquiring and releasing locks as well as connectors to a wide variety of lock providers. If you're writing your own distributed tasks but don't want to take on the complexity of an entire orchestration platform like Kubernetes, ShedLock is worth a look.","blip_selector":"shedlock","name":"ShedLock","display_name":"ShedLock","url":"/radar/languages-and-frameworks/shedlock","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202203003,"quadrant":"techniques","volume_date":"2022-03","description":"We previously featured platform engineering product teams in Adopt as a good way for internal platform teams to operate, thus enabling delivery teams to self-service deploy and operate systems with reduced lead time and stack complexity. Unfortunately we're seeing the \"platform team\" label applied to teams dedicated to projects that don't have clear outcomes or a well-defined set of customers. As a result, these miscellaneous platform teams, as we call them, struggle to deliver due to high cognitive loads and a lack of clearly aligned priorities as they're dealing with a miscellaneous collection of unrelated systems. They effectively become just another general support team for things that don't fit or that are unwanted elsewhere. We continue to believe platform engineering product teams focused around a clear and well-defined (internal) product offer a better set of outcomes.","blip_selector":"miscellaneous-platform-teams","name":"Miscellaneous platform teams","display_name":"Miscellaneous platform teams","url":"/radar/techniques/miscellaneous-platform-teams","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203022,"quadrant":"tools","volume_date":"2022-03","description":"kube-score is a tool that does static code analysis of your Kubernetes object definitions. The output is a list of recommendations for what you can improve to make your application more secure and resilient. It has a list of predefined checks which includes best practices such as running containers with non-root privileges and correctly specifying resource limits. It's been around for some time, and we've used it in a few projects as part of a CD pipeline for Kubernetes manifests. A major drawback of kube-score is that you can't add custom policies. We typically supplement it with tools like Conftest in these cases.","blip_selector":"kube-score","name":"kube-score","display_name":"kube-score","url":"/radar/tools/kube-score","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":513,"quadrant":"platforms","volume_date":"2022-03","description":"When we originally blipped Couchbase in 2013, it was seen primarily as a persistent cache that evolved from a merger of Membase and CouchDB. Since then, it has undergone steady improvement and an ecosystem of related tools and commercial offerings has grown up around it. Among the additions to the product suite are Couchbase Mobile and the Couchbase Sync Gateway. These features work together to keep persistent data on edge devices up-to-date even when the device is offline for periods of time due to intermittent connectivity. As these devices proliferate, we see increasing need for embedded persistence that continues to work whether or not the device happens to be connected. Recently, one of our teams evaluated Couchbase for its offline sync capability and found that this off-the-shelf capability saved them considerable effort that they otherwise would have had to invest themselves. | Couchbase is a persistent cache with auto-sharding features, master-less clusters and replicated data to avoid cachemisses. Because it supports the Memcached protocol, it allows drop-in replacement for Memcached based systems.","blip_selector":"couchbase","name":"Couchbase","display_name":"Couchbase","url":"/radar/platforms/couchbase","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203053,"quadrant":"tools","volume_date":"2022-03","description":"GitHub Codespaces allows developers to create development environments in the cloud and access them through an IDE as though the environment were local. GitHub isn't the first company to implement this idea; we previously blipped about Gitpod. We like that Codespaces allows environments to be standardized by using dotfiles configuration, making it quicker to onboard new team members, and that they offer VMs with up to 32 cores and 64GB memory. These VMs can be spun up in under ten seconds, potentially offering environments more powerful than a developer laptop.","blip_selector":"github-codespaces","name":"GitHub Codespaces","display_name":"GitHub Codespaces","url":"/radar/tools/github-codespaces","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203009,"quadrant":"platforms","volume_date":"2022-03","description":"VerneMQ is an open-source, high-performance, distributed MQTT broker. We've blipped other MQTT brokers in the past like Mosquitto and EMQ. Like EMQ and RabbitMQ, VerneMQ is also based on Erlang/OTP which makes it highly scalable. It scales horizontally and vertically on commodity hardware to support a high number of concurrent publishers and consumers while maintaining low latency and fault tolerance. In our internal benchmarks, we've been able to achieve a few million concurrent connections in a single cluster. While it's not new, we've used it in production for some time now, and it has worked well for us.","blip_selector":"vernemq","name":"VerneMQ","display_name":"VerneMQ","url":"/radar/platforms/vernemq","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203030,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"sqlc is a compiler that generates type-safe idiomatic Go code from SQL. Unlike other approaches based on object-relational mapping (ORM), you continue to write plain SQL for your needs. Once invoked, sqlc checks the correctness of the SQL and generates performant Go code, which can be directly called from the rest of the application. With stable support for both PostgreSQL and MySQL, sqlc is worth a look, and we encourage you to assess it.","blip_selector":"sqlc","name":"sqlc","display_name":"sqlc","url":"/radar/languages-and-frameworks/sqlc","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203047,"quadrant":"tools","volume_date":"2022-03","description":"By now many organizations have created sprawling landscapes of services in the cloud. Of course, this is only possible when using infrastructure as code and mature tooling. We still like Terraform, not the least because of its rich and growing ecosystem. However, the lack of abstractions in HCL, Terraform's default configuration language, effectively creates a glass ceiling. Using Terragrunt pushes that up a bit further, but more and more often our teams find themselves longing for the abstractions afforded by modern programming languages. Cloud Development Kit for Terraform (CDKTF), which resulted from a collaboration between AWS's CDK team and Hashicorp, makes it possible for teams to use several programming languages, including TypeScript and Java, to define and provision infrastructure. With this approach it follows the lead of Pulumi while remaining in the Terraform ecosystem. We've had good experiences with CDKTF but have decided to keep it in the Assess ring until it moves out of beta.","blip_selector":"cdktf","name":"CDKTF","display_name":"CDKTF","url":"/radar/tools/cdktf","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203027,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"We witnessed the migration from server-side rendering website to single-page application in the browser, now the pendulum of web development seems to swing back to the middle. Remix is one such example. It's a full-stack JavaScript framework. It provides fast page loads by leveraging distributed systems and native browsers instead of clumsy static builds. It has made some optimizations on nested routing and page loading, which makes page rendering seem especially fast. Many people will compare Remix with Next.js, which is similarly positioned. We're glad to see such frameworks cleverly combining the browser run time with the server run time to provide a better user experience.","blip_selector":"remix","name":"Remix","display_name":"Remix","url":"/radar/languages-and-frameworks/remix","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203071,"quadrant":"techniques","volume_date":"2022-03","description":"A transitional architecture is a useful practice used when replacing legacy systems. Much like scaffolding might be built, reconfigured and finally removed during construction or renovation of a building, you often need interim architectural steps during legacy displacement. Transitional architectures will be removed or replaced later on, but they're not just throwaway work given the important role they play in reducing risk and allowing a difficult problem to be broken into smaller steps. Thus they help with avoiding the trap of defaulting to a \"big bang\" legacy replacement approach, because you cannot make smaller interim steps line up with a final architectural vision. Care is needed to make sure the architectural \"scaffolding\" is eventually removed, lest it just become technical debt later on.","blip_selector":"transitional-architecture","name":"Transitional architecture","display_name":"Transitional architecture","url":"/radar/techniques/transitional-architecture","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203025,"quadrant":"tools","volume_date":"2022-03","description":"NUKE is a build system for .NET and an alternative to either the traditional MSBuild or Cake and Fake which we've featured previously in the Radar. NUKE represents build instructions as a C# DSL, making it easy to learn and with good IDE support. In our experience, NUKE made it really simple to build automation for .NET projects. We like the accurate static code checks and hints. We also like that we can use any NuGet package seamlessly and that the automation code can be compiled to avoid problems at runtime. NUKE isn't new, but its novel approach — using a C# DSL — and our positive overall experience prompted us to include it here.","blip_selector":"nuke","name":"NUKE","display_name":"NUKE","url":"/radar/tools/nuke","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110092,"quadrant":"platforms","volume_date":"2022-03","description":"Kubernetes natively supports a key-value object known as a secret. However, by default, Kubernetes secrets aren't really secret. They're handled separately from other key-value data so that precautions or access control can be applied separately. There is support for encrypting secrets before they are stored in etcd, but the secrets start out as plain text fields in configuration files. Sealed Secrets is a combination operator and command-line utility that uses asymmetric keys to encrypt secrets so that they can only be decrypted by the controller in the cluster. This process ensures that the secrets won't be compromised while they sit in the configuration files that define a Kubernetes deployment. Once encrypted, these files can be safely shared or stored alongside other deployment artifacts. | Kubernetes natively supports a key-value object known as a secret. However, by default, Kubernetes secrets aren't really secret. They're handled separately from other key-value data so that precautions or access control can be applied separately. There is support for encrypting secrets before they are stored in etcd, but the secrets start out as plain text fields in configuration files. Sealed Secrets is a combination operator and command-line utility that uses asymmetric keys to encrypt secrets so that they can only be decrypted by the controller in the cluster. This process ensures that the secrets won't be compromised while they sit in the configuration files that define a Kubernetes deployment. Once encrypted, these files can be safely shared or stored alongside other deployment artifacts.","blip_selector":"sealed-secrets","name":"Sealed Secrets","display_name":"Sealed Secrets","url":"/radar/platforms/sealed-secrets","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203014,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"We've been debating the merits of cross-platform mobile development tools for nearly as long as we've been publishing the Technology Radar. We first noted a new generation of tools in 2011 when blipping about cross-mobile platforms. Although we were skeptical of them at first, these tools have been perfected and widely adopted over the years. And nobody can debate the enduring popularity and usefulness of React Native. Capacitor is the latest generation of a line of tools starting with PhoneGap, then renamed to Apache Cordova. Capacitor is a complete rewrite from Ionic that embraces the progressive web app style for stand-alone applications. So far, our developers like that they can address web, iOS and Android applications with a single code base and that they can manage the native platforms separately with access to the native APIs when necessary. Capacitor offers an alternative to React Native, which has many years of cross-platform experience behind it.","blip_selector":"capacitor","name":"Capacitor","display_name":"Capacitor","url":"/radar/languages-and-frameworks/capacitor","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203049,"quadrant":"platforms","volume_date":"2022-03","description":"In the increasingly crowded space that is the enterprise data catalog market, our teams have enjoyed working with Collibra. They liked the deployment flexibility of either a SaaS or self-hosted instance, the wide range of functionality included out of the box, including data governance, lineage, quality and observability. Users also have the option to use a smaller subset of capabilities required by a more decentralized approach such as a data mesh. The real feather in its cap has been their often overlooked customer support, which our people have found to be collaborative and supportive. Of course, there's a tension between simple data catalogs and more full featured enterprise platforms, but so far the teams using it are happy with how Collibra has supported their needs.","blip_selector":"collibra","name":"Collibra","display_name":"Collibra","url":"/radar/platforms/collibra","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203087,"quadrant":"tools","volume_date":"2022-03","description":"One of the key elements of improving \"supply chain security\" is using a Software Bill of Materials (SBOM), which is why publishing an SBOM along with the software artifact is increasingly important. Syft is a CLI tool and Go library for generating an SBOM from container images and file systems. It can generate the SBOM output in multiple formats, including JSON, CycloneDX and SPDX. The SBOM output of Syft can be used by Grype for vulnerability scanning. One way to publish the generated SBOM along with the image is to add it as an attestation using Cosign. This allows consumers of the image to verify the SBOM and to use it for further analysis.","blip_selector":"syft","name":"Syft","display_name":"Syft","url":"/radar/tools/syft","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203055,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"We don't routinely feature new versions of languages, but we wanted to highlight the new long-term support (LTS) version of Java, version 17. While there are promising new features, such as the preview of pattern matching, it's the switch to the new LTS process that should interest many organizations. We recommend organizations assess new releases of Java as and when they become available, making sure they adopt new features and versions as appropriate. Surprisingly many organizations do not routinely adopt newer versions of languages even though regular updates help keep things small and manageable. Hopefully the new LTS process, alongside organizations moving to regular updates, will help avoid the \"too expensive to update\" trap that ends with production software running on an end-of-life version of Java.","blip_selector":"java-17","name":"Java 17","display_name":"Java 17","url":"/radar/languages-and-frameworks/java-17","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005040,"quadrant":"platforms","volume_date":"2022-03","description":"Since we last blipped about Google BigQuery ML, more sophisticated models such as Deep Neural Networks and AutoML Tables have been added by connecting BigQuery ML with TensorFlow and Vertex AI as its backend. BigQuery has also introduced support for time series forecasting. One of our concerns previously was explainability. Earlier this year, BigQuery Explainable AI was announced for general availability, taking a step in addressing this. We can also export BigQuery ML models to Cloud Storage as a Tensorflow SavedModel and use them for online prediction. There remain trade-offs like ease of \"continuous delivery for machine learning\" but with its low barrier to entry, BigQuery ML remains an attractive option, particularly when the data already resides in BigQuery. | Often training and predicting outcomes from machine learning models require code to take the data to the model. Google BigQuery ML inverts this by bringing the model to the data. Google BigQuery is a data warehouse designed to serve large-scale queries using SQL, for analytical use cases. Google BigQuery ML extends this function and its SQL interface to create, train and evaluate machine learning models using its data sets; and eventually run model predictions to create new BigQuery data sets. It supports a limited set of models out of the box, such as linear regression for forecasting or binary and multiclass regression for classification. It also supports, with limited functionality, importing previously trained TensorFlow models. Although BigQuery ML and its SQL-based approach lower the bar for using machine learning to make predictions and recommendations, particularly for quick explorations, this comes with a difficult trade-off: compromising on other aspects of model training such as ethical bias testing, explainability and continuous delivery for machine learning.","blip_selector":"google-bigquery-ml","name":"Google BigQuery ML","display_name":"Google BigQuery ML","url":"/radar/platforms/google-bigquery-ml","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202005105,"quadrant":"tools","volume_date":"2022-03","description":"For our projects using Terraform, tfsec has quickly become a default static analysis tool to detect potential security risks. It's easy to integrate into a CI pipeline and has a growing library of checks against all of the major cloud providers and platforms like Kubernetes. Given its ease of use, we believe tfsec could be a good addition to any Terraform project. | Security is everyone's concern, and capturing risks early is always better than facing problems later on. In the infrastructure as code space — where Terraform has been an obvious choice to manage cloud environments — we now also have tfsec, a static analysis tool that scans Terraform templates to find potential security issues. Our teams have been using tfsec quite successfully. The tool is easy to set up and use, which makes it a great choice for any development team determined to mitigate security risks to prevent breaches before they happen. Its preset rules for different cloud providers, including AWS and Azure, compliment the benefits that tfsec brings to the teams that use Terraform. | Security is everyone's concern and capturing risks early is always better than facing problems later on. In the infrastructure as code space, where Terraform is an obvious choice to manage cloud environments, we now also have tfsec, which is a static analysis tool that helps to scan Terraform templates and find any potential security issues. It comes with preset rules for different cloud providers including AWS and Azure. We always like tools that help to mitigate security risks, and tfsec not only excels in identifying security risks, it's also easy to install and use.","blip_selector":"tfsec","name":"tfsec","display_name":"tfsec","url":"/radar/tools/tfsec","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1298,"quadrant":"techniques","volume_date":"2022-03","description":"To measure software delivery performance, more and more organizations are defaulting to the four key metrics as defined by the DORA research program: change lead time, deployment frequency, mean time to restore (MTTR) and change fail percentage. This research and its statistical analysis have shown a clear link between high-delivery performance and these metrics; they provide a great leading indicator for how a delivery organization as a whole is doing.\n\nWe're still big proponents of these metrics, but we've also learned some lessons. We're still observing misguided approaches with tools that help teams measure these metrics based purely on their continuous delivery (CD) pipelines. In particular when it comes to the stability metrics (MTTR and change fail percentage), CD pipeline data alone doesn't provide enough information to determine what a deployment failure with real user impact is. Stability metrics only make sense if they include data about real incidents that degrade service for the users.\n\nWe recommend always to keep in mind the ultimate intention behind a measurement and use it to reflect and learn. For example, before spending weeks building up sophisticated dashboard tooling, consider just regularly taking the DORA quick check in team retrospectives. This gives the team the opportunity to reflect on which capabilities they could work on to improve their metrics, which can be much more effective than overdetailed out-of-the-box tooling. Keep in mind that these four key metrics originated out of the organization-level research of high-performing teams, and the use of these metrics at a team level should be a way to reflect on their own behaviors, not just another set of metrics to add to the dashboard. | To measure software delivery performance, more and more organizations are turning to the four key metrics as defined by the DORA research program: change lead time, deployment frequency, mean time to restore (MTTR) and change fail percentage. This research and its statistical analysis have shown a clear link between high delivery performance and these metrics; they provide a great leading indicator for how a team, or even a whole delivery organization, is doing.\n\nWe're still big proponents of these metrics, but we've also learned some lessons since we first started monitoring them. And we're increasingly seeing misguided measurement approaches with tools that help teams measure these metrics based purely on their continuous delivery (CD) pipelines. In particular when it comes to the stability metrics (MTTR and change fail percentage), CD pipeline data alone doesn't provide enough information to determine what a deployment failure with real user impact is. Stability metrics only make sense if they include data about real incidents that degrade service for the users.\n\nAnd as with all metrics, we recommend to always keep in mind the ultimate intention behind a measurement and use them to reflect and learn. For example, before spending weeks to build up sophisticated dashboard tooling, consider just regularly taking the DORA quick check in team retrospectives. This gives the team the opportunity to reflect on which capabilities they could work on to improve their metrics, which can be much more effective than overdetailed out-of-the-box tooling. | The thorough State of DevOps reports have focused on data-driven and statistical analysis of high-performing organizations. The result of this multiyear research, published in Accelerate, demonstrates a direct link between organizational performance and software delivery performance. The researchers have determined that only four key metrics differentiate between low, medium and high performers: lead time, deployment frequency, mean time to restore (MTTR) and change fail percentage. Indeed, we've found that these four key metrics are a simple and yet powerful tool to help leaders and teams focus on measuring and improving what matters. A good place to start is to instrument the build pipelines so you can capture the four key metrics and make the software delivery value stream visible. GoCD pipelines, for example, provide the ability to measure these four key metrics as a first-class citizen of the GoCD analytics. | The State of DevOps report, first published in 2014, states that high-performing teams create high-performing organizations. Recently, the team behind the report released Accelerate, which describes the scientific method they've used in the report. A key takeaway of both are the four key metrics to support software delivery performance: lead time, deployment frequency, mean time to restore (MTTR), and change fail percentage. As a consultancy that has helped many organizations transform, these metrics have come up time and time again as a way to help organizations determine whether they're improving the overall performance. Each metric creates a virtuous cycle and focuses the teams on continuous improvement: to reduce lead time, you reduce wasteful activities which, in turn, lets you deploy more frequently; deployment frequency forces your teams to improve their practices and automation; your speed to recover from failure is improved by better practices, automation and monitoring which reduces the frequency of failures.","blip_selector":"four-key-metrics","name":"Four key metrics","display_name":"Four key metrics","url":"/radar/techniques/four-key-metrics","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203004,"quadrant":"tools","volume_date":"2022-03","description":"Chrome Recorder panel is a preview feature in Google Chrome 97 that allows for simple record and playback of user journeys. While this definitely isn't a new idea, the way in which it is integrated into Chrome allows for quick creation, editing and running of scripts. The panel also integrates nicely with the performance panel, which makes getting repeated consistent feedback on page performance easier. While record/playback style testing always needs to be used with care in order to avoid brittle tests, we think this preview feature is worth assessing, especially if you're already using the Chrome Performance panel to measure your pages.","blip_selector":"chrome-recorder-panel","name":"Chrome Recorder panel","display_name":"Chrome Recorder panel","url":"/radar/tools/chrome-recorder-panel","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104064,"quadrant":"tools","volume_date":"2022-03","description":"As an alternative to Docker, Podman has been validated by many of our teams. Podman introduces a daemonless engine for managing and running containers which is an interesting approach in comparison to what Docker does. Additionally, Podman can be easily run as a normal user without requiring root privileges, which reduces the attack surface. By using either Open Container Initiative (OCI) images built by Buildah or Docker images, Podman can be adapted to most container use cases. Apart from some compatibility issues with macOS, our team has had generally good experiences with Podman on Linux distributions. | Even though Docker has become the sensible default for containerization, we're seeing new players in this space that are catching our attention. That is the case for Buildah and Podman, which are complementary projects to build images (Buildah) and run containers (Podman) using a rootless approach in multiple Linux distributions. Podman introduces a daemonless engine for managing and running containers which is an interesting approach in comparison to what Docker does. The fact that Podman can use either Open Container Initiative (OCI) images built by Buildah or Docker images makes this tool even more attractive and easy to use.","blip_selector":"podman","name":"Podman","display_name":"Podman","url":"/radar/tools/podman","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203082,"quadrant":"languages-and-frameworks","volume_date":"2022-03","description":"MistQL is a small domain-specific language for performing computations on JSON-like structures. Originally built for handcrafted feature extraction of machine-learning models on the frontend, MistQL currently supports a JavaScript implementation for browsers and a Python implementation for server-side use cases. We quite like its clean composable functional syntax, and we encourage you to assess it based on your needs.","blip_selector":"mistql","name":"MistQL","display_name":"MistQL","url":"/radar/languages-and-frameworks/mistql","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203001,"quadrant":"platforms","volume_date":"2022-03","description":"If you're using GitLab to manage your software delivery, you should also look at GitLab CI/CD for your continuous integration and continuous delivery needs. We've found it especially useful when used with on-premise GitLab and self-hosted runners, as this combination gets around authorization headaches often caused by using a cloud-based solution. Self-hosted runners can be fully configured for your purposes with the right OS and dependencies installed, and as a result pipelines can run much faster than using a cloud-provisioned runner that needs to be configured each time.\n\nApart from the basic build, test and deploy pipeline, GitLab's product supports Services, Auto Devops and ChatOps among other advanced features. Services are useful in running Docker services such as Postgres or Testcontainer linked to a job for integration and end-to-end testing. Auto Devops creates pipelines with zero configuration which is very useful for teams that are new to continuous delivery or for organizations with many repositories that would otherwise need to create many pipelines manually.","blip_selector":"gitlab-ci-cd","name":"GitLab CI/CD","display_name":"GitLab CI/CD","url":"/radar/platforms/gitlab-ci-cd","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203034,"quadrant":"platforms","volume_date":"2022-03","description":"CycloneDX is a standard for describing a machine-readable Software Bill of Materials (SBOM). As software and compute fabrics increase in complexity, software becomes harder to define. Originating with OWASP, CycloneDX improves on the older SPDX standard with a broader definition that extends beyond the local machine dependencies to include runtime service dependencies. You'll also find implementations in several languages, an ecosystem of supporting integrations and a CLI tool that lets you analyze and change SBOMs with appropriate signing and verification.","blip_selector":"cyclonedx","name":"CycloneDX","display_name":"CycloneDX","url":"/radar/platforms/cyclonedx","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202203067,"quadrant":"platforms","volume_date":"2022-03","description":"Temporal is a platform for developing long-running workflows, particularly for microservice architectures. A fork of Uber’s previous OSS Cadence project, it has an event-sourcing model for long-running workflows so they can survive process/machine crashes. Although we don’t recommend using distributed transactions in microservice architectures, if you do need to implement them or long-running Sagas, you may want to look at Temporal.","blip_selector":"temporal","name":"Temporal","display_name":"Temporal","url":"/radar/platforms/temporal","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202203099,"quadrant":"techniques","volume_date":"2022-03","description":"The term standup originated from the idea of standing up during this daily sync meeting, with the goal of making it short. It's a common principle many teams try to abide by in their standups: keep it crisp and to the point. But we're now seeing teams challenge that principle and rethinking remote standups. When co-located, there are lots of opportunities during the rest of the day to sync up with each other spontaneously, as a complement to the short standup. Remotely, some of our teams are now experimenting with a longer meeting format, similar to what the folks at Honeycomb call a “meandering team sync.”\n\nIt's not about getting rid of a daily sync altogether; we still find that very important and valuable, especially in a remote setup. Instead, it's about extending the time blocked in everybody's calendars for the daily sync to up to an hour, and use it in a way that makes some of the other team meetings obsolete and brings the team closer together. Activities can still include the well-tried walkthrough of the team board but are then extended by more detailed clarification discussions, quick decisions, and taking time to socialize. The technique is considered successful if it reduces the overall meeting load and improves team bonding.","blip_selector":"rethinking-remote-standups","name":"Rethinking remote standups","display_name":"Rethinking remote standups","url":"/radar/techniques/rethinking-remote-standups","isCurrentEdition":false,"isRecentEdition":true,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110037,"quadrant":"tools","volume_date":"2021-10","description":"It may not be a tool that you need everyday, but when you're in the weeds trying to diagnose a nasty network problem, it's very useful to be able to reach for a feature-rich HTTP debugging proxy. Proxyman is just such a tool. Quite a few of our teams have been using it for a while now as a macOS-specific drop-in replacement for Charles and really like its streamlined interface and cert management.","blip_selector":"proxyman","name":"Proxyman","display_name":"Proxyman","url":"/radar/tools/proxyman","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110023,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Chakra UI is a UI component library for React.js that is designed for accessibility. We like it, especially for its accessibility features, including dark mode and compatibility with the Web Accessibility Initiative – Accessible Rich Internet Applications (WAI-ARIA) guidelines. Moreover, it is easy to test and customize which makes for a good development experience, accelerating the development process of UI solutions in production environments.","blip_selector":"chakra-ui","name":"Chakra UI","display_name":"Chakra UI","url":"/radar/languages-and-frameworks/chakra-ui","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1145,"quadrant":"tools","volume_date":"2021-10","description":"gopass is a password manager for teams, built on GPG and Git. It's a descendant of pass and adds several features, including interactive search and multiple password stores in a single tree. Since we first mentioned gopass, our teams have used it on several projects, sometimes stretching it beyond its limits. A sorely missed feature was the ability to deprecate secrets. Discoverability was already an issue, but not being able to mark secrets as no longer in use compounded this problem. The biggest issue, though, was scale. When you have teams with 50+ people using the same repository for several years, we found that the repository could grow to multiple gigabytes in size. Re-encrypting the secrets when onboarding new members could take more than half an hour. The underlying issue seems to be that in our teams everything changes all the time: people come and go, secrets are rotated, the architecture evolves, new secrets are added, old ones are no longer needed. gopass seems to work well, even for large numbers of users, when there's less change. | gopass is a password management solution for teams, built on GPG and Git. It's a descendant of pass and adds features such as: support for recipient management and multiple password stores in a single tree; an interactive search functionality; time-based one-time password (TOTP) support; and storage of binary data. Migration of your pass store is fairly straightforward, because gopass is largely compatible with the format pass uses. This also means integration into provisioning workflows can be achieved with a single call to a stored secret. | gopass is a password management solution for teams, built on GPG and Git. It's a descendant of pass and adds features such as: support for recipient management and multiple password stores in a single tree; an interactive search functionality; time-based one-time password (TOTP) support; and storage of binary data. Migration of your pass store is fairly straightforward, because gopass is largely compatible with the format pass uses. This also means integration into provisioning workflows can be achieved with a single call to a stored secret.","blip_selector":"gopass","name":"gopass","display_name":"gopass","url":"/radar/tools/gopass","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104048,"quadrant":"tools","volume_date":"2021-10","description":"We continue to see the adoption of Kubernetes in new and novel scenarios. For example, we see Kubernetes is being extended to manage resources running outside of its cluster or across multiple infrastructure providers, or it is used in managing stateful applications beyond Kubernetes's original scope. These extensions are possible using the Kubernetes Operator pattern: building Kubernetes controllers that have the domain-specific knowledge of the custom resource they manage. For example, an operator that manages a stateful application can use the Kubernetes primitives to automate an application's specific tasks beyond its deployment, such as restore, backup and upgrade its database.\n\nOperator Framework is a set of open-source tools that simplifies building and managing the lifecycle of Kubernetes operators. Although there are multiple frameworks to help you build Kubernetes operators, Operator Framework remains a good choice. It supports rich operator lifecycle management using its Operator Lifecycle Manager module; it supports multiple languages to build the operator code itself using its Operator SDK; and it provides a catalog for publishing and sharing the operators. If you're planning to build Kubernetes operators, we recommend giving the Operator Framework a try to accelerate your development reliably. | Operator Framework is a set of open-source tools that simplifies building and managing the lifecycle of Kubernetes operators. The Kubernetes operator pattern, originally introduced by CoreOS, is an approach to encapsulate the knowledge of operating an application using Kubernetes native capabilities; it includes resources to be managed and controller code that ensures the resources are matching their target state. This approach has been used to extend Kubernetes to manage many applications, particularly the stateful ones, natively. Operator Framework has three components: Operator SDK, which simplifies building, testing and packaging Kubernetes operators; Operator lifecycle manager to install, manage and upgrade the operators; and a catalog to publish and share third-party operators. Our teams have found Operator SDK particularly powerful in rapidly developing Kubernetes-native applications.","blip_selector":"operator-framework","name":"Operator Framework","display_name":"Operator Framework","url":"/radar/tools/operator-framework","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110015,"quadrant":"tools","volume_date":"2021-10","description":"Contrast Security offers a security platform with multiple components, including static application security testing (SAST), interactive application security testing (IAST), open-source scanning and runtime application self-protection (RASP). It's been around for a few years now, and we've used it in multiple projects. One of the things we quite like about the Contrast platform is its run-time analysis of libraries; it helps identify libraries that are not used, which in turn helps our teams prioritize vulnerabilities and potentially get rid of unused libraries. This is particularly relevant given the increased importance of securing the software supply chain. We also quite like its IAST component; we've found it effective in our continuous delivery (CD) pipeline with reduced false positives, and it manages to catch a good range of vulnerabilities.","blip_selector":"contrast-security","name":"Contrast Security","display_name":"Contrast Security","url":"/radar/tools/contrast-security","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110081,"quadrant":"tools","volume_date":"2021-10","description":"Crossplane is another entry in the class of tools implemented by the Kubernetes Operator pattern but with side effects that extend beyond the Kubernetes cluster. In our last Radar we mentioned Kube-managed cloud services as a technique, and Crossplane does just that. The idea is to leverage the Kubernetes control plane to provision cloud services on which your deployment is dependent, even if they aren't deployed on the cluster itself. Examples include managed database instances, load balancers or access control policies. This tool is noteworthy for two reasons. First, it demonstrates the powerful and flexible execution environment of the underlying Kubernetes control plane. There is no real limit to the range of supported custom resources. Second, Crossplane provides an alternative to the usual options of Terraform, CDK or Pulumi. Crossplane comes with a set of predefined providers for the major cloud services that cover the most commonly provisioned services. It isn't trying to be a general-purpose infrastructure-as-code (IaC) tool but rather a companion to workloads being deployed in Kubernetes. Often associated with the practice of GitOps, Crossplane stands on its own and allows you to stay within the Kubernetes ecosystem when it's necessary to manage external cloud resources. However, Crossplane doesn't help with provisioning Kubernetes itself; you'll need at least one other IaC tool to bootstrap the cluster.","blip_selector":"crossplane","name":"Crossplane","display_name":"Crossplane","url":"/radar/tools/crossplane","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110075,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Originally type annotations were added to Python to support static analysis. However, considering how widely type annotations, and annotations in general, are used in other programming languages, it was only a matter of time before developers would begin to use Python's type annotations for other purposes. pydantic falls into this category. It allows you to use type annotations for data validation and settings management at run time. When data arrives as, say, a JSON document and needs to be parsed into a complex Python structure, pydantic ensures that the incoming data matches the expected types or reports an error if it doesn't. Although you can use pydantic directly, many developers have used it as part of FastAPI, one of the most popular Python web frameworks. In fact, using pydantic in FastAPI is considered so indispensable that a recently proposed change to Python, aimed at reducing the cost of loading annotated code into memory, was reconsidered because it would have broken the use of type annotations at run time.","blip_selector":"pydantic","name":"pydantic","display_name":"pydantic","url":"/radar/languages-and-frameworks/pydantic","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110013,"quadrant":"tools","volume_date":"2021-10","description":"JetBrains' collaborative coding tool, Code With Me, has been increasing in popularity as many teams use various JetBrains tools in this remote-first world. Along with other remote collaboration tools such as VSCode's Visual Studio Live Share, Code With Me gives development teams an improved experience with remote pairing and collaboration. Code With Me's abilities to invite teammates into the IDE projects and collaborate in real time are worth exploring. However, we've seen some limitations with regard to refactoring seamlessly and some issues in high-latency environments. We'll continue to watch this tool in this space.","blip_selector":"code-with-me","name":"Code With Me","display_name":"Code With Me","url":"/radar/tools/code-with-me","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110031,"quadrant":"tools","volume_date":"2021-10","description":"Sometimes you come across a tool that you didn't realize you needed until you do; mob is just such a tool. Living as we do in a world where remote pair programming has become the norm for many teams, having a tool that allows for seamless handover either between pairs or a wider group as part of a mob programming session is super useful. mob hides all the version control paraphernalia behind a command-line interface that makes participating in mob programming sessions simpler. It also provides specific advice around how to participate remotely, for example, to \"steal the screenshare\" in Zoom rather than ending a screenshare, ensuring the video layout doesn't change for participants. A useful tool and thoughtful advice, what's not to like?","blip_selector":"mob","name":"mob","display_name":"mob","url":"/radar/tools/mob","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110041,"quadrant":"platforms","volume_date":"2021-10","description":"Since we first evaluated JAMstack, we've seen more and more web applications of this style. However, when the infrastructure for building traditional dynamic websites and back-end services is too heavy for JAMstack, our teams choose Vercel. Vercel is a cloud platform for static site hosting. More importantly, it provides a seamless workflow for developing, previewing and shipping JAMstack sites. The configuration for the deployment is quite simple. By integrating with GitHub, each code commit or pull request could trigger a new website deployment that has a URL for preview, which greatly accelerates development feedback. Vercel also uses CDN to scale and speed up production sites. It's worth mentioning that the team behind Vercel also supports another popular framework, Next.js.","blip_selector":"vercel","name":"Vercel","display_name":"Vercel","url":"/radar/platforms/vercel","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110005,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Jetpack Hilt has recently reached version 1.0, and we can report that we've had good experiences with it. Jetpack Hilt offers extensions for integrating Hilt with various other AndroidX libraries, such as WorkManager and Navigation. It further expands the reach of Hilt, to provide developers with a standard way of incorporating Dagger dependency injection into Android applications. We've featured Koin as a Kotlin-native dependency injection framework in the Radar before, and we would advise against attempting to replace Koin in a large existing codebase. However, when starting a new project, Hilt, it seems, is now the way to go.","blip_selector":"jetpack-hilt","name":"Jetpack Hilt","display_name":"Jetpack Hilt","url":"/radar/languages-and-frameworks/jetpack-hilt","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110019,"quadrant":"platforms","volume_date":"2021-10","description":"Konga is an open-source UI for administering the Kong API Gateway, previously featured in the Radar in Trial. Our teams liked the quick setup and rich feature set that allowed them to experiment with and try out configurations easily. And the fact it's open-source software eases concerns about licensing costs.","blip_selector":"konga","name":"Konga","display_name":"Konga","url":"/radar/platforms/konga","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110027,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"lifelines is a library for survival analysis in Python. Originally developed for birth and death events, it has evolved into a complete survival analysis library to predict any duration of time. Beyond medical use cases (such as answering, How long does this population live for?), we've used it in retail and manufacturing to answer questions like How long users are subscribed to a service? or When should we do the next preventive maintenance?","blip_selector":"lifelines","name":"lifelines","display_name":"lifelines","url":"/radar/languages-and-frameworks/lifelines","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110073,"quadrant":"tools","volume_date":"2021-10","description":"Over the years we've debated several times whether to feature monorepos in the Radar. Each time we ended up concluding that the trade-offs introduced by monorepos require a nuanced discussion and the technique is \"too complex to blip.\" Now we're seeing increased interest in monorepos in the JavaScript community, for example, for building applications composed of micro frontends, as discussed in this podcast episode. Whether this is a good idea depends a lot on your situation, and we certainly don't want to give a general recommendation. What we do want to comment on is the tooling. In our teams we see a shift away from Lerna and a strong preference to use Nx for managing JavaScript-based monorepos.","blip_selector":"nx","name":"Nx","display_name":"Nx","url":"/radar/tools/nx","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010050,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Web applications, especially those for internal use in enterprises, are usually written in two parts. The user interface and some business logic run in the web browser while business logic, authorization and persistence run on a server. These two halves normally communicate via JSON over HTTP. The endpoints shouldn't be mistaken for a real API; they're simply an implementation detail of an application that is split across two run-time environments. At the same time, they provide a valid seam to test the pieces individually. When testing the JavaScript part, the server side can be stubbed and mocked at the network level by a tool such as Mountebank. Mock Service Worker offers an alternative approach of intercepting requests in the browser. This simplifies manual tests as well. Like Mountebank, Mock Service Worker is run outside the browser as a Node.js process for testing network interactions. In addition to REST interactions, it mocks GraphQL APIs — a bonus because GraphQL can be complex to mock manually at the network level. | Web applications, especially those written for internal use in enterprises, are usually written in two parts. The user interface and some business logic run in the web browser while business logic, authorization and persistence run on a server. These two halves normally communicate via JSON over HTTP. The endpoints shouldn't be mistaken for a real API; they're simply an implementation detail of an application that is split across two run-time environments. At the same time, they provide a valid seam to test the pieces individually. When testing the JavaScript part, the server side can be stubbed and mocked at the network level by a tool such as Mountebank. An alternative approach is to intercept the requests in the browser. We like the approach taken by Mock Service Worker because with service workers it uses an abstraction familiar to developers. This approach results in a simpler setup and faster test execution. However, because these tests don't test the actual network layer, you want to implement some end-to-end tests as part of a healthy test pyramid.","blip_selector":"mock-service-worker","name":"Mock Service Worker","display_name":"Mock Service Worker","url":"/radar/languages-and-frameworks/mock-service-worker","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911020,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"React Hooks have introduced a new approach to managing stateful logic; given React components have always been closer to functions than classes, Hooks have embraced this and brought state to the functions, instead of using classes to take function to the state with methods. Another staple of state management in React applications is Redux, and we've already noted that it has come under scrutiny, suggesting that sometimes the complexity of Redux isn't worth it and in such cases a simple approach using Hooks is preferable. Rolling this completely on your own can quickly become tricky; therefore we recommend considering a combination of React Context and the useContext and useReducer hooks, along the lines explained in this blog post. | React Hooks have introduced a new approach to managing stateful logic; given React components have always been closer to functions than classes, Hooks have embraced this and brought state to the functions, instead of taking function as methods to the state with classes. Based on our experience, Hooks improve reuse of functionality among components and code readability. Given Hooks’ testability improvements, using React Test Renderer and React Testing Library, and their growing community support, we consider them our approach of choice. | Earlier this year, React Hooks were introduced to the popular JavaScript framework. They make it possible to use state and other React features without writing a class, offering a cleaner approach than higher-order components or render-props for use cases. Libraries such as Material UI and Apollo have already switched to using Hooks. There are some issues with testing Hooks, especially with Enzyme, which contributed to our reassessment of Enzyme as the tool of choice.","blip_selector":"react-hooks","name":"React Hooks","display_name":"React Hooks","url":"/radar/languages-and-frameworks/react-hooks","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104042,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"For many organizations, cross-platform mobile development is becoming a strong option especially as the end-to-end experience of building mobile cross-platform applications becomes more enjoyable and efficient. Kotlin Multiplatform Mobile (KMM) is an SDK provided by JetBrains that leverages the multiplatform capabilities in Kotlin and includes tools and features designed to streamline the developer experience. With KMM you write code once for business logic and the app core in Kotlin and then share it with both Android and iOS applications. You write platform-specific code only when necessary, for example, to take advantage of native UI elements; and the specific code is kept in different views for each platform. We're moving KMM to Trial as it is evolving rapidly and we're seeing a few organizations use this as their default. | Following the trend of cross-platform mobile development, Kotlin Multiplatform Mobile (KMM) is a new entry in this space. KMM is an SDK provided by JetBrains that leverages the multiplatform capabilities in Kotlin and includes tools and features designed to make the end-to-end experience of building mobile cross-platform applications more enjoyable and efficient. With KMM you write code once for business logic and the app core in Kotlin and then  share it with both Android and iOS applications. Write platform-specific code only when necessary, for example, to take advantage of native UI elements; and the specific code is kept in different views for each platform. Although still in Alpha, Kotlin Multiplatform Mobile is evolving rapidly. We'll certainly keep an eye on it, and you should too.","blip_selector":"kotlin-multiplatform-mobile","name":"Kotlin Multiplatform Mobile","display_name":"Kotlin Multiplatform Mobile","url":"/radar/languages-and-frameworks/kotlin-multiplatform-mobile","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1125,"quadrant":"tools","volume_date":"2021-10","description":"In recent years we've seen the rise of generic and domain-specific workflow management tools. The drivers behind this rise include the increased usage of data-processing pipelines and the automation of the machine-learning (ML) model development process. Airflow is one of the early open-source task orchestration tools that popularized the definition of directed acyclic graphs (DAGs) as code, an improvement over an XML/YAML pipeline configuration. Although Airflow remains one of the most widely adopted orchestration tools, we encourage you to evaluate other tools based on your unique situation. For example, you may want to choose Prefect, which supports dynamic data-processing tasks as a first-class concern with generic Python functions as tasks; or Argo if you prefer a tight integration with Kubernetes; or Kubeflow or MLflow for ML-specific workflows. Given the rise of new tools, combined with some of the shortfalls of Airflow (such as lack of native support for dynamic workflows and its centralized approach to scheduling pipelines), we no longer recommend Airflow as the default orchestration tool.\n\nWe believe that with the increased usage of streaming in analytics and data pipelines, as well as managing data through a decentralized data mesh, the need for orchestration tools to define and manage complex data-processing pipelines is reduced. | Airflow remains our most widely used and favorite open-source workflow management tool for data-processing pipelines as directed acyclic graphs (DAGs). This is a growing space with open-source tools such as Luigi and Argo and vendor-specific tools such as Azure Data Factory or AWS Data Pipeline. However, Airflow differentiates itself with its programmatic definition of workflows over limited low-code configuration files, support for automated testing, open-source and multiplatform installation, rich set of integration points to the data ecosystem and large community support. In decentralized data architectures such as data mesh, however, Airflow currently falls short as a centralized workflow orchestration. | Airflow is a tool to programmatically create, schedule and monitor data pipelines. By treating Directed Acyclic Graphs (DAGs) as code, it encourages maintainable, versionable and testable data pipelines. We've leveraged this configuration in our projects to create dynamic pipelines that resulted in lean and explicit data workflows. Airflow makes it easy to define your operators and executors and to extend the library so that it fits the level of abstraction that suits your environment.","blip_selector":"airflow","name":"Airflow","display_name":"Airflow","url":"/radar/tools/airflow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110022,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Arium is an automated testing framework for 3D applications written in Unity. Functional tests are an important part of a healthy test pyramid. Arium, which is built as a wrapper on the Unity Test framework, lets you write functional tests for 3D apps on multiple platforms. We've used it successfully in a few of our projects.","blip_selector":"arium","name":"Arium","display_name":"Arium","url":"/radar/languages-and-frameworks/arium","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110097,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Transloco is a library for Angular to build multilingual applications. It can be used in templates and offers a function to cover more complex use cases. Because the translations are loaded on-demand at run time, Transloco makes it easy to implement language switching in the web browser. It also covers localization of numbers, dates and more using template pipes.","blip_selector":"transloco","name":"Transloco","display_name":"Transloco","url":"/radar/languages-and-frameworks/transloco","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911009,"quadrant":"platforms","volume_date":"2021-10","description":"XTDB is an open-source document database with bitemporal graph queries. It natively supports two time axes for each record: valid time, when a fact occurs, and transaction time, when a fact is processed and recorded by the database. Support for bitemporality is beneficial in numerous scenarios, including analytical use cases executing time-aware queries; auditing historical changes to facts; supporting distributed data architectures that must guarantee globally consistent point-in-time queries such as data mesh; and preserving data immutability. XTDB takes information in document form, expressed in the Extensible Data Notation (EDN) format, a subset of the Clojure language. XTDB supports graph as well as SQL queries and is extensible through a REST API layer and Kafka Connect, among other modules. We're excited to see a growth in adoption of XTDB and the addition of features such as support for transactions and SQL. | Crux is an open-source document database with bitemporal graph queries. Most database systems are temporal, meaning they help us model facts along with the time at which they occurred. Bitemporal database systems let you model not just the valid time the fact occurred but also the transaction time when it was received. If you need a document store with graph capabilities for querying the content, then give Crux a try. It's currently in alpha and lacks SQL support, but you can use a Datalog query interface for reading and traversing relationships.","blip_selector":"xtdb","name":"XTDB","display_name":"XTDB","url":"/radar/platforms/xtdb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110071,"quadrant":"tools","volume_date":"2021-10","description":"This edition of the Radar introduces two tools that search and replace code using an abstract syntax tree (AST) representation. They occupy a similar space as jscodeshift but contain parsers for a wide range of programming languages. Although they share some similarities, they also differ in several ways. One of these tools, Comby, is unique in its simple, command-line interface designed in the spirit of Unix tools such as awk and sed. While the Unix commands are based on regular expressions operating matching text, Comby employs a pattern syntax that is specific to programming language constructs and parses the code before searching. This helps developers search large code bases for structural patterns. Like sed, Comby can replace the patterns it matches with new structures. This is useful for automating wholesale changes to large codebases or for making repetitive changes across a suite of microservice repositories. Since these tools are fairly new, we expect to see a range of creative uses that have yet to be discovered.","blip_selector":"comby","name":"Comby","display_name":"Comby","url":"/radar/tools/comby","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104066,"quadrant":"techniques","volume_date":"2021-10","description":"After successfully launching their email application HEY as a server-side application, Basecamp reported migrating its flagship product, Basecamp 3, to Hotwire this summer. As organizations increasingly default to single-page applications (SPAs) for new web development, we continue to be excited by Hotwire swimming against the stream. Unlike SPAs, Hotwire applications keep most of the logic and navigation on the server, relying on a minimal amount of browser JavaScript. Hotwire modularizes HTML pages into a set of components (called Turbo Frames) that can be lazy loaded, provide independent contexts and send HTML updates to those contexts based on user actions. SPAs offer undeniable user responsiveness, but the simplicity of traditional server-side web programming combined with modern browser tooling provides a refreshing take on balancing developer effectiveness and user responsiveness. | Hotwire (HTML over the wire) is a technique to build web applications. Pages are constructed out of components, but unlike modern SPAs the HTML for the components is generated on the server side and then sent \"over the wire\" to the browser. The application has only a small amount of JavaScript code in the browser to stitch the HTML fragments together. Our teams, and doubtlessly others too, experimented with this technique after asynchronous web requests gained cross-browser support around 2005, but for various reasons it never gained much traction.\n\nToday, Hotwire uses modern web browser and HTTP capabilities to achieve the speed, responsiveness and dynamic nature of single-page apps (SPAs). It embraces simpler web application design by localizing the logic to the server and keeping the client-side code simple. The team at Basecamp has released a few Hotwire frameworks that power their own application, including Turbo and Stimulus. Turbo includes a set of techniques and frameworks to speed up the application responsiveness by preventing whole page reloading, page preview from cache and decomposing the page into fragments with progressive enhancements on request. Stimulus is designed to enhance static HTML in the browser by connecting JavaScript objects to the page elements on the HTML.","blip_selector":"hotwire","name":"Hotwire","display_name":"Hotwire","url":"/radar/techniques/hotwire","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110035,"quadrant":"tools","volume_date":"2021-10","description":"Prefect is a data workflow management tool that makes it easy to add semantics such as retries, dynamic mapping, caching and failure notifications to data pipelines. You can mark Python functions as tasks and chain them together through function calls to build the data flow. The Python API combined with a collection of predefined tasks for common data operations makes Prefect a noteworthy option to assess for your data pipeline needs.","blip_selector":"prefect","name":"Prefect","display_name":"Prefect","url":"/radar/tools/prefect","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110025,"quadrant":"platforms","volume_date":"2021-10","description":"OPA Gatekeeper for Kubernetes is a customizable admission webhook for Kubernetes that enforces policies executed by the Open Policy Agent (OPA). We're using this extension of the Kubernetes platform to add a security layer to clusters, providing automated governance mechanisms that ensure applications are compliant with defined policies. Our teams like it because of its customization capability; using CustomResourceDefinitions (CRD) allows us to define ConstraintTemplates and Constraints which make defining rules and the objects (e.g., deployments, jobs, cron jobs) and namespaces under evaluation an easy task.","blip_selector":"opa-gatekeeper-for-kubernetes","name":"OPA Gatekeeper for Kubernetes","display_name":"OPA Gatekeeper for Kubernetes","url":"/radar/platforms/opa-gatekeeper-for-kubernetes","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110079,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"InsightFace is an open source 2D and 3D deep face analysis toolbox, mainly based on PyTorch and MXNet. InsightFace uses some of the most recent and accurate methods for face detection, face recognition and face alignment. We're particularly interested in it, because it has one of the best implementations for ArcFace, a cutting-edge machine-learning model that detects the similarities of two images. InsightFace with ArcFace received a 99.83% accuracy score on the Labeled Faces in the Wild (LFW) data set. We're experimenting with it in the context of facial deduplication and have seen promising results.","blip_selector":"insightface","name":"InsightFace","display_name":"InsightFace","url":"/radar/languages-and-frameworks/insightface","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110094,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Tauri is an Electron alternative for building desktop applications using a combination of Rust tools and HTML, CSS, and JavaScript rendered in System's WebView. Unlike Electron which bundles Chromium, the applications built with Tauri leverage the underlying WebView, that is, WebKit on macOS, WebView2 on Windows and WebKitGTK on Linux. This approach has interesting trade-offs — on one hand you get small and fast application binaries; on the other hand, you need to verify compatibility quirks across WebViews of different systems.","blip_selector":"tauri","name":"Tauri","display_name":"Tauri","url":"/radar/languages-and-frameworks/tauri","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110085,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Kats is a lightweight framework for performing time series analyses, recently released by Facebook Research. Time series analysis is an important area in data science; it encompasses the problem domains of forecasting, detection (including the detection of seasonalities, outliers and change points), feature extraction and multivariate analysis. Typically we tend to have different libraries for different techniques in a time series analysis. Kats though aims to be a one-stop shop for time series analyses and provides a set of algorithms and models for all the time series analysis problem domains. Previously we mentioned Prophet, also by Facebook Research,  which is one of the models Kats implements for forecasting. We're looking forward to trying Kats in problems involving time series analyses.","blip_selector":"kats","name":"Kats","display_name":"Kats","url":"/radar/languages-and-frameworks/kats","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110086,"quadrant":"platforms","volume_date":"2021-10","description":"Milvus 2.0 is a cloud-native, open-source vector database built to search and manage embedding vectors generated by machine-learning models and neural networks. It supports several vector indexes for approximate nearest neighbors (ANN) search across embedding vectors of audio, video, image or any unstructured data. Milvus 2.0 is a relatively new database, and we recommend you assess it for your similarity search needs.","blip_selector":"milvus-2-0","name":"Milvus 2.0","display_name":"Milvus 2.0","url":"/radar/platforms/milvus-2-0","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110098,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"When creating a user interface with SwiftUI, the idea is to build a view model that can be mapped easily to the elements of the user interface. In such cases, most of the testing can be done on the model, using the standard unit testing frameworks which makes these tests straightforward to write and fast to run. To test the bindings between the model and the views, developers usually reach for XCUITest, a test automation framework that launches the full application and remote controls the interface. It works, tests are reasonably stable, but they take a long time to run.\n\nFor a faster approach to writing unit tests for SwiftUI, try ViewInspector, an open-source framework that uses Swift's public reflection API to access the underlying views created by SwiftUI. With ViewInspector, a test simply instantiates a SwiftUI view, locates the interface elements that need to be tested and then makes assertions against them. Basic interactions such as taps can be tested, too. Like many UI testing frameworks, it provides an API to locate interface elements, either by specifying a path through the view hierarchy or by using a set of finder methods. These tests are usually simpler than XCUITests, and they run much faster. As a word of caution, though, given the ease with which tests can be written using ViewInspector, you might be tempted to over-test the interface. Testing simple one-to-one mappings is just double-entry bookkeeping. And even though ViewInspector makes it easier to test the SwiftUI code, remember to keep most of the logic in the model.","blip_selector":"viewinspector","name":"ViewInspector","display_name":"ViewInspector","url":"/radar/languages-and-frameworks/viewinspector","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110011,"quadrant":"tools","volume_date":"2021-10","description":"Berglas is a tool for managing secrets on Google Cloud Platform (GCP). We've recommended secrets as a service as a technique to store and share secrets in modern distributed architectures in the past, and GCP offers Secret Manager for that purpose, and Berglas works well with Secret Manager. This is especially useful for those GCP services that don't have direct integration with Secret Manager yet; the alternative in such cases would be to write custom code or scripts. Berglas ships as a command-line tool and as a library, and both also come in handy in use cases beyond secrets as a service. The author of Berglas, who also happens to be the original author of HashiCorp Vault, now works at Google; however, Berglass is not an official Google product.","blip_selector":"berglas","name":"Berglas","display_name":"Berglas","url":"/radar/tools/berglas","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110070,"quadrant":"techniques","volume_date":"2021-10","description":"Many augmented reality (AR) applications depend on knowing the location and orientation of the user's device. The default is to use GPS-based solutions, but spatial anchors, a newer technique to address this requirement, are also worth considering. Spatial anchors work with the image recorded by the device's camera, using image features and their relative position in 3D space to recognize a real-world location. For this location a corresponding anchor is created in the AR space. Although spatial anchors can't replace all GPS and marker-based anchors, they do provide more accuracy than most GPS-based solutions and are more resilient to different viewing angles than marker-based anchors. Our experience is currently limited to Google's Cloud Anchors for Android, which worked well for us. Somewhat uncharacteristically Google also offers Cloud Anchors for iOS and with Azure Spatial Anchors Microsoft supports even more platforms.","blip_selector":"ar-spatial-anchors","name":"AR spatial anchors","display_name":"AR spatial anchors","url":"/radar/techniques/ar-spatial-anchors","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1060,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"We first mentioned Three.js in the Radar in Assess back in 2017. Since then, this 3D rendering library for the web has evolved and improved rapidly. The standard WebGL APIs have improved, and Three.js has added support for WebXR, turning it into a viable tool for creating immersive experiences. At the same time, browser support for 3D rendering and WebXR device APIs has improved, making the web an increasingly attractive platform for 3D content. Although there are other 3D rendering libraries, our teams have come to prefer Three.js, especially when paired with React Three Fiber to abstract away some of the low-level details. We've found that developers still need to be conscious of performance issues and will sometimes need to restructure data to optimize rendering speed. | Despite the fervor surrounding the spate of new headsets, we believe there are many VR and AR scenarios that make sense in the browser, particularly on mobile. Given this trend, we have seen an uptick in usage of Three.js, a powerful JavaScript visualization and 3D rendering framework. The growth in support for WebGL, which it is based on, has helped adoption, as has the vibrant community supporting this open source project. | Despite the fervor surrounding the spate of new headsets, we believe there are many VR and AR scenarios that make sense in the browser, particularly on mobile. Given this trend, we have seen an uptick in usage of Three.js, a powerful JavaScript visualization and 3D rendering framework. The growth in support for WebGL, which it is based on, has helped adoption, as has the vibrant community supporting this open source project.","blip_selector":"three-js","name":"Three.js","display_name":"Three.js","url":"/radar/languages-and-frameworks/three-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110002,"quadrant":"platforms","volume_date":"2021-10","description":"ClickHouse is an open-source, columnar online analytical processing (OLAP) database for real-time analytics. It started as an experimental project in 2009 and since has matured into a highly performant and linearly scalable analytical database. It's efficient query processing engine together with data compression makes it suitable to run interactive queries without pre-aggregation. We've used ClickHouse and are quite impressed with its performance.","blip_selector":"clickhouse","name":"ClickHouse","display_name":"ClickHouse","url":"/radar/platforms/clickhouse","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110093,"quadrant":"tools","volume_date":"2021-10","description":"Cosign is a container signing and verification tool. Part of Sigstore — a project under the Cloud Native Computing Foundation (CNCF) umbrella aimed at simplifying software signing and transparency — Cosign supports not only Docker and Open Container Initiative (OCI) images but also other artifacts that can be stored in a container registry. We previously talked about Docker Notary, which also operates in this space; Notary v1, however, has some disadvantages: it's not registry native and needs a separate Notary server. Cosign avoids this problem and stores the signatures in the registry next to an image. It currently has integrations with GitHub actions and Kubernetes using a Webhook with further integrations in the pipeline. We've used Cosign in some of our projects and it looks quite promising.","blip_selector":"cosign","name":"Cosign","display_name":"Cosign","url":"/radar/tools/cosign","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1283,"quadrant":"platforms","volume_date":"2021-10","description":"We've been seeing an increase in teams using Pulumi in various organizations. Pulumi fills a gaping hole in the infrastructure coding world where Terraform maintains a firm hold. While Terraform is a tried-and-true standby, its declarative nature suffers from inadequate abstraction facilities and limited testability. Terraform is adequate when the infrastructure is entirely static, but dynamic infrastructure definitions call for a real programming language. Pulumi distinguishes itself by allowing configurations to be written in TypeScript/JavaScript, Python and Go — no markup language or templating required. Pulumi is tightly focused on cloud-native architectures — including containers, serverless functions and data services — and provides good support for Kubernetes. Recently, AWS CDK has mounted a challenge, but Pulumi remains the only cloud-neutral tool in this area. | We've seen interest in Pulumi slowly but steadily rising. Pulumi fills a gaping hole in the infrastructure coding world where Terraform maintains a firm hold. While Terraform is a tried-and-true standby, its declarative nature suffers from inadequate abstraction facilities and limited testability. Terraform is adequate when the infrastructure is entirely static, but dynamic infrastructure definitions call for a real programming language. Pulumi distinguishes itself by allowing configurations to be written in TypeScript/JavaScript, Python and Go — no markup language or templating required. Pulumi is tightly focused on cloud-native architectures — including containers, serverless functions and data services — and provides good support for Kubernetes. Recently, AWS CDK has mounted a challenge, but Pulumi remains the only cloud-neutral tool in this area. We're anticipating wider Pulumi adoption in the future and looking forward to a viable tool and knowledge ecosystem emerging to support it. | We've seen interest in Pulumi slowly but steadily rising. Pulumi fills a gaping hole in the infrastructure coding world where Terraform maintains a firm hold. While Terraform is a tried-and-true standby, its declarative nature suffers from inadequate abstraction facilities and limited testability. Terraform is adequate when the infrastructure is entirely static, but dynamic infrastructure definitions call for a real programming language. Pulumi distinguishes itself by allowing configurations to be written in TypeScript/JavaScript, Python and Go — no markup language or templating required. Pulumi is tightly focused on cloud-native architectures — including containers, serverless functions and data services — and provides good support for Kubernetes. Recently, AWS CDK has mounted a challenge, but Pulumi remains the only cloud-neutral tool in this area. We're anticipating wider Pulumi adoption in the future and looking forward to a viable tool and knowledge ecosystem emerging to support it. | We're quite interested in Pulumi, a promising entrant in cloud infrastructure automation. Pulumi distinguishes itself by allowing configurations to be written in TypeScript/JavaScript, Python, and Go—no YAML required. Pulumi is tightly focused on cloud-native architectures—including containers, serverless functions and data services—and provides good support for Kubernetes.","blip_selector":"pulumi","name":"Pulumi","display_name":"Pulumi","url":"/radar/platforms/pulumi","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110012,"quadrant":"techniques","volume_date":"2021-10","description":"Although it’s been around for a while, we're seeing more and more use cases where using the CBOR specification for data interchange makes sense — especially in environments containing multiple types of applications communicating with one another: service to service, browser to service, and so on. One thing we've found useful with Borer, a Scala implementation of a CBOR encoder/decoder, is the ability for clients to negotiate content between the binary representation and plain old JSON format. It's quite useful to have a text version viewable in a browser as well as the concise binary format. We foresee CBOR/JSON bilingual protocols picking up in popularity with the continuing rise of IoT and edge computing and other situations where the environment is tightly constrained.","blip_selector":"cbor-json-bilingual-protocols","name":"CBOR/JSON bilingual protocols","display_name":"CBOR/JSON bilingual protocols","url":"/radar/techniques/cbor-json-bilingual-protocols","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110003,"quadrant":"platforms","volume_date":"2021-10","description":"Kafka is a common default for event-driven architectures, but adapting it to legacy environments introduces an impedance mismatch. In a few cases, we've had success minimizing the legacy complexity using Confluent Kafka REST Proxy. The proxy allows developers to access Kafka through an HTTP interface, which is useful in environments that make using the native Kafka protocol difficult. For example, we were able to consume events emitted through SAP simply by having the SAP team invoke an HTTP POST command through a preconfigured SAP remote function call, avoiding the need to spin up a Java abstraction around SAP (and a team to manage it). The proxy is quite full-featured, although, as with any such adapter tool, we recommend caution and a clear-eyed view of the trade-offs involved. We believe the proxy is valuable when it enables legacy producers to send events, but would be careful creating event consumers through the proxy as the abstraction gets more complex. The proxy doesn't change the fact that Kafka consumers are stateful, which means that consumer instances created through the REST API are tied to a specific proxy, and the need to make an HTTP call to consume messages from a topic changes the standard semantics of Kafka eventing.","blip_selector":"confluent-kafka-rest-proxy","name":"Confluent Kafka REST Proxy","display_name":"Confluent Kafka REST Proxy","url":"/radar/platforms/confluent-kafka-rest-proxy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911026,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"We assessed Quarkus two years ago, and now our teams have more experience with it. Quarkus is a Kubernetes-native Java stack tailored for OpenJDK HotSpot and GraalVM. Over the past two years, Quarkus has wired those best-of-breed libraries in the Java world and streamlined the code configuration, giving our teams a pretty good developer experience. Quarkus has a very fast boot time (tens of milliseconds) and a low RSS memory footprint; this is because of its container-first building approach: it uses ahead-of-time compilation techniques to do dependency injection at compile time and thus avoids the run-time costs of reflection. Our team has also had to endure the trade-offs: it takes nearly 10 minutes for Quarkus to build on our pipeline; some features that rely on annotations and reflection (such as ORM and serializer) are also limited. Part of these trade-offs are the result of using GraalVM. So if your application is not running for FaaS, using Quarkus with HotSpot is also a good choice. | Quarkus is a cloud-native, container-first framework by Red Hat for writing Java applications. It has a very fast startup time (tens of milliseconds) and has low memory utilization which makes it a good candidate for FaaS or frequent scaling up and down in a container orchestrator. Like Micronaut, Quarkus achieves this by using ahead-of-time compilation techniques to do dependency injection at compile time and avoid the runtime costs of reflection. It also works well with GraalVM's Native Image which further reduces startup time. Quarkus supports both imperative and reactive models. Along with Micronaut and Helidon, Quarkus is leading the charge on the new generation of Java frameworks which attempt to address startup performance and memory without sacrificing developer effectiveness. It's gained a lot of community attention and is worth keeping an eye on.","blip_selector":"quarkus","name":"Quarkus","display_name":"Quarkus","url":"/radar/languages-and-frameworks/quarkus","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202104068,"quadrant":"techniques","volume_date":"2021-10","description":"Some organizations seem to think peer review equals pull request; they've taken the view that the only way to achieve a peer review of code is via a pull request. We've seen this approach create significant team bottlenecks as well as significantly degrade the quality of feedback as overloaded reviewers begin to simply reject requests. Although the argument could be made that this is one way to demonstrate code review \"regulatory compliance,\" one of our clients was told this was invalid since there was no evidence the code was actually read by anyone prior to acceptance. Pull requests are only one way to manage the code review workflow; we urge people to consider other approaches, especially where there is a need to coach and pass on feedback carefully. | Some organizations seem to think peer review equals pull request; they've taken the view that the only way to achieve a peer review of code is via a pull request. We've seen this approach create significant team bottlenecks as well as significantly degrade the quality of feedback as overloaded reviewers begin to simply reject requests. Although the argument could be made that this is one way to demonstrate code review \"regulatory compliance\" one of our clients was told this was invalid since there was no evidence the code was actually read by anyone prior to acceptance. Pull requests are only one way to manage the code review workflow; we urge people to consider other approaches, especially where there is a need to coach and pass on feedback carefully.","blip_selector":"peer-review-equals-pull-request","name":"Peer review equals pull request","display_name":"Peer review equals pull request","url":"/radar/techniques/peer-review-equals-pull-request","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110082,"quadrant":"tools","volume_date":"2021-10","description":"Dive is a tool for analyzing Docker images; it helps explore each layer in the image and identify what's changed in each layer. Dive estimates image efficiency and wasted space in an image and can be integrated into the continuous integration (CI) pipeline to fail the build based on the efficiency score or amount of wasted space. We've used it in a few projects, and it has proven to be a useful tool — particularly if you're building images with a very low tolerance for additional tools or space consumption.","blip_selector":"dive","name":"Dive","display_name":"Dive","url":"/radar/tools/dive","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110030,"quadrant":"tools","volume_date":"2021-10","description":"Micoo is a new entrant into the crowded space of visual regression tools; it's an open-source solution and is self-contained, providing Docker images to enable an easy and quick environment setup. It also provides different clients for Node.js, Java and Python as well as a Cypress plugin so it can be easily integrated with most of the common frontend UI automation testing frameworks or solutions. Although Micoo doesn't provide all the functionality of some of the SaaS-based or other commercial solutions, our teams have been using it extensively and have had positive experiences. They've especially called out that it works for mobile and desktop apps as well as the web.","blip_selector":"micoo","name":"Micoo","display_name":"Micoo","url":"/radar/tools/micoo","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110042,"quadrant":"platforms","volume_date":"2021-10","description":"Weights & Biases is a machine learning (ML) platform for building models faster through experiment tracking, data set versioning, visualizing model performance and model management. You can integrate it with existing ML code and quickly get live metrics, terminal logs and system statistics streamed to the dashboard for further analysis. Our teams have used Weights & Biases, and we like its collaborative approach to model building.","blip_selector":"weights-biases","name":"Weights & Biases","display_name":"Weights & Biases","url":"/radar/platforms/weights-biases","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110043,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Zap is a super performant structured logging library for GoLang which is faster than the standard Log implementation and other logging libraries. Zap has both a \"pretty\" logger, providing a structured and printf-style interface, as well as an (even) faster implementation with just the structured interface. Our teams have used it extensively at scale and are happy to recommend it as their go-to solution.","blip_selector":"zap","name":"Zap","display_name":"Zap","url":"/radar/languages-and-frameworks/zap","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110028,"quadrant":"platforms","volume_date":"2021-10","description":"Mambu is a SaaS cloud banking platform. It empowers customers to easily and flexibly build and change their banking and lending products. Unlike other out-of-box core banking platforms that you can only adapt with hard-coded integration, Mambu is designed for constantly changing financial offerings. It comes with an opinionated workflow, while also providing an API-driven approach to customize business logic, process and integrations. We currently have several projects using Mambu. With its cloud-based scalability and highly customizable capabilities, it's becoming one of the sensible default domain systems when building financial products.","blip_selector":"mambu","name":"Mambu","display_name":"Mambu","url":"/radar/platforms/mambu","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1244,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Since we first mentioned TensorFlow Lite in the Radar in 2018, we've used it in several products and are happy to report that it works as advertised. The standard use case is to integrate pretrained models into mobile apps, but TensorFlow Lite also supports on-device learning which opens further areas of application. Numerous examples on the website showcase many common areas of application, such as image classification and object detection, but also hint at new ways of interaction using, for example, pose estimation and gesture recognition. | TensorFlow Lite is the designated successor of TensorFlow Mobile, which we mentioned in our previous Radar. Like Mobile it is a lightweight solution tuned and optimized for mobile devices (Android and iOS). We expect the standard use case to be the deployment of pretrained models into mobile apps but TensorFlow Lite also supports on-device learning which opens further areas of application.","blip_selector":"tensorflow-lite","name":"TensorFlow Lite","display_name":"TensorFlow Lite","url":"/radar/languages-and-frameworks/tensorflow-lite","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202110072,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"In a move that mirrors Apple's introduction of SwiftUI, Google introduced Jetpack Compose as a new and quite different approach to building user interfaces for modern Android applications. Compose brings more powerful tools and an intuitive Kotlin API. In most cases less code is needed, and it has become easier to create user interfaces at runtime rather than defining a static UI that can be filled with data. With Compose Multiplatform and Kotlin Multiplatform developers now have a unified toolkit to build desktop, web and native Android apps. Wear OS 3.0+ is included, too, and with support for iOS already present in Kotlin Multiplatform Mobile, it's likely that iOS will be supported by Compose in the future.","blip_selector":"jetpack-compose","name":"Jetpack Compose","display_name":"Jetpack Compose","url":"/radar/languages-and-frameworks/jetpack-compose","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110083,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"DoWhy is a Python library to perform end-to-end causal inference and analysis. Although machine-learning models can make predictions based on factual data, exploiting the correlation of variables that were present at the time, they're insufficient in scenarios where we need to ask What if and Why questions: What if a variable changed? What would be the impact on the outcome? Causal inference is an approach to answer such questions. It estimates the causal effect, that is, the magnitude by which an outcome would change, if we changed one of the causal variables. This approach is applied when we can't arrive at the answer through observations and collecting data from A/B testing — due to the cost of experiments or limitations. The DoWhy library estimates the causal effect based on a process that uses the past collected facts and data as well as assumptions one can make knowing the domain. It uses a four-step process of modeling the causal relationships graph based on assumptions, identifying a cause for an outcome, estimating the causal effect and finally challenging those assumptions by refuting the result. We've used this library successfully in production, and it's one of the commonly used libraries in causal estimation use cases.","blip_selector":"dowhy","name":"DoWhy","display_name":"DoWhy","url":"/radar/languages-and-frameworks/dowhy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110024,"quadrant":"platforms","volume_date":"2021-10","description":"Azure Cognitive Search provides search as a service for applications that require text search over heterogeneous content. It provides push or pull-based APIs to upload and index images, unstructured text or structured document content, with limitations on supported pull-based data source types. It provides APIs over REST and .NET SDK to execute search queries, either using a simple query language or more powerful Apache Lucene queries with field-scoped queries, fuzzy search, infix and suffix wildcard search and regular expression search, among other features. We've successfully used Azure Cognitive Search alongside other Azure services, including searching content uploaded from Cosmos DB.","blip_selector":"azure-cognitive-search","name":"Azure Cognitive Search","display_name":"Azure Cognitive Search","url":"/radar/platforms/azure-cognitive-search","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110018,"quadrant":"platforms","volume_date":"2021-10","description":"MirrorMaker 2.0 (also known as MM2), built using the Kafka Connect framework, solves many tool shortcomings of previous Kafka replication approaches. It can successfully geo-replicate topic data and metadata across clusters, including offsets, consumer groups and authorization command lines (ACLs). MM2 preserves partitioning and detects new topics and partitions. We appreciated the ability to stage a cluster migration over time, an approach that can be useful in migrating from an on-prem cluster to a cloud cluster. After synchronizing the topics and consumer groups, we first migrated the clients to the new cluster location, then we migrated the producers to the new location and finally turned off MM2 and decommissioned the old cluster. We've also seen MM2 used in disaster recovery and high-availability scenarios.","blip_selector":"mirrormaker-2-0","name":"MirrorMaker 2.0","display_name":"MirrorMaker 2.0","url":"/radar/platforms/mirrormaker-2-0","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1101,"quadrant":"techniques","volume_date":"2021-10","description":"We continue to see platform engineering product teams as a sensible default with the key insight being that they're just another product team, albeit one focused on internal platform customers. Thus it is critical to have clearly defined customers and products while using the same engineering disciplines and ways of working as any other (externally focused) product team; platform teams aren't special in this regard. We strongly caution against just renaming existing internal teams \"platform teams\" while leaving ways of working and organizational structures unchanged. We're still big fans of using concepts from Team Topologies as we think about how best to organize platform teams. We consider platform engineering product teams to be a standard approach and a significant enabler for high-performing IT. | As noted in one of the themes for this edition, the industry is increasingly gaining experience with platform engineering product teams that create and support internal platforms. These platforms are used by teams across an organization and accelerate application development, reduce operational complexity and improve time to market. With increasing adoption we're also clearer on both good and bad patterns for this approach. When creating a platform, it’s critical to have clearly defined customers and products that will benefit from it rather than building in a vacuum. We caution against layered platform teams that simply preserve existing technology silos but apply the \"platform team\" label as well as against ticket-driven platform operating models. We're still big fans of using concepts from Team Topologies as we think about how best to organize platform teams. We consider platform engineering product teams to be a standard approach and a significant enabler for high-performing IT. | The adoption of cloud and DevOps — while increasing the productivity of teams who can now move more quickly with reduced dependency on centralized operations teams and infrastructure — also has constrained teams that lack the skills to self-manage a full application and operations stack. Some organizations have tackled this challenge by creating platform engineering product teams. These teams maintain an internal platform that enables delivery teams to deploy and operate systems with reduced lead time and stack complexity. The emphasis here is on API-driven self-service and supporting tools, with delivery teams still responsible for supporting what they deploy onto the platform. Organizations that consider establishing such a platform team should be very cautious not to accidentally create a separate DevOps team, nor should they simply relabel their existing hosting and operations structure as a platform. If you're wondering how to best set up platform teams, we've been using the concepts from Team Topologies to split platform teams in our projects into Enabling Teams, core \"platform within a platform\" teams and Stream-aligned Teams. | The adoption of cloud and DevOps, while increasing the productivity of teams who can now move more quickly with reduced dependency on centralized operations teams and infrastructure, also has constrained teams who lack the skills to self-manage a full application and operations stack. Some organizations have tackled this challenge by creating platform engineering product teams. These teams operate an internal platform which enables delivery teams to self-service deploy and operate systems with reduced lead time and stack complexity. The emphasis here is on API-driven self-service and supporting tools, with delivery teams still responsible for supporting what they deploy onto the platform. Organizations that consider establishing such a platform team should be very cautious not to accidentally create a separate DevOps team, nor should they simply relabel their existing hosting and operations structure as a platform. | The adoption of cloud and DevOps, while increasing the productivity of teams who can now move more quickly with reduced dependency on centralized operations teams and infrastructure, also has constrained teams who lack the skills to self-manage a full application and operations stack. Some organizations have tackled this challenge by creating platform engineering product teams. These teams operate an internal platform which enables delivery teams to self-service deploy and operate systems with reduced lead time and stack complexity. The emphasis here is on API-driven self-service and supporting tools, with delivery teams still responsible for supporting what they deploy onto the platform. Organizations that consider establishing such a platform team should be very cautious not to accidentally create a separate DevOps team, nor should they simply relabel their existing hosting and operations structure as a platform.","blip_selector":"platform-engineering-product-teams","name":"Platform engineering product teams","display_name":"Platform engineering product teams","url":"/radar/techniques/platform-engineering-product-teams","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202005092,"quadrant":"techniques","volume_date":"2021-10","description":"We keep hearing about enterprises finding their security badly compromised due to an overreliance on the \"secure\" network perimeter. Once this external perimeter is breached, internal systems prove to be poorly protected with attackers quickly and easily able to deploy automated data extraction tools and ransomware attacks that all too often remain undetected for long periods. This leads us to recommend zero trust architecture (ZTA) as a now sensible default.\n\nZTA is a paradigm shift in security architecture and strategy. It’s based on the assumption that a network perimeter is no longer representative of a secure boundary and no implicit trust should be granted to users or services based solely on their physical or network location. The number of resources, tools and platforms available to implement aspects of ZTA keeps growing and includes enforcing policies as code based on the least privilege and as-granular-as-possible principles and continuous monitoring and automated mitigation of threats; using service mesh to enforce security control application-to-service and service-to-service; implementing binary attestation to verify the origin of the binaries; and including secure enclaves in addition to traditional encryption to enforce the three pillars of data security: in transit, at rest and in memory. For introductions to the topic, consult the NIST ZTA publication and Google's white paper on BeyondProd. | While the fabric of computing and data continues to shift in enterprises — from monolithic applications to microservices, from centralized data lakes to data mesh, from on-prem hosting to polycloud, with an increasing proliferation of connected devices — the approach to securing enterprise assets for the most part remains unchanged, with heavy reliance and trust in the network perimeter: Organizations continue to make heavy investments to secure their assets by hardening the virtual walls of their enterprises, using private links and firewall configurations and replacing static and cumbersome security processes that no longer serve the reality of today. This continuing trend compelled us to highlight zero trust architecture (ZTA) again.\n\nZTA is a paradigm shift in security architecture and strategy. It’s based on the assumption that a network perimeter is no longer representative of a secure boundary and no implicit trust should be granted to users or services based solely on their physical or network location. The number of resources, tools and platforms available to implement aspects of ZTA keeps growing and includes: enforcing policies as code based on the least privilege and as granular as possible principles and continuous monitoring and automated mitigation of threats; using service mesh to enforce security control application-to-service and service-to-service; implementing binary attestation to verify the origin of the binaries; and including secure enclaves in addition to traditional encryption to enforce the three pillars of data security: in transit, at rest and in memory. For introductions to the topic, consult the NIST ZTA publication and Google's white paper on BeyondProd. | The technology landscape of organizations today is increasingly more complex with assets — data, functions, infrastructure and users — spread across security boundaries, such as local hosts, multiple cloud providers and a variety of SaaS vendors. This demands a paradigm shift in enterprise security planning and systems architecture, moving from static and slow-changing security policy management, based on trust zones and network configurations, to dynamic, fine-grained security policy enforcement based on temporal access privileges.\n\nZero trust architecture (ZTA) is an organization's strategy and journey to implement zero-trust security principles for all of their assets — such as devices, infrastructure, services, data and users — and includes implementing practices such as securing all access and communications regardless of the network location, enforcing policies as code based on the least privilege and as granular as possible, and continuous monitoring and automated mitigation of threats. Our Radar reflects many of the enabling techniques such as security policy as code, sidecars for endpoint security and BeyondCorp. If you're on your journey toward ZTA, refer to the NIST ZTA publication to learn more about principles, enabling technology components and migration patterns as well as Google's publication on BeyondProd.","blip_selector":"zero-trust-architecture","name":"Zero trust architecture","display_name":"Zero trust architecture","url":"/radar/techniques/zero-trust-architecture","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911025,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Although several frameworks promise the same ease of development and scalability typical of static site generators, we continue to have good experiences with Gatsby.js. In particular we've used it to build and deploy websites that scale to very large numbers of users without having to worry about capacity planning or deployment infrastructure. Our developers have also been impressed by the focus on accessibility and support for old browsers and that they could reuse their React.js experience. All in all, we feel Gatsby has matured well and is a solid choice in this space. | Gatsby.js is a framework to write web applications in an architectural style known as JAMstack. Part of the application is generated at build time and deployed as a static site, while the remainder of the functionality is implemented as a progressive web application (PWA) running in the browser. Such applications work without code running on the server side. Usually, though, the PWA makes calls to third-party APIs and SaaS solutions for content management, for example. In the case of Gatsby.js, all client and build time code is written using React. The framework includes some optimizations to make the web application feel fast. It provides code and data splitting out of the box to minimize load times and speeds up performance when navigating the application by prefetching resources. APIs are called via GraphQL and several plugins simplify integration with existing services.","blip_selector":"gatsby-js","name":"Gatsby.js","display_name":"Gatsby.js","url":"/radar/languages-and-frameworks/gatsby-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110021,"quadrant":"techniques","volume_date":"2021-10","description":"We're seeing continued innovation in remote collaboration tools. The new Huddles feature in Slack provides a Discord-like experience of persistent audio calls that users can jump in and out of at any time. Gather provides a creative way to emulate a virtual office with avatars and video. IDEs provide direct collaboration features for pairing and debugging: we've previously blipped Visual Studio Live Share and included JetBrains Code With Me to the list in this edition. As tools continue to evolve modalities for collaboration in addition to video conferencing, we're increasingly seeing teams participating in remote spontaneous huddling, recreating the spontaneity of informal conversations over the intentionality of scheduling a Zoom or Microsoft Teams meeting. We don't expect to ever fully recreate the richness of face-to-face communication through digital tools, but we do see improved remote team effectiveness by giving teams multiple channels of collaboration rather than relying on one toolchain for everything.","blip_selector":"remote-spontaneous-huddling","name":"Remote spontaneous huddling","display_name":"Remote spontaneous huddling","url":"/radar/techniques/remote-spontaneous-huddling","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904061,"quadrant":"tools","volume_date":"2021-10","description":"Batect continues to gain traction among our developers and is considered by many to be a default approach for configuring local development and test environments. This open-source tool (which happens to be developed by a Thoughtworker) makes it easy to set up and share a build environment based on Docker. Batect then becomes the entry point for your build system, replacing the ubiquitous go script as the basis for a “check out and go” approach. Batect continues to evolve in response to developer feedback and recently added support for Docker's BuildKit and shell tab completion. | So much energy and effort continue to be wasted on configuring local development environments and troubleshooting the \"works on my machine\" problem. For many years our teams have adopted the \"check out and go\" approach where we use a scripted approach to ensure the local development environment is configured consistently. batect is an open source tool developed by a ThoughtWorker that makes it easy to set up and share a build environment based on Docker. batect becomes the entry point script for your build system, launching containers to perform build tasks that don't rely at all on local setup. Changes to build configuration and dependencies are simply shared through source control without requiring any changes or installations on local machines or CI agents. While we like Cage, among other tools, in this space, we see batect quickly growing in favor with our teams.","blip_selector":"batect","name":"Batect","display_name":"Batect","url":"/radar/tools/batect","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110090,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"With the rising interest in — and viability of — 3D and extended reality (XR) applications in web browsers, our teams have been experimenting with React Three Fiber for developing 3D experiences on the web. React Three Fiber is a library that takes the React.js component and state model and translates it to 3D objects rendered with the Three.js library. This approach opens up 3D web programming to the wider group of developers already familiar with React.js and the rich ecosystem of tools and libraries surrounding it. However, when developing applications with React Three Fiber, our teams often have to manipulate the 3D scene imperatively. This doesn't mix well with the reactive component paradigm provided by React. There is no escaping the need to understand the basic 3D rendering mechanisms. The jury is still out on whether React Three Fiber offers enough abstraction to warrant learning its idiosyncrasies or if it's better just to work with Three.js directly.","blip_selector":"react-three-fiber","name":"React Three Fiber","display_name":"React Three Fiber","url":"/radar/languages-and-frameworks/react-three-fiber","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110084,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Headless UI is an unstyled component library for either React.js or Vue.js from the same people that created Tailwind CSS. Our developers like that they don't have to customize or work around the default styles that other component libraries come with. The components' rich functionality and full accessibility, combined with the frictionless styling, allows developers to focus more productively on the business problem and user experience. Unsurprisingly, Headless UI also pairs well with Tailwind CSS classes.","blip_selector":"headless-ui","name":"Headless UI","display_name":"Headless UI","url":"/radar/languages-and-frameworks/headless-ui","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005054,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Our developers have continued to be productive with Tailwind CSS and are impressed with its ability to scale with large teams and codebases. Tailwind CSS offers an alternative approach to CSS tools and frameworks that reduces complexity through lower-level utility CSS classes. The Tailwind CSS classes can easily be customized to suit any customer's visual identity. We've also found that it pairs particularly well with Headless UI. Tailwind CSS allows you to avoid writing any classes or CSS on your own which leads to a more maintainable codebase in the long term. It seems that Tailwind CSS offers the right balance between reusability and customization to create visual components. | CSS tools and frameworks offer predesigned components for fast results; after a while, however, they can complicate customization. Tailwind CSS proposes an interesting approach by providing lower-level utility CSS classes to create building blocks without opinionated styles and aiming for easy customization. The breadth of the low-level utilities allows you to avoid writing any classes or CSS on your own which leads to a more maintainable codebase in the long term. It seems that Tailwind CSS offers the right balance between reusability and customization to create visual components.","blip_selector":"tailwind-css","name":"Tailwind CSS","display_name":"Tailwind CSS","url":"/radar/languages-and-frameworks/tailwind-css","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110038,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Micro frontends have continued to gain in popularity since they were first introduced. However, it's easy to fall into micro frontend anarchy if teams fail to maintain consistency across an application, from styling technique to state management. Qiankun, which means heaven and earth in Chinese, is a JavaScript library built to provide an out-of-the-box solution for this. Qiankun is based on single-spa, so it allows different frameworks to coexist in a single application. It also provides style isolation and JavaScript sandbox to ensure the style or state of microapplications do not interfere with each other. Qiankun has received some attention in the community; our teams are also assessing it, hoping that it can support more friendly debugging.","blip_selector":"qiankun","name":"Qiankun","display_name":"Qiankun","url":"/radar/languages-and-frameworks/qiankun","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110032,"quadrant":"tools","volume_date":"2021-10","description":"There are many reasons to love Unix, but the one that has profoundly affected our industry is the Unix philosophy of building applications that \"do one thing and do it well.\" Unix commands embody this philosophy. A set of small functions that can be piped together to create more complex solutions. In recent years, programmers have contributed to a growing set of modern Unix commands. These modern versions attempt to be smaller and faster, often written in Rust. They include additional features such as syntax highlighting and utilize features of modern terminals. They aim to support programmers natively by integrating nicely with git and recognizing source code files. For example, bat is a replacement for cat with paging and syntax highlighting; exa is a replacement for ls with extended file information and ripgrep is a faster grep replacement that by default ignores gitignore, binary and hidden files. The Modern Unix repository has a reference to some of these commands. We've been enjoying using these Unix commands. You should try them in improving your command-line experience. However, we caution against using them in scripts as replacements for the standard command-line utilities that are shipped in default OS distributions, because they reduce the scripts' portability running on other machines.","blip_selector":"modern-unix-commands","name":"Modern Unix commands","display_name":"Modern Unix commands","url":"/radar/tools/modern-unix-commands","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110078,"quadrant":"platforms","volume_date":"2021-10","description":"It's rare for us to feature commercial, off-the-shelf software in the Radar, much less a core banking platform. However, Thought Machine Vault (no connection to Thoughtworks) is an example of a product in this class designed to support good software engineering practices such as test-driven development, continuous delivery and infrastructure as code. Developers define banking products in Vault by writing smart contracts in Python code. This is distinctly different from the standard no-code approach where customization is done through graphical interfaces or proprietary configuration files or both. Because products are defined in regular Python code, developers have access to a range of tools such as test frameworks and version control to ensure that their work is safe and accurate. We wish more financial services platforms were designed with developer effectiveness in mind.","blip_selector":"thought-machine-vault","name":"Thought Machine Vault","display_name":"Thought Machine Vault","url":"/radar/platforms/thought-machine-vault","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104052,"quadrant":"techniques","volume_date":"2021-10","description":"We continue to see many teams working and collaborating remotely; for these teams remote mob programming is a technique that is well worth trying. Remote mob programming allows teams to quickly \"mob\" around an issue or piece of code without the physical constraints of only being able to fit so many people around a pairing station. Teams can quickly collaborate on an issue or piece of code using their video conferencing tool of choice without having to connect to a big display, book a physical meeting room or find a whiteboard. | Mob programming is one of those techniques that our teams have found to be easier when done remotely. Remote mob programming is allowing teams to quickly \"mob\" around an issue or piece of code without the physical constraints of only being able to fit so many people around a pairing station. Teams can quickly collaborate on an issue or piece of code without having to connect to a big display, book a physical meeting room or find a whiteboard.","blip_selector":"remote-mob-programming","name":"Remote mob programming","display_name":"Remote mob programming","url":"/radar/techniques/remote-mob-programming","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110045,"quadrant":"platforms","volume_date":"2021-10","description":"ExternalDNS synchronizes Kubernetes ingresses and services with external DNS providers, filling a hole previously filled by kops dns-controller, Zalando's Mate or route53-kubernetes — the last two of which have been deprecated in favor of ExternalDNS. The tool makes internal Kubernetes resources discoverable via public DNS servers, removing a sometimes manual step to update DNS records when an ingress host or service's IP address changes. It supports a huge list of DNS service providers out of the box with more being added via community support. As the old joke goes, it's always DNS.","blip_selector":"externaldns","name":"ExternalDNS","display_name":"ExternalDNS","url":"/radar/platforms/externaldns","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110087,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"Polars is an in-memory data frame library implemented in Rust. Unlike other data frames (such as Pandas), Polars is multithreaded and safe for parallel operations. The in-memory data is organized in the Apache Arrow format for efficient analytic operations and to enable interoperability with other tools. If you're familiar with Pandas, you can quickly get started with Polars' Python bindings. We believe Polars, with Rust implementation and Python bindings, is a performant in-memory data frame to assess for your analytical needs.","blip_selector":"polars","name":"Polars","display_name":"Polars","url":"/radar/languages-and-frameworks/polars","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110080,"quadrant":"platforms","volume_date":"2021-10","description":"Even today, considering all the development and infrastructure tools at our disposal, we often reach a point where we need a script to glue several things together or to automate a recurring task. Current favorites for writing these scripts are bash and Python, but we're happy to report that there's a new, exciting option: Clojure. This is made possible with Babashka, a complete Clojure run time implemented with GraalVM. Babashka ships with libraries that cover most of the use cases for which you'd use a scripting tool, and loading of further libraries is possible, too. The use of GraalVM brings startup times within range of native tools, and it also makes Babashka one of the few options for a multithreaded scripting environment, for those rare cases when it's needed.","blip_selector":"babashka","name":"Babashka","display_name":"Babashka","url":"/radar/platforms/babashka","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110100,"quadrant":"tools","volume_date":"2021-10","description":"Wav2Vec 2.0 is a self-supervised learning framework for speech recognition. With this framework the model is trained in two phases. First, it begins in self-supervised mode using unlabeled data and tries to achieve the best possible speech representation. Then it uses supervised fine-tuning, during which labeled data teaches the model to predict particular words or phonemes. We've used Wav2Vec and find its approach quite powerful for building automatic speech recognition models for regional languages with limited availability of labeled data.","blip_selector":"wav2vec-2-0","name":"Wav2Vec 2.0","display_name":"Wav2Vec 2.0","url":"/radar/tools/wav2vec-2-0","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110088,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"PyTorch Geometric is a geometric deep learning extension library for PyTorch. Geometric deep learning aims to build neural networks that can learn from non-Euclidean data like graphs. Graph network-based machine-learning approaches have been of increasing interest in social network modeling and in biomedical fields, specifically in drug discovery. PyTorch Geometric provides an easy-to-use library to design complicated graph network problems like protein structure representation. It has GPU and CPU support and includes a good collection of graph-based machine-learning algorithms based on recent research.","blip_selector":"pytorch-geometric","name":"PyTorch Geometric","display_name":"PyTorch Geometric","url":"/radar/languages-and-frameworks/pytorch-geometric","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1018,"quadrant":"tools","volume_date":"2021-10","description":"Releasing applications for iOS involves a code-signing step. Although supported by Apple's toolchain, the process can be cumbersome, error prone and full of unexpected surprises. We're happy to report that fastlane, already our tool of choice for automating the release process of mobile applications, provides a better solution: match is integrated into fastlane's smooth process, and it implements a new approach to manage code signing for teams. Instead of storing the signing keys in the developer's macOS keychain — the default approach — the new approach revolves around storing the keys and certificates in a Git repository. This not only makes it easier to on-board new team members and set up new development machines; in our experience, it also is the easiest method to integrate signing into continuous delivery pipelines. | Web application developers have it easy when it comes to simplifying and automating diverse application workflows; they can choose from a variety of solutions to help automate release processes. When developing for mobile, however, we're dealing with two operating systems with two different ways of building, testing, distribution, generating screenshots, signing and distributing applications. To help ease the pain, our teams have adopted fastlane as the go-to tool to automate the release process for iOS and Android applications. Using simple configurations and multiple pipelines, they can achieve continuous delivery for mobile development. | Web application developers have it easy when it comes to simplifying and automating diverse application workflows; they can choose from a variety of solutions to help automate release processes. When developing for mobile, however, we're dealing with two operating systems with two different ways of building, testing, distribution, generating screenshots, signing and distributing applications. To help ease the pain, our teams have adopted fastlane as the go-to tool to automate the release process for iOS and Android applications. Using simple configurations and multiple pipelines, they can achieve continuous delivery for mobile development. | fastlane is our go-to tool for automating most of the boring activities involved in getting iOS and Android mobile apps built, tested, documented and provisioned. Simple configuration, a range of tooling and multiple pipelines make this a key ingredient in doing continuous delivery for mobile.","blip_selector":"fastlane","name":"fastlane","display_name":"fastlane","url":"/radar/tools/fastlane","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110095,"quadrant":"tools","volume_date":"2021-10","description":"Telepresence is a tool that helps shorten the feedback loop of changes that usually require a deployment for proper testing. Developers can use it to plug a process that is running on their local machines into a remote Kubernetes cluster. This gives the local process access to the remote cluster's services and features, and the local service can also temporarily replace one of the cluster services.\n\nIn situations where the service integration setup has become somewhat unwieldy, Telepresence can boost developer productivity and enable more effective local testing. However, if you get into the habit of using a clever tool like this, you may have bigger problems. For example, if you use Telepresence because it has become impossible to set up all necessary dependencies for local development, you may want to investigate the complexity of your setup and architecture. If it becomes the only way for you to do service integration tests, consider looking into consumer-driven contract testing or other automated ways of integration testing.","blip_selector":"telepresence","name":"Telepresence","display_name":"Telepresence","url":"/radar/tools/telepresence","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110091,"quadrant":"tools","volume_date":"2021-10","description":"One of the key tenets of infrastructure as code (IaC) is automated testing. If we have a solid test pyramid with good code-level coverage at the bottom, we can produce a better and more secure infrastructure. Unfortunately, tools to assist in this space have been sparse. Conftest is frequently used to test Terraform JSON and HCL code, but it is a general-purpose tool. Regula is an attractive alternative. Similar to Conftest, Regula checks for compliance of infrastructure code by applying rules written in Open Policy Agent's Rego language, but it also provides a set of primitives specifically for validating infrastructure configurations. Because both tools are based on the Rego language, Regula rules can be run by Conftest. However, Regula comes with its own command-line tool for running tests as part of a pipeline with no dependence on Conftest or OPA. Our developers have found that Regula saves time and produces much more readable, maintainable and succinct test code. Still, both tools only validate the infrastructure code. A complete suite should also test the infrastructure to ensure the code is being accurately interpreted.","blip_selector":"regula","name":"Regula","display_name":"Regula","url":"/radar/tools/regula","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005050,"quadrant":"tools","volume_date":"2021-10","description":"Our teams continue to report good results when using Lens to visualize and manage their Kubernetes clusters. Billed as an \"IDE for Kubernetes,\" Lens makes it possible to interact with the cluster without having to memorize commands or manifest file structures. Kubernetes can be complex, and we understand that a tool for visualizing cluster metrics and deployed workloads can save time and reduce some of the toil involved in maintaining a Kubernetes cluster. Instead of hiding complexity behind a simple point-and-click interface, Lens brings together the tools an administrator would run from the command line. But be cautious about interactively making changes to a running cluster via any mechanism. We generally prefer that infrastructure changes be implemented in code so they are repeatable, testable and less prone to human error. However, Lens does excel as a one-stop tool to interactively navigate through and comprehend your cluster status. | One of the strengths of Kubernetes is its flexibility and range of configuration possibilities along with the API-driven, programmable configuration mechanisms and command-line visibility and control using manifest files. However, that strength can also be a weakness: when deployments are complex or when managing multiple clusters, it can be difficult to get a clear picture of the overall status through command-line arguments and manifests alone. Lens attempts to solve this problem with an integrated environment for viewing the current state of the cluster and its workloads, visualizing cluster metrics and changing configurations through an embedded text editor. Rather than a simple point-and-click interface, Lens brings together the tools an administrator would run from the command line into a single, navigable interface. This tool is one of several approaches that are trying to tame the complexity of Kubernetes management. We've yet to see a clear winner in this space, but Lens strikes an interesting balance between a graphical UI and command-line–only tools.","blip_selector":"lens","name":"Lens","display_name":"Lens","url":"/radar/tools/lens","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110039,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"If we want animations in React Native applications, React Native Reanimated 2.0 is the way to go. We previously had Reanimated 1.x, but it had issues related to the complexity of the Reanimated declarative language and also had some additional performance costs related to initialization and communication between the React Native JavaScript thread and the UI thread. Reanimated 2.0 is an attempt at reimagining how to run animations in the UI thread; it allows us to code the animations in JavaScript and run them on the UI thread using a new API called animation worklets. It does this by spawning a secondary JavaScript context on the UI thread that then is able to run JavaScript functions. We're using it in our React Native projects which need animations and like it a lot.","blip_selector":"react-native-reanimated-2-0","name":"React Native Reanimated 2.0","display_name":"React Native Reanimated 2.0","url":"/radar/languages-and-frameworks/react-native-reanimated-2-0","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110020,"quadrant":"techniques","volume_date":"2021-10","description":"Living documentation, which comes from the behavior-driven development (BDD) community, is often considered a privilege for those well-maintained codebases with executable specifications. We found that this technique can also be applied to legacy systems. Lack of business knowledge is a common obstacle encountered by teams when doing system modernization. Code is usually the only trustworthy source of truth because staff turnover and existing documentation are outdated. Therefore it's very important to reestablish the association between the documentation and the code and spread the business knowledge among the team when we take over a legacy system. In practice, we would first try to go to the codebase and deepen our understanding of the business through simple cleanup and safe refactoring. During the process, we'll need to add annotations to the code so that we're able to automatically generate living documentation later. This is very different from doing BDD in green-field projects, but it's a good start in legacy systems. Based on the generated documentation, we would try to convert some of the specs into executable high-level automation tests. Do this iteratively, and eventually you could get living documentation in legacy systems that is closely associated with the code and partially executable.","blip_selector":"living-documentation-in-legacy-systems","name":"Living documentation in legacy systems","display_name":"Living documentation in legacy systems","url":"/radar/techniques/living-documentation-in-legacy-systems","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202110006,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"If you're using Apache Kafka and building stream-processing applications, ksqlDB is a great framework for writing simple applications using SQL-like statements. ksqlDB is not a traditional SQL database. However, it allows you to use lightweight SQL-like statements to build new Kafka streams or tables on top of the existing Kafka topics. The queries can pull data, similar to reading from a traditional database, or push results to the application when incremental changes occur. You can choose to run it as a standalone server natively as part of your existing Apache Kafka installation or as a fully managed service on Confluent Cloud. We're using ksqlDB in simple data-processing use cases. In more complex use cases, where an application requires programming code beyond algebraic SQL queries, we continue to use data-processing frameworks such as Apache Spark or Apache Flink on top of Kafka. We recommend experimenting with ksqlDB in scenarios where the simplicity of the application allows it.","blip_selector":"ksqldb","name":"ksqlDB","display_name":"ksqlDB","url":"/radar/languages-and-frameworks/ksqldb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202110034,"quadrant":"languages-and-frameworks","volume_date":"2021-10","description":"State management in React applications has been a recurring topic in the Radar, and we've recently clarified our position on Redux, a popular framework in this space. NgRx is, in essence, Redux for Angular. It's a framework for building reactive applications with Angular, providing ways to manage state and to isolate side effects. Our teams report that picking up NgRx was straightforward, not the least because it is built with RxJS, and they highlight a trade-off similar to the one we know from Redux: adding reactive state management comes with added complexity that only pays off in larger applications. The developer experience is enhanced by schematics, a scaffolding library and a set of tools that enable visual tracking of state and time-travel debugging.","blip_selector":"ngrx","name":"NgRx","display_name":"NgRx","url":"/radar/languages-and-frameworks/ngrx","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104027,"quadrant":"tools","volume_date":"2021-04","description":"Tuple is a relatively new tool optimized for remote paired programming, designed to fill the gap Slack left in the marketplace after abandoning Screenhero. Although it still exhibits some growing pains — platform availability is limited to Mac OS for now (with Linux support coming soon), and it has some UI quirks to work through — we've had good experience using it within those constraints. Unlike general-purpose video- and screen-sharing tools like Zoom, Tuple supports dual control with two mouse cursors, and unlike options such as Visual Studio Live Share, it isn't tied to an IDE. Tuple supports voice and video calls, clipboard sharing, and lower latency than general-purpose tools; and its ability to let you draw and erase in your pair's screen with ease makes Tuple a very intuitive and developer-friendly tool.","blip_selector":"tuple","name":"Tuple","display_name":"Tuple","url":"/radar/tools/tuple","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911040,"quadrant":"techniques","volume_date":"2021-04","description":"As application development becomes increasingly dynamic and complex, it's a challenge to deliver accessible and usable products with consistent style. This is particularly true in larger organizations with multiple teams working on different products. Design systems define a collection of design patterns, component libraries and good design and engineering practices that ensure consistent digital products. Built on the corporate style guides of the past, design systems offer shared libraries and documents that are easy to find and use. Generally, guidance is written down as code and kept under version control so that the guide is less ambiguous and easier to maintain than simple documents. Design systems have become a standard approach when working across teams and disciplines in product development because they allow teams to focus. They can address strategic challenges around the product itself without reinventing the wheel every time a new visual component is needed. | As application development becomes increasingly dynamic and complex, it's a challenge to achieve the effective delivery of accessible and usable products that are consistent in style. Design systems define a collection of design patterns, component libraries and good design and engineering practices that ensure consistency in the development of digital products. We've found design systems a useful addition to our toolbox when working across teams and disciplines in product development, because they allow teams to focus on more strategic challenges around the product itself without the need to reinvent the wheel every time they need to add a visual component. The types of components and tools you use to create design systems can vary greatly.","blip_selector":"design-systems","name":"Design systems","display_name":"Design systems","url":"/radar/techniques/design-systems","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104033,"quadrant":"tools","volume_date":"2021-04","description":"We've always been keen to find tools that can shorten the software development feedback cycle; esbuild is such an example. As the front-end codebase grows larger, we usually face a packaging time of minutes. As a JavaScript bundler optimized for speed, esbuild can reduce this time by a factor of 10 to 100. It is written in Golang and uses a more efficient approach in the process of parsing, printing and source map generation which significantly surpasses build tools such as Webpack and Parcel in building time. esbuild may not be as comprehensive as those tools in JavaScript syntax transformation; however, this doesn't stop many of our teams from switching to esbuild as their default.","blip_selector":"esbuild","name":"esbuild","display_name":"esbuild","url":"/radar/tools/esbuild","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202104054,"quadrant":"techniques","volume_date":"2021-04","description":"We strongly advise organizations to make sure, when they really need to use cloud service accounts, that they are rotating the credentials. Rotation is one of the three R's of security. It is far too easy for organizations to forget about these accounts unless an incident occurs. This is leading to accounts with unnecessarily broad permissions remaining in use for long periods alongside a lack of planning for how to replace or rotate them. Regularly applying a cloud service account rotation approach also provides a chance to exercise the principle of least privilege.","blip_selector":"service-account-rotation-approach","name":"Service account rotation approach","display_name":"Service account rotation approach","url":"/radar/techniques/service-account-rotation-approach","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104051,"quadrant":"tools","volume_date":"2021-04","description":"Adopting a \"you build it, you run it\" DevOps philosophy means teams have increased attention on both technical and business metrics that can be extracted from the systems they deploy. Often we find that analytics tooling is difficult to access for most developers, so the work to capture and present metrics is left to other teams — long after features are shipped to end users. Our teams have found Redash to be very useful for querying product metrics and creating dashboards in a way that can be self-served by general developers, shortening feedback cycles and focusing the whole team on the business outcomes.","blip_selector":"redash","name":"Redash","display_name":"Redash","url":"/radar/tools/redash","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104077,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"AWS Data Wrangler is an open-source library that extends the capabilities of Pandas to AWS by connecting data frames to AWS data-related services. In addition to Pandas, this library leverages Apache Arrow and Boto3 to expose several APIs to load, transform and save data from data lakes and data warehouses. An important limitation is that you can't do large distributed data pipelines with this library. However, you can leverage the native data services — like Athena, Redshift and Timestream — to do the heavy lifting and pull data in order to express complex transformations that are well suited for data frames. We've used AWS Data Wrangler in production and like that it lets you focus on writing transformations without spending too much time on the connectivity to AWS data services.","blip_selector":"aws-data-wrangler","name":"AWS Data Wrangler","display_name":"AWS Data Wrangler","url":"/radar/languages-and-frameworks/aws-data-wrangler","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202104105,"quadrant":"techniques","volume_date":"2021-04","description":"Password policies are a standard default for many organizations today. However, we're still seeing organizations requiring passwords to include a variety of symbols, numbers, uppercase and lowercase letters as well as inclusion of special characters. These are naive password complexity requirements that lead to a false sense of security as users will opt for more insecure passwords because the alternative is difficult to remember and type. According to NIST recommendations, the primary factor in password strength is password length, and therefore users should choose long passphrases with a maximum requirement of 64 characters (including spaces). These passphrases are more secure and memorable.","blip_selector":"naive-password-complexity-requirements","name":"Naive password complexity requirements","display_name":"Naive password complexity requirements","url":"/radar/techniques/naive-password-complexity-requirements","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104119,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"In the previous Radar, we commented on the beginning of a phase of experimentation with state management in React applications. We moved Redux back into the Trial ring, documenting that it is no longer our default choice, and we mentioned Facebook's Recoil. In this volume we want to highlight Jotai and Zustand: Both are state management libraries for React; both aim to be small and simple to use; and, perhaps not by complete coincidence, both names are translations of the word state into Japanese and German, respectively. Beyond these similarities, however, they differ in their design. Jotai's design is closer to that of Recoil in that state consists of atoms stored within the React component tree, whereas Zustand stores the state outside of React in a single state object, much like the approach taken by Redux. The authors of Jotai provide a helpful checklist to decide when to use which.","blip_selector":"jotai-and-zustand","name":"Jotai and Zustand","display_name":"Jotai and Zustand","url":"/radar/languages-and-frameworks/jotai-and-zustand","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104120,"quadrant":"techniques","volume_date":"2021-04","description":"Privacy-focused web analytics is a technique for gathering web analytics without compromising end user privacy by keeping the end users truly anonymous. One surprising consequence of General Data Protection Regulation (GDPR) compliance is the decision taken by many organizations to degrade the user experience with complex cookie consent processes, especially when the user doesn't immediately consent to the \"all the cookies\" default settings. Privacy-focused web analytics has the dual benefit of both observing the spirit and letter of GDPR while also avoiding the need to introduce intrusive cookie consent forms. One implementation of this approach is Plausible.","blip_selector":"privacy-focused-web-analytics","name":"Privacy-focused web analytics","display_name":"Privacy-focused web analytics","url":"/radar/techniques/privacy-focused-web-analytics","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104069,"quadrant":"tools","volume_date":"2021-04","description":"While duck typing is certainly seen as a feature by many Python programmers, sometimes — especially for larger codebases — type checking can be useful, too. For that reason a number of type annotations are proposed as Python Enhancement Proposals (PEPs), and Pyright is a type checker that works with these annotations. In addition, it provides some type inference and guards that understand conditional code flow constructs. Designed with large codebases in mind, Pyright is fast, and its watch mode checks happen incrementally as files are changed to further shorten the feedback cycle. Pyright can be used directly on the command line, but integrations for VS Code, Emacs, vim, Sublime, and possibly other editors are available, too. In our experience, Pyright is preferable to alternatives like mypy.","blip_selector":"pyright","name":"Pyright","display_name":"Pyright","url":"/radar/tools/pyright","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104108,"quadrant":"platforms","volume_date":"2021-04","description":"Redpanda is a streaming data platform for developers. It is API-compatible with Apache Kafka, allowing architectures already running in the Kafka ecosystem to benefit from Redpanda’s improved simplicity, performance and hardware efficiency over Kafka. Redpanda is free from external dependencies such as ZooKeeper, and written in C++, eliminating the need for JVM management. It leverages the Raft protocol for data replication and is Jepsen-tested to validate for correctness. Redpanda also offers much-reduced tail latencies and increased throughput due to a series of optimizations. Some advanced capabilities available for enterprise customers include tiered storage and inline WebAssembly (WASM) transformations.","blip_selector":"redpanda","name":"Redpanda","display_name":"Redpanda","url":"/radar/platforms/redpanda","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911076,"quadrant":"tools","volume_date":"2021-04","description":"Since we last wrote about dbt, we've used it in a few projects and like what we've seen. For example, we like that dbt makes the transformation part of ELT pipelines more accessible to consumers of the data as opposed to just the data engineers building the pipelines. It does this while encouraging good engineering practices such as versioning, automated testing and deployment. SQL continues to be the lingua franca of the data world (including databases, warehouses, query engines, data lakes and analytical platforms) and most of these systems support it to some extent. This allows dbt to be used against these systems for transformations by just building adaptors. The number of native connectors has grown to include Snowflake, BigQuery, Redshift and Postgres, as has the range of community plugins. We see tools like dbt helping data platforms become more \"self service\" capable. | Data transformation is an essential part of data-processing workflows: filtering, grouping or joining multiple sources into a format that is suitable for analyzing data or feeding machine-learning models. dbt is an open-source tool and a commercial SaaS product that provides simple and effective transformation capabilities for data analysts. The current frameworks and tooling for data transformation fall either into the group of powerful and flexible — requiring intimate understanding of the programming model and languages of the framework such as Apache Spark — or in the group of dumb drag-and-drop UI tools that don't lend themselves to reliable engineering practices such as automated testing and deployment. dbt fills a niche: it uses SQL — an interface widely understood — to model simple batch transformations, while it provides command-line tooling that encourages good engineering practices such as versioning, automated testing and deployment; essentially it implements SQL-based transformation modeling as code. dbt currently supports multiple data sources, including Snowflake and Postgres, and provides various execution options, such as Airflow and Apache's own cloud offering. Its transformation capability is limited to what SQL offers, and it doesn't support real-time streaming transformations at the time of writing.","blip_selector":"dbt","name":"dbt","display_name":"dbt","url":"/radar/tools/dbt","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104092,"quadrant":"tools","volume_date":"2021-04","description":"HashiCorp Boundary combines the secure networking and identity management capabilities needed for brokering access to your hosts and services in one place and across a mix of cloud and on-premise resources if needed. Key management can be done by integrating the key management service of your choice, be it from a cloud vendor or something like HashiCorp Vault. HashiCorp Boundary supports a growing number of identity providers and can be integrated with parts of your service landscape to help define permissions, not just on host but also on a service level. For example, it enables you to control fine-grained access to a Kubernetes cluster, and dynamically pulling in service catalogs from various sources is on the roadmap. All of this stays out of the way of the engineering end users who get the shell experience they're used to, securely connected through Boundary's network management layer.","blip_selector":"hashicorp-boundary","name":"HashiCorp Boundary","display_name":"HashiCorp Boundary","url":"/radar/tools/hashicorp-boundary","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104028,"quadrant":"platforms","volume_date":"2021-04","description":"Bit.dev is a cloud-hosted collaborative platform for UI components extracted, modularized and reused with Bit. Web components have been around for a while, but building a modern front-end application by assembling small, independent components extracted from other projects has never been easy. Bit was designed to let you do exactly that: extract a component from an existing library or project. You can either build your own service on top of Bit for component collaboration or use Bit.dev.","blip_selector":"bit-dev","name":"Bit.dev","display_name":"Bit.dev","url":"/radar/platforms/bit-dev","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202104043,"quadrant":"techniques","volume_date":"2021-04","description":"The explosion of interest around software platforms has created a lot of value for organizations, but the path to building a platform-based delivery model is fraught with potential dead ends. It's common in the excitement of new paradigms to see a resurgence of older techniques rebranded with the new vernacular, making it easy to lose sight of the reasons we moved past those techniques in the first place. For an example of this rebranding, see our blip on traditional ESBs make a comeback as API gateways in the previous Radar. Another example we're seeing is rehashing the approach of dividing teams by technology layer but calling them platforms. In the context of building an application, it used to be common to have a front-end team separate from the business logic team separate from the data team, and we see analogs to that model when organizations segregate platform capabilities among teams dedicated to a business or data layer. Thanks to Conway's Law, we know that organizing platform capability teams around business capabilities is a more effective model, giving the team end-to-end ownership of the capability, including data ownership. This helps to avoid the dependency management headaches of layered platform teams, with the front-end team waiting on the business logic team waiting on the data team to get anything done.","blip_selector":"layered-platform-teams","name":"Layered platform teams","display_name":"Layered platform teams","url":"/radar/techniques/layered-platform-teams","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104096,"quadrant":"platforms","volume_date":"2021-04","description":"As more businesses turn to events as a way to share data among microservices, collect analytics or feed data lakes, Apache Kafka has become a favorite platform to support an event-driven architectural style. Although Kafka was a revolutionary concept in scalable persistent messaging, a lot of moving parts are required to make it work, including ZooKeeper, brokers, partitions, and mirrors. While these can be particularly tricky to implement and operate, they do offer great flexibility and power when needed, especially at an industrial enterprise scale. Because of the high barrier to entry presented by the full Kafka ecosystem, we welcome the recent explosion of platforms offering the Kafka API without Kafka. Recent entries such as Kafka on Pulsar and Redpanda offer alternative architectures, and Azure Event Hubs for Kafka provides some compatibility with Kafka producer and consumer APIs. Some features of Kafka, like the streams client library, are not compatible with these alternative brokers, so there are still reasons to choose Kafka over alternative brokers. It remains to be seen, however, if developers actually adopt this strategy or if it is merely an attempt by competitors to lure users away from the Kafka platform. Ultimately, perhaps Kafka's most enduring impact could be the convenient protocol and API provided to clients.","blip_selector":"kafka-api-without-kafka","name":"Kafka API without Kafka","display_name":"Kafka API without Kafka","url":"/radar/platforms/kafka-api-without-kafka","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104128,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"So far, Flutter has primarily supported native iOS and Android applications. However, the Flutter team's vision is to support building applications on every platform. Flutter for Web is one step in that direction — it allows us to build apps for iOS, Android and the browser from the same codebase. It has been available for over a year now on the \"Beta\" channel, but with the recent Flutter 2.0 release, Flutter for Web has hit the stable milestone. In the initial release of web support, the Flutter team is focusing on progressive web apps, single-page apps and expanding existing mobile apps to the web. The application and framework code (all in Dart) are compiled to JavaScript instead of ARM machine code, which is used for mobile applications. Flutter’s web engine offers a choice of two renderers: an HTML renderer, which uses HTML, CSS, Canvas and SVG, and a CanvasKit renderer that uses WebAssembly and WebGL to render Skia paint commands to the browser canvas. A few of our teams have started using Flutter for Web and like the initial results.","blip_selector":"flutter-for-web","name":"Flutter for Web","display_name":"Flutter for Web","url":"/radar/languages-and-frameworks/flutter-for-web","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010056,"quadrant":"tools","volume_date":"2021-04","description":"Playwright allows you to write Web UI tests for Chromium and Firefox as well as WebKit, all through the same API. The tool has gained some attention for its support of all the major browser engines which it achieves by including patched versions of Firefox and Webkit. We continue to hear positive experience reports with Playwright, in particular its stability. Teams have also found it easy to migrate from Puppeteer, which has a very similar API. | Web UI testing continues to be an active space. Some of the folks who built Puppeteer have since moved on to Microsoft and are now applying their learnings to Playwright, which allows you to write tests for Chromium and Firefox as well as WebKit, all through the same API. Playwright has gained some attention for its support of all the major browser engines, which it currently achieves by including patched versions of Firefox and Webkit. It remains to be seen how quickly other tools can catch up, with more and more support for the Chrome DevTools Protocol as a common API for automating browsers.","blip_selector":"playwright","name":"Playwright","display_name":"Playwright","url":"/radar/tools/playwright","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104109,"quadrant":"tools","volume_date":"2021-04","description":"OR-Tools is an open-source software suite for solving combinatorial optimization problems. These optimization problems have a very large set of possible solutions, and tools like OR-Tools are quite helpful in seeking the best solution. You can model the problem in any one of the supported languages — Python, Java, C# or C++ — and choose the solvers from several supported open-source or commercial solvers. We've successfully used OR-Tools in multiple optimization projects with integer and mixed-integer programming.","blip_selector":"or-tools","name":"OR-Tools","display_name":"OR-Tools","url":"/radar/tools/or-tools","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104070,"quadrant":"techniques","volume_date":"2021-04","description":"All major cloud providers offer a dazzling array of machine-learning (ML) solutions. These powerful tools can provide a lot of value, but come at a cost. There is the pure run cost for these services charged by the cloud provider. In addition, there is a kind of operations tax. These complex tools need to be understood and operated, and with each new tool added to the architecture this tax burden increases. In our experience, teams often choose complex tools because they underestimate the power of simpler tools such as linear regression. Many ML problems don't require a GPU or neural networks. For that reason we advocate for the simplest possible ML, using simple tools and models and a few hundred lines of Python on the compute platform you have at hand. Only reach for the complex tools when you can demonstrate the need for them.","blip_selector":"simplest-possible-ml","name":"Simplest possible ML","display_name":"Simplest possible ML","url":"/radar/techniques/simplest-possible-ml","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104039,"quadrant":"platforms","volume_date":"2021-04","description":"JuiceFS is an open-source, distributed POSIX file system built on top of Redis and an object store service (for example, Amazon S3). If you're building new applications, then our recommendation has always been to interact directly with the object store without going through another abstraction layer. However, JuiceFS can be an option if you're migrating legacy applications that depend on traditional POSIX file systems to the cloud.","blip_selector":"juicefs","name":"JuiceFS","display_name":"JuiceFS","url":"/radar/platforms/juicefs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104073,"quadrant":"tools","volume_date":"2021-04","description":"The number of services offered by the big cloud providers keeps growing, but so does the convenience and maturity of tools that help you use them securely and efficiently. Recommender is a service on Google Cloud that analyzes your resources and gives you recommendations on how to optimize them based on your actual usage. The service consists of a range of \"recommenders\" in areas such as security, compute usage or cost savings. For example, the IAM Recommender helps you better implement the principle of least privilege by pointing out permissions that are never actually used and therefore are potentially too broad.","blip_selector":"recommender","name":"Recommender","display_name":"Recommender","url":"/radar/tools/recommender","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104053,"quadrant":"techniques","volume_date":"2021-04","description":"Secure multiparty computing (MPC) solves the problem of collaborative computing that protects privacy between parties that do not trust each other. It's aim is to safely calculate an agreed-upon problem without a trusted third party, while each participant is required to partake in the calculation result and can't be obtained by other entities. A simple illustration for MPC is the millionaires' problem: two millionaires want to understand who is the richest, but neither want to share their actual net worth with each other nor trust a third party. The implementation approaches of MPC vary; scenarios may include secret sharing, oblivious transfer, garbled circuits or homomorphic encryption. Some commercial MPC solutions that have recently appeared (e.g., Antchain Morse) claim to help solve the problems of secret sharing and secure machine learning in scenarios such as multiparty joint credit investigation and medical records data exchange. Although these platforms are attractive from a marketing perspective, we've yet to see whether they're really useful.","blip_selector":"secure-multiparty-computing","name":"Secure multiparty computing","display_name":"Secure multiparty computing","url":"/radar/techniques/secure-multiparty-computing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911005,"quadrant":"platforms","volume_date":"2021-04","description":"Since we last mentioned Snowflake in the Radar, we've gained more experience with it as well as with data mesh as an alternative to data warehouses and lakes. Snowflake continues to impress with features like time travel, zero-copy cloning, data sharing and its marketplace. We also haven't found anything we don't like about it, all of which has led to our consultants generally preferring it over the alternatives. Redshift is moving toward storage and compute separation, which has been a strong point of Snowflake, but even with Redshift Spectrum it isn't as convenient and flexible to use, partly because it is bound by its Postgres heritage (we do still like Postgres, by the way). Federated queries can be a reason to go with Redshift. When it comes to operations, Snowflake is much simpler to run. BigQuery, which is another alternative, is very easy to operate, but in a multicloud setup Snowflake is a better choice. We can also report that we've used Snowflake successfully with GCP, AWS, and Azure. | Snowflake has proven to be a robust SaaS big data storage, warehouse or lake solution for many of our clients. It has a superior architecture to scale storage, compute, and services to load, unload and use data. It's also very flexible: it supports storage of structured, semi-structured and unstructured data; provides a growing list of connectors for different access patterns such as Spark for data science and SQL for analytics; and runs on multiple cloud providers. Our advice to many of our clients is to use managed services for their utility technology such as big data storage; however, if the risk and regulations prohibit the use of managed services, then Snowflake is a good candidate for companies with large volumes of data and heavy processing workloads. Although we've been successful using Snowflake in our medium-sized engagements, we've yet to experience Snowflake in large ecosystems where data need to be owned across segments of the organization. | We often relate data warehousing to a central infrastructure that is hard to scale and manage with the growing demands around data. Snowflake, however, is a new SQL Data Warehouse as a Service solution built from the ground up for the cloud. With a bunch of neatly crafted features such as database-level atomicity, structured and semi-structured data support, in-database analytics functions and above all with a clear separation of storage, compute and services layer, Snowflake addresses most of the challenges faced in data warehousing.","blip_selector":"snowflake","name":"Snowflake","display_name":"Snowflake","url":"/radar/platforms/snowflake","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104009,"quadrant":"techniques","volume_date":"2021-04","description":"As the cloud is becoming more and more a commodity and being able to spin up cloud sandboxes is easier and available at scale, our teams prefer cloud-only (as opposed to local) development environments to reduce maintenance complexity. We're seeing that the tooling to do local simulation of cloud-native services limits the confidence in developer build and test cycles; therefore, we're looking to focus on standardizing cloud sandboxes over running cloud-native components on a developer machine. This will drive good infrastructure-as-code practices as a forcing function and good onboarding processes for provisioning sandbox environments for developers. There are risks associated with this transition, as it assumes that developers will have an absolute dependency on cloud environment availability, and it may slow down the developer feedback loop. We strongly recommend you adopt some lean governance practices regarding standardization of these sandbox environments, especially with regard to security, IAM and regional deployments.","blip_selector":"cloud-sandboxes","name":"Cloud sandboxes","display_name":"Cloud sandboxes","url":"/radar/techniques/cloud-sandboxes","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104008,"quadrant":"techniques","volume_date":"2021-04","description":"We're often asked to refresh, update or remediate legacy systems that we didn't originally build. Sometimes, technical issues need our attention such as improving performance or reliability. One common approach to address these issues is to create \"technical stories\" using the same format as a user story but with a technical outcome rather than a business one. But these technical tasks are often difficult to estimate, take longer than anticipated or don't end up having the desired outcome. An alternative, more successful method is to apply hypothesis-driven legacy renovation. Rather than working toward a standard backlog, the team takes ownership of a measurable technical outcome and collectively establishes a set of hypotheses about the problem. They then conduct iterative, time-boxed experiments to verify or disprove each hypothesis in order of priority. The resulting workflow is optimized for reducing uncertainty rather than following a plan toward a predictable outcome.","blip_selector":"hypothesis-driven-legacy-renovation","name":"Hypothesis-driven legacy renovation","display_name":"Hypothesis-driven legacy renovation","url":"/radar/techniques/hypothesis-driven-legacy-renovation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104104,"quadrant":"platforms","volume_date":"2021-04","description":"Opstrace is an open-source observability platform intended to be deployed in the user's own network. If we don't use commercial solutions like Datadog (for example, because of cost or data residency concerns), the only solution is to build your own platform composed of open-source tools. This can take a lot of effort — Opstrace is intended to fill this gap. It uses open-source APIs and interfaces such as Prometheus and Grafana and adds additional features on top like TLS and authentication. At the heart of Opstrace runs a Cortex cluster to provide the scalable Prometheus API as well as a Loki cluster for the logs. It's fairly new and still misses features when compared to solutions like Datadog or SignalFX. Still, it's a promising addition to this space and worth keeping an eye on.","blip_selector":"opstrace","name":"Opstrace","display_name":"Opstrace","url":"/radar/platforms/opstrace","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904030,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"We've had a bit more experience using Next.js for React codebases since the last time we wrote about it. Next.js is an opinionated, zero-configuration framework that includes simplified routing, automatic compilation and bundling with Webpack and Babel, fast hot reloading for a convenient developer workflow among other features. It provides server-side rendering by default, improves search engine optimization and the initial load time and supports incremental static generation. We've had positive experience reports from teams using Next.js and, given its large community, continue to be excited about the evolution of the framework. | React.js has revolutionized the way most people write single-page JavaScript applications. Generally, we recommend you use Create React App throughout the application lifecycle so you don't have to configure your setup, builds and packages manually. But some developers will prefer a tool whose initial defaults reflect a sound set of opinions. Next.js is just such an opinionated framework and it is garnering quite a bit of interest among our front-end enthusiasts. Next.js simplifies routing, renders on the server side by default and streamlines dependencies and builds. We're keen to see if it lives up to expectations on our own projects.","blip_selector":"next-js","name":"Next.js","display_name":"Next.js","url":"/radar/languages-and-frameworks/next-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202104022,"quadrant":"techniques","volume_date":"2021-04","description":"The API expand-contract pattern, sometimes called parallel change, will be familiar to many, especially when used with databases or code; however, we only see low levels of adoption with APIs. Specifically, we're seeing complex versioning schemes and breaking changes used in scenarios where a simple expand and then contract would suffice. For example, first adding to an API while deprecating an existing element, and then only later removing the deprecated elements once consumers are switched to the newer schema. This approach does require some coordination and visibility of the API consumers, perhaps through a technique such as consumer-driven contract testing.","blip_selector":"api-expand-contract","name":"API expand-contract","display_name":"API expand-contract","url":"/radar/techniques/api-expand-contract","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104029,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"bUnit is a testing library for Blazor that makes it easy to create tests for Blazor components in existing unit testing frameworks such as NUnit, xUnit or MSUnit. It provides a facade around the component allowing it to be run and tested within the familiar unit test paradigm, thus allowing very fast feedback and testing of the component in isolation. If you're developing for Blazor, we recommend that you add bUnit to your list of tools to try out.","blip_selector":"bunit","name":"bUnit","display_name":"bUnit","url":"/radar/languages-and-frameworks/bunit","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104087,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"We're seeing more teams adopting Python as the preferred language to build solutions, not just for data science but for back-end services too. In these scenarios, we're having good experiences with FastAPI — a modern, fast (high-performance), web framework for building APIs with Python 3.6 or later. Additionally, this framework and its ecosystem include features such as API documentation using OpenAPI that allow our teams to focus on the business functionalities and quickly create REST APIs, which makes FastAPI a good alternative to existing solutions in this space.","blip_selector":"fastapi","name":"FastAPI","display_name":"FastAPI","url":"/radar/languages-and-frameworks/fastapi","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1232,"quadrant":"tools","volume_date":"2021-04","description":"Sentry has become the default choice for many of our teams when it comes to front-end error reporting. The convenience of features like the grouping of errors or defining patterns for discarding errors with certain parameters helps deal with the flood of errors coming from many end user devices. Integrating Sentry in your CD pipeline allows you to upload source maps for more efficient error debugging, and it helps easily trace back which errors occurred in which version of the software. We also appreciate that while Sentry is primarily a SaaS offering, its source code is publicly available and it can be used for free for smaller use cases and self-hosting. | Sentry is a cross-platform application monitoring tool with a focus on error reporting. Tools like Sentry distinguish themselves from traditional logging solutions such as the ELK Stack in their focus on discovering, investigating and fixing errors. Sentry has been around for a while and supports several languages and frameworks. We've used Sentry in many projects, and it has been really useful in tracking errors, finding out if a commit actually fixed an issue and alerting us if an issue resurfaces due to a regression. | Sentry is an error-tracking tool that helps monitor and fix errors in real time. Error tracking and management tools such as Sentry distinguish themselves from traditional logging solutions such as the ELK Stack in their focus on discovering, investigating and fixing errors. Sentry has been around for some time and is quite popular — error-tracking tools are increasingly useful with the current focus on \"mean time to recovery\". Sentry — with its integration options with Github, Hipchat, Heroku, Slack, among other platforms — enables us to keep a close eye on our apps. It can provide error notifications following a release, enable us to track whether new commits actually fix the issue and alert us if an issue comes back due to a regression.","blip_selector":"sentry","name":"Sentry","display_name":"Sentry","url":"/radar/tools/sentry","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104035,"quadrant":"techniques","volume_date":"2021-04","description":"Fully homomorphic encryption (HE) refers to a class of encryption methods that allow computations (such as search and arithmetic) to be performed directly on encrypted data. The result of such a computation remains in encrypted form, which at a later point can be decrypted and revealed. Although the HE problem was first proposed in 1978, a solution wasn't constructed until 2009. With advances in computing power and the availability of easy-to-use open-source libraries — including SEAL, Lattigo, HElib and partially homomorphic encryption in Python — HE is becoming feasible in real-world applications. The motivating scenarios include privacy-preserving use cases, where computation can be outsourced to an untrusted party, for example, running computation on encrypted data in the cloud, or enabling a third party to aggregate homomorphically encrypted intermediate federated machine learning results. Moreover, most HE schemes are considered to be secure against quantum computers, and efforts are underway to standardize HE. Despite its current limitations, namely performance and feasibility of the types of computations, HE is worth your attention.","blip_selector":"homomorphic-encryption","name":"Homomorphic encryption","display_name":"Homomorphic encryption","url":"/radar/techniques/homomorphic-encryption","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201904066,"quadrant":"techniques","volume_date":"2021-04","description":"We see continuous delivery for machine learning (CD4ML) as a good default starting point for any ML solution that is being deployed into production. Many organizations are becoming more reliant on ML solutions for both customer offerings and internal operations so it makes sound business sense to apply the lessons and good practice captured by continuous delivery (CD) to ML solutions. | About a decade ago we introduced continuous delivery (CD), our default way to deliver software solutions. Today's solutions increasingly include machine-learning models and we find them no exception in adopting continuous delivery practices. We call this continuous delivery for machine learning (CD4ML). Although the principles of CD remain the same, the practices and tools to implement the end-to-end process of training, testing, deploying and monitoring models require some modifications. For example: version control must not only include code but also the data, the models and its parameters; the testing pyramid extends to include model bias, fairness and data and feature validation; the deployment process must consider how to promote and evaluate the performance of new models against current champion models. While the industry is celebrating the new buzzword of MLOps, we feel CD4ML is our holistic approach to implement an end-to-end process to reliably release and continuously improve machine-learning models, from idea to production. | Applying machine learning to make the business applications and services intelligent is more than just training models and serving them. It requires implementing end-to-end and continuously repeatable cycles of training, testing, deploying, monitoring and operating the models. Continuous delivery for machine learning (CD4ML) is a technique that enables reliable end-to-end cycles of development, deploying and monitoring machine learning models. The underpinning technology stack to enable CD4ML includes tooling for accessing and discovering data, version control of artefacts (such as data, model and code), continuous delivery pipelines, automated environment provisioning for various deployments and experiments, model performance assessment and tracking, and model operational observability. Companies can choose their own tool set depending on their existing tech stack. CD4ML emphasizes automation and removing manual handoffs. CD4ML is our de facto approach for developing ML models. | With an increased popularity of ML-based applications, and the technical complexity involved in building them, our teams rely heavily on continuous delivery for machine learning (CD4ML) to deliver such applications safely, quickly and in a sustainable manner. CD4ML is the discipline of bringing CD principles and practices to ML applications. It removes long cycle times between training models and deploying them to production. CD4ML removes manual handoffs between different teams, data engineers, data scientists and ML engineers in the end-to-end process of build and deployment of a model served by an application. Using CD4ML, our teams have successfully implemented the automated versioning, testing and deployment of all components of ML-based applications: data, model and code. | Continuous delivery for machine learning (CD4ML) apply continuous delivery practices to developing machine learning models so that they are always ready for production. This technique addresses two main problems of traditional machine learning model development: long cycle time between training models and deploying them to production, which often includes manually converting the model to production-ready code; and using production models that had been trained with stale data.\n\nA continuous delivery pipeline of a machine learning model has two triggers: (1) changes to the structure of the model and (2) changes to the training and test data sets. For this to work we need to both version the data sets and the model's source code. The pipeline often includes steps such as testing the model against the test data set, applying automatic conversion of the model (if necessary) with tools such as H2O, and deploying the model to production to deliver value.","blip_selector":"continuous-delivery-for-machine-learning-cd4ml","name":"Continuous delivery for machine learning (CD4ML)","display_name":"Continuous delivery for machine learning (CD4ML)","url":"/radar/techniques/continuous-delivery-for-machine-learning-cd4ml","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104086,"quadrant":"techniques","volume_date":"2021-04","description":"The group behind Ethical OS — the Omidyar Network, a self-described social change venture created by eBay founder Pierre Omidyar — has released a new iteration called Ethical Explorer. The new Ethical Explorer pack draws on lessons learned from using Ethical OS and adds further questions for product teams to consider. The kit, which can be downloaded for free and folded into cards to trigger discussion, has open-ended question prompts for several technical \"risk zones,\" including surveillance (\"can someone use our product or service to track or identify other users?\"), disinformation, exclusion, algorithmic bias, addiction, data control, bad actors and outsized power. The included field guide has activities and workshops, ideas for starting conversations and tips for gaining organizational buy-in. While we've a long way to go as an industry to better represent the ethical externalities of our digital society, we've had some productive conversations using Ethical Explorer, and we're encouraged by the broadening awareness of the importance of product decisions in addressing societal issues.","blip_selector":"ethical-explorer","name":"Ethical Explorer","display_name":"Ethical Explorer","url":"/radar/techniques/ethical-explorer","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104060,"quadrant":"techniques","volume_date":"2021-04","description":"Many of our developers coding iOS in Xcode often get headaches because the Xcodeproj file changes with every project change. The Xcodeproj file format is not human-readable, hence trying to handle merge conflicts is quite complicated and can lead to productivity loss and risk of messing up the entire project — if anything goes wrong with the file, Xcode won't work properly and developers will very likely be blocked. Instead of trying to merge and fix the file manually or version it, we recommend you use a tool-managed Xcodeproj approach: Define your Xcode project configuration in YAML (XcodeGen, Struct), Ruby (Xcake) or Swift (Tuist). These tools generate the Xcodeproj file based on a configuration file and the project structure. As a result, merge conflicts in the Xcodeproj file will be a thing of the past, and when they do happen in the configuration file, they're much easier to handle.","blip_selector":"tool-managed-xcodeproj","name":"Tool-managed Xcodeproj","display_name":"Tool-managed Xcodeproj","url":"/radar/techniques/tool-managed-xcodeproj","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104115,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"SSL public key pinning is tricky. If you select the wrong policy or don't have a backup pin, your application will stop working unexpectedly. This is where TrustKit is useful — it's an open-source framework that makes SSL public key pinning easier for iOS applications. There is an equivalent framework for Android as well. Picking the correct pinning strategy is a nuanced topic, and you can find more details about it in the TrustKit Getting Started guide. We've used TrustKit in several projects in production, and it has worked out well.","blip_selector":"trustkit","name":"TrustKit","display_name":"TrustKit","url":"/radar/languages-and-frameworks/trustkit","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1173,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"Our mobile teams now view LeakCanary as a good default choice for Android development. It detects annoying memory leaks in Android apps, is extremely simple to hook up and provides notifications with a clear trace-back to the cause of the leak. LeakCanary can save you tedious hours troubleshooting out-of-memory errors on multiple devices, and we recommend you add it to your toolkit. | Our mobile teams have been excited about LeakCanary, a tool for detecting annoying memory leaks in Android and Java. It's simple to hook up and provides notifications with a clear trace-back to the cause of the leak. Adding this to your toolkit can save tedious hours troubleshooting out-of-memory errors on multiple devices.","blip_selector":"leakcanary","name":"LeakCanary","display_name":"LeakCanary","url":"/radar/languages-and-frameworks/leakcanary","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202104002,"quadrant":"tools","volume_date":"2021-04","description":"Based on the experiences of multiple ThoughtWorks teams we suggest approaching AWS CodePipeline with caution. Specifically, we've found that once teams move beyond simple pipelines, this tool can become hard to work with. While it may seem like a \"quick win\" when first starting out with AWS, we suggest taking a step back and checking whether AWS CodePipeline will meet your longer-term needs, for example, pipeline fan-out and fan-in or more complex deployment and testing scenarios featuring nontrivial dependencies and triggers.","blip_selector":"aws-codepipeline","name":"AWS CodePipeline","display_name":"AWS CodePipeline","url":"/radar/tools/aws-codepipeline","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104017,"quadrant":"tools","volume_date":"2021-04","description":"Remember the research project pix2code that showed how to automatically generate code from GUI screenshots? Now there is a productized version of this technique — imgcook is a SaaS product from Alibaba that can intelligently transform various design files (Sketch/PSD/static images) into front-end code. Alibaba needs to customize a large number of campaign pages during the Double Eleven shopping festival. These are usually one-time pages that need to be developed quickly. Through the deep-learning method, the UX's design is initially processed into front-end code and then adjusted by the developer. Our team is evaluating this tech: although the image processing takes place on the server side while the main interface is on the web, imgcook provides tools that could integrate with the software design and development lifecycle. imgcook can generate static code as well as some data-binding component code if you define a DSL. The technology is not perfect yet; designers need to refer to certain specifications to improve the accuracy of code generation (which still needs to be adjusted by developers afterward). We've always been cautious about magic code generation, because the generated code is usually difficult to maintain in the long run, and imgcook is no exception. But if you limit the usage to a specific context, such as one-time campaign pages, it's worth a try.","blip_selector":"imgcook","name":"imgcook","display_name":"imgcook","url":"/radar/tools/imgcook","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202104031,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"A long time ago we placed ReactiveX — a family of open-source frameworks for reactive programming — into the Adopt ring of the Radar. In 2017, we mentioned the addition of RxSwift, which brought reactive programming to iOS development using Swift. Since then, Apple has introduced its own take on reactive programming in the form of the Combine framework. Combine has become our default choice for apps that support iOS 13 as an acceptable deployment target. It's easier to learn than RxSwift and integrates really well with SwiftUI. If you're planning to convert an existing application from RxSwift to Combine or work with both in the same project, you might want to look at RxCombine.","blip_selector":"combine","name":"Combine","display_name":"Combine","url":"/radar/languages-and-frameworks/combine","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202104006,"quadrant":"techniques","volume_date":"2021-04","description":"We suggest approaching GitOps with a degree of care, especially with regard to branching strategies. GitOps can be seen as a way of implementing infrastructure as code that involves continuously synchronizing and applying infrastructure code from Git into various environments. When used with a \"branch per environment\" infrastructure, changes are promoted from one environment to the next by merging code. While treating code as the single source of truth is clearly a sound approach, we're seeing branch per environment lead to environmental drift and eventually environment-specific configs as code merges become problematic or even stop entirely. This is very similar to what we've seen in the past with long-lived branches with GitFlow.","blip_selector":"gitops","name":"GitOps","display_name":"GitOps","url":"/radar/techniques/gitops","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010082,"quadrant":"tools","volume_date":"2021-04","description":"MLflow is an open-source tool for machine-learning experiment tracking and lifecycle management. The workflow to develop and continuously evolve a machine-learning model includes a series of experiments (a collection of runs), tracking the performance of these experiments (a collection of metrics) and tracking and tweaking models (projects). MLflow facilitates this workflow nicely by supporting existing open standards and integrates well with many other tools in the ecosystem. MLflow as a managed service by Databricks on the cloud, available in AWS and Azure, is rapidly maturing, and we've used it successfully in our projects. We find MLflow a great tool for model management and tracking, supporting both UI-based and API-based interaction models. Our only growing concern is that MLflow is attempting to deliver too many conflating concerns as a single platform, such as model serving and scoring. | MLflow is an open-source tool for machine-learning experiment tracking and lifecycle management. The workflow to develop and continuously evolve a machine-learning model includes a series of experiments (a collection of runs), tracking the performance of these experiments (a collection of metrics) and tracking and tweaking models (projects). MLflow facilitates this workflow nicely by supporting existing open standards and integrates well with many other tools in the ecosystem. MLflow as a managed service by Databricks on the cloud, available in AWS and Azure, is rapidly maturing and we've used it successfully in our projects. We find MLflow a great tool for model management and tracking, supporting both UI-based and API-based interaction models. Our only growing concern is that MLflow is attempting to deliver too many conflating concerns as a single platform, such as model serving and scoring.","blip_selector":"mlflow","name":"MLflow","display_name":"MLflow","url":"/radar/tools/mlflow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010022,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"Although JavaScript and its ecosystem is dominant in the web UI development space, new opportunities are opening up with the emergence of WebAssembly. Blazor continues to demand our attention; it's producing good results with our teams building interactive rich user interfaces using C# on top of WebAssembly. The fact that our teams can use C# on the frontend too allows them to share code and reuse existing libraries. That, along with the existing tooling for debugging and testing, such as bUnit, make this open-source framework worth trying. | Although JavaScript and its ecosystem is dominant in the web UI development space, new opportunities are opening up with the emergence of WebAssembly. We see Blazor as an interesting option for building interactive web UIs using C#. We especially like this open-source framework because it allows running C# code in the browser on top of WebAssembly, leveraging the .NET Standard runtime and ecosystem as well as custom libraries developed in this programming language. Additionally, it can interoperate bidirectionally with JavaScript code in the browser if needed.","blip_selector":"blazor","name":"Blazor","display_name":"Blazor","url":"/radar/languages-and-frameworks/blazor","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104041,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"The introduction of coroutines to Kotlin opened the door for several innovations — Kotlin Flow is one of them, directly integrated into the coroutines library. It's an implementation of Reactive Streams on top of coroutines. Unlike RxJava, flows are a native Kotlin API similar to the familiar sequence API with methods that include map and filter. Like sequences, flows are cold, meaning that the values of the sequence are only constructed when needed. All of this makes writing multithreaded code much simpler and easier to understand than other approaches. The toList method, predictably, converts a flow into a list which is a common pattern in tests.","blip_selector":"kotlin-flow","name":"Kotlin Flow","display_name":"Kotlin Flow","url":"/radar/languages-and-frameworks/kotlin-flow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005083,"quadrant":"techniques","volume_date":"2021-04","description":"In 2016, Christopher Allen, a key contributor to SSL/TLS, inspired us with an introduction of 10 principles underpinning a new form of digital identity and a path to get there, the path to self-sovereign identity. Self-sovereign identity, also known as decentralized identity, is a “lifetime portable identity for any person, organization, or thing that does not depend on any centralized authority and can never be taken away,” according to the Trust over IP standard. Adopting and implementing decentralized identity is gaining momentum and becoming attainable. We see its adoption in privacy-respecting customer health applications, government healthcare infrastructure and corporate legal identity. If you want to rapidly get started with decentralized identity, you can assess Sovrin Network, Hyperledger Aries and Indy OSS, as well as decentralized identifiers and verifiable credentials standards. We're watching this space closely as we help our clients with their strategic positioning in the new era of digital trust. | In 2016, Christopher Allen, a key contributor to SSL/TLS, inspired us with an introduction of 10 principles underpinning a new form of digital identity and a path to get there, the path to self-sovereign identity. Self-sovereign identity, also known as decentralized identity, is a “lifetime portable identity for any person, organization, or thing that does not depend on any centralized authority and can never be taken away,” according to the Trust over IP standard. Adopting and implementing decentralized identity is gaining momentum and becoming attainable. We see its adoption in privacy-respecting customer health applications, government healthcare infrastructure and corporate legal identity. If you want to rapidly get started with decentralized identity, you can assess Sovrin Network, Hyperledger Aries and Indy OSS, as well as decentralized identifiers and verifiable credentials standards. We're watching this space closely as we help our clients with their strategic positioning in the new era of digital trust. | Since the birth of the internet, the technology landscape has experienced an accelerated evolution toward decentralization. While protocols such as HTTP and architectural patterns such as microservices or data mesh enable decentralized implementations, identity management remains centralized. The emergence of distributed ledger technology (DLT), however, provides the opportunity to enable the concept of decentralized identity. In a decentralized identity system, entities — that is, discrete identifiable units such as people, organizations and things — are free to use any shared root of trust. In contrast, conventional identity management systems are based on centralized authorities and registries such as corporate directory services, certificate authorities or domain name registries.\n\nThe development of decentralized identifiers — globally unique, persistent and self-sovereign identifiers that are cryptographically verifiable — is a major enabling standard. Although scaled implementations of decentralized identifiers in the wild are still rare, we're excited by the premise of this movement and have started using the concept in our architecture. For the latest experiments and industry collaborations, check out Decentralized Identity Foundation.","blip_selector":"decentralized-identity","name":"Decentralized identity","display_name":"Decentralized identity","url":"/radar/techniques/decentralized-identity","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104093,"quadrant":"techniques","volume_date":"2021-04","description":"Contextual bandits is a type of reinforcement learning that is well suited for problems with exploration/exploitation trade-offs. Named after \"bandits,\" or slot machines, in casinos, the algorithm explores different options to learn more about expected outcomes and balances it by exploiting the options that perform well. We've successfully used this technique in scenarios where we've had little data to train and deploy other machine-learning models. The fact that we can add context to this explore/exploit trade-off makes it suitable for a wide variety of use cases including A/B testing, recommendations and layout optimizations.","blip_selector":"contextual-bandits","name":"Contextual bandits","display_name":"Contextual bandits","url":"/radar/techniques/contextual-bandits","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904011,"quadrant":"tools","volume_date":"2021-04","description":"Terratest caught our attention in the past as an interesting option for infrastructure testing. Since then, our teams have been using it, and they're very excited about it because of its stability and the experience it provides. Terratest is a Golang library that makes it easier to write automated tests for infrastructure code. Using infrastructure-as-code tools such as Terraform, you can create real infrastructure components (such as servers, firewalls, or load balancers) to deploy applications on them and then validate the expected behavior using Terratest. At the end of the test, Terratest can undeploy the apps and clean up resources. This makes it largely useful for end-to-end tests of your infrastructure in a real environment. | We widely use Terraform as code to configure a cloud infrastructure. Terratest is a Golang library that makes it easier to write automated tests for infrastructure code. A test run creates real infrastructure components (such as servers, firewalls or load balancers), deploys applications on them and validates the expected behavior using Terratest. At the end of the test, Terratest can undeploy the apps and clean up resources. This makes it largely useful for end-to-end tests of your infrastructure in a real environment.","blip_selector":"terratest","name":"Terratest","display_name":"Terratest","url":"/radar/tools/terratest","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202104025,"quadrant":"techniques","volume_date":"2021-04","description":"One of the ultimate goals of a platform should be to reduce ticket-based processes to an absolute minimum, as they create queues in the value stream. Sadly, we still see organizations not pushing forcefully enough toward this important goal, resulting in a ticket-driven platform operating model. This is particularly frustrating when ticket-based processes are put in front of platforms that are built on top of the self-service and API-driven features of public cloud vendors. It's hard and not necessary to achieve self-service with very few tickets right from the start, but it needs to be the destination.\n\nOver-reliance on bureaucracy and lack of trust are among the causes of this reluctance to move away from ticket-based processes. Baking more automated checks and alerts into your platform is one way to help cut the cord from approval processes with tickets. For example, provide teams with visibility into their run costs and put in automated guardrails to avoid accidental explosion of costs. Implement security policy as code and use configuration scanners or analyzers like Recommender to help teams do the right thing.","blip_selector":"ticket-driven-platform-operating-models","name":"Ticket-driven platform operating models","display_name":"Ticket-driven platform operating models","url":"/radar/techniques/ticket-driven-platform-operating-models","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104023,"quadrant":"techniques","volume_date":"2021-04","description":"When composing an application out of several micro frontends, some part of the system needs to decide which micro frontends to load and where to load them from. So far, we've either built custom solutions or relied on a broader framework like single-spa. Now there is a new standard, import maps, that helps in both cases. Our first experiences show that using import maps for micro frontends allows for a neat separation of concerns. The JavaScript code states what to import and a small script tag in the initial HTML response specifies where to load the frontends from. That HTML is obviously generated on the server side, which makes it possible to use some dynamic configuration during its rendering. In many ways this technique reminds us of linker/loader paths for dynamic Unix libraries. At the moment import maps are only supported by Chrome, but with the SystemJS polyfill they're ready for wider use.","blip_selector":"import-maps-for-micro-frontends","name":"Import maps for micro frontends","display_name":"Import maps for micro frontends","url":"/radar/techniques/import-maps-for-micro-frontends","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010046,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"Steady progress has been made since we first wrote about Web Components in 2014. LitElement, part of the Polymer Project, is a simple library that you can use to create lightweight web components. It's really just a base class that removes the need for a lot of the common boilerplate, making writing web components a lot easier. We've had success using it on projects, and as we see the technology maturing and the library being well liked, LitElement is becoming more commonly used in our Web Components-based projects. | Steady progress has been made since we first wrote about web components in 2014. LitElement, part of the Polymer Project, is a simple library that you can use to create lightweight web components. It's really just a base class that removes the need for a lot of the common boilerplate making writing web components a lot easier. We've had early success using it on projects and are excited to see the technology maturing.","blip_selector":"litelement","name":"LitElement","display_name":"LitElement","url":"/radar/languages-and-frameworks/litelement","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104013,"quadrant":"techniques","volume_date":"2021-04","description":"With TypeScript becoming a common language for front-end development and Node.js becoming the preferred BFF technology, we're seeing increasing use of UI/BFF shared types. In this technique, a single set of type definitions is used to define both the data objects returned by front-end queries and the data served to satisfy those queries by the back-end server. Ordinarily, we would be cautious about this practice because of the unnecessarily tight coupling it creates across process boundaries. However, many teams are finding that the benefits of this approach outweigh any risks of tight coupling. Since the BFF pattern works best when the same team owns both the UI code and the BFF, often storing both components in the same repository, the UI/BFF pair can be viewed as a single cohesive system. When the BFF offers strongly typed queries, the results can be tailored to the specific needs of the frontend rather than reusing a single, general-purpose entity that must serve the needs of many consumers and contain more fields than actually required. This reduces the risk of accidentally exposing data that the user shouldn't see, prevents incorrect interpretation of the returned data object and makes the query more expressive. This practice is particularly useful when implemented with io-ts to enforce the run-time type safety.","blip_selector":"ui-bff-shared-types","name":"UI/BFF shared types","display_name":"UI/BFF shared types","url":"/radar/techniques/ui-bff-shared-types","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104122,"quadrant":"tools","volume_date":"2021-04","description":"Graal Native Image is a technology that compiles Java code into an operating system's native binary — in the form of a statically linked executable or a shared library. A native image is optimized to reduce the memory footprint and startup time of an application. Our teams have successfully used Graal native images, executed as small Docker containers, in the serverless architecture where reducing start time matters. Although designed for use with programming languages such as Go or Rust that natively compile and require smaller binary sizes and shorter start times, Graal Native Image can be equally useful to teams that have other requirements and want to use JVM-based languages.\n\nGraal Native Image Builder, native-image, supports JVM-based languages — such as Java, Scala, Clojure and Kotlin — and builds executables on multiple operating systems including Mac OS, Windows and multiple distributions of Linux. Since it requires a closed-world assumption, where all code is known at compile time, additional configuration is needed for features such as reflection or dynamic class loading where types can't be deduced at build time from the code alone.","blip_selector":"graal-native-image","name":"Graal Native Image","display_name":"Graal Native Image","url":"/radar/tools/graal-native-image","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104044,"quadrant":"techniques","volume_date":"2021-04","description":"As organizations drive toward evolutionary architecture, it's important to capture decisions around design, architecture, techniques and teams' ways of workings. The process of collecting and aggregating feedback that will lead to these decisions begin with Request for Comments (RfCs). RfCs are a technique for collecting context, design and architectural ideas and collaborating with teams to ultimately come to decisions along with their context and consequences. We recommend that organizations take a lightweight approach to RFCs by using a simple standardized template across many teams as well as version control to capture RfCs.\n\nIt's important to capture these in an audit of these decisions to benefit future team members and to capture the technical and business evolution of an organization. Mature organizations have used RfCs in autonomous teams to drive better communication and collaboration especially in cross-team relevant decisions.","blip_selector":"lightweight-approach-to-rfcs","name":"Lightweight approach to RFCs","display_name":"Lightweight approach to RFCs","url":"/radar/techniques/lightweight-approach-to-rfcs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104050,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"Building forms for the web remains one of the perennial challenges of front-end development, in particular with React. Many of our teams working with React have been using Formik to make this easier, but some are now assessing React Hook Form as a potential alternative. React Hooks already existed when React Hook Form was created, so it could use them as a first-class concept: the framework is registering and tracking form elements as uncontrolled components via a hook, thereby significantly reducing the need for re-rendering. It's also quite lightweight in size and in the amount of boilerplate code needed.","blip_selector":"react-hook-form","name":"React Hook Form","display_name":"React Hook Form","url":"/radar/languages-and-frameworks/react-hook-form","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104107,"quadrant":"tools","volume_date":"2021-04","description":"We welcome the increased availability and maturity of infrastructure configuration scanning tools: Prowler helps teams scan their AWS infrastructure setups and improve security based on the results. Although Prowler has been around for a while, it has evolved a lot over the past few years, and we've found it very valuable to enable teams to take responsibility for proper security with a short feedback loop. Prowler categorizes AWS CIS benchmarking checks into different groups (Identity and Access Management, Logging, Monitoring, Networking, CIS Level 1, CIS Level 2, EKS-CIS), and it includes many checks that help you gain insights into your PCI DSS and GDPR compliance.","blip_selector":"prowler","name":"Prowler","display_name":"Prowler","url":"/radar/tools/prowler","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104100,"quadrant":"platforms","volume_date":"2021-04","description":"NATS is a fast, secure message queueing system with an unusually wide range of features and potential deployment targets. At first glance, you would be forgiven for asking why the world needs another message queueing system. Message queues have been around in various forms for nearly as long as businesses have been using computers and have undergone years of refinement and optimization for various tasks. But NATS has several interesting characteristics and is unique in its ability to scale from embedded controllers to global, cloud-hosted superclusters. We're particularly intrigued by NATS's intent to support a continuous streaming flow of data from mobile devices and IoT and through a network of interconnected systems. However, some tricky issues need to be addressed, not the least of which is ensuring consumers see only the messages and topics to which they're allowed access, especially when the network spans organizational boundaries. NATS 2.0 introduced a security and access control framework that supports multitenant clusters where accounts restrict a user's access to queues and topics. Written in Go, NATS has primarily been embraced by the Go language community. Although clients exist for pretty much all widely used programming languages, the Go client is by far the most popular. However, some of our developers have found that all the language client libraries tend to reflect the Go origins of codebase. Increasing bandwidth and processing power on small, wireless devices means that the volume of data businesses must consume in real time will only increase. Assess NATS as a possible platform for streaming that data within and among businesses.","blip_selector":"nats","name":"NATS","display_name":"NATS","url":"/radar/platforms/nats","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104015,"quadrant":"tools","volume_date":"2021-04","description":"Making the web inclusive requires serious attention to ensure accessibility is considered and validated at all stages of software delivery. Many of the popular accessibility testing tools are designed for testing after a web application is complete; as a result, issues are detected late and often are harder to fix, accumulating as debt. In our recent internal work on ThoughtWorks websites, we included the open-source accessibility (a11y) testing engine axe-core as part of our build processes. It provided team members with early feedback on adherence to accessibility rules, even during early increments. Not every issue can be found through automated inspection, though. Extending the functionality of axe-core is the commercially available axe DevTools, including functionality that guides team members through exploratory testing for a majority of accessibility issues.","blip_selector":"axe-core","name":"axe-core","display_name":"axe-core","url":"/radar/tools/axe-core","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104014,"quadrant":"platforms","volume_date":"2021-04","description":"Variable fonts are a way of avoiding the need to find and include separate font files for different weights and styles. Everything is in one font file, and you can use properties to select which style and weight you need. While not new, we still see sites and projects that could benefit from this simple approach. If you have pages that are including many variations of the same font, we suggest trying out variable fonts.","blip_selector":"variable-fonts","name":"Variable fonts","display_name":"Variable fonts","url":"/radar/platforms/variable-fonts","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104075,"quadrant":"platforms","volume_date":"2021-04","description":"Apache Pinot is a distributed OLAP data store, built to deliver real-time analytics with low latency. It can ingest from batch data sources (such as Hadoop HDFS, Amazon S3, Azure ADLS or Google Cloud Storage) as well as stream data sources (such as Apache Kafka). If the need is user-facing, low-latency analytics, SQL-on-Hadoop solutions don't offer the low latency that is needed. Modern OLAP engines like Apache Pinot (or Apache Druid and Clickhouse among others) can achieve much lower latency and are particularly suited in contexts where fast analytics, such as aggregations, are needed on immutable data, possibly, with real-time data ingestion. Originally built by LinkedIn, Apache Pinot entered Apache incubation in late 2018 and has since added a plugin architecture and SQL support among other key capabilities. Apache Pinot can be fairly complex to operate and has many moving parts, but if your data volumes are large enough and you need low-latency query capability, we recommend you assess Apache Pinot.","blip_selector":"apache-pinot","name":"Apache Pinot","display_name":"Apache Pinot","url":"/radar/platforms/apache-pinot","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1330,"quadrant":"techniques","volume_date":"2021-04","description":"When building Docker images for our applications, we're often concerned with two things: the security and the size of the image. Traditionally, we've used container security scanning tools to detect and patch common vulnerabilities and exposures and small distributions such as Alpine Linux to address the image size and distribution performance. But with rising security threats, eliminating all possible attack vectors is more important than ever. That's why distroless Docker images are becoming the default choice for deployment containers. Distroless Docker images reduce the footprint and dependencies by doing away with a full operating system distribution. This technique reduces security scan noise and the application attack surface. Moreover, fewer vulnerabilities need to be patched and as a bonus, these smaller images are more efficient. Google has published a set of distroless container images for different languages. You can create distroless application images using the Google build tool Bazel or simply use multistage Dockerfiles. Note that distroless containers by default don't have a shell for debugging. However, you can easily find debug versions of distroless containers online, including a BusyBox shell. Distroless Docker images is a technique pioneered by Google and, in our experience, is still largely confined to Google-generated images. We would be more comfortable if there were more than one provider to choose from. Also, use caution when applying  Trivy or similar vulnerability scanners since distroless containers are only supported in more recent versions. | When building Docker images for our applications, we're often concerned with two things: the security and the size of the image. Traditionally, we've used container security scanning tools to detect and patch common vulnerabilities and exposures and small distributions such as Alpine Linux to address the image size and distribution performance. We've now gained more experience with distroless Docker images and are ready to recommend this approach as another important security precaution for containerized applications. Distroless Docker images reduce the footprint and dependencies by doing away with a full operating system distribution. This technique reduces security scan noise and the application attack surface. There are fewer vulnerabilities that need to be patched and as a bonus, these smaller images are more efficient. Google has published a set of distroless container images for different languages. You can create distroless application images using the Google build tool Bazel or simply use multistage Dockerfiles. Note that distroless containers by default don't have a shell for debugging. However, you can easily find debug versions of distroless containers online, including a BusyBox shell. Distroless Docker images is a technique pioneered by Google and, in our experience, is still largely confined to Google-generated images. We're hoping that the technique catches on beyond this ecosystem. | When building Docker images for our applications, we're often concerned with two things: the security and the size of the image. Traditionally, we've used container security scanning tools to detect and patch common vulnerabilities and exposures and small distributions such as Alpine Linux to address the image size and distribution performance. In this Radar, we're excited about addressing the security and size of containers with a new technique called distroless docker images , pioneered by Google. With this technique, the footprint of the image is reduced to the application, its resources and language runtime dependencies, without operating system distribution. The advantages of this technique include reduced noise of security scanners, smaller security attack surface, reduced overhead of patching vulnerabilities and even smaller image size for higher performance. Google has published a set of distroless container images for different languages. You can create distroless application images using the Google build tool Bazel, which has rules for creating distroless containers or simply use multistage Dockerfiles. Note that distroless containers by default don't have a shell for debugging. However, you can easily find debug versions of distroless containers online, including a busybox shell.","blip_selector":"distroless-docker-images","name":"Distroless Docker images","display_name":"Distroless Docker images","url":"/radar/techniques/distroless-docker-images","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104058,"quadrant":"tools","volume_date":"2021-04","description":"As the API specification ecosystem matures, we're seeing more tools built to automate style checks. Zally is a minimalist OpenAPI linter that helps to ensure an API conforms to the team's API style guide. Out of the box, it will validate against a rule set developed for Zalando's API style guide, but it also supports a Kotlin extension mechanism to develop custom rules. Zally includes a web UI that provides an intuitive interface for understanding style violations and includes a CLI that makes it easy to plug into your CD pipeline.","blip_selector":"zally","name":"Zally","display_name":"Zally","url":"/radar/tools/zally","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":793,"quadrant":"techniques","volume_date":"2021-04","description":"Our positioning regarding \"being agile before doing agile\" and our opinions around this topic shouldn't come as a surprise; but since SAFe™ (Scaled Agile Framework®), per Gartner’s May 2019 report, is the most considered and most used enterprise agile framework, and since we're seeing more and more enterprises going through organizational changes, we thought it was time to raise awareness on this topic again. We've come across organizations struggling with SAFe's over-standardized, phase-gated processes. Those processes create friction in the organizational structure and its operating model. It can also promote silos in the organization, preventing platforms from becoming real business capabilities enablers. The top-down control generates waste in the value stream and discourages engineering talent creativity, while limiting autonomy and experimentation in the teams. Rather than measuring effort and focusing on standardized ceremonies, we recommend a leaner, value-driven approach and governance to help eliminate organizational friction such as EDGE, as well as a team cognitive load assessment to identify types of teams and determine how they should better interact with each other.\n\nScaled Agile Framework® and SAFe™ are trademarks of Scaled Agile, Inc. | The Scaled Agile Framework® (aka  SAFe™ ) continues to gain mindshare in many organizations at scale. In addition, tools and certification are becoming a significant aspect of the adoption of SAFe™. We continue to be concerned that actual adoptions are prone to over-standardization and are tending towards large release practices, resulting in practices that hinder agile adoption. In its place, we continue to recommend lean approaches that include experimentation and incorporate continuous improvement practices like the Improvement Katas offer organizations a better model for scaling agile.\n\nScaled Agile Framework® and SAFe™ are trademarks of Scaled Agile, Inc. | Scaling agile across enterprises is a continuing challenge. Several approaches have been proposed, with SAFe™ being one gaining significant mindshare. While SAFe™ provides a useful checklist for areas of concern, they are easy to misuse, by introducing the same kind of large release tendencies like the release train and gated control processes that agile removes. Enterprises in particular look for a degree of commonality across endeavors that SAFe™ seems to provide, promoting aggressive standardization when some degree of customization provides significant value. Other lean approaches that include experimentation and incorporate continuous improvement practices like the Improvement Katas offer organizations a better model for scaling agile.\n\nScaled Agile Framework® and SAFe™ are trademarks of Scaled Agile, Inc.","blip_selector":"safe","name":"SAFe™","display_name":"SAFe™","url":"/radar/techniques/safe","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104047,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"On-demand modules for Android is a framework that allows tailored APKs containing only required functionality to be downloaded and installed for a suitably structured app. This could be worth trialing for larger apps where download speed might be an issue, or if a user is likely only to use some functionality on initial installation. It can also simplify the handling of multiple devices without requiring different APKs. A similar framework is available for iOS.","blip_selector":"on-demand-modules","name":"On-demand modules","display_name":"On-demand modules","url":"/radar/languages-and-frameworks/on-demand-modules","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010048,"quadrant":"platforms","volume_date":"2021-04","description":"Materialize is a streaming database that enables you to do incremental computation without complicated data pipelines. Just describe your computations via standard SQL views and connect Materialize to the data stream. The underlying differential data flow engine performs incremental computation to provide consistent and correct output with minimal latency. Unlike traditional databases, there are no restrictions in defining these materialized views, and the computations are executed in real time. We've used Materialize, together with Spring Cloud Stream and Kafka, to query over streams of events for insights in a distributed event-driven system, and we quite like the setup. | Materialize is a streaming database that enables you to do incremental computation without complicated data pipelines. Just describe your computations via standard SQL views and connect Materialize to the data stream. The underlying differential data flow engine performs incremental computation to provide consistent and correct output with minimal latency. Unlike traditional databases, there are no restrictions in defining these materialized views and the computations are executed in real time.","blip_selector":"materialize","name":"Materialize","display_name":"Materialize","url":"/radar/platforms/materialize","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104057,"quadrant":"tools","volume_date":"2021-04","description":"Yelp detect-secrets is a Python module for detecting secrets within a codebase; it scans files within a directory looking for secrets. It can be used as a Git pre-commit hook or to perform a scan in multiple places within the CI/CD pipeline. It comes with a default configuration that makes it very easy to use but can be modified to suit your needs. You can also install custom plugins to add to its default heuristic searches. Compared to similar offerings, we found that this tool detects more types of secrets with its out-of-the-box configuration.","blip_selector":"yelp-detect-secrets","name":"Yelp detect-secrets","display_name":"Yelp detect-secrets","url":"/radar/tools/yelp-detect-secrets","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104019,"quadrant":"tools","volume_date":"2021-04","description":"Longhorn is a distributed block storage system for Kubernetes. There are many persistent storage options for Kubernetes; unlike most, however, Longhorn is built from the ground up to provide incremental snapshots and backups, thereby easing the pain of running a replicated storage for non–cloud-hosted Kubernetes. With the recent experimental support for ReadWriteMany (RWX) you can even mount the same volume for read and write access across many nodes. Choosing the right storage system for Kubernetes is a nontrivial task, and we recommend you assess Longhorn based on your needs.","blip_selector":"longhorn","name":"Longhorn","display_name":"Longhorn","url":"/radar/tools/longhorn","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104026,"quadrant":"techniques","volume_date":"2021-04","description":"A deployment drift radiator makes version drift visible for deployed software across multiple environments. Organizations using automated deployments may require manual approvals for environments that get closer to production, meaning the code in these environments might well be lagging several versions behind current development. This technique makes this lag visible via a simple dashboard showing how far behind each deployed component is for each environment. This helps to highlight the opportunity cost of completed software not yet in production while drawing attention to related risks such as security fixes not yet deployed.","blip_selector":"deployment-drift-radiator","name":"Deployment drift radiator","display_name":"Deployment drift radiator","url":"/radar/techniques/deployment-drift-radiator","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202104007,"quadrant":"platforms","volume_date":"2021-04","description":"Products supported by companies or communities are in constant evolution, at least the ones that get traction in the industry. Sometimes organizations tend to build frameworks or abstractions on top of the existing external products to cover very specific needs, thinking that the adaptation will provide more benefits than the existing ones. We're seeing organizations trying to create homemade infrastructure-as-code (IaC) products on top of the existing ones; they underestimate the required effort to keep those solutions evolving according to their needs, and after a short period of time, they realize that the original version is in much better shape than their own; there are even cases where the abstraction on top of the external product reduces the original capabilities. Although we've seen success stories of organizations building homemade solutions, we want to caution about this approach as the effort required to do so isn't negligible, and a long-term product vision is required to have the expected outcomes.","blip_selector":"homemade-infrastructure-as-code-iac-products","name":"Homemade infrastructure-as-code (IaC) products","display_name":"Homemade infrastructure-as-code (IaC) products","url":"/radar/platforms/homemade-infrastructure-as-code-iac-products","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104126,"quadrant":"tools","volume_date":"2021-04","description":"Over the past few years Windows Subsystem for Linux (WSL) has come up a few times in our discussions. Although we liked what we saw, including the improvements in WSL 2, it never made it into the Radar. In this edition we want to highlight an extension for Visual Studio Code that greatly improves the experience working with WSL. Although Windows-based editors could always access files on a WSL file system, they were unaware of the isolated Linux environment. With the Remote - WSL extension, Visual Studio Code becomes aware of WSL, allowing developers to launch a Linux shell. This also enables debugging of binaries running inside WSL from Windows. Jetbrains' IntelliJ too has seen steady improvement in its support for WSL.","blip_selector":"remote-wsl","name":"Remote - WSL","display_name":"Remote - WSL","url":"/radar/tools/remote-wsl","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202104110,"quadrant":"techniques","volume_date":"2021-04","description":"Ideally, but especially when teams are practicing DevOps, the deployment pipeline and the code being deployed should be owned by the same team. Unfortunately, we still see organizations where there is separate code and pipeline ownership, with the deployment pipeline configuration owned by the infrastructure team; this results in delays to changes, barriers to improvements and a lack of development team ownership and involvement in deployments. One cause of this can clearly be the separate team, another can be the desire to retain “gatekeeper” processes and roles. Although there can be legitimate reasons for using this approach (e.g., regulatory control), in general we find it painful and unhelpful.","blip_selector":"separate-code-and-pipeline-ownership","name":"Separate code and pipeline ownership","display_name":"Separate code and pipeline ownership","url":"/radar/techniques/separate-code-and-pipeline-ownership","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104116,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"The release of the Webpack 5 Module Federation feature has been highly anticipated by developers of micro frontend architectures. The feature introduces a more standardized way to optimize how module dependencies and shared code are managed and loaded. Module federation allows for the specification of shared modules, which helps with the deduplication of dependencies across micro frontends by loading code used by multiple modules only once. It also lets you distinguish between local and remote modules, where the remote modules are not actually part of the build itself but loaded asynchronously. Compared to build-time dependencies like npm packages, this can significantly simplify the deployment of a module update with many downstream dependencies. Be aware, though, that this requires you to bundle all of your micro frontends with Webpack, as opposed to approaches such as import maps, which might eventually become part of the W3C standard.","blip_selector":"webpack-5-module-federation","name":"Webpack 5 Module Federation","display_name":"Webpack 5 Module Federation","url":"/radar/languages-and-frameworks/webpack-5-module-federation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104045,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"With the increasing popularity of smart home and wearable devices, demand for intuitive graphical user interfaces (GUIs)is increasing. However, if you're engaged in embedded device development, rather than Android/iOS, GUI development may take a lot of effort. As an open-source embedded graphics library, LVGL has become increasingly popular. LVGL has been adapted to mainstream embedded platforms such as NXP, STM32, PIC, Arduino, and ESP32. It has a very small memory footprint: 64 kB flash and 8 kB RAM is enough to make it work, and it can run smoothly on various Cortex-M0 low-power MCUs. LVGL supports input types such as touchscreen, mouse and buttons and contains more than 30 controls, including TileView suitable for smart watches. The MIT license it chose doesn’t restrict enterprise and commercial use. Our teams’ feedback on this tool has been positive and one of our projects using LVGL is already in production, more specifically in small batch manufacturing.","blip_selector":"lvgl","name":"LVGL","display_name":"LVGL","url":"/radar/languages-and-frameworks/lvgl","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010032,"quadrant":"techniques","volume_date":"2021-04","description":"The Open Application Model (OAM) is an attempt to bring some standardization to the space of shaping infrastructure platforms as products. Using the abstractions of components, application configurations, scopes and traits, developers can describe their applications in a platform-agnostic way, while platform implementers define their platform in terms of workload, trait and scope. Since we last talked about the OAM, we've followed one of its first implementations with interest, KubeVela. KubeVela is close to release 1.0, and we're curious to see if implementations like this can substantiate the promise of the OAM idea. | We've talked a lot about the benefits of creating platform engineering product teams in support of your other product teams, but actually doing it is hard. It seems that the industry is still searching for the right abstraction in the world of infrastructure as code. Although tools such as Terraform and Helm are steps in the right direction, the focus is still on managing infrastructure as opposed to application development. There are also shifts toward the concept of infrastructure as software with new tools such as Pulumi and CDK being released. The Open Application Model (OAM) is an attempt to bring some standardization to this space. Using the abstractions of components, application configurations, scopes and traits, developers can describe their applications in a platform-agnostic way, while platform implementers define their platform in terms of workload, trait and scope. Whether the OAM will be widely adopted remains to be seen, but we recommend keeping an eye on this interesting and needed idea.","blip_selector":"open-application-model-oam","name":"Open Application Model (OAM)","display_name":"Open Application Model (OAM)","url":"/radar/techniques/open-application-model-oam","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104083,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"Dagster is an open-source data orchestration framework for machine learning, analytics and plain ETL data pipelines. Unlike other task-driven frameworks, Dagster is aware of data flowing through the pipeline and can provide type-safety. With this unified view of pipelines and assets produced, Dagster can schedule and orchestrate Pandas, Spark, SQL or anything else that Python can invoke. The framework is relatively new, and we recommend that you assess its capabilities for your data pipelines.","blip_selector":"dagster","name":"Dagster","display_name":"Dagster","url":"/radar/languages-and-frameworks/dagster","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104089,"quadrant":"tools","volume_date":"2021-04","description":"Flipper is an extensible mobile application debugger. Out of the box it supports profiling, interactive layout inspection, log viewer and a network inspector for iOS, Android and React Native applications. Compared to other debugging tools for mobile apps, we find Flipper to be lightweight, feature rich and easy to set up.","blip_selector":"flipper","name":"Flipper","display_name":"Flipper","url":"/radar/tools/flipper","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010088,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"Streamlit is an open-source application framework in Python used by data scientists for building interactive data applications. Tuning machine-learning models takes time; instead of going back and forth on the main application (the one that uses these models), we've found value in quickly building standalone prototypes in Streamlit and gathering feedback during experimentation cycles. Streamlit stands out from competitors such as Dash because of its focus on rapid prototyping and support for a wide range of visualization libraries, including Plotly and Bokeh. We're using it in a few projects and like how we can put together interactive visualizations with very little effort. | Streamlit is an open-source application framework in Python used by data scientists for building good-looking data visualization applications. Streamlit stands out from competitors such as Dash with its focus on rapid prototyping and support for a wide range of visualization libraries, including Plotly and Bokeh. For data scientists who need quick showcases during the experimentation cycle, Streamlit is a solid choice. We're using it in a few projects and like how we can put together interactive visualizations with very little effort.","blip_selector":"streamlit","name":"Streamlit","display_name":"Streamlit","url":"/radar/languages-and-frameworks/streamlit","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202104003,"quadrant":"platforms","volume_date":"2021-04","description":"We've observed before that the cloud providers push more and more services onto the market. We've also documented our concerns that sometimes the services are made available when they're not quite ready for prime time. Unfortunately, in our experience, Azure Machine Learning falls into the latter category. One of several recent entrants in the field of bounded low-code platforms, Azure ML promises more convenience for data scientists. Ultimately, however, it doesn't live up to its promise; in fact, it still feels easier for our data scientists to work in Python. Despite significant efforts, we struggled to make it scale and lack of adequate documentation proved to be another issue which is why we moved it to the Hold ring.","blip_selector":"azure-machine-learning","name":"Azure Machine Learning","display_name":"Azure Machine Learning","url":"/radar/platforms/azure-machine-learning","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104118,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"As we continue developing web applications in JavaScript, we continue enjoying the Testing Library approach of testing applications; and carry on exploring and gaining experience with its packages — beyond that of React Testing Library. Angular Testing Library brings all the benefits of its family when testing UI components in a user-centric way, pushing for more maintainable tests focused primarily on behavior rather than testing UI implementation details. Although it falls short in documentation, Angular Testing Library does provide good sample tests that helped us in getting started faster for various cases. We've had great success with this testing library in our Angular projects and advise you to trial this solid testing approach.","blip_selector":"angular-testing-library","name":"Angular Testing Library","display_name":"Angular Testing Library","url":"/radar/languages-and-frameworks/angular-testing-library","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104012,"quadrant":"techniques","volume_date":"2021-04","description":"The strangler fig pattern is often the default strategy for legacy modernization, where the new code wraps around the old and slowly absorbs the ability to handle all the needed functionality. That sort of \"outside-in\" approach works well for a number of legacy systems, but now that we've had enough experience with single-page applications (SPA) for them to become legacy systems themselves, we're seeing the opposite \"inside-out\" approach used to replace them. Instead of wrapping the legacy system, we instead embed the beginning of the new SPA into the HTML document containing the old one and let it slowly expand in functionality. The SPA frameworks don't even need to be the same as long as users can tolerate the performance hit of the increased page size (e.g., embedding a new React app inside an old AngularJS one). SPA injection allows you to iteratively remove the old SPA until the new one completely takes over. Whereas a strangler fig can be viewed as a type of parasite that uses the host tree's stable external surface to support itself until it takes root and the host itself dies, this approach is more like injecting an outside agent into the host, relying on functionality of the original SPA until it can completely take over.","blip_selector":"spa-injection","name":"SPA injection","display_name":"SPA injection","url":"/radar/techniques/spa-injection","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010091,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"When used in appropriate circumstances, our teams have found that the React Hooks library SWR can result in cleaner code and much improved performance. SWR implements the stale-while-revalidate HTTP caching strategy, first returning data from cache (stale), then sending the fetch request (revalidate) and finally refreshing the values with the up-to-date response. We caution teams to only use the SWR caching strategy when an application is supposed to return stale data. Note that HTTP requires that caches respond to a request with the most up-to-date response; only in carefully considered circumstances is a stale response allowed to be returned. | SWR is a React Hooks library for fetching remote data. It implements the stale-while-revalidate HTTP caching strategy. SWR first returns data from cache (stale), then sends the fetch request (revalidate) and finally refreshes the values with the up-to-date response. Components receive a stream of data, first stale and then fresh, constantly and automatically. Our developers have had a good experience using SWR, dramatically improving the user experience with always having data on the screen. However, we caution teams to only use SWR caching strategy when appropriate for an application to return stale data. Note that HTTP requires that caches respond to a request with the most up-to-date response held that is appropriate to the request, and only in carefully considered circumstances is a stale response allowed to be returned.","blip_selector":"swr","name":"SWR","display_name":"SWR","url":"/radar/languages-and-frameworks/swr","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202104117,"quadrant":"tools","volume_date":"2021-04","description":"When working with React, we often encounter situations where our page is very slow because some components are re-rendering when they shouldn't be. Why Did You Render is a library that helps detect why a component is re-rendering. It does this by monkey patching React. We've used it in a few of our projects to debug performance issues with great effect.","blip_selector":"why-did-you-render","name":"Why Did You Render","display_name":"Why Did You Render","url":"/radar/tools/why-did-you-render","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202104021,"quadrant":"languages-and-frameworks","volume_date":"2021-04","description":"We don't call out every new .NET version in the Radar, but .NET 5 represents a significant step forward in bringing .NET Core and .NET Framework into a single platform. Organizations should start to develop a strategy to migrate their development environments — a fragmented mix of frameworks depending on the deployment target — to a single version of .NET 5 or 6 when it becomes available. The advantage of this approach will be a common development platform regardless of the intended environment: Windows, Linux, cross-platform mobile devices (via Xamarin) or the browser (using Blazor). While polyglot development will remain the preferred approach for companies with the engineering culture to support it, others will find it more efficient to standardize on a single platform for .NET development. For now, we want to keep this in the Assess ring to see how well the final unified framework performs in .NET 6.","blip_selector":"net-5","name":".NET 5","display_name":".NET 5","url":"/radar/languages-and-frameworks/net-5","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911007,"quadrant":"platforms","volume_date":"2021-04","description":"Many of our teams who are already on AWS have found AWS Cloud Development Kit (AWS CDK) to be a sensible AWS default for enabling infrastructure provisioning. In particular, they like the use of first-class programming languages instead of configuration files which allows them to use existing tools, test approaches and skills. Like similar tools, care is still needed to ensure deployments remain easy to understand and maintain. The development kit currently supports TypeScript, JavaScript, Python, Java, C# and .NET. New providers are being added to the CDK core. We've also used both AWS Cloud Development Kit and HashiCorp's Cloud Development Kit for Terraform to generate Terraform configurations and enable provisioning with the Terraform platform with success. | For many of our teams, Terraform has become the default choice for defining cloud infrastructure. However, some of our teams have been experimenting with AWS Cloud Development Kit (AWS CDK), and they like what they've seen so far. In particular, they like the use of first-class programming languages instead of configuration files which allows them to use existing tools, test approaches and skills. Like similar tools, care is still needed to ensure deployments remain easy to understand and maintain. It currently supports TypeScript, JavaScript, Python, Java and C# and .NET. We'll continue to watch AWS CDK, especially since the AWS and HashiCorp teams recently launched a preview for Cloud Development Kit for Terraform to generate Terraform configurations and enable provisioning with the Terraform platform. | For many of our teams Terraform has become the default choice for defining cloud infrastructure. However, some of our teams have been experimenting with AWS Cloud Development Kit (AWS CDK) and they like what they've seen so far. In particular, they like the use of first-class programming languages instead of configuration files which allows them to use existing tools, test approaches and skills. Like similar tools, care is still needed to ensure deployments remain easy to understand and maintain. Given that support for C# and Java is coming soon and ignoring for now some gaps in functionality, we think AWS CDK is worth watching as an alternative to other configuration file–based approaches.","blip_selector":"aws-cloud-development-kit","name":"AWS Cloud Development Kit","display_name":"AWS Cloud Development Kit","url":"/radar/platforms/aws-cloud-development-kit","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202005026,"quadrant":"platforms","volume_date":"2020-10","description":"Technologies, especially wildly popular ones, have a tendency to be overused. What we're seeing at the moment is Node overload, a tendency to use Node.js indiscriminately or for the wrong reasons. Among these, two stand out in our opinion. Firstly, we frequently hear that Node.js should be used so that all programming can be done in one programming language. Our view remains that polyglot programming is a better approach, and this still goes both ways. Secondly, we often hear teams cite performance as a reason to choose Node.js. Although there are myriads of more or less sensible benchmarks, this perception is rooted in history. When Node.js became popular, it was the first major framework to embrace a nonblocking programming model which made it very efficient for IO-heavy tasks. (We mentioned this in our write-up of Node.js in 2012.) Due to its single-threaded nature, Node.js was never a good choice for compute-heavy workloads, though, and now that capable nonblocking frameworks also exist on other platforms — some with elegant, modern APIs — performance is no longer a reason to choose Node.js. | Technologies, especially wildly popular ones, have a tendency to be overused. What we're seeing at the moment is Node overload, a tendency to use Node.js indiscriminately or for the wrong reasons. Among these, two stand out in our opinion. Firstly, we frequently hear that Node should be used so that all programming can be done in one programming language. Our view remains that polyglot programming is a better approach, and this still goes both ways. Secondly, we often hear teams cite performance as a reason to choose Node.js. Although there are myriads of more or less sensible benchmarks, this perception is rooted in history. When Node.js became popular, it was the first major framework to embrace a nonblocking programming model which made it very efficient for IO-heavy tasks. (We mentioned this in our write-up of Node.js in 2012.) Due to its single-threaded nature, Node.js was never a good choice for compute-heavy workloads, though, and now that capable nonblocking frameworks also exist on other platforms — some with elegant, modern APIs — performance is no longer a reason to choose Node.js.","blip_selector":"node-overload","name":"Node overload","display_name":"Node overload","url":"/radar/platforms/node-overload","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201904040,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"Arrow is promoted as the functional companion for Kotlin's standard library. Indeed, the package of ready-to-use higher-level abstractions delivered by Arrow has proven so useful that our teams now consider Arrow a sensible default when working with Kotlin. Recently, in preparation for the 1.0 release, the Arrow team introduced several changes, including the addition of new modules but also some deprecations and removals. | Arrow is a functional programming library for Kotlin, created by merging two existing popular libraries (kategory and funKTionale). While Kotlin provides building blocks for functional programming, Arrow delivers a package of ready-to-use higher-level abstractions for application developers. It provides data types, type classes, effects, optics and other functional programming patterns as well as integrations with popular libraries. Our initial positive impressions of Arrow were confirmed when using it to build applications that are now in production. | Arrow is a functional programming library for Kotlin, created by merging two existing popular libraries (kategory and funKTionale). While Kotlin provides building blocks for functional programming, Arrow delivers a package of ready-to-use higher-level abstractions for application developers. It provides data types, type classes, effects, optics and other functional programming patterns as well as integrations with popular libraries. With Arrow, existing libraries are unified, which should go a long way to avoid fractured communities in this space.","blip_selector":"arrow","name":"Arrow","display_name":"Arrow","url":"/radar/languages-and-frameworks/arrow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010030,"quadrant":"tools","volume_date":"2020-10","description":"When connecting to server instances on AWS, it is recommended to go through a bastion host instead of a direct connection. However, provisioning a bastion host just for that purpose can be frustrating, which is why AWS Systems Manager’s Session Manager provides tunneling to more comfortably connect to your servers. gossm is an open-source CLI tool that makes the use of the Session Manager even more convenient. gossm lets you leverage the security provided by Session Manager and IAM policies from your terminal using tools such as ssh and scp. It also has some capabilities that the AWS CLI is missing, including server discovery and SSH integration.","blip_selector":"gossm","name":"gossm","display_name":"gossm","url":"/radar/tools/gossm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010075,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"Flutter Driver is an integration testing library for Flutter applications. With Flutter Driver you can instrument and drive the test suite on either real devices or emulators. Our teams continue to write unit and widget tests to ensure most of the business functionality in Flutter apps is implemented. However, for testing the actual user interaction, we're assessing Flutter Driver, and you should too.","blip_selector":"flutter-driver","name":"Flutter Driver","display_name":"Flutter Driver","url":"/radar/languages-and-frameworks/flutter-driver","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904021,"quadrant":"platforms","volume_date":"2020-10","description":"Honeycomb is an observability service that ingests rich data from production systems and makes it manageable through dynamic sampling. Developers can log large amounts of rich events and later decide how to slice and correlate them. This interactive approach is useful when working with today's large distributed systems, because we've passed the point where we can reasonably anticipate which questions we might want to ask of production systems. The Honeycomb team is actively developing for a number of languages and frameworks with plugins now available for Go, Node, Java and Rails among others; other new features are being added at a rapid pace. The pricing model has also been simplified to make it more attractive. Our teams love it. | Honeycomb is an observability tool that ingests rich data from production systems and makes it manageable through dynamic sampling. Developers can log large amounts of rich events and decide later how to slice and correlate them. This interactive approach is useful when working with today's large distributed systems because we've passed the point where we can reasonably anticipate which questions we might want to ask of production systems.","blip_selector":"honeycomb","name":"Honeycomb","display_name":"Honeycomb","url":"/radar/platforms/honeycomb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010026,"quadrant":"tools","volume_date":"2020-10","description":"This edition of the Radar introduces several new tools for creating web applications that help end users visualize and interact with data. These are more than simple visualization libraries such as D3. Instead, they reduce the effort necessary to build standalone analytic applications for manipulating existing data sets. Dash from Plotly is gaining popularity among data scientists for creating richly functional analytics applications in Python. Dash augments Python data libraries much like Shiny sits on top of R. These applications are sometimes referred to as dashboards, but the range of possible functionality is really much greater than the term implies. Dash is particularly suited to building scalable, production-ready applications, unlike Streamlit, another tool in this class. Consider using Dash when you need to present more sophisticated analyses to business users than a low- or no-code solution such as Tableau can provide.","blip_selector":"dash","name":"Dash","display_name":"Dash","url":"/radar/tools/dash","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010002,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"In the case where implementation in Node.js is necessary, we see that Fastify is an option that our teams are very happy with. This web framework offers ease in handling request-response validations, support for TypeScript and a plugin ecosystem giving our teams an easier experience developing software. Although it's a good option in the Node.js ecosystem, we stand by our previous advice: don't fall into Node overload scenarios.","blip_selector":"fastify","name":"Fastify","display_name":"Fastify","url":"/radar/languages-and-frameworks/fastify","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1224,"quadrant":"tools","volume_date":"2020-10","description":"Among the available tools for keeping dependencies up to date, Dependabot is a solid default choice in our opinion. Dependabot's integration with GitHub is smooth and automatically sends you pull requests to update your dependencies to their latest versions. It can be enabled at the organization level, so it's very easy for teams to receive these pull requests. If you're not using GitHub, you can still use the Dependabot libraries within your build pipeline. If you're interested in an alternative tool, also consider Renovate, which supports a wider range of services, including GitLab, Bitbucket and Azure DevOps. | Keeping dependencies up to date is a chore, but for security reasons it's important to respond to updates in a timely manner. You can use tools to make this process as painless and automated as possible. In practical use our teams have had good experiences with Dependabot. It integrates with GitHub repositories and automatically checks dependencies for new versions. When required, Dependabot will open a pull request with upgraded dependencies. | Keeping dependencies up to date is a chore, but it's important to manage upgrades frequently and incrementally. We want the process to be as painless and automated as possible. Our teams have often hand-rolled scripts to automate parts of the process; now, however, we integrate commercial offerings to do that work. Dependabot is a service that integrates with your GitHub repositories and automatically checks your project dependencies for new versions. When required, Dependabot will open a pull request with upgraded dependencies. Using features of your CI server, you can automatically test upgrades for compatibility and automatically merge compatible upgrades to master. There are alternatives to Dependabot, including Renovate for JavaScript projects and Depfu for JavaScript and Ruby projects. Our teams, however, recommend Dependabot because of its multilanguage support and ease of use.","blip_selector":"dependabot","name":"Dependabot","display_name":"Dependabot","url":"/radar/tools/dependabot","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201904057,"quadrant":"techniques","volume_date":"2020-10","description":"Over the last few decades computational notebooks, first introduced by Wolfram Mathematica, have evolved to support scientific research, exploration and educational workflows. Naturally, in support of data science workflows and with the likes of Jupyter notebooks and Databricks notebooks, they've become a great companion by providing a simple and intuitive interactive computation environment for combining code to analyze data with rich text and visualization to tell a data story. Notebooks were designed to provide an ultimate medium for modern scientific communication and innovation. In recent years, however, we've seen a trend for notebooks to be the medium for running the type of production-quality code typically used to drive enterprise operations. We see notebook platform providers advertising the use of their exploratory notebooks in production. This is a case of good intentions — democratizing programming for data scientists — implemented poorly and at the cost of scalability, maintainability, resiliency and all the other qualities that a long-lived production code needs to support. We don't recommend productionizing notebooks and instead encourage empowering data scientists to build production-ready code with the right programming frameworks, thus simplifying the continuous delivery tooling and abstracting complexity away through end-to-end ML platforms. | Jupyter Notebooks have gained in popularity among data scientists who use them for exploratory analyses, early-stage development and knowledge sharing. This rise in popularity has led to the trend of productionizing Jupyter Notebooks, by providing the tools and support to execute them at scale. Although we wouldn't want to discourage anyone from using their tools of choice, we don't recommend using Jupyter Notebooks for building scalable, maintainable and long-lived production code — they lack effective version control, error handling, modularity and extensibility among other basic capabilities required for building scalable, production-ready code. Instead, we encourage developers and data scientists to work together to find solutions that empower data scientists to build production-ready machine learning models using continuous delivery practices with the right programming frameworks. We caution against productionization of Jupyter Notebooks to overcome inefficiencies in continuous delivery pipelines for machine learning, or inadequate automated testing.","blip_selector":"productionizing-notebooks","name":"Productionizing notebooks","display_name":"Productionizing notebooks","url":"/radar/techniques/productionizing-notebooks","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202010014,"quadrant":"techniques","volume_date":"2020-10","description":"Since we originally introduced the term in 2016, micro frontends have grown in popularity and achieved mainstream acceptance. But like any new technique with an easy-to-remember name, it has occasionally been misused and abused. Particularly concerning is the tendency to use this architecture as an excuse to mix a range of competing technologies, tools or frameworks in a single page, leading to micro frontend anarchy. A particularly egregious form of this syndrome is using multiple frontend frameworks — for example, React.js and Angular — in the same \"single-page\" application. Although this might be technically possible, it is far from advisable when not part of a deliberate transition strategy. Other properties that should be consistent from team to team include the styling technique (e.g., CSS-in-JS or CSS modules) and the means by which the individual components are integrated (e.g., iFrames or web components). Furthermore, organizations should decide whether to standardize on consistent approaches or to leave it up to their teams to decide on state management, data fetching, build tooling, analytics and a host of other choices in a micro frontend application.","blip_selector":"micro-frontend-anarchy","name":"Micro frontend anarchy","display_name":"Micro frontend anarchy","url":"/radar/techniques/micro-frontend-anarchy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911077,"quadrant":"tools","volume_date":"2020-10","description":"Build pipelines that create and deploy containers should include container security scanning. Our teams particularly like Trivy, a vulnerability scanner for containers. We've tried Clair and Anchore Engine among other good tools in this field. Unlike Clair, Trivy doesn’t only check containers but also dependencies in the codebase. Also, because Trivy ships as a stand-alone binary, it's easier to set up and run the scan locally. Other benefits of Trivy are that it's open-source software and that it supports distroless containers. | Build pipelines that create and deploy containers should include container security scanning. Our teams particularly like Trivy, a vulnerability scanner for containers, because it's easier to set up than other tools,  thanks to it shipping as a stand-alone binary. Other benefits of Trivy are that it's open-source software and that it supports distroless containers.","blip_selector":"trivy","name":"Trivy","display_name":"Trivy","url":"/radar/tools/trivy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911034,"quadrant":"techniques","volume_date":"2020-10","description":"As the technology landscape is becoming more complex, concerns such as security need more automation and engineering practices. When building systems, we need to take into consideration security policies, which are rules and procedures to protect our systems from threats and disruption. For example, access control policies define and enforce who can access which services and resources under what circumstances; by contrast, network security policies can dynamically limit the traffic rate to a particular service.\n\nSeveral of our teams have had a great experience treating security policy as code. When we say as code, we not only mean to write these security policies in a file but also to apply practices such as keeping the code under version control, introducing automatic validation in the pipeline, automatically deploying them in the environments and observing and monitoring their performance. Based on our experience and the maturity of the existing tools — including Open Policy Agent and platforms such as Istio which provide flexible policy definition and enforcement mechanisms that support the practice of security policy as code — we highly recommend using this technique in your environment. | Security policies are rules and procedures that protect our systems from threats and disruption. For example, access control policies define and enforce who can access which services and resources under what circumstances; or network security policies can dynamically limit the traffic rate to a particular service. The complexity of the technology landscape today demands treating security policy as code: define and keep policies under version control, automatically validate them, automatically deploy them and monitor their performance. Tools such as Open Policy Agent or platforms such as Istio provide flexible policy definition and enforcement mechanisms that support the practice of security policy as code. | Security policies are rules and procedures that protect our systems from threats and disruption. For example, access control policies define and enforce who can access which services and resources under what circumstances; or network security policies can dynamically limit the traffic rate to a particular service. The complexity of the technology landscape today demands treating security policy as code: define and keep policies under version control, automatically validate them, automatically deploy them and monitor their performance. Tools such as Open Policy Agent, or platforms such as Istio provide flexible policy definition and enforcement mechanisms that support the practice of security policy as code.","blip_selector":"security-policy-as-code","name":"Security policy as code","display_name":"Security policy as code","url":"/radar/techniques/security-policy-as-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1174,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"single-spa is a JavaScript framework for bringing together multiple micro frontends in a single front-end application. Although we advise against micro frontend anarchy, the use of micro frontends as an excuse to mix and match multiple frameworks, single-spa supports just that. We understand that there are legitimate scenarios such as upgrading to a new revision of a framework across multiple micro frontends where integration across multiple frameworks is necessary. single-spa has been a go-to framework for micro frontend integration for our teams, and they're finding it to work well with SystemJS and managing different versions of a single dependency. | single-spa is a JavaScript metaframework that allows us to build micro frontends using different frameworks that can coexist in a single application. In general, we don't recommend using more than one framework for an application, but there are times when we can't avoid doing so. For instance, single-spa can be quite useful when you're working with a legacy application and you want to experiment by developing a new feature, with either a new version of the existing framework or a completely different one. Given the short life span of many JavaScript frameworks, we see a need for a solution that would allow for future framework changes and localized experimentation, without affecting the entire application. single-spa seems to be a good start in that direction.","blip_selector":"single-spa","name":"single-spa","display_name":"single-spa","url":"/radar/languages-and-frameworks/single-spa","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1288,"quadrant":"platforms","volume_date":"2020-10","description":"Debezium is a change data capture (CDC) platform that can stream database changes onto Kafka topics. CDC is a popular technique with multiple use cases, including replicating data to other databases, feeding analytics systems, extracting microservices from monoliths and invalidating caches. Debezium reacts to changes in the database's log files and has CDC connectors for multiple databases, including Postgres, MySQL, Oracle and MongoDB. We're using Debezium in many projects, and it has worked very well for us. | Debezium is a change data capture (CDC) platform that can stream database changes onto Kafka topics. CDC is a popular technique with multiple use cases, including replicating data to other databases, feeding analytics systems, extracting microservices from monoliths and invalidating caches. We're always on the lookout for tools or platforms in this space (we talked about Bottled Water in a previous Radar) and Debezium is an excellent choice. It uses a log-based CDC approach which means it works by reacting to changes in the database's log files. Debezium uses Kafka Connect which makes it highly scalable and resilient to failures and has CDC connectors for multiple databases including Postgres, Mysql and MongoDB. We're using it in a few projects and it has worked very well for us.","blip_selector":"debezium","name":"Debezium","display_name":"Debezium","url":"/radar/platforms/debezium","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904034,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"With the increasing complexity of single-page JavaScript applications, managing state predictably is becoming more and more important. Immutability can help to ensure our applications behave consistently but unfortunately JavaScript doesn't offer built-in deeply immutable data structures (see the ES Record and Tuple proposal). Immer — German for always — is a tiny package that lets you work with immutable state in a more convenient way. It's based on the copy-on-write mechanism, has a minimal API and operates on normal JavaScript objects and arrays. This means that data access is seamless and no large refactoring efforts are needed when introducing immutability to an existing codebase. Many of our teams now use it in their JavaScript codebases and prefer it to Immutable.js, which is why we're moving it to Trial. | With the increasing complexity of single-page JavaScript applications, managing state predictably is becoming more and more important. Immutability can help to ensure our applications behave consistently, but unfortunately JavaScript doesn't natively support the ability to create immutable objects. Libraries such as Immutable.js filled that gap but introduced new problems because now two kinds of objects and arrays existed in the application, the library's version and the native JavaScript ones. Immer — German for always — is a tiny package that lets you work with immutable state in a more convenient way. It's based on the copy-on-write mechanism, has a minimal API and operates on normal JavaScript objects and arrays. This means that data access is seamless and no large refactoring efforts are needed when introducing immutability to an existing codebase.","blip_selector":"immer","name":"Immer","display_name":"Immer","url":"/radar/languages-and-frameworks/immer","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010012,"quadrant":"techniques","volume_date":"2020-10","description":"As many more companies migrate away from their legacy systems, we feel it's worth highlighting an alternative to change data capture (CDC) as a mechanism for getting data from these systems. Martin Fowler described event interception back in 2004. In modern terms it involves forking requests on ingress to a system so that it's possible to gradually build a replacement. Often this is done by copying events or messages but forking HTTP requests is equally valid. Examples include forking events from point-of-sale systems before they're written to a mainframe and forking payment transactions before they're written to a core banking system. Both lead to the gradual replacement of parts of the legacy systems. We feel that as a technique, obtaining state changes from the source, rather than trying to recreate them postprocessing using CDC, has been overlooked which is why we're highlighting it in this issue of the Radar.","blip_selector":"event-interception","name":"Event interception","display_name":"Event interception","url":"/radar/techniques/event-interception","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":970,"quadrant":"tools","volume_date":"2020-10","description":"Traditional testing approaches focus on evaluating if our production code is doing what it's supposed to do. However, we could make mistakes in the testing code introducing incomplete or useless assertions that create a false sense of confidence. This is where mutation testing comes in; it assesses the quality of the tests themselves, finding corner cases that are hard to realize. Our teams have used Pitest for a while now, and we recommend its use in Java projects to measure the health of the test suite. In short, mutation testing introduces changes in the production code and executes the same tests a second time; if the tests are still green it means that the tests are not good and need to improve. When you’re using programming languages other than Java Stryker is a good choice in this space. | Pitest is a test coverage analysis tool for Java that uses a mutation-testing technique. Traditional test coverage analysis tends to measure the number of lines that are executed by your tests. It is therefore only able to identify code that is definitely not tested. Mutation testing, on the other hand, tries to test the quality of those lines that are executed by your test code and yet might contain general errors. Several problems can be spotted this way, helping the team to measure and grow a healthy test suite. Most of such tools tend to be slow and difficult to use, but Pitest has proven to have better performance, is easy to set up, and is actively supported.","blip_selector":"pitest","name":"Pitest","display_name":"Pitest","url":"/radar/tools/pitest","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010080,"quadrant":"tools","volume_date":"2020-10","description":"Given the increased use of service mesh to deploy collections of containerized microservices, we can expect to see tools emerge that automate and simplify the administrative tasks associated with this architectural style. Kiali is one such tool. Kiali provides a graphical user interface to observe and control networks of services deployed with Istio. We've found Kiali useful for visualizing the topology of services in a network and understanding the traffic routed between them. For example, when used in conjunction with Flagger, Kiali can display requests that have been routed to a canary service release. We particularly like Kiali's ability to artificially inject network faults into a service mesh to test resilience in the face of network interruptions. This practice is all too often ignored due to the complexity of configuring and running failure tests in a complex mesh of microservices.","blip_selector":"kiali","name":"Kiali","display_name":"Kiali","url":"/radar/tools/kiali","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005084,"quadrant":"techniques","volume_date":"2020-10","description":"Many data pipelines are defined in a large, more or less imperative script written in Python or Scala. The script contains the logic of the individual steps as well as the code chaining the steps together. When faced with a similar situation in Selenium tests, developers discovered the Page Object pattern, and later many behavior-driven development (BDD) frameworks implemented a split between step definitions and their composition. Some teams are now experimenting with bringing the same thinking to data engineering. A separate declarative data pipeline definition, maybe written in YAML, contains only the declaration and sequence of steps. It states input and output data sets but refers to scripts if and when more complex logic is needed. A La Mode is a relatively new tool that takes a DSL approach to defining pipelines, but airflow-declarative, a tool that turns directed acyclic graphs defined in YAML into Airflow task schedules, seems to have the most momentum in this space. | Many data pipelines are defined in a large, more or less imperative script written in Python or Scala. The script contains the logic of the individual steps as well as the code chaining the steps together. When faced with a similar situation in Selenium tests, developers discovered the Page Object pattern, and later many behavior-driven development (BDD) frameworks implemented a split between step definitions and their composition. Some teams are now experimenting with bringing the same thinking to data engineering. A separate declarative data pipeline definition, maybe written in YAML, contains only the declaration and sequence of steps. It states input and output data sets but refers to scripts if and when more complex logic is needed. With A La Mode, we're seeing the first open source tool appear in this space.","blip_selector":"declarative-data-pipeline-definition","name":"Declarative data pipeline definition","display_name":"Declarative data pipeline definition","url":"/radar/techniques/declarative-data-pipeline-definition","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":951,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"We've decided to move Redux back into the Trial ring to show that we no longer consider it the default approach for state management in React applications. Our experience shows that Redux is still a valuable framework in many cases but compared to other approaches, it also leads to more verbose and harder-to-follow code. Throwing Redux Sagas into the mix usually compounds this issue. As an alternative, you can often use the features in recent versions of React to manage state effectively without an additional framework. However, we want to highlight that when you reach the point at which your simple state management solution starts to become complex, it might be worth reaching for Redux after all or perhaps even Facebook’s recently published Recoil. | With the increasing complexity of single-page JavaScript applications, we have seen a more pressing need to make client-side state management predictable. Redux, with its three principles of restrictions for updating state, has proven to be invaluable in a number of projects we have implemented. Getting Started with Redux and idiomatic Redux tutorials are a good starting point for new and experienced users. Its minimal library design has spawned a rich set of tools, and we encourage you to check out the redux-ecosystem-links project for examples, middleware and utility libraries. We also particularly like the testability story: Dispatching actions, state transitions and rendering can be unit-tested separately from one another and with minimal amounts of mocking. | With the increasing complexity of single-page JavaScript applications, we have seen a more pressing need to make client-side state management predictable. Redux, with its three principles of restrictions for updating state, has proven to be invaluable in a number of projects we have implemented. Getting Started with Redux and idiomatic Redux tutorials are a good starting point for new and experienced users. Its minimal library design has spawned a rich set of tools, and we encourage you to check out the redux-ecosystem-links project for examples, middleware and utility libraries. We also particularly like the testability story: Dispatching actions, state transitions and rendering can be unit-tested separately from one another and with minimal amounts of mocking. | Redux is a great, mature tool that has helped many of our teams reframe how they think about managing state in client-side apps. Using a Flux-style approach, it enables a loosely coupled state-machine architecture that's easy to reason about. We've found it a good companion to some of our favored JavaScript frameworks, such as Ember and React.","blip_selector":"redux","name":"Redux","display_name":"Redux","url":"/radar/languages-and-frameworks/redux","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010060,"quadrant":"techniques","volume_date":"2020-10","description":"Secure enclaves, also identified as trusted execution environments (TEE), refer to a technique that isolates an environment — processor, memory and storage — with a higher level of security and only provides a limited exchange of information with its surrounding untrusted execution context. For example, a secure enclave at the hardware and OS levels can create and store private keys and perform operations with them such as encrypt data or verify signatures without the private keys leaving the secure enclave or being loaded in the untrusted application memory. Secure enclave provides a limited set of instructions to perform trusted operations, isolated from an untrusted application context.\n\nThe technique has long been supported by many hardware and OS providers (including Apple), and developers have used it in IoT and edge applications. Only recently, however, has it gained attention in enterprise and cloud-based applications. Cloud providers have started to introduce confidential computing features such as hardware-based secure enclaves: Azure confidential computing infrastructure promises TEE-enabled VMs and access through the Open Enclave SDK open-source library to perform trusted operations. Similarly, GCP Confidential VMs and Compute Engine, still in beta, allow using VMs with data encryption in memory, and AWS Nitro Enclaves is following them with its upcoming preview release. With the introduction of cloud-based secure enclaves and confidential computing, we can add a third pillar to data protection: in rest, in transit and now in memory.\n\nEven though we're still in the very early days of secure enclaves for enterprise, we encourage you to consider this technique, while staying informed about known vulnerabilities that can compromise the secure enclaves of the underlying hardware providers.","blip_selector":"secure-enclaves","name":"Secure enclaves","display_name":"Secure enclaves","url":"/radar/techniques/secure-enclaves","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010018,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"Testing Library is a family of packages for testing applications in numerous frameworks such as React, Vue, React Native and Angular among others. This set of libraries helps you test UI components in a user-centric way by encouraging you to test user behavior rather than implementation details, such as the presence of elements in the UI at a certain moment in time. One of the benefits of this mindset is more reliable tests, and this is what we call out as its main differentiator. We recommend you assess this family of libraries when testing your web applications in any framework. Although our direct experience is limited to React Testing Library and Angular Testing Library, we've been impressed with what we've seen.","blip_selector":"testing-library","name":"Testing Library","display_name":"Testing Library","url":"/radar/languages-and-frameworks/testing-library","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010031,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"Although we're big advocates of defining security policy as code, the tooling in this space has been fairly limited. If you're using HashiCorp products (such as Terraform or Vault) and don't mind paying for the enterprise versions, you have the option of using HashiCorp Sentinel. Sentinel is, in effect, a complete programming language for defining and implementing context-based policy decisions. For example, in Terraform it can be used to test for policy violations before applying infrastructure changes. In Vault, Sentinel can be used to define fine-grained access control on the APIs. This approach has all the benefits of encapsulation, maintainability, readability and extensibility that high-level programming languages offer, creating an attractive alternative to traditional, declarative security policy. Sentinel is in the same class of tools as Open Policy Agent but is proprietary, closed-source and only works with HashiCorp products.","blip_selector":"hashicorp-sentinel","name":"HashiCorp Sentinel","display_name":"HashiCorp Sentinel","url":"/radar/languages-and-frameworks/hashicorp-sentinel","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1287,"quadrant":"tools","volume_date":"2020-10","description":"Bitrise, a domain-specific CD tool for mobile applications, continues to be a useful part of the mobile workflow, and teams really should be using it. Bitrise can build, test and deploy mobile applications all the way from developer laptop to app store publishing. It's easy to set up and provides a comprehensive set of prebuilt steps for most mobile development needs. | Building, testing and deploying mobile applications entails complex steps, especially when we consider a pipeline from source code repository to app stores. All of these steps can be automated with scripts and build pipelines in generic CI/CD tools. However, our teams have found Bitrise, a domain-specific CD tool for mobile applications, useful for mobile applications when there was no need to integrate with build pipelines for back-end systems. Bitrise is easy to set up and provides a comprehensive set of prebuilt steps for most mobile development needs. | Building, testing and deploying mobile applications entails a number of complex steps, especially when we consider a pipeline from source code repositories to app stores. All these steps can be automated with scripts and build pipelines in generic CI/CD tools. However, for teams that focus on mobile development, and have little or no requirement to integrate with build pipelines for back-end systems, a domain-specific tool can reduce the complexity and maintenance overhead. Bitrise is easy to set up and provides a comprehensive set of prebuilt steps for most mobile development needs.","blip_selector":"bitrise","name":"Bitrise","display_name":"Bitrise","url":"/radar/tools/bitrise","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":843,"quadrant":"tools","volume_date":"2020-10","description":"Bokeh is one of the principal libraries in Python for creating scientific plots and data visualizations that render in the browser via JavaScript. Such tools, compared to desktop tools that create static images, make it easy to reuse code for exploratory work in web applications. Bokeh is particularly good for this. The library is mature and full-featured. What we like about Bokeh: it's great at keeping to its concern as a presentation layer tool and not trying to take on concerns such as data aggregation (see ggplot) or web app development (such as Shiny or Dash). This makes it a joy to use when separation of concerns is important to you. Bokeh does provide web UI widgets and can run in server mode, but you can take or leave these features as you see fit. Bokeh is flexible, and it doesn't make too many assumptions about how you'll use it nor does it have many dependencies (such as pandas or notebooks). | In the world of data science and analytics, much of the work is done using Python and R, languages which sadly offer few options for web-accessible plotting of visualizations. One approach is to convert the result of analysis into something that can be easily visualized and interacted with in the browser. We’re aware of two tools that are an attempt to do this. Bokeh is a Python and JavaScript library that allows you to create interactive visualizations “in the style of D3.js” but with high performance over large or streaming data sets. Vega is a declarative visualization grammar for D3 that consumes server-generated JSON datasets and translates visualization descriptions into D3.js code.","blip_selector":"bokeh","name":"Bokeh","display_name":"Bokeh","url":"/radar/tools/bokeh","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010079,"quadrant":"tools","volume_date":"2020-10","description":"Katran is a high-performance layer 4 load balancer. It's not for everyone, but if you need redundancy for layer 7 load balancers (such as HAProxy or NGINX) or need to scale load balancers to two or more servers, then we recommend assessing Katran. We see Katran as a flexible and efficient choice over techniques such as round-robin DNS over L7 load balancers or the IPVS Kernel model that network engineers usually adopt to solve similar challenges.","blip_selector":"katran","name":"Katran","display_name":"Katran","url":"/radar/tools/katran","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010038,"quadrant":"tools","volume_date":"2020-10","description":"Maintaining large-scale JavaScript codebases is never easy, but it's especially challenging when migrating breaking changes. IDEs with refactoring capabilities may help in simple scenarios. However, when your codebase is a widely dependent library, every time you make a breaking change you have to go through a series of client codebases to make the appropriate updates — which requires human oversight and needs to be done manually. jscodeshift, a toolkit to refactor JavaScript and TypeScript, helps relieve this pain. It can parse your code to abstract syntax trees (AST) and provides an API to manipulate the tree with various transformations (e.g., adding, renaming and deleting properties from existing components) and then exports the tree as final source code. jscodeshift also comes with a simple unit testing utility, which can apply test-driven development for writing migration codemods. We've found jscodeshift to be quite helpful when maintaining design systems.","blip_selector":"jscodeshift","name":"jscodeshift","display_name":"jscodeshift","url":"/radar/tools/jscodeshift","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010086,"quadrant":"tools","volume_date":"2020-10","description":"Even though tooling has vastly improved in the infrastructure space, writing a shell script may make sense in some cases. Of course, the syntax of shell scripts can only be described as arcane, and as we've less practice writing shell scripts these days, we've come to like ShellCheck, a linter for shell scripts. ShellCheck can be used from the command line, as part of a build or, even better, as an extension in many popular IDEs. The wiki contains a detailed description of several hundred issues that ShellCheck can detect, and most tools and IDEs provide a way to conveniently access the respective wiki page when an issue is found.","blip_selector":"shellcheck","name":"ShellCheck","display_name":"ShellCheck","url":"/radar/tools/shellcheck","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010040,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"In the past we've talked about the improving tooling for applying good engineering practices in data science projects. Kedro is another good addition in this space. It's a development workflow framework for data science projects that brings a standardized approach to building production-ready data and machine-learning pipelines. We like the focus on software engineering practices and good design with its emphasis on test-driven development, modularity, versioning and good hygiene practices such as keeping credentials out of the codebase.","blip_selector":"kedro","name":"Kedro","display_name":"Kedro","url":"/radar/languages-and-frameworks/kedro","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010097,"quadrant":"platforms","volume_date":"2020-10","description":"Continuous challenges with how individuals and organizations establish trust digitally, over the internet, is giving rise to a new approach on how to prove identity, how to share and verify attributes needed to establish trust and how to securely transact. Our Radar features some of the foundational technologies such as decentralized identity and verifiable credentials that enable this new era of digital trust.\n\nHowever, such a global scale change won't be possible without a standardization of a technical governance stack that enables interoperability. The new Trust over IP Foundation, part of the Linux Foundation, has set out to do just that. Taking its inspiration from how TCP/IP standardization as the narrow waist of the internet has enabled interoperability across billions of devices, the group is defining a four-layer technical and governance Trust over IP stack. The stack includes public utilities such as decentralized identifiers, decentralized identity comms to standardized protocols for agents such as digital wallets to communicate, data exchange protocols such as flows to issue and verify verifiable credentials, as well as the application ecosystems such as education, finance, healthcare, etc. If you're revisiting your identity systems and how you establish trust with your ecosystem, we suggest looking into ToIP stack and its supporting tooling, Hyperledger Aries.","blip_selector":"trust-over-ip-stack","name":"Trust over IP stack","display_name":"Trust over IP stack","url":"/radar/platforms/trust-over-ip-stack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005067,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"We've featured several state management libraries in the Radar before, but XState takes a slightly different approach. It's a simple JavaScript and TypeScript framework for creating finite state machines and visualizing them as state charts. It integrates with the more popular reactive JavaScript frameworks (Vue.js, Ember.js, React.js and RxJS) and is based on the W3C standard for finite state machines. Another notable feature is the serialization of machine definitions. One thing that we've found helpful when creating finite state machines in other contexts (particularly when writing game logic) is the ability to visualize states and their possible transitions; we like that it's really easy to do this with XState's visualizer. | We've featured several state management libraries in the Radar before, but XState takes a slightly different approach. It's a simple JavaScript and TypeScript framework for creating finite state machines and visualizing them as state charts. It integrates with the more popular reactive JavaScript frameworks (Vue.js, Ember.js, React.js and RxJS) and is based on the W3C standard for finite state machines. Another notable feature is the serialization of machine definitions. One thing that we've found helpful when creating finite state machines in other contexts (particularly when writing game logic) is the ability to visualize states and their possible transitions; we like the fact that it's really easy to do this with XState's visualizer.","blip_selector":"xstate","name":"XState","display_name":"XState","url":"/radar/languages-and-frameworks/xstate","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1112,"quadrant":"tools","volume_date":"2020-10","description":"Yarn continues to be the package manager of choice for many teams. We're excited about Yarn 2, a major new release with a long list of changes and improvements. In addition to usability tweaks and improvements in the area of workspaces, Yarn 2 introduces the concept of zero-installs, which allows developers to run a project directly after cloning it. However, Yarn 2 includes some breaking changes which makes the upgrade nontrivial. It also defaults to plug'n'play (PnP) environments and at the same time doesn't support React Native in PnP environments. Teams can, of course, opt out of PnP or stay on Yarn 1. They should be aware, though, that Yarn 1 is now in maintenance mode. | Yarn is a fast, reliable and secured package manager for JavaScript. Using a lock file and a deterministic algorithm, Yarn is able to guarantee that an installation that worked on one system will work exactly the same way on any other system. By efficiently queuing up requests, Yarn maximizes network utilization and as a result we’ve seen faster package downloads. Yarn continues to be our tool of choice for JavaScript package management in spite of the latest improvements in npm (version 5). | Yarn is a new package manager that replaces the existing workflow for the npm client while remaining compatible with the npm registry. With the npm client, we may end up with a different tree structure under node_modules based on the order that dependencies are installed. This nondeterministic nature can cause \"works on my machine\" problems. By breaking the installation steps into resolution, fetching and linking, Yarn avoids these issues using deterministic algorithms and lockfiles and thus guarantees repeatable installations. We've also seen significantly faster builds in our continuous integration (CI) environment because of Yarn caching all the packages it downloads. | Yarn is a new package manager that replaces the existing workflow for the npm client while remaining compatible with the npm registry. With the npm client, we may end up with a different tree structure under node_modules based on the order that dependencies are installed. This nondeterministic nature can cause \"works on my machine\" problems. By breaking the installation steps into resolution, fetching and linking, Yarn avoids these issues using deterministic algorithms and lockfiles and thus guarantees repeatable installations. We've also seen significantly faster builds in our continuous integration (CI) environment because of Yarn caching all the packages it downloads.","blip_selector":"yarn","name":"Yarn","display_name":"Yarn","url":"/radar/tools/yarn","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010089,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"The Kotlin ecosystem keeps growing and more libraries are taking advantage of Kotlin language features to replace their Java alternatives. Strikt is an assertion library that allows you to write test assertions in a very fluent style. It uses Kotlin features such as blocks and lambdas to help make your tests less verbose while maintaining readability. Strikt also supports building custom assertions, which can make your tests more domain specific.","blip_selector":"strikt","name":"Strikt","display_name":"Strikt","url":"/radar/languages-and-frameworks/strikt","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202010074,"quadrant":"techniques","volume_date":"2020-10","description":"We've long warned against centralized enterprise services buses and defined \"smart endpoints, dumb pipes\" as one of the core characteristics of a microservices architecture. Unfortunately, we're observing a pattern of traditional ESBs rebranding themselves, creating ESBs in API gateway's clothing that naturally encourage overambitious API gateways. Don't let the marketing fool you: regardless of what you call it, putting business logic (including orchestration and transformation) in a centralized tool creates architectural coupling, decreases transparency, and increases vendor lock-in with no clear upside. API gateways can still act as a useful abstraction for crosscutting concerns, but we believe the smarts should live in the APIs themselves.","blip_selector":"esbs-in-api-gateway-s-clothing","name":"ESBs in API Gateway's clothing","display_name":"ESBs in API Gateway's clothing","url":"/radar/techniques/esbs-in-api-gateway-s-clothing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010059,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"More and more teams using React are reevaluating their options for state management, something we also mention in our reassessment of Redux. Now, Facebook — the creators of React — have published Recoil, a new framework for managing state, which came out of an internal application that had to deal with large amounts of data. Even though we currently do not have much practical experience with Recoil, we see its potential and promise. The API is simple and easy to learn; it feels like idiomatic React. Unlike other approaches, Recoil provides an efficient and flexible way to have state shared across an application: it supports dynamically created state by derived data and queries as well as app-wide state observation without impairing code splitting.","blip_selector":"recoil","name":"Recoil","display_name":"Recoil","url":"/radar/languages-and-frameworks/recoil","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010057,"quadrant":"techniques","volume_date":"2020-10","description":"Cloud providers have slowly started supporting Kubernetes-style APIs, via custom resource definitions (CRDs), for managing their cloud services. In most cases these cloud services are a core part of the infrastructure, and we've seen teams use tools such as Terraform or Pulumi to provision them. With these new CRDs (ACK for AWS, Azure Service Operator for Azure and Config Connectors for GCP) you can use Kubernetes to provision and manage these cloud services. One advantage of these Kube-managed cloud services is that you can leverage the same Kubernetes control plane to enforce the declarative state of both your application and infrastructure. The downside is that it tightly couples your Kubernetes cluster with infrastructure, so we're carefully assessing it and you should too.","blip_selector":"kube-managed-cloud-services","name":"Kube-managed cloud services","display_name":"Kube-managed cloud services","url":"/radar/techniques/kube-managed-cloud-services","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010034,"quadrant":"tools","volume_date":"2020-10","description":"Sensei from Secure Code Warrior is a Java IDE plugin that makes it easy to create and distribute secure code quality guidelines. At ThoughtWorks we often advocate for \"tools over rules,\" that is, make it easy to do the right thing over applying checklist-like governance rules and procedures, and this tool fits this philosophy. Developers create recipes that can be easily shared with team members. These can be simple or complex and are implemented as queries targeting the Java AST. Examples include warnings for SQL injection, cryptographic weakness and many others. Another feature we like: Since it executes on code changes in the IDE, Sensei provides faster feedback than the more traditional static analysis tools.","blip_selector":"sensei","name":"Sensei","display_name":"Sensei","url":"/radar/tools/sensei","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":771,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"The Rust programming language continues to grow in popularity and has been voted Stack Overflow's \"most loved\" language by developers five years in a row. We like it too. It's a fast, safe and expressive language that is increasing in utility as its ecosystem grows. For example, Rust is starting to be used for data science and machine learning and can give a significant performance boost. Also, Materialize is a streaming-oriented, low-latency database written in Rust. | Rust is continuously gaining in popularity. We've had heated discussions about which is better, Rust or C++/Go, without a clear winner. However, we're glad to see Rust has improved significantly, with more built-in APIs being added and stabilized, including advanced async support, since we mentioned it in our previous Radar. In addition, Rust has also inspired the design of new languages. For example, the Move language on Libra borrows Rust's way of managing memory to manage resources, ensuring that digital assets can never be copied or implicitly discarded. | Since we last featured it on the Radar in January 2015, we've seen steadily increasing interest in Rust. Some of our clients are now using Rust, mostly in the context of infrastructure tooling but also in high-powered embedded devices. Interest was fuelled by a growing ecosystem as well as improvements to the language itself. The latter included straightforward performance improvements but also changes that make Rust more intuitive, for example the change to non-lexical scoping. Most of the significant changes are included in the Rust 2018 standard released last December. | Rust is a system programming language with modern affordances. It features a rich typing system, safe memory model and task-based concurrency. Compared to the Go language, Rust is more friendly to people who would like to write code in a functional style.","blip_selector":"rust","name":"Rust","display_name":"Rust","url":"/radar/languages-and-frameworks/rust","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005004,"quadrant":"techniques","volume_date":"2020-10","description":"As the pandemic stretches on it seems that highly distributed teams will be the \"new normal,\" at least for the time being. Over the past six months we've learnt a lot about effective remote working. On the positive side, good visual work-management and collaboration tools have made it easier than ever to collaborate remotely with colleagues. Developers, for example, can count on Visual Studio Live Share and GitHub Codespaces to facilitate teamwork and increase productivity. The biggest downside to remote work might be burnout: far too many people are scheduled for back-to-back video calls all day long, and this has begun to take its toll. While online visual tools make it easier to collaborate, it's also possible to build complex giant diagrams that end up being very hard to use, and the security aspects of tool proliferation also need to be carefully managed. Our advice is to remember to take a step back, talk to your teams, evaluate what's working and what's not and change processes and tools as needed. | Distributed teams come in many shapes and setups; delivery teams in a 100% single-site co-located setup, however, have become the exception for us. Most of our teams are either multisite teams or have at least some team members working off-site. Therefore, using \"remote native\" processes and approaches by default can help significantly with the overall team flow and effectiveness. This starts with making sure that everybody has access to the necessary remote systems. Moreover, using tools such as Visual Studio Live Share, MURAL or Jamboard turn online workshops and remote pairing into routines instead of ineffective exceptions. But \"remote native\" goes beyond a lift-and-shift of co-location practices to the digital world: Embracing more asynchronous communication, even more discipline around decision documentation, and \"everybody always remote\" meetings are other approaches our teams practice by default to optimize for location fluidity.","blip_selector":"use-remote-native-processes-and-approaches","name":"Use \"remote native\" processes and approaches","display_name":"Use \"remote native\" processes and approaches","url":"/radar/techniques/use-remote-native-processes-and-approaches","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010045,"quadrant":"tools","volume_date":"2020-10","description":"pnpm is an up-and-coming package manager for Node.js that we're looking at closely because of its higher speed and greater efficiency compared to other package managers. Dependencies are saved in a single place on the disk and are linked into the respective node_modules directories. pnpm also supports incremental optimization on file level, provides a solid API foundation to allow extension/customization and supports store server mode, which speeds up dependency download even more. If your organization has a large number of projects with the same dependencies, you may want to take a closer look at pnpm.","blip_selector":"pnpm","name":"pnpm","display_name":"pnpm","url":"/radar/tools/pnpm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010043,"quadrant":"tools","volume_date":"2020-10","description":"Kustomize is a tool to manage and customize Kubernetes manifest files. It allows you to select and patch your base Kubernetes resources before applying them to different environments and is now natively supported by kubectl. We like it because it helps keep your code DRY and in contrast to Helm (which is trying to do many things — package management, version management and so on), we find Kustomize follows the Unix philosophy: do one thing well and expect the output of every program to be input to another.","blip_selector":"kustomize","name":"Kustomize","display_name":"Kustomize","url":"/radar/tools/kustomize","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010011,"quadrant":"tools","volume_date":"2020-10","description":"We've long liked the idea of using static site generators to avoid complexity and improve performance, whenever the use case allows it. Although Eleventy has been around for a few years, it's recently caught our attention as it's matured and previous favorites such as Gatsby.js displayed some scalability problems. Eleventy is quick to learn and easy to build sites with. We also like the ease with which you can create semantic (and therefore more accessible) markup with its templating and its simple and robust support for pagination.","blip_selector":"eleventy","name":"Eleventy","display_name":"Eleventy","url":"/radar/tools/eleventy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010053,"quadrant":"tools","volume_date":"2020-10","description":"The concept of differential privacy first appeared in the Radar in 2016. Although the problem of breaking privacy through systematic model inference queries was recognized at the time, it was largely a theoretical issue since remedies were few. The industry has lacked tools to prevent this from happening. Opacus is a new Python library that can be used in conjunction with PyTorch to help thwart one type of differential privacy attack. Although this is a promising development, finding the right model and data set to which it applies has been a challenge. The library is still quite new so we're looking forward to seeing how it'll be accepted going forward.","blip_selector":"opacus","name":"Opacus","display_name":"Opacus","url":"/radar/tools/opacus","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010044,"quadrant":"tools","volume_date":"2020-10","description":"Writing secure code is as important as ever, but it's only one of the many things developers have to prioritize. LGTM provides both a safety net and a means to benefit from a knowledge base of secure coding practices. It is a static code analysis tool with a focus on security that is backed by a (partially open-source) catalog of secure coding rules. The rules are implemented as queries over your codebase in the CodeQL query language. It can be used to integrate white-box security checks into your CD pipelines for Java, Go, JavaScript, Python, C# and C/C++. LGTM and CodeQL are part of the Github Security Lab.","blip_selector":"lgtm","name":"LGTM","display_name":"LGTM","url":"/radar/tools/lgtm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010081,"quadrant":"tools","volume_date":"2020-10","description":"Litmus is a chaos engineering tool with a low barrier to entry. It allows you to inject various error scenarios into your Kubernetes cluster with minimal effort. We're particularly excited by the range of capabilities Litmus offers beyond your random pod kill, including simulating network, CPU, memory and I/O issues. Litmus also supports tailored experiments to simulate errors for Kafka and Cassandra among other common services.","blip_selector":"litmus","name":"Litmus","display_name":"Litmus","url":"/radar/tools/litmus","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010035,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"Hermes is a JavaScript engine optimized for fast start-up of React Native applications on Android. JavaScript engines such as V8 have just-in-time (JIT) compilers that profile the code at run time to produce optimized instructions. Hermes, however, takes a different approach by compiling the JavaScript code ahead of time (AOT) into an optimized bytecode. As a result you get a smaller APK image size, lean memory consumption and faster startup time. We're carefully assessing Hermes in a few React Native apps and recommend you do the same.","blip_selector":"hermes","name":"Hermes","display_name":"Hermes","url":"/radar/languages-and-frameworks/hermes","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010065,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"When we wrote about VR beyond gaming a few years ago we made no prediction on how quickly and to what extent VR solutions would be found in fields other than video gaming. In hindsight, we've certainly seen interest and adoption grow but the uptake has been slower than some of us anticipated. One reason could be tooling. Unity and Unreal are two very mature and capable engines for developing VR applications. We also highlighted Godot. However, these engines are quite unlike what most web and enterprise teams are familiar with. As we continued exploring, we realized that web-based VR solutions have come a long way and we've had positive experience with Babylon.js. Written in TypeScript and rendering its applications in the browser, Babylon.js provides a familiar experience for many development teams. Additionally, Babylon.js is open-source software, mature and well-funded, which makes it even more attractive.","blip_selector":"babylon-js","name":"Babylon.js","display_name":"Babylon.js","url":"/radar/languages-and-frameworks/babylon-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005034,"quadrant":"platforms","volume_date":"2020-10","description":"Since introducing JupyterLab in the Assess ring in our last issue, it has become the preferred web-based user interface for Project Jupyter for many of our data practitioners. JupyterLab use is rapidly overtaking Jupyter Notebooks, which it will eventually replace. If you're still using Jupyter Notebooks, you should give JupyterLab a try. Its interactive environment is an evolution of Jupyter Notebook: it extends the original capabilities with drag-and-drop cells and tab autocompletion among other new features. | JupyterLab is the next-generation web-based user interface for Project Jupyter. If you've been using Jupyter Notebooks, JupyterLab is worth a try; it gives you an interactive environment for Jupyter notebooks, code and data. We see it as an evolution of Jupyter Notebook: it provides a better experience by extending its original capabilities of allowing code, visualization and documentation to exist in one place.","blip_selector":"jupyterlab","name":"JupyterLab","display_name":"JupyterLab","url":"/radar/platforms/jupyterlab","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010087,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"Modern ML models are very complex and require massive amounts of labeled training data sets to learn from. Snorkel started at the Stanford AI lab with the realization that manually labeling data is very expensive and often not feasible. Snorkel allows us to label training data programmatically via the creation of labeling functions. Snorkel employs supervised learning techniques to assess the accuracies and correlations of these labeling functions, and then reweighs and combines their output labels, leading to high-quality training labels. The creators of Snorkel have since come out with a commercial platform called Snorkel Flow. While Snorkel itself is no longer actively developed, it's still significant for its ideas on the use of weakly supervised methods to label data.","blip_selector":"snorkel","name":"Snorkel","display_name":"Snorkel","url":"/radar/languages-and-frameworks/snorkel","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010017,"quadrant":"tools","volume_date":"2020-10","description":"Stryker is a relatively new entry in the mutation testing space. Similar to Pitest, Stryker lets you evaluate the quality of your tests. We've been using it quite successfully in JavaScript projects, but it also supports C# and Scala projects. Stryker is very user friendly and highly customizable, and we've been able to increase code coverage as well as confidence in the applications we're delivering for our clients.","blip_selector":"stryker","name":"Stryker","display_name":"Stryker","url":"/radar/tools/stryker","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202005090,"quadrant":"techniques","volume_date":"2020-10","description":"Several years ago, a new generation of log aggregation platforms emerged that were capable of storing and searching over vast amounts of log data to uncover trends and insights in operational data. Splunk was the most prominent but by no means the only example of these tools. Because these platforms provide broad operational and security visibility across the entire estate of applications, administrators and developers have grown increasingly dependent on them. This enthusiasm spread as stakeholders discovered that they could use log aggregation for business analytics. However, business needs can quickly outstrip the flexibility and usability of these tools. Logs intended for technical observability are often inadequate to infer deep customer understanding. We prefer either to use tools and metrics designed for customer analytics or to take a more event-driven approach to observability where both business and operational events are collected and stored in a way they can be replayed and processed by more purpose-built tools. | Several years ago, a new generation of log aggregation platforms emerged that were capable of storing and searching over vast amounts of log data to uncover trends and insights in operational data. Splunk was the most prominent but by no means the only example of these tools. Because these platforms provide broad operational and security visibility across the entire estate of applications, administrators and developers have grown increasingly dependent on them. This enthusiasm spread as stakeholders discovered that they could use log aggregation for business analytics. However, business needs can quickly outstrip the flexibility and usability of these tools. Logs intended for technical observability are often inadequate to infer deep customer understanding. We prefer either to use tools and metrics designed for customer analytics or to take a more event-driven approach to observability where both business and operational events are collected and stored in a way they can be replayed and processed by more purpose-built tools.","blip_selector":"log-aggregation-for-business-analytics","name":"Log aggregation for business analytics","display_name":"Log aggregation for business analytics","url":"/radar/techniques/log-aggregation-for-business-analytics","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010029,"quadrant":"tools","volume_date":"2020-10","description":"Service meshes and API gateways provide a convenient way to route traffic to a variety of microservices, all of which implement the same API interface. Flagger uses this feature to dynamically adjust the portion of traffic that is routed to a new version of a service. This is a common technique for canary releases or blue/green deployment. Flagger works in conjunction with a variety of popular proxies (including Envoy and Kong) to progressively ramp up requests to a service and report metrics on the load in order to provide fast feedback on a new release. We like that Flagger simplifies this valuable practice so that it can be more widely adopted. Although Flagger is sponsored by Weaveworks, it stands on its own with no obligation to use it in conjunction with Weaveworks' other tooling.","blip_selector":"flagger","name":"Flagger","display_name":"Flagger","url":"/radar/tools/flagger","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010069,"quadrant":"tools","volume_date":"2020-10","description":"We've included continuous delivery for machine learning as a technique in previous Radars, and in this edition we want to highlight a promising new tool called Continuous Machine Learning (or CML) from the people who made DVC. CML aims to bring the best engineering practices of CI and CD to AI and ML teams and can help to organize your MLOps infrastructure on top of a traditional software engineering stack, instead of creating separate AI platforms. We like that they've prioritized support for DVC and see this as a good sign for this burgeoning new tool.","blip_selector":"cml","name":"CML","display_name":"CML","url":"/radar/tools/cml","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010021,"quadrant":"platforms","volume_date":"2020-10","description":"Data scientists spend a large part of their time on data discovery, which means tooling to help in this space is bound to generate some excitement. Although the Apache Atlas project has become the de facto tool for metadata management, data discovery is still not easily accomplished. Enter Amundsen, which can be deployed in concert with Apache Atlas to provide a much nicer search interface for data discovery.","blip_selector":"amundsen","name":"Amundsen","display_name":"Amundsen","url":"/radar/platforms/amundsen","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1222,"quadrant":"tools","volume_date":"2020-10","description":"Helm is a package manager for Kubernetes. It comes with a repository of curated Kubernetes applications that are maintained in the official Charts repository. Since we last talked about Helm, Helm 3 has been released, and the most significant change is the removal of Tiller, the server-side component of Helm 2. The benefit of a design without Tiller is that you can only make changes to the Kubernetes cluster from the client side, that is, you can only modify the cluster according to the permissions you have as a user of the Helm command. We've used Helm in a number of client projects and its dependency management, templating and hook mechanism has greatly simplified the application lifecycle management in Kubernetes. | Helm is a package manager for Kubernetes. It comes with a repository of curated Kubernetes applications that are maintained in the official Charts repository. Helm has two components: a command line utility called Helm and a cluster component called Tiller. Securing a Kubernetes cluster is a wide and nuanced topic, but we highly recommend setting up Tiller in a role-based access control (RBAC) environment. We've used Helm in a number of client projects and its dependency management, templating and hook mechanism has greatly simplified the application lifecycle management in Kubernetes. However, we recommend proceeding with caution — Helm's YAML templating can be difficult to understand, and Tiller still has some rough edges. Helm 3 is expected to address these issues. | Helm is a package manager for Kubernetes. The set of Kubernetes resources that together define an application is packaged as charts. These charts can describe a single resource, such as a Redis pod, or a full stack of a web application: HTTP servers, databases and caches. Helm, by default, comes with a repository of curated Kubernetes applications that are maintained in the official charts repository. It’s also easy to set up a private chart repository for internal usage. Helm has two components: a command line utility called Helm and a cluster component called Tiller. Securing a Kubernetes cluster is a wide and nuanced topic, but we highly recommend setting up Tiller in a role-based access control (RBAC) environment. We’ve used Helm in a number of client projects and it’s dependency management, templating and hook mechanism has greatly simplified the application lifecycle management in Kubernetes.","blip_selector":"helm","name":"Helm","display_name":"Helm","url":"/radar/tools/helm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202010003,"quadrant":"techniques","volume_date":"2020-10","description":"When we first covered GraphQL in the Radar, we cautioned that its misuse can lead to antipatterns which, in the long run, has more disadvantages than benefits. Nevertheless, we’ve seen an increasing interest in GraphQL among our teams because of its ability to aggregate information from different resources. This time we want to caution you about using Apollo Federation and its  strong support for a single unified data graph for your company. Even though at first glance the idea of having ubiquitous concepts across the organization is tempting, we have to take into account previous similar attempts in the industry — such as MDM and canonical data model among others — that have exposed the pitfalls of this approach. The challenges can be significant, especially when the domain we find ourselves in is complex enough to create a unique unified model.","blip_selector":"apollo-federation","name":"Apollo Federation","display_name":"Apollo Federation","url":"/radar/techniques/apollo-federation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010094,"quadrant":"platforms","volume_date":"2020-10","description":"Tekton is a young Kubernetes-native platform for managing continuous integration and delivery (CI/CD) pipelines. It not only installs and runs on Kubernetes but also defines its CI/CD pipelines as Kubernetes custom resources. This means the pipelines can now be controlled by native Kubernetes clients (CLI or APIs) and can take advantage of underlying resource management features such as rollbacks. The pipeline declaration format is flexible and allows defining workflows with conditions, parallel execution paths and handling final tasks to clean up among other features. As a result, Tekton can support complex and hybrid deployment workflows with rollbacks, canary release and more. Tekton is open source and also offered as a managed service by GCP. Although the documentation has room for improvement and the community is growing, we've been using Tekton successfully for production workloads on AWS.","blip_selector":"tekton","name":"Tekton","display_name":"Tekton","url":"/radar/platforms/tekton","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010099,"quadrant":"tools","volume_date":"2020-10","description":"Zola is a static site generator written in Rust. As such it comes as a single executable with no dependencies, is very fast and supports all the usual things you'd expect such as Sass, content in markdown and hot reloading. We've had success building static sites with Zola and appreciate how intuitive it is to use.","blip_selector":"zola","name":"Zola","display_name":"Zola","url":"/radar/tools/zola","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911030,"quadrant":"languages-and-frameworks","volume_date":"2020-10","description":"jest-when is a lightweight JavaScript library that complements Jest by matching mock function call arguments. Jest is a great tool for testing the stack; jest-when allows you to expect specific arguments for mock functions which enables you to write more robust unit tests of modules with many dependencies. It's easy to use and provides great support for multiple matchers, which is why our teams have made jest-when their default choice for mocking in this space. | jest-when is a lightweight JavaScript library that complements Jest by matching mock function call arguments. Jest is a great tool for testing the stack; jest-when allows you to expect specific arguments for mock functions and thus lets you write more robust unit tests of modules with many dependencies.","blip_selector":"jest-when","name":"jest-when","display_name":"jest-when","url":"/radar/languages-and-frameworks/jest-when","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1266,"quadrant":"tools","volume_date":"2020-10","description":"We've used Terraform extensively to create and manage cloud infrastructure. In our experience with larger setups, where code is divided into modules that are included in different ways, teams eventually hit a wall of unavoidable repetition caused by a lack of flexibility. We've addressed this by using Terragrunt, a thin wrapper for Terraform that implements the practices advocated by Yevgeniy Brikman’s Terraform: Up and Running. We've found Terragrunt helpful because it encourages versioned modules and reusability for different environments. Lifecycle hooks are another useful feature providing additional flexibility. In terms of packaging, Terragrunt has the same limitations as Terraform: there is no proper way to define packages or dependencies between packages. As a workaround, you can use modules and specify a version associated with a Git tag. | We widely use Terraform as code to configure a cloud infrastructure. Terragrunt is a thin wrapper for Terraform that implements the practices advocated by the Terraform: Up and Running book. We've found Terragrunt helpful as it encourages versioned modules and reusability for different environments with some handy features, including recursive code execution in subdirectories. We'd like to see the tool evolve to support CD practices natively, where all code can be packaged, versioned and reused across different environments on CD pipelines. Our team achieves this today with workarounds.","blip_selector":"terragrunt","name":"Terragrunt","display_name":"Terragrunt","url":"/radar/tools/terragrunt","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010062,"quadrant":"techniques","volume_date":"2020-10","description":"Replacing legacy code at scale is always a difficult endeavor and one that often benefits from executing a parallel run with reconciliation. In practice, the technique relies on executing the same production flow through both the old and new code, returning the response from the legacy code but comparing the results to gain confidence in the new code. Despite being an old technique, we've seen more robust implementations in recent years building on continuous delivery practices such as canary releases and feature toggles and extending them by adding an extra layer of experimentation and data analysis to compare live results. We've even used the approach to compare cross-functional results such as response time. Although we've used the technique multiple times with bespoke tooling, we certainly owe a nod to GitHub's Scientist tool, which they used to modernize a critical piece of their application and which has now been ported to multiple languages.","blip_selector":"parallel-run-with-reconciliation","name":"Parallel run with reconciliation","display_name":"Parallel run with reconciliation","url":"/radar/techniques/parallel-run-with-reconciliation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010008,"quadrant":"techniques","volume_date":"2020-10","description":"Polyfills are extremely useful to help the web evolve, providing substitute implementations of modern features for browsers that don't implement them (yet). Too often, though, web applications ship polyfills to browsers that don't need them, which causes unnecessary download and parsing overhead. The situation is becoming more pronounced now as only a few rendering engines remain and the bulk of the polyfills target only one of them: the Trident renderer in IE11. Further, market share of IE11 is dwindling with support ending in less than a year. We therefore suggest that you make use of browser-tailored polyfills, shipping only necessary polyfills to a given browser. This technique can even be implemented as a service with Polyfill.io.","blip_selector":"browser-tailored-polyfills","name":"Browser-tailored polyfills","display_name":"Browser-tailored polyfills","url":"/radar/techniques/browser-tailored-polyfills","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":928,"quadrant":"tools","volume_date":"2020-10","description":"Implementing sustainable continuous delivery pipelines that can build and deploy production software across multiple environments requires a tool that treats build pipelines and artifacts as first-class citizens. When we first started assessing Concourse, we liked its simple and flexible model, the principle of container-based builds and the fact that it forces you to define pipelines as code. Since then, the usability has improved, and the simple model has stood the test of time. Many of our teams and clients have successfully been using Concourse for large pipeline setups over longer periods of time. We also often leverage Concourse's flexibility to run workers anywhere, for example, when hardware integration tests require a local setup. | Many development teams are making the move from simple continuous integration servers to Continuous Delivery pipelines, often spanning multiple environments, reaching into production. To implement such a pipeline successfully and operate it in a sustainable way requires a CI/CD tool that treats build pipelines and artifacts as first-class citizens; and unfortunately there aren’t many. Concourse CI is a promising new entrant in this field, and our teams that have tried it are excited about its setup, which enables builds that run in containers, has a clean, usable UI and discourages snowflake build servers.","blip_selector":"concourse","name":"Concourse","display_name":"Concourse","url":"/radar/tools/concourse","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010090,"quadrant":"techniques","volume_date":"2020-10","description":"Controlled experiments using A/B testing is a great way to inform decisions around product development. But it doesn't work well when we can't establish independence between the two groups involved in the A/B test — i.e., adding someone to the \"A\" group impacts the \"B\" group and vice versa. One technique to address this problem space is Switchback experimentation. The core concept here is we switch back and forth between the \"A\" and \"B\" modes of the experiment in a certain region at alternating time periods instead of both running during the same time period. We then compare the customer experience and other key metrics between the two time buckets. We've tried this to good effect in some of our projects — it's a good tool to have in our experiments toolbelt.","blip_selector":"switchback-experimentation","name":"Switchback experimentation","display_name":"Switchback experimentation","url":"/radar/techniques/switchback-experimentation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010073,"quadrant":"platforms","volume_date":"2020-10","description":"Dremio is a cloud data lake engine that powers interactive queries against cloud data lake storage. With Dremio, you don't have to manage data pipelines in order to extract and transform data into a separate data warehouse for predictive performance. Dremio creates virtual data sets from data ingested into a data lake and provides a uniform view to consumers. Presto popularized the technique of separating storage from the compute layer, and Dremio takes it further by improving performance and optimizing cost of operation.","blip_selector":"dremio","name":"Dremio","display_name":"Dremio","url":"/radar/platforms/dremio","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":777,"quadrant":"techniques","volume_date":"2020-10","description":"Since we last mentioned tailored service templates, we've seen a broader adoption of the pattern to help pave the road for organizations moving to microservices. With constant advances in observability tooling, container orchestration and service mesh sidecars, a template provides sensible defaults to bootstrap a new service, removing a great deal of setup needed to make the service work well with the surrounding infrastructure. We've had success applying product management principles to tailored service templates, treating internal developers as customers and making it easier for them to push code to production and operate it with appropriate observability. This has the added benefit of acting as a lightweight governance mechanism to centralize default technical decisions. | We see multiple organizations creating a Tailored Service Template which can be used to quickly seed new services, pre-configured to operate within that organization's production environment. The template contains a default set of decisions such as web frameworks, logging, monitoring, build, packaging, and deployment approaches. This is a very useful technique for encouraging collaborative evolution while retaining lightweight governance. | We see multiple organizations creating a Tailored Service Template which can be used to quickly seed new services, pre-configured to operate within that organization's production environment. The template contains a default set of decisions such as web frameworks, logging, monitoring, build, packaging, and deployment approaches. This is a very useful technique for encouraging collaborative evolution while retaining lightweight governance.","blip_selector":"tailored-service-templates","name":"Tailored service templates","display_name":"Tailored service templates","url":"/radar/techniques/tailored-service-templates","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202010055,"quadrant":"tools","volume_date":"2020-10","description":"It's important for a development team to identify whether the dependencies of their application have known vulnerabilities. OSS Index could be used to achieve this goal. OSS Index is a free catalog of open-source components and scanning tools designed to help developers identify vulnerabilities, understand risk and keep their software safe. Our teams are already integrating this index into pipelines via different languages, including AuditJS and Gradle plugin. The speed is fast, vulnerabilities are identified accurately and few false positives occur.","blip_selector":"oss-index","name":"OSS Index","display_name":"OSS Index","url":"/radar/tools/oss-index","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911044,"quadrant":"techniques","volume_date":"2020-10","description":"Fitness functions introduced by evolutionary architecture, borrowed from evolutionary computing, are executable functions that inform us if our applications and architecture are objectively moving away from their desired characteristics. They're essentially tests that can be incorporated into our release pipelines. One of the major characteristics of an application is the freshness of its dependencies to other libraries, APIs or environmental components that a dependency drift fitness function tracks to flag the out-of-date dependencies that require updating. With the growing and maturing number of tools that detect dependency drifts, such as Dependabot or Snyk, we can easily incorporate dependency drift fitness functions into our software release process to take timely action in keeping our application dependencies up to date. | Many teams and organizations have no formal or consistent way of tracking technical dependencies in their software. This issue often shows itself when that software needs to be changed, at which point the use of an outdated version of a library, API or component will cause problems or delay. Dependency drift fitness function is a technique to introduce a specific evolutionary architecture fitness function to track these dependencies over time, thus giving an indication of the possible work needed and whether a potential issue is getting better or worse.","blip_selector":"dependency-drift-fitness-function","name":"Dependency drift fitness function","display_name":"Dependency drift fitness function","url":"/radar/techniques/dependency-drift-fitness-function","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202010027,"quadrant":"techniques","volume_date":"2020-10","description":"We're seeing more and more tools that enable you to create software architecture and other diagrams as code. There are benefits to using these tools over the heavier alternatives, including easy version control and the ability to generate the DSLs from many sources. Tools in this space that we like include Diagrams, Structurizr DSL, AsciiDoctor Diagram and stables such as WebSequenceDiagrams, PlantUML and the venerable Graphviz. It's also fairly simple to generate your own SVG these days, so don't rule out quickly writing your own tool either. One of our authors wrote a small Ruby script to quickly create SVGs, for example.","blip_selector":"diagrams-as-code","name":"Diagrams as code","display_name":"Diagrams as code","url":"/radar/techniques/diagrams-as-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202005002,"quadrant":"techniques","volume_date":"2020-05","description":"Unfortunately, feature toggles are less common than we'd like, and quite often we see people mixing up its types and use cases. It's quite common to come across teams that use heavyweight platforms such as LaunchDarkly to implement feature toggles, including release toggles, to benefit from Continuous Integration, when all you need are if/else conditionals. Therefore, unless you need A/B testing or canary release or hand over feature release responsibility to business folks, we encourage you to use the simplest possible feature toggle instead of unnecessarily complex feature toggle frameworks.","blip_selector":"simplest-possible-feature-toggle","name":"Simplest possible feature toggle","display_name":"Simplest possible feature toggle","url":"/radar/techniques/simplest-possible-feature-toggle","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1149,"quadrant":"tools","volume_date":"2020-05","description":"Cypress is still a favorite among our teams where developers manage end-to-end tests themselves, as part of a healthy test pyramid, of course. We decided to call it out again in this Radar because recent versions of Cypress have added support for Firefox, and we strongly suggest testing on multiple browsers. The dominance of Chrome and Chromium-based browsers has led to a worrying trend of teams seemingly only testing with Chrome which can lead to nasty surprises. | We keep receiving positive feedback on \"post-Selenium\" web UI testing tools such as Cypress, TestCafe and Puppeteer. Running end-to-end tests can present challenges, such as the long duration of the running process, the flakiness of some tests and the challenges of fixing failures in CI when running tests in headless mode. Our teams have had very good experiences with Cypress by solving common issues such as lack of performance and long wait time for responses and resources to load. Cypress has become the tool of choice for end-to-end testing within our teams. | Running end-to-end tests can present challenges, such as the long duration of the running process, the flakiness of some tests and the challenges of fixing failures in CI when running tests in headless mode. Our teams have had very good experiences with Cypress by solving common issues such as lack of performance and long wait time for responses and resources to load. Cypress is a useful tool that helps developers build end-to-end tests and records all test steps as a video in an MP4 file to make it easier to identify errors. | Fixing end-to-end test failures in CI can be a painful experience, especially in headless mode. Cypress is a useful tool that helps developers build end-to-end tests easily and records all test steps as a video in an MP4 file. Instead of reproducing the issue in headless mode, developers can watch the testing video in order to fix it. Cypress is a powerful platform, not only a testing framework. Currently, we've integrated its CLI with headless CI in our projects. | Fixing end-to-end test failures in CI can be a painful experience, especially in headless mode. Cypress is a useful tool that helps developers build end-to-end tests easily and records all test steps as a video in an MP4 file. Instead of reproducing the issue in headless mode, developers can watch the testing video in order to fix it. Cypress is a powerful platform, not only a testing framework. Currently, we've integrated its CLI with headless CI in our projects.","blip_selector":"cypress","name":"Cypress","display_name":"Cypress","url":"/radar/tools/cypress","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005037,"quadrant":"platforms","volume_date":"2020-05","description":"Ultraleap (previously Leap Motion) has been a leader in the XR space for some time, creating remarkable hand-tracking hardware that allows a user's hands to make the leap into virtual reality. Stratos is Ultraleap's underlying haptics, sensors and software platform, and it can use targeted ultrasound to create haptic feedback in mid-air. A use case is responding to a driver's hand gesture to change the air conditioning in the car and providing haptic feedback as part of the interface. We're excited to see this technology and what creative technologists might do to incorporate it into their use cases.","blip_selector":"stratos","name":"Stratos","display_name":"Stratos","url":"/radar/platforms/stratos","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005108,"quadrant":"tools","volume_date":"2020-05","description":"A few years ago, Docker — and containers in general — radically changed how we think about packaging, deploying and running our applications. But despite this improvement in production, developers still spend a lot of time setting up development environments and regularly run into \"but it works on my machine\" style problems. Dojo aims to fix this by creating standard development environments, versioned and released as Docker images. Several of our teams use Dojo to streamline developing, testing and building code from local development through production pipelines.","blip_selector":"dojo","name":"Dojo","display_name":"Dojo","url":"/radar/tools/dojo","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005044,"quadrant":"tools","volume_date":"2020-05","description":"As mentioned in our description of Crowdin, you now have a choice of platforms to manage the translation of a product into multiple languages instead of emailing large spreadsheets. Our teams report positive experiences with Phrase, emphasizing that it's easy to use for all key user groups. Translators use a convenient browser-based UI. Managers can add new fields and synchronize translations with other teams in the same UI. Developers can access Phrase locally and from a build pipeline. A feature that deserves a specific mention is the ability to apply versioning to translations through tags, which makes it possible to compare the look of different translations inside the actual product.","blip_selector":"phrase","name":"Phrase","display_name":"Phrase","url":"/radar/tools/phrase","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005024,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"There are still some tool gaps when applying good software engineering practices in data engineering. Attempting to automate data quality checks between different steps in a data pipeline, one of our teams was surprised when they found only a few tools in this space. They settled on Deequ, a library for writing tests that resemble unit tests for data sets. Deequ is built on top of Apache Spark, and even though it's published by AWS Labs it can be used in environments other than AWS.","blip_selector":"deequ","name":"Deequ","display_name":"Deequ","url":"/radar/languages-and-frameworks/deequ","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201904035,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"The JavaScript world moves pretty fast, and as we gain more experience using a framework our recommendations change. The React Testing Library is a good example of a framework that with deeper usage has eclipsed the alternatives to become the sensible default when testing React-based frontends. Our teams like the fact that tests written with this framework are less brittle than with alternative frameworks such as Enzyme, because you're encouraged to test component relationships individually as opposed to testing all implementation details. This mindset is brought by Testing Library which React Testing Library is part of and which provides a whole family of libraries for Angular and Vue.js, for example. | The JavaScript world moves pretty fast, and as we gain more experience using a framework our recommendations change. The React Testing Library is a good example of a framework that with deeper usage has eclipsed the alternatives to become the sensible default when testing React-based frontends. Our teams like the fact that tests written with this framework are less brittle than with alternative frameworks such as Enzyme because you're encouraged to test component relationships individually as opposed to testing all implementation details. | As the pace of change in JavaScript frameworks has slowed, our teams have more time to work with specific frameworks and are gaining deeper insights as a result. With React and the dominant testing framework, Enzyme, we've observed a worrying trend of unit tests becoming tightly coupled to implementation details without providing — because the focus is on shallow details — much confidence that features work as expected. These unit tests make evolving the design difficult and they shift too much responsibility up the test pyramid to functional testing. This has made us revisit the idea of subcutaneous testing. Additionally, because of its design, Enzyme has issues trying to keep up with React's development. All this has pushed us toward assessing react-testing-library as a new framework for testing React applications.","blip_selector":"react-testing-library","name":"React Testing Library","display_name":"React Testing Library","url":"/radar/languages-and-frameworks/react-testing-library","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1037,"quadrant":"techniques","volume_date":"2020-05","description":"The pipelines as code technique emphasizes that the configuration of delivery pipelines that build, test and deploy our applications or infrastructure should be treated as code; they should be placed under source control and modularized in reusable components with automated testing and deployment. As organizations move to decentralized autonomous teams building microservices or micro frontends, the need for engineering practices in managing pipelines as code increases to keep building and deploying software consistent within the organization. This need has given rise to delivery pipeline templates and tooling that enable a standardized way to build and deploy services and applications. Such tools use the declarative delivery pipelines of applications, adopting a pipeline blueprint to execute the underlying tasks for various stages of a delivery lifecycle such as build, test and deployment; and they abstract away implementation details. The ability to build, test and deploy pipelines as code should be one of the evaluation criteria for choosing a CI/CD tool. | Teams are pushing for automation across their environments(testing), including their development infrastructure. Pipelines as code is defining the deployment pipeline through code instead of configuring a running CI/CD tool. LambdaCD, Drone, GoCD and Concourse are examples that allow usage of this technique. Also, configuration automation tools for CI/CD systems like GoMatic can be used to treat the deployment pipeline as code—versioned and tested. | Teams are pushing for automation across their environments, including their development infrastructure. Pipelines as code is defining the deployment pipeline through code instead of configuring a running CI/CD tool. LambdaCD, Drone, GoCD and Concourse are examples that allow usage of this technique. Also, configuration automation tools for CI/CD systems like GoMatic can be used to treat the deployment pipeline as code—versioned and tested.","blip_selector":"pipelines-as-code","name":"Pipelines as code","display_name":"Pipelines as code","url":"/radar/techniques/pipelines-as-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005103,"quadrant":"tools","volume_date":"2020-05","description":"Building web applications that look just as intended on a large number of devices and screen sizes can be cumbersome. Sizzy is a SaaS solution that shows many viewports in a single browser window. The application is rendered in all viewports simultaneously and interactions with the application are also synched across the viewports. In our experience interacting with an application in this way can make it easier to spot potential issues earlier, before a visual regression testing tool flags the issue in the build pipeline. We should mention, though, that some of our developers who tried Sizzy for a while did, on balance, prefer to work with the tooling provided by Chrome.","blip_selector":"sizzy","name":"Sizzy","display_name":"Sizzy","url":"/radar/tools/sizzy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":866,"quadrant":"platforms","volume_date":"2020-05","description":"We previously had .NET Core in Adopt, indicating that it had become our default for .NET projects. But we felt it's worth again calling attention to .NET Core. With the release of .NET Core 3.x last year, the bulk of the features from .NET Framework have now been ported into .NET Core. With the announcement that .NET Framework is on its last release, Microsoft have reinforced the view that .NET Core is the future of .NET. Microsoft has done a lot of work to make .NET Core container friendly. Most of our .NET Core–based projects target Linux and are often deployed as containers. The upcoming .NET 5 release looks promising, and we're looking forward to it. | Our teams have confirmed that .NET Core has reached a level of maturity that makes it the default for .NET server applications. The open source .NET Core framework enables the development and deployment of .NET applications on Windows, macOS and Linux with first-class cross-platform tooling. Microsoft provides blessed Docker images which make it easy to deploy .NET Core applications in a containerized environment. Positive directions in the community and feedback from our projects indicate that .NET Core is the future for .NET development. | We're seeing increased adoption of .NET Core, the open source cross-platform software framework. .NET Core enables the development and deployment of .NET applications on Windows, macOS and Linux. With the release of .NET Standard 2.0 increasing the number of standard APIs across .NET platforms, the migration path to .NET Core has become clearer. Issues related to library support on .NET Core are becoming less problematic, and first-class cross-platform tooling is now available, allowing for productive development on non-Windows platforms. Blessed Docker images are provided to make it easy to integrate .NET Core services into a containerized environment. Positive directions in the community and feedback from our projects indicate that .NET Core is ready for widespread use. | .NET Core is an open source modular product for creating applications that can be easily deployed in Windows, macOS and Linux. .NET Core makes it possible to build cross-platform web applications using ASP.NET Core with a set of tools, libraries and frameworks—another choice for microservices architecture. The community around .NET Core and other related projects has been growing. New tools have appeared and evolved quickly, such as Visual Studio Code. There are Docker images based on both Linux and Windows (Nano Server) with .NET Core that simplify applying a microservice architecture. CoreCLR and CoreFX appeared in the Radar in the past. However, a few months ago Microsoft announced the release of .NET Core 1.0, the first stable version. We see good new opportunities, changes and a vibrant community as reasons to keep assessing this product. | .NET Core is an open source modular product for creating applications that can be easily deployed in Windows, macOS and Linux. .NET Core makes it possible to build cross-platform web applications using ASP.NET Core with a set of tools, libraries and frameworks—another choice for microservices architecture. The community around .NET Core and other related projects has been growing. New tools have appeared and evolved quickly, such as Visual Studio Code. There are Docker images based on both Linux and Windows (Nano Server) with .NET Core that simplify applying a microservice architecture. CoreCLR and CoreFX appeared in the Radar in the past. However, a few months ago Microsoft announced the release of .NET Core 1.0, the first stable version. We see good new opportunities, changes and a vibrant community as reasons to keep assessing this product. | CoreCLR and** CoreFX** is the core platform and framework for .NET. Although not new, they have recently been open sourced by Microsoft. A key change is that these dependencies are bin-deployable, they do not need to be installed on a machine in advance.  This eases side-by-side deployments, allowing applications to use different framework versions without conflicts. Something written in .NET is then an implementation detail, you can install a .NET dependency into any environment. A .NET tool is no different than something written in C from an external dependency perspective, making it a much more attractive option for general purpose applications and utilities. CoreFX is also being factored into individual NuGet dependencies, so that applications can pull what they need, keeping the footprint for .NET applications and libraries small and making it easier to replace part of the framework.","blip_selector":"net-core","name":".NET Core","display_name":".NET Core","url":"/radar/platforms/net-core","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1095,"quadrant":"platforms","volume_date":"2020-05","description":"OpenTelemetry is an open source observability project that merges OpenTracing and OpenCensus. The OpenTelemetry project includes specification, libraries, agents, and other components needed to capture telemetry from services to better observe, manage and debug them. It covers the three pillars of observability — distributed tracing, metrics and logging (currently in beta) — and its specification connects these three pieces through correlations; thus you can use metrics to pinpoint a problem, locate the corresponding traces to discover where the problem occured, and ultimately study the corresponding logs to find the exact root cause. OpenTelemetry components can be connected to back-end observability systems such as Prometheus and Jaeger among others. Formation of OpenTracing is a positive step toward the convergence of standardization and the simplification of tooling. | As monolithic applications are being replaced with more complex (micro)service ecosystems, tracing requests across multiple services is becoming the norm. With majority contribution from LightStep and Uber OpenTracing is rapidly becoming the de facto standard for distributed tracing. There is a growing number of tracers supporting OpenTracing standard, including Zipkin, Instana, and Jaeger. OpenTracing currently provides vendor-neutral implementation in multiple languages including: Go, JavaScript, Java, Python, Objective-C, C#, C++, Ruby and PHP. | As monolithic applications are being replaced with more complex (micro)service ecosystems, tracing requests across multiple services is becoming the norm. With majority contribution from LightStep and Uber OpenTracing is rapidly becoming the de facto standard for distributed tracing. There is a growing number of tracers supporting OpenTracing standard, including Zipkin, Instana, and Jaeger. OpenTracing currently provides vendor-neutral implementation in multiple languages including: Go, JavaScript, Java, Python, Objective-C, C#, C++, Ruby and PHP.","blip_selector":"opentelemetry","name":"OpenTelemetry","display_name":"OpenTelemetry","url":"/radar/platforms/opentelemetry","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005033,"quadrant":"platforms","volume_date":"2020-05","description":"The performance of blockchain technology has been greatly improved since we initially assessed this area in the Radar. However, there's still no single blockchain that could achieve \"internet-level\" throughput. As various blockchain platforms develop, we're seeing new data and value silos. That's why cross-chain tech has always been a key topic in the blockchain community: the future of blockchain may be a network of independent parallel blockchains. This is also the vision of Cosmos. Cosmos releases Tendermint and CosmosSDK to let developers customize independent blockchains. These parallel blockchains could exchange value through the Inter-Blockchain Communication (IBC) protocol and Peg-Zones. Our teams have had great experiences with CosmosSDK, and the IBC protocol is maturing. This architecture could solve blockchain interoperability and scalability issues.","blip_selector":"cosmos","name":"Cosmos","display_name":"Cosmos","url":"/radar/platforms/cosmos","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005076,"quadrant":"platforms","volume_date":"2020-05","description":"Matomo (formerly Piwik) is an open source web analytics platform that provides you with full control over your data. You can self-host Matomo and secure your web analytics data from third parties. Matomo also makes it easy to integrate web analytics data with your in-house data platform and lets you build usage models that are tailored to your needs.","blip_selector":"matomo","name":"Matomo","display_name":"Matomo","url":"/radar/platforms/matomo","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005032,"quadrant":"platforms","volume_date":"2020-05","description":"Apache Pulsar is an open source pub-sub messaging/streaming platform, competing in a similar space with Apache Kafka. It provides expected functionality — such as low-latency async and sync message delivery and scalable persistent storage of messages — as well as various client libraries. What has excited us to evaluate Pulsar is its ease of scalability, particularly in large organizations with multiple segments of users. Pulsar natively supports multitenancy, georeplication, role-based access control and segregation of billing. We're also looking to Pulsar to solve the problem of a never-ending log of messages for our large-scale data systems where events are expected to persist indefinitely and subscribers are able to start consuming messages retrospectively. This is supported through a tiered storage model. Although Pulsar is a promising platform for large organizations, there is room for improvement. Its current installation requires administering ZooKeeper and BookKeeper among other pieces of technology. We hope that with its growing adoption, users can soon count on wider community support.","blip_selector":"apache-pulsar","name":"Apache Pulsar","display_name":"Apache Pulsar","url":"/radar/platforms/apache-pulsar","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005025,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"The Golang community has had its fair share of dependency injection skeptics, partly because they confused the pattern with specific frameworks, and developers with a system-programming background naturally dislike runtime overhead caused by reflection. Then along came Wire, a compile-time dependency injection tool that can generate code and wire components together. Wire has no additional runtime overhead, and the static dependency graph is easier to reason about. Whether you handwrite your code or use frameworks, we recommend using dependency injection to encourage modular and testable designs.","blip_selector":"wire","name":"Wire","display_name":"Wire","url":"/radar/languages-and-frameworks/wire","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005065,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"MediaPipe is a framework for building MultiModal (such as video, audio, time series data, etc.), cross-platform (for example, Android, iOS, Web, and edge devices) and applied ML pipelines. It provides multiple capabilities, including face detection, hand tracking, gesture detection and object detection. Although MediaPipe is primarily deployed to mobile devices, it's started to show up in the browser thanks to WebAssembly and XNNPack ML Inference Library. We're exploring MediaPipe for some AR use cases and like what we see so far.","blip_selector":"mediapipe","name":"MediaPipe","display_name":"MediaPipe","url":"/radar/languages-and-frameworks/mediapipe","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1047,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"We don't always move deprecated tools to Hold in the Radar, but our teams feel strongly that Enzyme has been replaced for unit testing React UI components by React Testing Library. Teams using Enzyme have found that its focus on testing component internals leads to brittle, unmaintainable tests. | We don't always move deprecated tools to Hold in the Radar, but our teams feel strongly that Enzyme has been replaced for unit testing React UI components by React Testing Library. Teams using Enzyme have found that its focus on testing component internals leads to brittle, unmaintainable tests. | Enzyme has become the defacto standard for unit testing React UI components. Unlike many other snapshot-based testing utilities, Enzyme enables you to test without doing on-device rendering, which results in faster and more granular testing. This is a contributing factor in our ability to massively reduce the amount of functional testing we find we have to do in React applications. In many of our projects it’s used within a unit testing framework such as Jest. | We've been enjoying the rapid component-level UI testing that Enzyme provides for React.js applications. Unlike many other snapshot-based testing frameworks, Enzyme allows you to test without doing on-device rendering, which results in faster and more granular testing. This is a contributing factor in our ability to massively reduce the amount of functional testing we find we have to do in React applications. | We’ve been enjoying the rapid component-level UI testing that Enzyme provides for React.js applications. Unlike many other snapshot-based testing frameworks, Enzyme allows you to test without doing on-device rendering, which results in faster and more granular testing. This is a contributing factor in our ability to massively reduce the amount of functional testing we find we have to do in React applications.","blip_selector":"enzyme","name":"Enzyme","display_name":"Enzyme","url":"/radar/languages-and-frameworks/enzyme","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005058,"quadrant":"tools","volume_date":"2020-05","description":"We continue to be ardent supporters of infrastructure as code, and we continue to believe that a robust monitoring solution is a prerequisite for operating distributed applications. Sometimes an interactive tool such as the AWS web console can be a useful addition. It allows us to explore all kinds of resources in an ad-hoc fashion without having to remember every single obscure command. Using an interactive tool to make manual modifications on the fly is still a questionable practice, though. For Kubernetes we now have k9s, which provides an interactive interface for basically everything that kubectl can do. And to boot, it's not a web application but runs inside a terminal window, evoking fond memories of Midnight Commander for some of us.","blip_selector":"k9s","name":"k9s","display_name":"k9s","url":"/radar/tools/k9s","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911003,"quadrant":"platforms","volume_date":"2020-05","description":"Most of the projects with multilingual support start with development teams building features in one language and managing the rest through offline translation via emails and spreadsheets. Although this simple setup works, things can quickly get out of hand. You may have to keep answering the same questions for different language translators, sucking the energy out of the collaboration between translators, proofreaders and the development team. Crowdin is one of a handful of platforms that help in streamlining the localization workflow of your project. With Crowdin the development team can continue building features, while the platform streamlines the text that needs translation into an online workflow. We like that Crowdin nudges the teams to continuously and incrementally incorporate translations rather than managing them in large batches toward the end. | Most of the projects with multilingual support start with development teams building features in one language and managing the rest through offline translation via emails and spreadsheets. Although this simple setup works, things can quickly get out of hand. You may have to keep answering the same questions for different language translators, sucking the energy out of the collaboration between translators, proofreaders and the development team. Crowdin is one of a handful of platforms that help in streamlining the localization workflow of your project. With Crowdin the development team can continue building features and the platform streamlines the text that needs translation into an online workflow. We like that Crowdin nudges the teams to continuously and incrementally incorporate translation rather than managing them in large batches toward the end.","blip_selector":"crowdin","name":"Crowdin","display_name":"Crowdin","url":"/radar/platforms/crowdin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005015,"quadrant":"platforms","volume_date":"2020-05","description":"Google's Firebase has undergone significant evolution since we mentioned it as part of a serverless architecture in 2016. Firebase is a comprehensive platform for building mobile and web apps in a way that's supported by Google's underlying scalable infrastructure. We particularly like Firebase App Distribution, which makes it easy to publish test versions of an app via a CD pipeline, and Firebase Remote Config, which allows configuration changes to be dynamically pushed to apps without needing to republish them.","blip_selector":"firebase","name":"Firebase","display_name":"Firebase","url":"/radar/platforms/firebase","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005109,"quadrant":"tools","volume_date":"2020-05","description":"We mentioned Goss, a tool for provisioning testing, in passing in previous Radars, for example, when describing the technique of TDD'ing containers. Although Goss isn't always an alternative to Serverspec, simply because it doesn't offer the same amount of features, you may want to consider it when its features meet your needs, especially since it comes as a small, self-contained binary (rather than requiring a Ruby environment). A common anti-pattern with using tools such as Goss is double-entry bookkeeping, where each change in the actual infrastructure as code files requires a corresponding change in the test assertions. Such tests are maintenance heavy and because of the close correspondence between code and test, failures mostly occur when an engineer updates one side and forgets the other. And these tests rarely catch genuine problems.","blip_selector":"goss","name":"Goss","display_name":"Goss","url":"/radar/tools/goss","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1035,"quadrant":"techniques","volume_date":"2020-05","description":"We've seen significant benefits from introducing microservices, which have allowed teams to scale the delivery of independently deployed and maintained services. Unfortunately, we've also seen many teams create a front-end monolith — a large, entangled browser application that sits on top of the back-end services — largely neutralizing the benefits of microservices. Micro frontends have continued to gain in popularity since they were first introduced. We've seen many teams adopt some form of this architecture as a way to manage the complexity of multiple developers and teams contributing to the same user experience. In June of last year, one of the originators of this technique published an introductory article that serves as a reference for micro frontends. It shows how this style can be implemented using various web programming mechanisms and builds out an example application using React.js. We're confident this style will grow in popularity as larger organizations try to decompose UI development across multiple teams. | We've seen significant benefits from introducing microservices, which have allowed teams to scale the delivery of independently deployed and maintained services. Unfortunately, we've also seen many teams create a front-end monolith — a large, entangled browser application that sits on top of the back-end services — largely neutralizing the benefits of microservices. Micro frontends have continued to gain in popularity since they were first introduced. We've seen many teams adopt some form of this architecture as a way to manage the complexity of multiple developers and teams contributing to the same user experience. In June of this year, one of the originators of this technique published an introductory article that serves as a reference for micro frontends. It shows how this style can be implemented using various web programming mechanisms and builds out an example application using React.js. We're confident this style will grow in popularity as larger organizations try to decompose UI development across multiple teams. | We've seen significant benefits from introducing microservices, which have allowed teams to scale the delivery of independently deployed and maintained services. Unfortunately, we've also seen many teams create a frontend monolith — a large, entangled browser application that sits on top of the backend services — largely neutralizing the benefits of microservices. Since we first described micro frontends as a technique to address this issue, we've had almost universally positive experiences with the approach and have found a number of patterns to use micro frontends even as more and more code shifts from the server to the web browser. So far, web components have been elusive in this field, though. | We've seen significant benefits from introducing microservices architectures, which have allowed teams to scale the delivery of independently deployed and maintained services. Unfortunately, we've also seen many teams create front-end monoliths — a single, large and sprawling browser application — on top of their back-end services. Our preferred (and proven) approach is to split the browser-based code into micro frontends. In this approach, the web application is broken down into its features, and each feature is owned, frontend to backend, by a different team. This ensures that every feature is developed, tested and deployed independently from other features. Multiple techniques exist to recombine the features — sometimes as pages, sometimes as components — into a cohesive user experience. | We've seen significant benefits from introducing microservices architectures, which have allowed teams to scale the delivery of independently deployed and maintained services. Unfortunately, we've also seen many teams create front-end monoliths — a single, large and sprawling browser application — on top of their back-end services. Our preferred (and proven) approach is to split the browser-based code into micro frontends. In this approach, the web application is broken down into its features, and each feature is owned, frontend to backend, by a different team. This ensures that every feature is developed, tested and deployed independently from other features. Multiple techniques exist to recombine the features — sometimes as pages, sometimes as components — into a cohesive user experience. | We've seen significant benefit from introducing microservice architectures, which have allowed teams to scale delivery of independently deployed and maintained services. However, teams have often struggled to avoid the creation of front-end monoliths—large and sprawling browser applications that are as difficult to maintain and evolve as the monolithic server-side applications we've abandoned. We're seeing an approach emerge that our teams call micro frontends. In this approach, a web application is broken up by its pages and features, with each feature being owned end-to-end by a single team. Multiple techniques exist to bring the application features—some old and some new—together as a cohesive user experience, but the goal remains to allow each feature to be developed, tested and deployed independently from others. The BFF - backend for frontends approach works well here, with each team developing a BFF to support its set of application features. | We've seen significant benefit from introducing microservice architectures, which have allowed teams to scale delivery of independently deployed and maintained services. However, teams have often struggled to avoid the creation of front-end monoliths—large and sprawling browser applications that are as difficult to maintain and evolve as the monolithic server-side applications we've abandoned. We're seeing an approach emerge that our teams call micro frontends. In this approach, a web application is broken up by its pages and features, with each feature being owned end-to-end by a single team. Multiple techniques exist to bring the application features—some old and some new—together as a cohesive user experience, but the goal remains to allow each feature to be developed, tested and deployed independently from others. The BFF - backend for frontends approach works well here, with each team developing a BFF to support its set of application features.","blip_selector":"micro-frontends","name":"Micro frontends","display_name":"Micro frontends","url":"/radar/techniques/micro-frontends","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911052,"quadrant":"techniques","volume_date":"2020-05","description":"Semi-supervised learning loops are a class of iterative machine-learning workflows that take advantage of the relationships to be found in unlabeled data. These techniques may improve models by combining labeled and unlabeled data sets in various ways. In other cases they compare models trained on different subsets of the data. Unlike either unsupervised learning where a machine infers classes in unlabeled data or supervised techniques where the training set is entirely labeled, semi-supervised techniques take advantage of a small set of labeled data and a much larger set of unlabeled data. Semi-supervised learning is also closely related to active learning techniques where a human is directed to selectively label ambiguous data points. Since expert humans that can accurately label data are a scarce resource and labeling is often the most time-consuming activity in the machine-learning workflow, semi-supervised techniques lower the cost of training and make machine learning feasible for a new class of users. We're also seeing the application of weakly supervised techniques where machine-labeled data is used but is trusted less than the data labeled by humans. | Semi-supervised learning loops are a class of iterative machine-learning workflows that take advantage of the relationships to be found in unlabeled data. These techniques may improve models by combining labeled and unlabeled data sets in various ways. In other cases they compare models trained on different subsets of the data. Unlike either unsupervised learning where a machine infers classes in unlabeled data or supervised techniques where the training set is entirely labeled, semi-supervised techniques take advantage of a small set of labeled data and a much larger set of unlabeled data. Semi-supervised learning is also closely related to active learning techniques where a human is directed to selectively label ambiguous data points. Since expert humans that can accurately label data are a scarce resource and labeling is often the most time-consuming activity in the machine-learning workflow, semi-supervised techniques lower the cost of training and make machine learning feasible for a new class of users.","blip_selector":"semi-supervised-learning-loops","name":"Semi-supervised learning loops","display_name":"Semi-supervised learning loops","url":"/radar/techniques/semi-supervised-learning-loops","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005066,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"If you need to ingest data from relational databases into a Kafka topic, consider Tamer, which labels itself \"a domesticated JDBC source connector for Kafka.\" Despite being a relatively new framework, we've found Tamer to be more efficient than the Kafka JDBC connector, especially when huge amounts of data are involved.","blip_selector":"tamer","name":"Tamer","display_name":"Tamer","url":"/radar/languages-and-frameworks/tamer","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":717,"quadrant":"tools","volume_date":"2020-05","description":"Since we first mentioned visual regression testing tools in 2014, the use of the technique has spread and the tools landscape has evolved. BackstopJS remains an excellent choice with new features being added regularly, including support for running inside Docker containers. Loki was featured in our previous Radar. Applitools, CrossBrowserTesting and Percy are SaaS solutions. Another notable mention is Resemble.js, an image diffing library. Although most teams use it indirectly as part of BackstopJS, some of our teams have been using it to analyze and compare images of web pages directly. In general, our experience shows that visual regression tools are less useful in the early stages when the interface goes through significant changes, but they certainly prove their worth as the product matures and the interface stabilizes. | Growing complexity in web applications has increased the awareness that appearance should be tested in addition to functionality. This has given rise to a variety of visual regression testing tools, including CSS Critic, dpxdt, Huxley, PhantomCSS, and Wraith. Techniques range from straightforward assertions of CSS values to actual screenshot comparison. While this is a field still in active development we believe that testing for visual regressions should be added to Continuous Delivery pipelines.","blip_selector":"visual-regression-testing-tools","name":"Visual regression testing tools","display_name":"Visual regression testing tools","url":"/radar/tools/visual-regression-testing-tools","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005068,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"Through their extended use of Kotlin, our development teams have gained experience with more frameworks designed specifically for Kotlin rather than using Java frameworks with Kotlin. Although it's been around for a while, Exposed has caught our attention as a lightweight object-relational mapper (ORM). Exposed has two flavors of database access: a typesafe internal DSL wrapping SQL and an implementation of the data access object (DAO) pattern. It supports features expected from a mature ORM such as handling of many-to-many references, eager loading, and support for joins across entities. We also like that the implementation works without proxies and doesn't rely on reflection, which is certainly beneficial to performance.","blip_selector":"exposed","name":"Exposed","display_name":"Exposed","url":"/radar/languages-and-frameworks/exposed","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1199,"quadrant":"platforms","volume_date":"2020-05","description":"If you're building and operating a scaled microservices architecture and have embraced Kubernetes, adopting service mesh to manage all cross-cutting aspects of running the architecture is a default position. Among various implementations of service mesh, Istio has gained majority adoption. It has a rich feature set, including service discovery, traffic management, service-to-service and origin-to-service security, observability (including telemetry and distributed tracing), rolling releases and resiliency. Its user experience has been improved in its latest releases, because of its ease of installation and control panel architecture. Istio has lowered the bar for implementing large-scale microservices with operational quality for many of our clients, while admitting that operating your own Istio and Kubernetes instances requires adequate knowledge and internal resources which is not for the fainthearted. | Istio is becoming the de facto infrastructure to operationalize a microservices ecosystem. Its out-of-the-box implementation of cross-cutting concerns — such as service discovery, service-to-service and origin-to-service security, observability (including telemetry and distributed tracing), rolling releases and resiliency — has been bootstrapping our microservices implementations very quickly. It's the main implementation of the service mesh technique we've been using. We've been enjoying its monthly releases and its continuous improvements with seamless upgrades. We use Istio to bootstrap our projects, starting with observability (tracing and telemetry) and service-to-service security. We're closely watching its improvements to service-to-service authentication everywhere in and outside of the mesh. We'd also like to see Istio establish best practices for configuration files to strike a balance between giving autonomy to service developers and control to the service mesh operators. | When building and operating a microservices ecosystem, one of the early questions to answer is how to implement cross-cutting concerns such as service discovery, service-to-service and origin-to-service security, observability (including telemetry and distributed tracing), rolling releases and resiliency. Over the last couple of years, our default answer to this question has been using a service mesh technique. A service mesh offers the implementation of these cross-cutting capabilities as an infrastructure layer that is configured as code. The policy configurations can be consistently applied to the whole ecosystem of microservices; enforced on both in and out of mesh traffic (via the mesh proxy as a gateway) as well as on the traffic at each service (via the same mesh proxy as a sidecar container). While we're keeping a close eye on the progress of different open source service mesh projects such as Linkerd, we've successfully used Istio in production with a surprisingly easy-to-configure operating model.","blip_selector":"istio","name":"Istio","display_name":"Istio","url":"/radar/platforms/istio","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1055,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"Vue.js has become one of the successfully applied, loved and trusted frontend JavaScript frameworks among our community. Although there are other, well-adopted alternatives, such as React.js, the simplicity of Vue.js in API design, its clear segregation of directives and components (one file per component idiom) and its simpler state management have made it a compelling option among others. | In the ever-changing world of front-end JavaScript frameworks, one of the emerging favorites appears to be Vue.js. Vue.js is a lightweight alternative to AngularJS. It is designed to be a very flexible—and a less opinionated—library that offers a set of tools for building interactive web interfaces around concepts such as modularity, components and reactive data flow. It has a low learning curve, which makes it interesting for less experienced developers and beginners. Note, though, that Vue.js is not a full-blown framework; it is focused on the view layer only and therefore is easy to integrate with other libraries or existing projects. | In the ever-changing world of front-end JavaScript frameworks, Vue.js has gained a lot of ground as a lightweight alternative to AngularJS. It is designed to be a very flexible—and a less opinionated—library that offers a set of tools for building interactive web interfaces around concepts like modularity, components and reactive data flow. It has a low learning barrier, which makes it interesting for junior developers and beginners. Vue.js itself is not a full-blown framework; it is focused on the view layer only and therefore is easy to integrate with other libraries or existing projects.","blip_selector":"vue-js","name":"Vue.js","display_name":"Vue.js","url":"/radar/languages-and-frameworks/vue-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005075,"quadrant":"platforms","volume_date":"2020-05","description":"Marquez is a relatively young open source project for collecting and serving metadata information about a data ecosystem. It represents a simple data model to capture metadata such as lineage, upstream and downstream data processing jobs and their status, and a flexible set of tags to capture the attributes of data sets. It provides a simple RESTful API to manage the metadata which eases the integration of Marquez to other tool sets within the data ecosystem.\n\nWe've used Marquez as a starting point and easily extended it to fit our needs such as enforcing security policies as well as changes to its domain language. If you're looking for a small and simple tool to bootstrap storage and visualization of your data-processing jobs and data sets, Marquez is a good place to start.","blip_selector":"marquez","name":"Marquez","display_name":"Marquez","url":"/radar/platforms/marquez","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1186,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"Since we first mentioned CSS-in-JS as an emerging technique in 2017, it has become much more popular, a trend we also see in our work. With some solid production experience under our belts, we can now recommend CSS-in-JS as a technique to trial. A good starting point is the styled components framework, which we mentioned in our previous Radar. Next to all the positives, though, there usually is a downside when using CSS-in-JS: the calculation of styles at runtime can cause a noticeable lag for end users. With Linaria we're now seeing a new class of frameworks that were created with this issue in mind. Linaria employs a number of techniques to shift most of the performance overhead to build time. Alas, this does come with its own set of trade-offs, most notably a lack of dynamic style support in IE11. | CSS in JS is a technique of writing CSS styling in the JavaScript programming language. This encourages a common pattern of writing the styling with the JavaScript component it applies to, co-locating presentational and logical concerns. The new players — including JSS, emotion and styled-components — rely on the tooling to translate the CSS-in-JS code to separate CSS stylesheets, to make them suitable for browser consumption. This is the second-generation approach to writing CSS in JavaScript and unlike the previous approaches doesn’t rely on in-line styles. That means it provides the benefit of supporting all CSS features, sharing of CSS using the npm ecosystem and utilization of components across multiple platforms. Our teams have found styled-components working well with component-based frameworks, such as React, and unit testing of CSS with jest-styled-components. This space is new and rapidly changing; the approach requires some effort for manual debugging of the generated class names in the browser, and it may not apply to some projects where the front-end architecture does not support reusing components and requires global styling.","blip_selector":"css-in-js","name":"CSS-in-JS","display_name":"CSS-in-JS","url":"/radar/languages-and-frameworks/css-in-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005028,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"GraphQL Inspector lets you compare changes between two GraphQL schemas. We've cautioned against the use of GraphQL in the past, and we're happy to see some improvements in tooling around GraphQL since. Most of our teams continue to use GraphQL for server-side resource aggregation, and by integrating GraphQL Inspector in their CI pipelines, we've been able to catch potential breaking changes in the GraphQL schema.","blip_selector":"graphql-inspector","name":"GraphQL Inspector","display_name":"GraphQL Inspector","url":"/radar/languages-and-frameworks/graphql-inspector","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904047,"quadrant":"platforms","volume_date":"2020-05","description":"The GraphQL ecosystem and community keep growing. Hot Chocolate is a GraphQL server for .NET (Core and Classic). It lets you build and host schemas and then serve queries against them using the same base components of GraphQL — data loader, resolver, schema, operations and types. The team behind Hot Chocolate has recently added schema stitching, which allows for a single entry point to query across multiple schemas aggregated from different locations. Despite the potential to misuse this approach, our teams are happy with Hot Chocolate — it’s well documented, and we're able to deliver value quickly to our clients. | The GraphQL ecosystem and community keep growing. Hot Chocolate is a GraphQL server for .NET (core and classic). It lets you build and host schemas and then serve queries against them. The team behind Hot Chocolate has recently added schema stitching which allows for a single entry point to query across multiple schemas aggregated from different locations. Although there are plenty of ways to misuse this approach, it's worth assessing whether to add it to your toolkit.","blip_selector":"hot-chocolate","name":"Hot Chocolate","display_name":"Hot Chocolate","url":"/radar/platforms/hot-chocolate","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9142,"quadrant":"techniques","volume_date":"2020-05","description":"Although infrastructure as code is a relatively old technique (we’ve featured it in the Radar in 2011), it has become vitally important in the modern cloud era where the act of setting up infrastructure has become the passing of configuration instructions to a cloud platform. When we say \"as code\" we mean that all the good practices we've learned in the software world should be applied to infrastructure. Using source control, adhering to the DRY principle, modularization, maintainability, and using automated testing and deployment are all critical practices. Those of us with a deep software and infrastructure background need to empathize with and support colleagues who do not. Saying \"treat infrastructure like code\" isn't enough; we need to ensure the hard-won learnings from the software world are also applied consistently throughout the infrastructure realm. | We continue to highlight infrastructure as code. This technique treats infrastructure configuration in the same way as code; checking configuration into source control, then carefully pushing changes out to the data center. | The DevOps movement continues to grow, with developers and operations staff working closely together to solve the “software last mile” problem. Infrastructure as code is a technique for treating infrastructure configuration in the same way as code; checking it into source control, then using it to push changes out to the data center. In addition to web server, application server and application configuration, we are seeing network configuration treated in the same way. Network switch, firewall and load balancer configuration can be infrastructure as code, and even changed at runtime. | The large number of hosts and devices in a modern datacenter or cloud deployment have made manually installing and configuring infrastructure unwise. Infrastructure as code is an approach whereby infrastructure configuration is scripted or described by files that are stored in version control, and changes are pushed out to the datacenter in a controlled manner. This parallels the discipline of source control and build promotion used in software development, hence ‘as code’. The two front-running open source tools for infrastructure automation are Chef and Puppet. They both use a textual DSL to script automation. Using this approach provides consistent and repeatable environment changes, reducing the manual effort involved, especially in troubleshooting environmental differences.","blip_selector":"infrastructure-as-code","name":"Infrastructure as code","display_name":"Infrastructure as code","url":"/radar/techniques/infrastructure-as-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":202005001,"quadrant":"techniques","volume_date":"2020-05","description":"We firmly believe that pair programming improves the quality of code, spreads knowledge throughout a team and allows overall faster delivery of software. In a post COVID-19 world, however, many software teams will be distributed or fully remote, and in this situation we recommend pragmatic remote pairing: adjusting pairing practices to what's possible given the tools at hand. Consider tools such as Visual Studio Live Share for efficient, low-latency collaboration. Only resort to pixel-sharing if both participants reside in relative geographic proximity and have high-bandwidth internet connections. Pair developers who are in similar time zones rather than expecting pairing to work between participants regardless of their location. If pairing isn't working for logistical reasons, fall back to practices such as individual programming augmented via code reviews, pull-request collaboration (but beware long-lived branches with Gitflow) or shorter pairing sessions for critical parts of the code. We've engaged in remote pairing for years, and we've found it to be effective if done with a dose of pragmatism.","blip_selector":"pragmatic-remote-pairing","name":"Pragmatic remote pairing","display_name":"Pragmatic remote pairing","url":"/radar/techniques/pragmatic-remote-pairing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005064,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"With the aim of improving performance in our code, profiling tools are very useful to identify bottlenecks or delays in code which are hard to identify, especially in asynchronous operations. Clinic.js Bubbleprof represents visually the async operations in Node.js processes, drawing a map of delays in the application's flow. We like this tool because it helps developers to easily identify and prioritize what to improve in the code.","blip_selector":"clinic-js-bubbleprof","name":"Clinic.js Bubbleprof","display_name":"Clinic.js Bubbleprof","url":"/radar/languages-and-frameworks/clinic-js-bubbleprof","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911054,"quadrant":"techniques","volume_date":"2020-05","description":"Over the past year, we've seen a shift in interest around machine learning and deep neural networks in particular. Until now, tool and technique development has been driven by excitement over the remarkable capabilities of these models. Currently, though, there is rising concern that these models could cause unintentional harm. For example, a model could be trained inadvertently to make profitable credit decisions by simply excluding disadvantaged applicants. Fortunately, we're seeing a growing interest in ethical bias testing that will help to uncover potentially harmful decisions. Tools such as lime, AI Fairness 360 or What-If Tool can help uncover inaccuracies that result from underrepresented groups in training data and visualization tools such as Google Facets or Facets Dive can be used to discover subgroups within a corpus of training data. We've used lime (local interpretable model-agnostic explanations) in addition to this technique in order to understand the predictions of any machine-learning classifier and what classifiers (or models) are doing. | Over the past year, we've seen a shift in interest around machine learning and deep neural networks in particular. Until now, tool and technique development has been driven by excitement over the remarkable capabilities of these models. Currently though, there is rising concern that these models could cause unintentional harm. For example, a model could be trained to make profitable credit decisions by simply excluding disadvantaged applicants. Fortunately, we're seeing a growing interest in ethical bias testing that will help to uncover potentially harmful decisions. Tools such as lime, AI Fairness 360 or What-If can help uncover inaccuracies that result from underrepresented groups in training data and visualization tools such as Google Facets or Facets Dive can be used to discover subgroups within a corpus of training data. However, this is a developing field and we expect standards and practices specific to ethical bias testing to emerge over time.","blip_selector":"ethical-bias-testing","name":"Ethical bias testing","display_name":"Ethical bias testing","url":"/radar/techniques/ethical-bias-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904027,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"Given our experience that tests are the only API specifications that really matter, we're always on the lookout for new tools that might help with testing. Karate is an API testing framework whose unique feature is that tests are written in Gherkin-based syntax without relying on a general-purpose programming language to implement test behavior. Karate uses a domain-specific language for describing HTTP-based API tests. Our teams like the readable specification that they get with this tool and recommend to keep tests with Karate in the upper levels of the testing pyramid and not overload its use by making very detailed assertions. | Given our experience that tests are the only API specifications that really matter, we're always on the lookout for new tools that might help. Karate is an API testing framework whose unique feature is that tests are written directly in Gherkin without relying on a general-purpose programming language to implement test behavior. Karate is really a domain-specific language for describing HTTP-based API tests. Although this approach is interesting and makes for some very readable specifications for simple tests, the special-purpose language for matching and validating payloads can become quite syntax-heavy and difficult to understand. It remains to be seen if more complex tests written in this style will be readable and maintainable over the long haul.","blip_selector":"karate","name":"Karate","display_name":"Karate","url":"/radar/languages-and-frameworks/karate","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005046,"quadrant":"tools","volume_date":"2020-05","description":"Open standards are one of the foundational pillars of building distributed systems. For example, the OpenAPI (formerly Swagger) specification, as an industry standard to define RESTful APIs, has been instrumental to the success of distributed architectures such as microservices. It has enabled a proliferation of tooling to support building, testing and monitoring RESTful APIs. However, such standardizations have been largely missing in distributed systems for event-driven APIs.\n\nAsyncAPI is an open source initiative to create a much needed event-driven and asynchronous API standardization and development tooling. The AsyncAPI specification, inspired by the OpenAPI specification, describes and documents event-driven APIs in a machine-readable format. It's protocol agnostic, so it can be used for APIs that work over many protocols, including MQTT, WebSockets, and Kafka. We're eager to see the ongoing improvements of AsyncAPI and further maturity of its tooling ecosystem.","blip_selector":"asyncapi","name":"AsyncAPI","display_name":"AsyncAPI","url":"/radar/tools/asyncapi","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005051,"quadrant":"tools","volume_date":"2020-05","description":"Snowpack is an interesting new entrant in the field of JavaScript build tools. The key improvement over other solutions is that Snowpack makes it possible to build applications with modern frameworks such as React.js, Vue.js, and Angular without the need for a bundler. Cutting out the bundling step dramatically improves the feedback cycle during development because changes become available in the browser almost immediately. For this magic to work, Snowpack transforms the dependencies in node_modules into single JavaScript files in a new web_modules directory, from where they can be imported as an ECMAScript module (ESM). For IE11 and other browsers that don't support ESM, a workaround is available. Unfortunately, because no browser today can import CSS from JavaScript, using CSS modules is not straightforward.","blip_selector":"snowpack","name":"Snowpack","display_name":"Snowpack","url":"/radar/tools/snowpack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005038,"quadrant":"platforms","volume_date":"2020-05","description":"Trillian is a cryptographically verifiable, centralized data store. For trustless, decentralized environments, you can use blockchain-based distributed ledgers. For enterprise environments, however, where the cost of CPU-heavy consensus protocols is unwarranted, we recommend you give Trillian a try.","blip_selector":"trillian","name":"Trillian","display_name":"Trillian","url":"/radar/platforms/trillian","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005006,"quadrant":"techniques","volume_date":"2020-05","description":"Even though we strongly advocate in favor of CI rather than Gitflow, we know that committing straight to the trunk and running the CI on a master branch can be ineffective if the team is too big, the builds are slow or flaky, or the team lacks the discipline to run the full test suite locally. In this situation a red build can block multiple devs or pairs of devs. Instead of fixing the underlying root cause — slow builds, the inability to run tests locally or monolithic architectures that necessitate many people working in the same area — teams usually rely on feature branches to bypass these issues. We discourage feature branches, given they may require significant effort to resolve merge conflicts, and they introduce longer feedback loops and potential bugs during conflict resolution. Instead, we propose using preflight builds as an alternative: these are pull request–based builds for “micro branches” that live only for the duration of the pipeline run, with the branch opened for every commit. To help automate this workflow, we've come across bots such as Bors, which automates merging to master and branch deletion in case the mini branch build succeeds. We're assessing this flow, and you should too; but don't use this to solve the wrong problem, as it can lead to misuse of branches and may cause more harm than benefit.","blip_selector":"preflight-builds","name":"Preflight builds","display_name":"Preflight builds","url":"/radar/techniques/preflight-builds","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005030,"quadrant":"tools","volume_date":"2020-05","description":"MURAL describes itself as a \"digital workspace for visual collaboration\" and allows teams to interact with a shared workspace based on a whiteboard/sticky notes metaphor. Its features include voting, commenting, notes and \"follow the presenter.\" We particularly like the template feature that allows a facilitator to design and then reuse guided sessions with a team. Each of the major collaboration suites have a tool in this space (for example, Google Jamboard and Microsoft Whiteboard) and these are worth investigating, but we've found MURAL to be slick, effective and flexible.","blip_selector":"mural","name":"MURAL","display_name":"MURAL","url":"/radar/tools/mural","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005059,"quadrant":"tools","volume_date":"2020-05","description":"mkcert is a convenient tool for creating locally trusted development certificates. Using certificates from real certificate authorities (CAs) for local development can be challenging if not impossible (for hosts such as example.test, localhost or 127.0.0.1). In such situations self-signed certificates may be your only option. mkcert lets you generate self-signed certificates and installs the local CA in the system root store. For anything other than local development and testing, we strongly recommend using certificates from real CAs to avoid trust issues.","blip_selector":"mkcert","name":"mkcert","display_name":"mkcert","url":"/radar/tools/mkcert","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":702,"quadrant":"techniques","volume_date":"2020-05","description":"It is rather curious, that after over a decade of industry experience with cloud migration, we still feel it's necessary to call out cloud lift and shift; a practice that views cloud simply as a hosting solution, resulting in the replication of an existing architecture, security practices and IT operational models in the cloud. This fails to realize the cloud's promises of agility and digital innovation. A cloud migration requires intentional change across multiple axes toward a cloud-native state, and depending on the unique migration circumstances, each organization might end up somewhere on the spectrum from cloud lift and shift to cloud native. Systems architecture, for example, is one of the pillars of delivery agility and often requires change. The temptation to simply lift and shift existing systems as containers to the cloud can be strong. While this tactic can speed up cloud migration, it falls short when it comes to creating agility and delivering features and value. Enterprise security in the cloud is fundamentally different from traditional perimeter-based security through firewalls and zoning, and it demands a journey toward zero trust architecture. The IT operating model too has to be reformed to safely provide cloud services through self-serve automated platforms and empower teams to take more of the operational responsibility and gain autonomy. Last but not least, organizations must build a foundation to enable continuous change, such as creating pipelines with continuous testing of applications and infrastructure as a part of the migration. These will help the migration process, result in a more robust and well-factored system and give organizations a way to continue to evolve and improve their systems. | As more organizations are choosing to deploy applications in the cloud, we're regularly finding IT groups that are wastefully trying to replicate their existing data center management and security approaches in the cloud. This often comes in the form of firewalls, load balancers, network proxies, access control, security appliances and services that are extended into the cloud with minimal rethinking. We've seen organizations build their own orchestration APIs in front of the cloud providers to constrain the services that can be utilized by teams. In most cases these layers serve only to cripple the capability, taking away most of the intended benefits of moving to the cloud. In this edition of the Radar, we've chosen to rehighlight cloud lift and shift as a technique to avoid. Organizations should instead look more deeply at the intent of their existing security and operational controls, and look for alternative controls that work in the cloud without creating unnecessary constraints. Many of those controls will already exist for mature cloud providers, and teams that adopt the cloud can use native APIs for self-serve provisioning and operations. | As cloud adoption grows we are unfortunately seeing a trend to treat the cloud as just another hosting provider. Cloud lift and shift is unfortunately being encouraged by large vendors re-branding existing hosting offerings as \"cloud.\" Few of these offer any real flexibility or pay-as-you-use pricing. If you think you can move to the cloud without re-architecting, you are probably not doing it right. | As cloud adoption grows we are unfortunately seeing a trend to treat the cloud as just another hosting provider. Cloud lift and shift is unfortunately being encouraged by large vendors re-branding existing hosting offerings as \"cloud.\" Few of these offer any real flexibility or pay-as-you-use pricing. If you think you can move to the cloud without re-architecting, you are probably not doing it right.","blip_selector":"cloud-lift-and-shift","name":"Cloud lift and shift","display_name":"Cloud lift and shift","url":"/radar/techniques/cloud-lift-and-shift","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911069,"quadrant":"tools","volume_date":"2020-05","description":"Open Policy Agent (OPA) has rapidly become a favorable component of many distributed cloud-native solutions that we build for our clients. OPA provides a uniform framework and language for declaring, enforcing and controlling policies for various components of a cloud-native solution. It's a great example of a tool that implements security policy as code. We've had a smooth experience using OPA in multiple scenarios, including deploying resources to K8s clusters, enforcing access control across services in a service mesh and fine-grained security controls as code for accessing application resources. A recent commercial offering, Styra's Declarative Authorization Service (DAS), eases the adoption of OPA for enterprises by adding a management tool, or control plane, to OPA for K8s with a prebuilt policy library, impact analysis of the policies and logging capabilities. We look forward to maturity and extension of OPA beyond operational services to (big) data-centric solutions. | Defining and enforcing security policies uniformly across a diverse technology landscape is a challenge. Even for simple applications, you have to control access to their components — such as container orchestrators, services and data stores to keep the services' state — using their components' built-in security policy configuration and enforcement mechanisms.\n\nWe're excited about Open Policy Agent (OPA), an open-source technology that attempts to solve this problem. OPA lets you define fine-grained access control and flexible policies as code, using the Rego policy definition language. Rego enforces the policies in a distributed and unobtrusive manner outside of the application code. At the time of this writing, OPA implements uniform and flexible policy definition and enforcement to secure access to Kubernetes APIs, microservices APIs through Envoy sidecar and Kafka. It can also be used as a sidecar to any service to verify access policies or filter response data. Styra, the company behind OPA, provides commercial solutions for centralized visibility to distributed policies. We like to see OPA mature through the CNCF incubation program and continue to build support for more challenging policy enforcement scenarios such as diverse data stores.","blip_selector":"open-policy-agent-opa","name":"Open Policy Agent (OPA)","display_name":"Open Policy Agent (OPA)","url":"/radar/tools/open-policy-agent-opa","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":202005089,"quadrant":"techniques","volume_date":"2020-05","description":"The value of snapshot testing is undeniable when working with legacy systems by ensuring that the system continues to work and the legacy code doesn't break. However, we're seeing the common, rather harmful practice of using snapshot testing only as the primary test mechanism. Snapshot tests validate the exact result generated in the DOM by a component, not the component's behavior; therefore, it can be weak and unreliable, fostering the \"only delete the snapshot and regenerate it\" bad practice. Instead, you should test the logic and behavior of the components emulating what users would do. This mindset is encouraged by tools in the Testing Library family.","blip_selector":"snapshot-testing-only","name":"Snapshot testing only","display_name":"Snapshot testing only","url":"/radar/techniques/snapshot-testing-only","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911042,"quadrant":"tools","volume_date":"2020-05","description":"The day-to-day work of machine learning often boils down to a series of experiments in selecting a modeling approach and the network topology, training data and optimizing or tweaking the model. Data scientists must use experience and intuition to hypothesize changes and then measure the impact those changes have on the overall performance of the model. As this practice has matured, our teams have found an increasing need for experiment tracking tools for machine learning. These tools help investigators keep track of the experiments and work through them methodically. Although no clear winner has emerged, tools such as MLflow and platforms such as Comet or Neptune have introduced rigor and repeatability into the entire machine learning workflow. | The day-to-day work of machine learning often boils down to a series of experiments in selecting a modeling approach, the network topology, training data and various optimizations or tweaks to the model. Because many of these models are still difficult to interpret or explain, data scientists must use experience and intuition to hypothesize changes and then measure the impact those changes have on the overall performance of the model. As these models have become increasingly common in business systems, several different experiment tracking tools for machine learning have emerged to help investigators keep track of these experiments and work through them methodically. Although no clear winner has emerged, tools such as MLflow or Weights & Biases and platforms such as Comet or Neptune have introduced rigor and repeatability into the entire machine learning workflow. They also facilitate collaboration and help turn data science from a solitary endeavor into a team sport.","blip_selector":"experiment-tracking-tools-for-machine-learning","name":"Experiment tracking tools for machine learning","display_name":"Experiment tracking tools for machine learning","url":"/radar/tools/experiment-tracking-tools-for-machine-learning","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005106,"quadrant":"tools","volume_date":"2020-05","description":"If you're looking for a service to support dynamic feature toggles (and bear in mind that simple feature toggles work well too), check out ConfigCat. We'd describe it as \"like LaunchDarkly but cheaper and a bit less fancy\" and find that it does most of what we need. ConfigCat supports simple feature toggles, user segmentation, and A/B testing and has a generous free tier for low-volume use cases or those just starting out.","blip_selector":"configcat","name":"ConfigCat","display_name":"ConfigCat","url":"/radar/tools/configcat","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1213,"quadrant":"techniques","volume_date":"2020-05","description":"We see more and more tools such as Apollo Federation that can aggregate multiple GraphQL endpoints into a single graph. However, we caution against misusing GraphQL, especially when turning it into a server-to-server protocol. Our practice is to use GraphQL for server-side resource aggregation only. When using this pattern, the microservices continue to expose well-defined RESTful APIs, while under-the-hood aggregate services or BFF (Backend for Frontends) patterns use GraphQL resolvers as the implementation for stitching resources from other services. The shape of the graph is driven by domain-modeling exercises to ensure ubiquitous language is limited to subgraphs where needed (in the case of one-microservice-per-bounded-context). This technique simplifies the internal implementation of aggregate services or BFFs, while encouraging good modeling of services to avoid anemic REST. | One pattern that comes up again and again when building microservice-style architectures is how to handle the aggregation of many resources server-side. In recent years, we've seen the emergence of a number of patterns such as Backend for Frontend (BFF) and tools such as Falcor to address this. Our teams have started using GraphQL for server-side resource aggregation instead. This differs from the usual mode of using GraphQL where clients directly query a GraphQL server. When using this technique, the services continue to expose RESTful APIs but under-the-hood aggregate services use GraphQL resolvers as the implementation for stitching resources from other services. This technique simplifies the internal implementation of aggregate services or BFFs by using GraphQL.","blip_selector":"graphql-for-server-side-resource-aggregation","name":"GraphQL for server-side resource aggregation","display_name":"GraphQL for server-side resource aggregation","url":"/radar/techniques/graphql-for-server-side-resource-aggregation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005048,"quadrant":"tools","volume_date":"2020-05","description":"You can build most software following a simple two-step process: check out a repository, and then run a single build script. The process of setting up a full coding environment can still be cumbersome, though. Gitpod addresses this by providing cloud-based, \"ready-to-code\" environments for Github or GitLab repositories. It offers an IDE based on Visual Studio Code that runs inside the web browser. By default, these environments are launched on the Google Cloud Platform, although you can also deploy on-premise solutions. We see the immediate appeal, especially for open source software where this approach can lower the bar for casual contributors. However, it remains to be seen how viable this approach will be in corporate environments.","blip_selector":"gitpod","name":"Gitpod","display_name":"Gitpod","url":"/radar/tools/gitpod","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005070,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"As Kotlin is used increasingly for both mobile and server-side development, the associated ecosystem continues to evolve. Koin is a Kotlin framework that handles one of the routine problems in software development: dependency injection. Although you can choose from a variety of dependency injection frameworks for Kotlin, our teams have come to prefer the simplicity of Koin. Koin avoids using annotations and injects either through constructors or by mimicking Kotlin's lazy initialization so that objects are injected only when needed. This is in contrast to the statically compiled Dagger injection framework for Android. Our developers like the lightweight nature of this framework and its built-in testability.","blip_selector":"koin","name":"Koin","display_name":"Koin","url":"/radar/languages-and-frameworks/koin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005009,"quadrant":"techniques","volume_date":"2020-05","description":"We recommend caution in managing stateful systems via container orchestration platforms such as Kubernetes. Some databases are not built with native support for orchestration — they don't expect a scheduler to kill and relocate them to a different host. Building a highly available service on top of such databases is not trivial, and we still recommend running them on bare metal hosts or a virtual machine (VM) rather than to force-fit them into a container orchestration platform.","blip_selector":"managing-stateful-systems-via-container-orchestration","name":"Managing stateful systems via container orchestration","display_name":"Managing stateful systems via container orchestration","url":"/radar/techniques/managing-stateful-systems-via-container-orchestration","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005031,"quadrant":"platforms","volume_date":"2020-05","description":"We see a shift from accidental hybrid or whole-of-estate cloud migration plans to intentional and sophisticated hybrid, poly or portable cloud strategies, where organizations apply multidimensional principles to establish and execute their cloud strategy: where to host their various data and functional assets based on risk, ability to control and performance profiles; how to utilize their on-premise infrastructure investments while reducing the cost of operations; and how to take advantage of multiple cloud providers and their unique differentiated services without creating complexity and friction for users building and operating applications.\n\nAnthos is Google's answer to enable hybrid and multicloud strategies by providing a high-level management and control plane on top of a set of open source technologies such as GKE, Service Mesh and a Git-based Configuration Management. It enables running portable workloads and other assets on different hosting environments, including Google Cloud and on-premises hardware. Although other cloud providers have comparative offerings, Anthos intends to go beyond a hybrid cloud to a portable cloud enabler using open source components, but that is yet to be seen. We're seeing a rising interest in Anthos. While Google's approach in managed hybrid cloud environments seems promising, it’s not a magic bullet and requires changes in both existing cloud and on-premise assets. Our advice for clients considering Anthos is to make measured tradeoffs between selecting services from the Google Cloud ecosystem and other options, to maintain their right level of neutrality and control.","blip_selector":"anthos","name":"Anthos","display_name":"Anthos","url":"/radar/platforms/anthos","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904062,"quadrant":"techniques","volume_date":"2020-05","description":"We had this technique in Assess previously. The innovations in the NLP landscape continue at a great pace, and we're able to leverage these innovations in our projects thanks to the ubiquitous transfer learning for NLP. The GLUE benchmark (a suite of language understanding tasks) scores have seen dramatic progress over the past couple of years with average scores moving from 70.0 at launch to some of the leaders crossing 90.0 as of April 2020. A lot of our projects in the NLP domain are able to make significant progress by starting from pretrained models from ELMo, BERT, and ERNIE, among others, and then fine-tuning them based on the project needs. | Transfer learning has been quite effective within the field of computer vision, speeding the time to train a model by reusing existing models. Those of us who work in machine learning are excited that the same techniques can be applied to natural language processing (NLP) with the publication of ULMFiT and open source pretrained models and code examples. We think transfer learning for NLP will significantly reduce the effort to create systems dealing with text classification.","blip_selector":"transfer-learning-for-nlp","name":"Transfer learning for NLP","display_name":"Transfer learning for NLP","url":"/radar/techniques/transfer-learning-for-nlp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":790,"quadrant":"techniques","volume_date":"2020-05","description":"Five years ago we highlighted the problems with long-lived branches with Gitflow. Essentially, long-lived branches are the opposite of continuously integrating all changes to the source code, and in our experience continuous integration is the better approach for most kinds of software development. Later we extended our caution to Gitflow itself, because we saw teams using it almost exclusively with long-lived branches. Today, we still see teams in settings where continuous delivery of web-based systems is the stated goal being drawn to long-lived branches. So we were delighted that the author of Gitflow has now added a note to his original article, explaining that Gitflow was not intended for such use cases. | Gitflow is a strict branching pattern for releases using Git. Although not an inherently bad pattern, we often see it misused. If the feature and develop branches are short lived and merged often, you are really using the power of Git, which makes these activities easy. However, a problem we often see is that these become long lived branches , which results in the dreaded merge conflicts many people began using Git to escape. A merge is a merge. Regardless of the source control tool or pattern you use. If you wait more than a day or two to merge, you could hit a big merge conflict. This becomes a real issue if you have a larger team. If you have more than a few people waiting to merge, you can have a serious a bottleneck. Introducing patterns like Gitflow require the discipline to merge often to be successful. So by all means use the pattern, but only if you have the discipline to prevent long lived branches","blip_selector":"long-lived-branches-with-gitflow","name":"Long-lived branches with Gitflow","display_name":"Long-lived branches with Gitflow","url":"/radar/techniques/long-lived-branches-with-gitflow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1282,"quadrant":"tools","volume_date":"2020-05","description":"UX research demands data collection and analysis to make better decisions about the products we need to build. Our teams find Optimal Workshop useful because it makes it easy to validate prototypes and configure tests for data collection and thus make better decisions. Features such as first-click, card sorting, or a heatmap of user interaction help to both validate prototypes and improve website navigation and information display. It's an ideal tool for distributed teams since it allows them to conduct remote research. | UX research demands data collection and analysis to make better decisions about the products we need to build. Optimal Workshop is a suite of tools that helps to do this digitally. Features such as first-click or card sorting help to both validate prototypes and improve website navigation and information display. For distributed teams, in particular, benefit from Optimal Workshop as it lets them conduct remote research.","blip_selector":"optimal-workshop","name":"Optimal Workshop","display_name":"Optimal Workshop","url":"/radar/tools/optimal-workshop","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911014,"quadrant":"platforms","volume_date":"2020-05","description":"Not everyone needs a self-hosted OAuth2 solution, but if you do, have a look at Hydra — a fully compliant open source OAuth2 server and OpenID connect provider. Hydra has in-memory storage support for development and a relational database (PostgreSQL) for production use cases. Hydra as such is stateless and easy to scale horizontally in platforms such as Kubernetes. Depending on your performance requirement, you may have to tune the number of database instances while scaling Hydra instances. And because Hydra doesn't provide any identity management solutions out of the box, you can integrate whatever flavor of identity management you have with Hydra through a clean API. This clear separation of identity from the rest of the OAuth2 framework makes it easier to integrate Hydra with an existing authentication ecosystem. | Not everyone needs a self-hosted OAuth2 solution, but if you do, we found Hydra — a fully compliant open-source OAuth2 server and OpenID connect provider — quite useful. We really like that Hydra doesn't provide any identity management solutions out of the box; so no matter what flavor of identity management you have, it's possible to integrate it with Hydra through a clean API. This clear separation of identity from the rest of the OAuth2 framework makes it easier to integrate Hydra with an existing authentication ecosystem.","blip_selector":"hydra","name":"Hydra","display_name":"Hydra","url":"/radar/platforms/hydra","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005035,"quadrant":"platforms","volume_date":"2020-05","description":"MeiliSearch is a fast, easy-to-use and easy-to-deploy text search engine. Over the years Elasticsearch has become the popular choice for scalable text searches. However, if you don't have the volume of data that warrants a distributed solution but still want to provide a fast typo-tolerant search engine, then we recommend assessing MeiliSearch.","blip_selector":"meilisearch","name":"MeiliSearch","display_name":"MeiliSearch","url":"/radar/platforms/meilisearch","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005049,"quadrant":"tools","volume_date":"2020-05","description":"With the increasing adoption of Kubernetes and service mesh, API gateways have been experiencing an existential crisis in cloud-native distributed systems. After all, many of their capabilities (such as traffic control, security, routing and observability) are now provided by the cluster’s ingress controller and mesh gateway. Gloo is a lightweight API gateway that embraces this change; it uses Envoy as its gateway technology, while providing added value such as a cohesive view of the APIs to the external users and applications. It also provides an administrative interface for controlling Envoy gateways and runs and integrates with multiple service mesh implementations such as Linkerd, Istio and AWS App Mesh. While its open source implementation provides the basic capabilities expected from an API gateway, its enterprise edition has a more mature set of security controls such as API key management or integration with OPA. Gloo is a promising lightweight API gateway that plays well with the ecosystem of cloud-native technology and architecture, while avoiding the API gateway trap of enabling business logic to glue APIs for the end user.","blip_selector":"gloo","name":"Gloo","display_name":"Gloo","url":"/radar/tools/gloo","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904010,"quadrant":"platforms","volume_date":"2020-05","description":"Anka is a set of tools to create, manage, distribute, build and test macOS reproducible virtual environments for iOS and macOS. It brings Docker-like experience to macOS environments: instant start, CLI to manage virtual machines and registry to version and tag virtual machines for distribution. We've used Anka to build a macOS private cloud for a client. This tool is worth considering when virtualizing iOS and macOS environments. | Anka is a set of tools to create, manage and distribute build and test macOS reproducible virtual environments for iOS and macOS development. It brings Docker-like experience to macOS environments: instant start, CLI to manage virtual machines and registry to version and tag virtual machines for distribution. We discovered Anka when we proposed a macOS private cloud solution to a client. This tool is worth considering when applying DevOps workflow to iOS and macOS environments.","blip_selector":"anka","name":"Anka","display_name":"Anka","url":"/radar/platforms/anka","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201911046,"quadrant":"techniques","volume_date":"2020-05","description":"We find that more and more organizations need to replace aging legacy systems to keep up with the demands of their customers (both internal and external). One antipattern we keep seeing is legacy migration feature parity , the desire to retain feature parity with the old. We see this as a huge missed opportunity. Often the old systems have bloated over time, with many features unused by users (50% according to a 2014 Standish Group report) and business processes that have evolved over time. Replacing these features is a waste. Our advice: Convince your customers to take a step back and understand what their users currently need and prioritize these needs against business outcomes and metrics — which often is easier said than done. This means conducting user research and applying modern product development practices rather than simply replacing the existing ones. | We find that more and more organizations need to replace aging legacy systems to keep up with the demands of their customers (both internal and external). One antipattern we keep seeing is legacy migration feature parity, the desire to retain feature parity with the old. We see this as a huge missed opportunity. Often the old systems have bloated over time, with many features unused by users (50% according to a 2014 Standish Group report) and business processes that have evolved over time. Replacing these features is a waste. Our advice: Convince your customers to take a step back and understand what their users currently need and prioritize these needs against business outcomes and metrics — which often is easier said than done. This means conducting user research and applying modern product development practices rather than simply replacing the existing ones.","blip_selector":"legacy-migration-feature-parity","name":"Legacy migration feature parity","display_name":"Legacy migration feature parity","url":"/radar/techniques/legacy-migration-feature-parity","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1265,"quadrant":"tools","volume_date":"2020-05","description":"Visual Studio Live Share is a suite of extensions for Visual Studio Code and Visual Studio. At a time when teams are searching for good remote collaboration options, we want to call attention to the excellent tooling here. Live Share provides a good, low-latency remote-pairing experience, and requires significantly less bandwidth than the brute-force approach of sharing your entire desktop. Importantly, developers can work with their preferred configuration, extensions and key mappings during a pairing session. In addition to real-time collaboration for editing and debugging code, Live Share allows voice calls and sharing terminals and servers. | Visual Studio Live Share is a suite of extensions for Visual Studio Code and Visual Studio. The real-time collaboration for editing and debugging of code, voice calls, sharing a terminal and exposing local ports have reduced some of the obstacles we'd otherwise encounter when pairing remotely. In particular, we like that Live Share allows developers to collaborate with each other, while continuing to use their preconfigured editor, which includes themes, key maps and extensions.","blip_selector":"visual-studio-live-share","name":"Visual Studio Live Share","display_name":"Visual Studio Live Share","url":"/radar/tools/visual-studio-live-share","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005085,"quadrant":"techniques","volume_date":"2020-05","description":"DeepWalk is an algorithm that helps apply machine learning on graphs. When working on data sets that are represented as graphs, one of the key problems is to extract features from the graph. This is where DeepWalk can help. It uses SkipGram to construct node embeddings by viewing the graph as a language where each node is a unique word in the language and random walks of finite length on the graph constitutes a sentence. These embeddings can then be used by various ML models. DeepWalk is one of the techniques we're trialling on some of our projects where we've needed to apply machine learning on graphs.","blip_selector":"deepwalk","name":"DeepWalk","display_name":"DeepWalk","url":"/radar/techniques/deepwalk","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1223,"quadrant":"tools","volume_date":"2020-05","description":"ScoutSuite is an expanded and updated tool based on Scout2 (featured in the Radar in 2018) that provides security posture assessment across AWS, Azure, GCP and other cloud providers. It works by automatically aggregating configuration data for an environment and applying rules to audit the environment. We've found this very useful across projects for doing point-in-time security assessments. | Scout2 is a security auditing tool for AWS environments. Instead of manually navigating through web pages, you can rely on Scout2 to fetch all the configuration data of an AWS environment for you; it even generates an attack surface report. Scout2 ships with preconfigured rules and can be easily extended to support more services and test cases. Since Scout2 only performs AWS API calls to fetch configuration data and identify security gaps, it is not necessary to complete and submit the AWS Vulnerability / Penetration Testing Request Form.","blip_selector":"scoutsuite","name":"ScoutSuite","display_name":"ScoutSuite","url":"/radar/tools/scoutsuite","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005100,"quadrant":"tools","volume_date":"2020-05","description":"Manifold is a model-agnostic visual debugger for machine learning (ML). Model developers spend a significant amount of time on iterating and improving an existing model rather than creating a new one. By shifting the focus from model space to data space, Manifold supplements the existing performance metrics with a visual characteristics of the data set that influences the model performance. We think Manifold will be a useful tool to assess in the ML ecosystem.","blip_selector":"manifold","name":"Manifold","display_name":"Manifold","url":"/radar/tools/manifold","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005020,"quadrant":"tools","volume_date":"2020-05","description":"kind is a tool for running local Kubernetes clusters using Docker container nodes. With kubetest integration, kind makes it easy to do end-to-end testing on Kubernetes. We've used kind to create ephemeral Kubernetes clusters to test Kubernetes resources such as Operators and Custom Resource Definitions (CRDs) in our CI pipelines.","blip_selector":"kind","name":"kind","display_name":"kind","url":"/radar/tools/kind","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":202005099,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"In the previous edition of the Radar we had BERT — which is a key milestone in the NLP landscape. Last year, Baidu released ERNIE 2.0 (Enhanced Representation through kNowledge IntEgration) which outperformed BERT on seven GLUE language understanding tasks and on all nine of the Chinese NLP tasks. ERNIE, like BERT, provides unsupervised pretrained language models, which can be fine-tuned by adding output layers to create state-of-the-art models for a variety of NLP tasks. ERNIE differs from traditional pretraining methods in that it is a continual pretraining framework. Instead of training with a small number of pretraining objectives, it could constantly introduce a large variety of pretraining tasks to help the model efficiently learn language representations. We're pretty excited about the advancements in NLP and are looking forward to experimenting with ERNIE on our projects.","blip_selector":"ernie","name":"ERNIE","display_name":"ERNIE","url":"/radar/languages-and-frameworks/ernie","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005110,"quadrant":"languages-and-frameworks","volume_date":"2020-05","description":"Sarama is a Go client library for Apache Kafka. If you’re developing your APIs in Go, you'll find Sarama quite easy to set up and manage as it doesn't depend on any native libraries. Sarama has two types of APIs — a high-level API for easily producing and consuming messages and a low-level API for controlling bytes on the wire.","blip_selector":"sarama","name":"Sarama","display_name":"Sarama","url":"/radar/languages-and-frameworks/sarama","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911070,"quadrant":"tools","volume_date":"2020-05","description":"Figma has demonstrated to be the go-to tool for collaborative design, not only for designers but for multidisciplinary teams too; it allows developers and other roles to view and comment on designs through the browser without the desktop version. Compared to its competitors (e.g., Invision or Sketch) which have you use more than one tool for versioning, collaborating and design sharing, Figma puts together all of these features in one tool that makes it easier for our teams to discover new ideas together. Our teams find Figma very useful, especially in remote and distributed design work enablement and facilitation. In addition to its real-time design and collaboration capabilities, Figma also offers an API that helps to improve the DesignOps process. | One of the great pain points in interaction and visual design is the lack of tools built for collaboration. This is where Figma comes in. It has the same functionalities of design programs such as Sketch and Invision, but by being able to collaborate with another person at the same time, it helps you discover new ideas together with real-time collaboration capabilities. Our teams find Figma very useful, especially in remote and distributed design work enablement and facilitation. In addition to its collaboration capabilities, Figma also offers an API that helps to improve the DesignOps process.","blip_selector":"figma","name":"Figma","display_name":"Figma","url":"/radar/tools/figma","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1285,"quadrant":"tools","volume_date":"2020-05","description":"Jaeger is an open source distributed tracing system. Similar to Zipkin, it's been inspired by the Google Dapper paper and complies with OpenTelemetry. We've used Jaeger successfully with Istio and Envoy on Kubernetes and like its UI. Jaeger exposes tracing metrics in the Prometheus format so they can be made available to other tools. However, a new generation of tools such as Honeycomb integrates traces and metrics into a single observability stream for simpler aggregate analysis. Jaeger joined CNCF in 2017 and has recently been elevated to CNCF's highest level of maturity, indicating its widespread deployment into production systems. | Jaeger is an open source distributed tracing system. Similar to Zipkin, it's been inspired by the Google Dapper paper and complies with OpenTracing. Jaeger is a younger open source project than Zipkin, but it's gained popularity quickly due to a larger number of supported languages for the client libraries and easy installation on Kubernetes. We've used Jaeger successfully with Istio, integrating application traces with Envoy on Kubernetes, and like its UI. With Jaeger joining CNCF, we anticipate a larger community engagement effort and deeper integration with other CNCF projects.","blip_selector":"jaeger","name":"Jaeger","display_name":"Jaeger","url":"/radar/tools/jaeger","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":202005041,"quadrant":"platforms","volume_date":"2020-05","description":"Without making a judgment of the GitOps technique, we'd like to talk about Argo CD within the scope of deploying and monitoring applications in Kubernetes environments. Based on its ability to automate the deployment of the desired application state in the specified target environments in Kubernetes and our good experience with troubleshooting failed deployments, verifying logs and monitoring deployment status, we recommend you give Argo CD a try. You can even see graphically what is going on in the cluster, how a change is propagated and how pods are created and destroyed in real time.","blip_selector":"argo-cd","name":"Argo CD","display_name":"Argo CD","url":"/radar/platforms/argo-cd","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911016,"quadrant":"platforms","volume_date":"2019-11","description":"Once exclusive to tech giants, self-driving technology isn't rocket science anymore, as demonstrated by Apollo Auto. The goal of the Baidu-owned Apollo program is to become the Android of the autonomous driving industry. The Apollo platform has components such as perception, simulation, planning and intelligent control that enable car companies to integrate their own autonomous driving systems into their vehicles' hardware. The developer community is still new but with a lot of vendors joining to contribute more ports. One of our projects helped our client to complete self-driving license exams with the Apollo-based autopilot system. Apollo also provides an evolutionary architecture approach to adopt advanced features gradually, which enables us to integrate more sensors and functions in an agile, iterative way.","blip_selector":"apollo-auto","name":"Apollo Auto","display_name":"Apollo Auto","url":"/radar/platforms/apollo-auto","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1041,"quadrant":"techniques","volume_date":"2019-11","description":"The continued adoption of containers for deployments, especially Docker, has made container security scanning a must-have technique and we've moved this technique into Adopt to reflect that. Specifically, containers introduced a new path for security issues; it's vital that you use tools to scan and check containers during deployment. We prefer using automated scanning tools that run as part of the deployment pipeline. | The container revolution around Docker has massively reduced the friction in moving applications between environments, fueling increased adoption of continuous delivery and continuous deployments. The latter, especially, has blown a rather large hole in the traditional controls over what can go to production. The technique of container security scanning is a necessary response to this threat vector. Tools in the build pipeline automatically check containers flowing through the pipeline against known vulnerabilities. Since our first mention of this technique, the tool landscape has matured and the technique has proven useful on development efforts with our clients. | The container revolution instigated by Docker has massively reduced the friction in moving applications between environments but at the same time has blown a rather large hole in the traditional controls over what can go to production. The technique of container security scanning is a necessary response to this threat vector. Docker now provides its own security scanning tools, as does CoreOS, and we've also had success with the CIS Security Benchmarks. Whichever approach you take, we believe the topic of automated container security validation is of high value and a necessary part of PaaS thinking. | The container revolution instigated by Docker has massively reduced the friction in moving applications between environments but at the same time has blown a rather large hole in the traditional controls over what can go to production. The technique of container security scanning is a necessary response to this threat vector. Docker now provides its own security scanning tools, as does CoreOS, and we’ve also had success with the CIS Security Benchmarks. Whichever approach you take, we believe the topic of automated container security validation is of high value and a necessary part of PaaS thinking.","blip_selector":"container-security-scanning","name":"Container security scanning","display_name":"Container security scanning","url":"/radar/techniques/container-security-scanning","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911002,"quadrant":"platforms","volume_date":"2019-11","description":"Kubernetes's serverless ecosystem is growing. We talked about Knative in a previous Radar; now we're seeing Fission gaining traction. Fission lets developers focus on writing short-lived functions and map them to HTTP requests while the framework handles the rest of the plumbing and automation of Kubernetes resources behind the scenes. Fission also lets you compose functions, integrate with third-party providers via web hooks and automate the management of the Kubernetes infrastructure.","blip_selector":"fission","name":"Fission","display_name":"Fission","url":"/radar/platforms/fission","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911006,"quadrant":"platforms","volume_date":"2019-11","description":"Kuma is a platform-agnostic service mesh for Kubernetes, VMs and bare metal environments. Kuma is implemented as a control plane on top of Envoy and as such can instrument any Layer 4/Layer 7 traffic to secure, observe, route and enhance connectivity between services. Most of the service mesh implementations are targeted natively at the Kubernetes ecosystem which in itself is not bad but hinders the adoption of service mesh for existing non-Kubernetes applications. Rather than waiting for large platform transformation efforts to be complete, you can now use Kuma and modernize the network infrastructure.","blip_selector":"kuma","name":"Kuma","display_name":"Kuma","url":"/radar/platforms/kuma","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911010,"quadrant":"platforms","volume_date":"2019-11","description":"GraalVM is a universal virtual machine by Oracle for running applications written in JVM languages, JavaScript, Python, Ruby and R, as well as C/C++ and other LLVM-based languages. At its simplest, GraalVM can be used as a more performant VM for JVM and other supported non-JVM languages. But it also allows us to write polyglot applications with very little performance impact; and its Native Image utility (currently only available as an Early Adopter Technology) lets us compile Java code ahead of time to stand-alone executables for faster startup and less memory use. GraalVM has generated a lot of excitement in the Java community, and a host of Java frameworks (including Micronaut, Quarkus, and Helidon) are already taking advantage of it.","blip_selector":"graalvm","name":"GraalVM","display_name":"GraalVM","url":"/radar/platforms/graalvm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1131,"quadrant":"techniques","volume_date":"2019-11","description":"The use of continuous delivery pipelines to orchestrate the release process for software has become a mainstream concept. CI/CD tools can be used to test server configuration (e.g., Chef cookbooks, Puppet modules, Ansible playbooks), server image building (e.g., Packer), environment provisioning (e.g., Terraform, CloudFormation) and the integration of environments. The use of pipelines for infrastructure as code lets you find errors before changes are applied to operational environments — including environments used for development and testing. They also offer a way to ensure that infrastructure tooling is run consistently, using CI/CD agents rather than individual workstations. Our teams have had good results adopting this technique on their projects. | The use of continuous delivery pipelines to orchestrate the release process for software has become a mainstream concept. However, automatically testing changes to infrastructure code isn’t as widely understood. Continuous integration (CI) and continuous delivery (CD) tools can be used to test server configuration (e.g., Chef cookbooks, Puppet modules, Ansible playbooks), server image building (e.g., Packer), environment provisioning (e.g., Terraform, CloudFormation) and integration of environments. The use of pipelines for infrastructure as code enables errors to be found before changes are applied to operational environments — including environments used for development and testing. They also offer a way to ensure that infrastructure tooling is run consistently, from CI/CD agents, as opposed to being run from individual workstations. Some challenges remain, however, such as the longer feedback loops associated with standing up containers and virtual machines. Still, we've found this to be a valuable technique. | The use of continuous delivery pipelines to orchestrate the release process for software has become a mainstream concept. However, automatically testing changes to infrastructure code isn’t as widely understood. Continuous integration (CI) and continuous delivery (CD) tools can be used to test server configuration (e.g., Chef cookbooks, Puppet modules, Ansible playbooks), server image building (e.g., Packer), environment provisioning (e.g., Terraform, CloudFormation) and integration of environments. The use of pipelines for infrastructure as code enables errors to be found before changes are applied to operational environments — including environments used for development and testing. They also offer a way to ensure that infrastructure tooling is run consistently, from CI/CD agents, as opposed to being run from individual workstations. Some challenges remain, however, such as the longer feedback loops associated with standing up containers and virtual machines. Still, we've found this to be a valuable technique.","blip_selector":"pipelines-for-infrastructure-as-code","name":"Pipelines for infrastructure as code","display_name":"Pipelines for infrastructure as code","url":"/radar/techniques/pipelines-for-infrastructure-as-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911045,"quadrant":"techniques","volume_date":"2019-11","description":"As the usage of containers, deployment of large fleet of services by autonomous teams and increased speed of continuous delivery become common practice for many organizations, the need for automated deploy-time software security controls arise. Binary attestation is a technique to implement deploy-time security control; to cryptographically verify that a binary image is authorized for deployment. Using this technique, an attestor, an automated build process or a security team signs off the binaries that have passed the required quality checks and tests and are authorized to be deployed. Services such as GCP Binary Authorization enabled by Grafeas, and tools such as in-toto and Docker Notary support creating attestations and validating the image signatures before deployment.","blip_selector":"binary-attestation","name":"Binary attestation","display_name":"Binary attestation","url":"/radar/techniques/binary-attestation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911039,"quadrant":"techniques","volume_date":"2019-11","description":"Deep neural networks have demonstrated remarkable recall and accuracy across a wide range of problems. Given sufficient training data and an appropriately chosen topology, these models meet and exceed human capabilities in certain select problem spaces. However, they're inherently opaque. Although parts of models can be reused through transfer learning, we're seldom able to ascribe any human-understandable meaning to these elements. In contrast, an explainable model is one that allows us to say how a decision was made. For example, a decision tree yields a chain of inference that describes the classification process. Explainability becomes critical in certain regulated industries or when we're concerned about the ethical impact of a decision. As these models are incorporated more widely into critical business systems, it's important to consider explainability as a first-class model selection criterion. Despite their power, neural networks might not be an appropriate choice when explainability requirements are strict.","blip_selector":"explainability-as-a-first-class-model-selection-criterion","name":"Explainability as a first-class model selection criterion","display_name":"Explainability as a first-class model selection criterion","url":"/radar/techniques/explainability-as-a-first-class-model-selection-criterion","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911064,"quadrant":"tools","volume_date":"2019-11","description":"The machine learning world has shifted emphasis slightly from exploring what models are capable of understanding to how they do it. Concerns about introducing bias or overgeneralizing a model's applicability have resulted in interesting new tools such as What-If Tool (WIT). This tool helps data scientists to dig into a model's behavior and to visualize the impact various features and data sets have on the output. Introduced by Google and available either through Tensorboard or Jupyter notebooks, WIT simplifies the tasks of comparing models, slicing data sets, visualizing facets and editing individual data points. Although WIT makes it easier to perform these analyses, they still require a deep understanding of the mathematics and theory behind the models. It is a tool for data scientists to gain deeper insights into model behavior. Naive users shouldn't expect any tool to remove the risk or minimize the damage done by a misapplied or poorly trained algorithm.","blip_selector":"what-if-tool","name":"What-If Tool","display_name":"What-If Tool","url":"/radar/tools/what-if-tool","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911013,"quadrant":"platforms","volume_date":"2019-11","description":"Ideally, containers should be managed and run by the respective container runtime without root privileges. This is not trivial but when achieved, it reduces the attack surface and avoids whole classes of security problems, notably privilege escalation out of the container. The community has discussed this as rootless containers for quite a while, and it is part of the open container runtime specification and its standard implementation runc, which underpins Kubernetes. Now, Docker 19.03 introduces rootless containers as an experimental feature. Although fully functional, the feature doesn't yet work with several other features such as cgroups resource controls and AppArmor security profiles.","blip_selector":"rootless-containers","name":"Rootless containers","display_name":"Rootless containers","url":"/radar/platforms/rootless-containers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911071,"quadrant":"tools","volume_date":"2019-11","description":"Pumba is a chaos testing and network emulation tool for Docker. Pumba can kill, stop, remove or pause Docker containers. Pumba can also emulate networks and simulate different network failures such as delays, packet loss and bandwidth rate limits. Pumba uses the tc tool for network emulation which means it needs to be available in our containers or we need to run Pumba in a sidecar container with tc. Pumba is particularly useful when we want to run some automated chaos tests against a distributed system running on a bunch of containers locally or in the build pipeline.","blip_selector":"pumba","name":"Pumba","display_name":"Pumba","url":"/radar/tools/pumba","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1136,"quadrant":"techniques","volume_date":"2019-11","description":"Many of the technical solutions we build today run in increasingly complex polycloud or hybrid-cloud environments with multiple distributed components and services. Under such circumstances, we apply two security principles early in implementation: zero trust network, never trust the network and always verify; and the principle of least privilege, granting the minimum permissions necessary for performing a particular job. Sidecars for endpoint security is a common technique we use to implement these principles to enforce security controls at every component's endpoint, e.g., APIs of services, data stores or Kubernetes control interface. We do this using an out-of-process sidecar — a process or a container that is deployed and scheduled with each service sharing the same execution context, host and identity. Open Policy Agent and Envoy are tools that implement this technique. Sidecars for endpoint security minimize the trusted footprint to a local endpoint rather than the network perimeter. We like to see the responsibility of sidecar’s security policy configuration left with the team that is responsible for the endpoint and not a separate centralized team. | Microservices architecture, with a large number of services exposing their assets and capabilities through APIs and an increased attack surface, demand a zero trust security architecture — ‘never trust, always verify’. However, enforcing security controls for communication between services is often neglected, due to increased service code complexity and lack of libraries and language support in a polyglot environment. To get around this complexity, some teams delegate security to an out-of-process sidecar — a process or a container that is deployed and scheduled with each service sharing the same execution context, host and identity. Sidecars implement security capabilities, such as transparent encryption of the communication and TLS (Transport Layer Security) termination, as well as authentication and authorization of the calling service or the end user. We recommend you look into using Istio, linkerd or Envoy before implementing your own sidecars for endpoint security. | Microservices architecture, with a large number of services exposing their assets and capabilities through APIs and an increased attack surface, demand a zero trust security architecture — ‘never trust, always verify’. However, enforcing security controls for communication between services is often neglected, due to increased service code complexity and lack of libraries and language support in a polyglot environment. To get around this complexity, some teams delegate security to an out-of-process sidecar — a process or a container that is deployed and scheduled with each service sharing the same execution context, host and identity. Sidecars implement security capabilities, such as transparent encryption of the communication and TLS (Transport Layer Security) termination, as well as authentication and authorization of the calling service or the end user. We recommend you look into using Istio, linkerd or Envoy before implementing your own sidecars for endpoint security.","blip_selector":"sidecars-for-endpoint-security","name":"Sidecars for endpoint security","display_name":"Sidecars for endpoint security","url":"/radar/techniques/sidecars-for-endpoint-security","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":982,"quadrant":"languages-and-frameworks","volume_date":"2019-11","description":"With its 2.0 release, TensorFlow retains its prominence as the industry’s leading machine learning framework. TensorFlow began as a numerical processing package that gradually expanded to include libraries supporting a variety of ML approaches and execution environments, ranging from mobile CPU to large GPU clusters. Along the way, a slew of frameworks became available to simplify the tasks of network creation and training. At the same time, other frameworks, notably PyTorch, offered an imperative programming model that made debugging and execution simpler and easier. TensorFlow 2.0 now defaults to imperative flow (eager execution) and adopts Keras as the single high-level API. While these changes modernize TensorFlow's usability and make it more competitive with PyTorch, it is a significant rewrite that often breaks backward compatibility — many tools and serving frameworks in the TensorFlow ecosystem won't immediately work with the new version. For the time being, consider whether you want to design and experiment in TensorFlow 2.0 but revert to version 1 to serve and run your models in production. | Google's TensorFlow is an open source machine-learning platform that can be used for everything from research through to production and will run on hardware from a mobile CPU all the way to a large GPU compute cluster. It's an important platform because it makes implementing deep-learning algorithms much more accessible and convenient. Despite the hype, though, TensorFlow isn't really anything new algorithmically: All of these techniques have been available in the public domain via academia for some time. It's also important to realize that most businesses are not yet doing even basic predictive analytics and that jumping to deep learning likely won't help make sense of most data sets. For those who do have the right problem and data set, however, TensorFlow is a useful toolkit.","blip_selector":"tensorflow","name":"TensorFlow","display_name":"TensorFlow","url":"/radar/languages-and-frameworks/tensorflow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911012,"quadrant":"platforms","volume_date":"2019-11","description":"We talked about Kubernetes in the past and it continues to be the default choice for deploying and managing containers in production clusters. However, it's getting increasingly difficult to provide a similar experience offline for developers. Among other options, we've found MicroK8s to be quite useful. To install the MicroK8s snap, pick a release channel (stable, candidate, beta or edge), and you can get Kubernetes running with a few commands. You can also keep track of mainstream releases and choose to upgrade your setup automatically.","blip_selector":"microk8s","name":"MicroK8s","display_name":"MicroK8s","url":"/radar/platforms/microk8s","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911035,"quadrant":"techniques","volume_date":"2019-11","description":"One of the main points of friction for data scientists and analysts, in their workflow, is to locate the data they need, make sense of it and evaluate whether it's trustworthy to use it. This remains a challenge due to the missing metadata about the available data sources and lack of adequate functionality needed to search and locate data. We encourage teams who are providing analytical data sets or building data platforms to make data discoverability a first-class function of their environments; to provide the ability to easily locate available data, detect its quality, understand its structure and lineage and get access to it. Traditionally this function has been provided by bloated data cataloguing solutions. In recent years, we've seen the growth of open-source projects that are improving developer experiences for both data providers and data consumers to do one thing really well: to make data discoverable. Amundsen by Lyft and WhereHows by LinkedIn are among these tools. What we like to see is a change in providers' behavior to intentionally share the metadata that help discoverability in favor of discoverability tools that infer partial metadata information from silos of application databases.","blip_selector":"data-discoverability","name":"Data discoverability","display_name":"Data discoverability","url":"/radar/techniques/data-discoverability","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911017,"quadrant":"platforms","volume_date":"2019-11","description":"ROS (Robot Operating System) is a set of libraries and tools to help software developers create robot applications. It's a development framework that provides hardware abstraction, device drivers, libraries, visualizers, message-passing, package management and more. Apollo Auto is based on ROS. In our other ADAS simulation project, we've also used ROS's messaging system (bag). The technology isn't new, but it has regained developers’ attention with the development of ADAS.","blip_selector":"ros","name":"ROS","display_name":"ROS","url":"/radar/platforms/ros","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911080,"quadrant":"tools","volume_date":"2019-11","description":"React Styleguidist is a development environment for React components. It includes a dev server with hot reloading capabilities and generates an HTML style guide for sharing with teams. The style guide shows a live version of all components in one place with documentation and a list of their props. We've mentioned React Styleguidist as a UI dev environment before, and over time it has become our default choice among similar tools in this space.","blip_selector":"react-styleguidist","name":"React Styleguidist","display_name":"React Styleguidist","url":"/radar/tools/react-styleguidist","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904032,"quadrant":"languages-and-frameworks","volume_date":"2019-11","description":"Micronaut is a JVM framework for building services using Java, Kotlin or Groovy. It distinguishes itself through a small memory footprint and short startup time; it achieves these improvements by avoiding runtime reflection for dependency injection (DI) and proxy generation, a common shortcoming of traditional frameworks, and instead uses a DI/AOP container which performs dependency injection at compile time. This makes it attractive not just for standard server-side microservices but also in the context of, for example, the Internet of Things, Android applications and serverless functions. Micronaut uses Netty and has first-class support for reactive programming. It also includes features such as service discovery and circuit breaking that make it cloud-native friendly. Micronaut is a very promising entrant to the full-stack framework for the JVM space, and we're seeing it in more and more projects in production, prompting us to move it to Trial. | Micronaut is a new JVM framework for building microservices using Java, Kotlin or Groovy. It distinguishes itself through a small memory footprint and short startup time. It achieves these improvements by avoiding runtime reflection for DI and proxy generation, a common shortcoming of traditional frameworks, and instead uses a DI/AOP container which performs dependency injection at compile time. This makes it attractive not just for standard server-side microservices but also in the context of, for example, the Internet of Things, Android applications and serverless functions. Micronaut uses Netty and has first-class support for reactive programming. It also includes many features that make it cloud-native friendly such as service discovery and circuit breaking. Micronaut is a very promising entrant to the full stack framework for the JVM space and we're keenly watching it.","blip_selector":"micronaut","name":"Micronaut","display_name":"Micronaut","url":"/radar/languages-and-frameworks/micronaut","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201911056,"quadrant":"techniques","volume_date":"2019-11","description":"When teams embrace the concept of micro frontends they have a number of patterns at their disposal to integrate the individual micro frontends into one application. As always there are antipatterns, too. A common one in this case is front-end integration via artifact. For each micro frontend an artifact is built, usually an NPM package, which is pushed into a registry. A later step, sometimes in a different build pipeline, then combines the individual packages into a final package that contains all micro frontends. From a purely technical perspective this integration at build time results in a working application. However, integrating via artifact implies that for each change the full artifact needs to be rebuilt, which is time consuming and will likely have a negative impact on developer experience. Worse, this style of integrating frontends also introduces direct dependencies between the micro frontends at build time and therefore causes considerable coordination overhead.","blip_selector":"front-end-integration-via-artifact","name":"Front-end integration via artifact","display_name":"Front-end integration via artifact","url":"/radar/techniques/front-end-integration-via-artifact","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1256,"quadrant":"platforms","volume_date":"2019-11","description":"Mongoose OS remains one of our preferred open-source microcontroller operating systems and embedded firmware development frameworks. It's worth noting that Mongoose OS fills a noticeable gap for embedded software developers: the gap between Arduino firmware suitable for prototyping and bare-metal microcontrollers' native SDKs. Our teams have successfully used Cesanta's new end-to-end device management platform, mDash, for small-scale greenfield hardware projects. Major Internet of Things (IoT) cloud platform providers today support the Mongoose OS development framework for their device management, connectivity, and over-the-air (OTA) firmware upgrades. Since we last reported on Mongoose OS, the number of supported boards and microcontrollers has grown to include STM, Texas Instruments and Espressif. We continue to enjoy its seamless support for OTA updates and its built-in security at the individual device level. | With an accelerated growth of connected embedded devices and wider accessibility of hardware, Mongoose OS fills a noticeable gap for embedded software developers: the gap between Arduino firmware suitable for prototyping and bare-metal microcontrollers' native SDKs. Mongoose OS is a microcontroller operating system that comes with a set of libraries and a development framework to support typical Internet of Things (IoT) applications with connectivity to generic MQTT servers and popular IoT cloud platforms such as Google Cloud IoT Core and AWS IoT by default. In fact, Google recommends a Mongoose starter kit for its Cloud IoT Core. We’ve had a seamless experience using Mongoose OS in our embedded projects building connected workspaces. We especially liked its built-in security at the individual device level and OTA firmware updates, among other features. At the time of writing, only a limited number of microcontrollers and boards are supported with more popular ARM-based microcontrollers still under development.","blip_selector":"mongoose-os","name":"Mongoose OS","display_name":"Mongoose OS","url":"/radar/platforms/mongoose-os","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911079,"quadrant":"tools","volume_date":"2019-11","description":"ESLint is being used as a standard in many of our projects. As a linting tool for JavaScript it has multiple rule sets, recommended rules and plugins in order to extend to frameworks or JavaScript flavors. We've seen it leveraged heavily to help teams create and enforce norms in their code by allowing for real-time analysis of code during development. It can be used to standardize coding practices by enforcing best practices and code styling, and identify vulnerabilities in your code. It does so by integrating well with most IDEs and giving live feedback while coding. It's styling rules in particular will automatically fix the linting errors, making the process seamless and effective without incurring additional development cost. Developers can quickly get up to speed with the rules thanks to the community documentation, which does a good job of explaining coding patterns.  As ESLint becomes more common and powerful, it has gained traction in the industry, and this is illustrated by the TypeScript team's move to support and work with ESLint rather than investing in TSLint.","blip_selector":"eslint","name":"ESLint","display_name":"ESLint","url":"/radar/tools/eslint","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911048,"quadrant":"techniques","volume_date":"2019-11","description":"Linking records from different data providers in the presence of a shared key is trivial. However, you may not always have a shared key; even if you do, it may not be a good idea to expose it due to privacy concerns. Privacy-preserving record linkage (PPRL) using Bloom filter (a space-efficient probabilistic data structure) is an established technique that allows probabilistic linkage of records from different data providers without exposing privately identifiable personal data. For example, when linking data from two data providers, each provider encrypts its personally identifiable data using Bloom filter to get cryptographic linkage keys and then sends them to you via a secure channel. Once data is received, the records can be linked by computing similarity scores between sets of cryptographic linkage keys from each provider. Among other techniques, we found PPRL using Bloom filters to be scalable for large data sets.","blip_selector":"privacy-preserving-record-linkage-pprl-using-bloom-filter","name":"Privacy-preserving record linkage (PPRL) using Bloom filter","display_name":"Privacy-preserving record linkage (PPRL) using Bloom filter","url":"/radar/techniques/privacy-preserving-record-linkage-pprl-using-bloom-filter","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911047,"quadrant":"platforms","volume_date":"2019-11","description":"The tools and frameworks ecosystem around neural networks have been evolving rapidly. The interoperability between them, however, has been a challenge. It's not uncommon in the ML industry to quickly prototype and train the model in one tool and then deploy it in a different tool for inference. Because the internal format of these tools aren't compatible, we need to implement and maintain messy convertors to make the models compatible. The Open Neural Network Exchange format ONNX  addresses this problem. In ONNX, the neural networks are represented as graphs using standard operator specifications, and together with a serialization format for trained weights, neural network models can be transferred from one tool to another. This opens up lots of possibilities, including Model Zoo, a collection of pretrained models in ONNX format.","blip_selector":"onnx","name":"ONNX","display_name":"ONNX","url":"/radar/platforms/onnx","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911011,"quadrant":"platforms","volume_date":"2019-11","description":"Azure Pipelines is a product of Azure DevOps that offers cloud-based solutions to implement pipelines as code for projects hosted in Azure DevOps Git server or other Git solution such as GitHub or Bitbucket. The interesting part of this solution is the ability to run your scripts in Linux, MacOS and Windows agents without the overhead of managing a virtual machine on your own. This represents a big step forward, especially for teams that work on Windows environments with .NET Framework solutions; we're also assessing this service for continuous delivery in iOS.","blip_selector":"azure-pipelines","name":"Azure Pipelines","display_name":"Azure Pipelines","url":"/radar/platforms/azure-pipelines","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201911057,"quadrant":"techniques","volume_date":"2019-11","description":"The old term 10x engineer has come under scrutiny these past few months. A widely shared Twitter thread essentially suggests companies should excuse antisocial and damaging behaviors in order to retain engineers who are perceived as having immense individual output. Thankfully, many people on social media made fun of the concept, but the stereotype of the \"rockstar developer\" is still pervasive. In our experience, great engineers are driven not by individual output but by working in amazing teams. It's more effective to build teams of talented individuals with mixed experiences and diverse backgrounds and provide the right ingredients for teamwork, learning and continuous improvement. These 10x teams can move faster, scale more quickly and are much more resilient — without needing to pander to bad behaviors.","blip_selector":"10x-engineers","name":"10x engineers","display_name":"10x engineers","url":"/radar/techniques/10x-engineers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911081,"quadrant":"tools","volume_date":"2019-11","description":"Commitizen is a simple tool to help streamline the commit process when using Git. It prompts you to provide any required fields and also formats your commit message appropriately. It supports different conventions for describing the required check-in formats, and you can add your own via an adapter. This simple tool saves time and avoids later rejections from a commit hook.","blip_selector":"commitizen","name":"Commitizen","display_name":"Commitizen","url":"/radar/tools/commitizen","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911029,"quadrant":"languages-and-frameworks","volume_date":"2019-11","description":"Flair is a simple Python-based framework for NLP processing. It allows users to do standard NLP tasks such as named entity recognition (NER), part-of-speech tagging (PoS), word-sense disambiguation and classification and performs well on a range of NLP tasks. Flair presents a simple and unified interface for a variety of word and document embeddings, including BERT, Elmo and its own Flair embeddings. It also has multilingual support. The framework itself is built on top of PyTorch. We're using it in some of our projects and like its ease of use and powerful abstractions.","blip_selector":"flair","name":"Flair","display_name":"Flair","url":"/radar/languages-and-frameworks/flair","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911078,"quadrant":"languages-and-frameworks","volume_date":"2019-11","description":"Using tagged template literals styled components make it possible to put the CSS needed to style a React component directly into the JavaScript code that creates the component. This greatly reduces the pain with managing CSS and obviates the need for naming conventions or other means of avoiding naming conflicts in CSS. Developers can see the styling when looking at the component definition, and they don't have to memorize several megabytes worth of CSS. Of course, placing the CSS into the JavaScript code can make it harder to get a consistent view across the styling of different components, which is why we recommend understanding the trade-offs with this approach.","blip_selector":"styled-components","name":"Styled components","display_name":"Styled components","url":"/radar/languages-and-frameworks/styled-components","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911015,"quadrant":"platforms","volume_date":"2019-11","description":"GCP Pub/Sub is Google Cloud's event streaming platform. It's a popular piece of infrastructure for many of our architectures running Google Cloud Platform, including mass event ingestion, communication of serverless workloads and streaming data-processing workflows. One of its unique features is support of pull and push subscriptions: subscribing to receive all published messages available at the time of subscription or pushing messages to a particular endpoint. Our teams have enjoyed its reliability and scale and that it just works as advertised.","blip_selector":"gcp-pub-sub","name":"GCP Pub/Sub","display_name":"GCP Pub/Sub","url":"/radar/platforms/gcp-pub-sub","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911063,"quadrant":"tools","volume_date":"2019-11","description":"With increased adoption of Kubernetes as container orchestrator, the security toolset around containers and Kubernetes is evolving rapidly. Falco is one such container-native tool aimed at addressing runtime security. Falco leverages Sysdig's Linux kernel instrumentation and system call profiling and lets us gain deep insights into system behavior and helps us detect abnormal activities in applications, containers, underlying host or Kubernetes orchestrator itself. We like Falco's capability to detect threats without injecting third-party code or sidecar containers.","blip_selector":"falco","name":"Falco","display_name":"Falco","url":"/radar/tools/falco","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911067,"quadrant":"tools","volume_date":"2019-11","description":"asdf-vm is a command-line tool to manage runtime versions of multiple languages, per project. It's similar to other command-line version management tools, such as RVM for Ruby and nvm for Node.js, with the advantage of an extensible plugin architecture to handle multiple languages. Its list of current plugins include many languages as well as tools such as Bazel or tflint, whose runtime version you may need to manage per project.","blip_selector":"asdf-vm","name":"asdf-vm","display_name":"asdf-vm","url":"/radar/tools/asdf-vm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911059,"quadrant":"tools","volume_date":"2019-11","description":"Google brings us Skaffold, an open-source tool to automate local development workflows, including deployment on Kubernetes. Skaffold detects changes in source code and triggers workflows to build, tag and deploy into a K8s cluster including capturing application logs back to the command line. The workflows are pluggable with different build and deployment tools, but this comes with an opinionated default configuration to make it easier to get started.","blip_selector":"skaffold","name":"Skaffold","display_name":"Skaffold","url":"/radar/tools/skaffold","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911101,"quadrant":"tools","volume_date":"2019-11","description":"Given the growing amount of weighty decisions that are derived from large data sets, either directly or as training input for machine learning models, it's important to understand the gaps, flaws and potential biases in your data. Google's Facets project provides two helpful tools in this space: Facets Overview and Facets Dive. Facets Overview visualizes the distribution of values for features in a data set, can show training and validation set skew and can be used to compare multiple data sets; Facets Dive is for drilling down and visualizing individual data points in large data sets, using different visual dimensions to explore the relationships between attributes. They're both useful tools in carrying out ethical bias testing.","blip_selector":"facets","name":"Facets","display_name":"Facets","url":"/radar/tools/facets","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911062,"quadrant":"tools","volume_date":"2019-11","description":"AWSume is a convenient script to manage AWS session tokens and assume role credentials from the command line. We find AWSume quite handy when we deal with multiple AWS accounts at the same time. Instead of specifying profiles individually in every command, the script reads from the CLI cache and exports them to environment variables. As a result, both the commands and AWS SDKs pick up the right credentials.","blip_selector":"awsume","name":"AWSume","display_name":"AWSume","url":"/radar/tools/awsume","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911072,"quadrant":"tools","volume_date":"2019-11","description":"Building containerized applications can require complex configurations in development environments and on build agents. If you're building a Java application and use Docker, you might consider using Google's Jib. Jib is an open-source plugin supporting both Maven and Gradle. The Jib plugin uses information from your build config to build your application directly as a Docker image without requiring a Dockerfile or Docker daemon. Jib optimizes around image layering, promising to speed up subsequent builds.","blip_selector":"jib","name":"Jib","display_name":"Jib","url":"/radar/tools/jib","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911065,"quadrant":"tools","volume_date":"2019-11","description":"We're seeing increased use of Binary attestation for securing the software supply chain, particularly within regulated industries. The currently favored approaches seem to involve either building a custom system for implementing the binary verification or relying on a cloud vendor's service. We're encouraged to see the open-source in-toto enter this space. in-toto is a framework for cryptographically verifying every component and step along the path to production for a software artifact. The project includes a number of integrations into many widely used build, container auditing and deployment tools. A software supply chain tool can be a critical piece of an organization's security apparatus, so we like that as an open-source project, in-toto's behavior is transparent, and its own integrity and supply chain can be verified by the community. We'll have to wait and see if it'll gain a critical mass of users and contributors to compete in this space.","blip_selector":"in-toto","name":"in-toto","display_name":"in-toto","url":"/radar/tools/in-toto","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911074,"quadrant":"tools","volume_date":"2019-11","description":"Increasingly we're seeing powerful Internet of Things devices that run Linux rather than a special embedded OS. In order to reduce resource usage and decrease the attack surface, it makes sense to build a custom Linux distribution that only contains the tools and dependencies needed to run the software on the device. In this context the Yocto Project has renewed relevance as a tool to create a Linux distribution tailored to the needs of a specific case. The learning curve is steep and due to its flexibility, it can be easy to do the wrong thing. However, over the many years of its existence, the Yocto Project has attracted an active community that can help. Compared to similar tools, it's easier to integrate into a CD workflow and, unlike Android Things or Ubuntu core for example, it's not tied to a specific ecosystem.","blip_selector":"yocto-project","name":"Yocto Project","display_name":"Yocto Project","url":"/radar/tools/yocto-project","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911061,"quadrant":"tools","volume_date":"2019-11","description":"If your application handles sensitive information (such as cryptographic keys) as plain text in memory, there's a high probability that someone could potentially exploit it as an attack vector and compromise the information. Most of the cloud-based solutions often use hardware security modules (HSM) to avoid such attacks. However, if you're in a situation where you need to do this in a self-hosted manner without access to HSMs, then we've found MemGuard to be quite useful. MemGuard acts as a secured software enclave for storage of sensitive information in memory. Although MemGuard is not a replacement for HSMs, it does deploy a number of security tactics such as protection against cold boot attacks, avoiding interference with garbage collection and fortifying with guard pages to reduce the likelihood of sensitive data being exposed.","blip_selector":"memguard","name":"MemGuard","display_name":"MemGuard","url":"/radar/tools/memguard","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911037,"quadrant":"techniques","volume_date":"2019-11","description":"The power and promise of machine learning has created a demand for expertise that outstrips the supply of data scientists who specialize in this area. In response to this skills gap, we've seen the emergence of Automated machine learning (AutoML) tools that purport to make it easy for nonexperts to automate the end-to-end process of model selection and training. Examples include Google's AutoML, DataRobot and the H2O AutoML interface. Although we've seen promising results from these tools, we'd caution businesses against viewing them as the sum total of their machine-learning journey. As stated on the H2O website, \"there is still a fair bit of knowledge and background in data science that is required to produce high-performing machine learning models.\" Blind trust in automated techniques also increases the risk of introducing ethical bias or making decisions that disadvantage minorities. While businesses may use these tools as a starting point to generate useful, trained models, we encourage them to seek out experienced data scientists to validate and refine the results.","blip_selector":"automated-machine-learning-automl","name":"Automated machine learning (AutoML)","display_name":"Automated machine learning (AutoML)","url":"/radar/techniques/automated-machine-learning-automl","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904007,"quadrant":"tools","volume_date":"2019-11","description":"Detekt is a static code analysis tool for Kotlin. It provides code smell analysis and complexity reports based on highly configurable rule sets. It can be run from the command line and, using plugins, via Gradle, SonarQube and IntelliJ. Our teams have found great value in using Detekt to maintain high code quality. When analysis and report generation are integrated into a build pipeline, it's obviously important that the reports are checked on a regular basis and the team sets aside time to act on the findings. | Detekt is a static code analysis tool for Kotlin. It finds code smells and code complexity. You can run it from the command line or use its plugins for integration with popular developer tools such as Gradle (to perform code analysis via builds) or SonarQube (to perform code coverage in addition to static code analysis), and IntelliJ. Detekt is a great addition to build pipelines of Kotlin applications.","blip_selector":"detekt","name":"Detekt","display_name":"Detekt","url":"/radar/tools/detekt","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":972,"quadrant":"platforms","volume_date":"2019-11","description":"Apache Flink has seen increasing adoption since our initial assessment on 2016. Flink is recognized as the leading stream-processing engine and also gradually matured in the fields of batch processing and machine learning. One of Flink's key differentiator from other stream-processing engines is its use of consistent checkpoints of an application's state. In the event of failure, the application is restarted and its state is loaded from the latest checkpoint — so that the application can continue processing as if the failure had never happened. This helps us to reduce complexity of building and operating external systems for fault tolerance. We see more and more companies using Flink to build their data-processing platform. | Interest continues to build for Apache Flink, a new-generation platform for scalable distributed batch and stream processing. At the core of Apache Flink is a streaming data-flow engine, with support for tabular (SQL-like), graph-processing and machine learning operations. Apache Flink stands out with feature rich capabilities for stream processing: event time, rich streaming window operations, fault tolerance and exactly-once semantics. The project shows significant ongoing activity, with the latest release (1.1) introducing new datasource/sink integrations as well as improved streaming features. | Apache Flink is a new-generation platform for scalable distributed batch and stream processing. At its core is a streaming data-flow engine. It also supports tabular (SQL-like), graph-processing and machine-learning operations. Apache Flink stands out with feature-rich capabilities for stream processing: event time, rich streaming window operations, fault tolerance and exactly-once semantics. While it hasn't reached version 1.0, it has raised significant community interest due to innovations in stream processing, memory handling, state management and simplicity of configuration.","blip_selector":"apache-flink","name":"Apache Flink","display_name":"Apache Flink","url":"/radar/platforms/apache-flink","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911111,"quadrant":"platforms","volume_date":"2019-11","description":"FoundationDB is an open-source multimodel database, acquired by Apple in 2015 and then open sourced in April 2018. The core of FoundationDB is a distributed key-value store, which provides strict serializability transactions. One of the interesting aspects of FoundationDB is its concept of layers to offer additional models. These layers are essentially stateless components built on top of the core key-value store, such as the Record layer and the Document layer. FoundationDB sets a high standard with its Simulation testing where they run daily tests simulating various system failures. With its performance, rigorous testing and easy operability, FoundationDB is not just a database but can also be used by those looking to build distributed systems where they can use FoundationDB as a core primitive on which to build their system.","blip_selector":"foundationdb","name":"FoundationDB","display_name":"FoundationDB","url":"/radar/platforms/foundationdb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911001,"quadrant":"platforms","volume_date":"2019-11","description":"We've long tracked AR/VR (Augmented/Virtual Reality) in our Radar, but its appeal has been limited to specific platforms and tethering options. The Oculus Quest changes the game, becoming one of the first consumer mass-market standalone VR headsets that requires no tethering or support outside a smartphone. This device opens the door for a huge jump in potential exposure to VR applications, whose demand will in turn drive the market toward more aggressive innovation. We applaud the democratization of VR this device helps usher in and can't wait to see what's on the horizon.","blip_selector":"oculus-quest","name":"Oculus Quest","display_name":"Oculus Quest","url":"/radar/platforms/oculus-quest","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911073,"quadrant":"tools","volume_date":"2019-11","description":"Loki is a visual regression tool that works with Storybook, which we mentioned previously in the context of UI dev environments. With a few lines of configuration, Loki can be used to test all UI components. The preferred mode of operation is using Chrome in a Docker container as this avoids one-pixel differences when tests are run in nonidentical environments. Our experience has been that the tests are very stable, but updates to Storybook tend to cause tests to fail with minor differences. It also seems impossible to test components which use position: fixed but you can work around that by wrapping the component with a fixed.","blip_selector":"loki","name":"Loki","display_name":"Loki","url":"/radar/tools/loki","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":986,"quadrant":"languages-and-frameworks","volume_date":"2019-11","description":"We've seen many successful GraphQL implementations on our projects. We've seen some interesting patterns of use too, including GraphQL for server-side resource aggregation. That said, we've concerns about misuse of this framework and some of the problems that can occur. Examples include performance gotchas around N+1 queries and lots of boilerplate code needed when adding new models, leading to complexity. There are workarounds to these gotchas such as query caching. Even though it's not a silver bullet, we still think it's worth assessing as part of your architecture. | When we look at REST implementations in the wild, we frequently see REST misused to naively retrieve object graphs through chatty interactions between client and server. Facebook's GraphQL is an interesting alternative to REST that might be a better approach for this very common use case. As a protocol for remotely retrieving object graphs, GraphQL has received enormous attention recently. One of GraphQL's most interesting features is its consumer-oriented nature: The structure of a response is driven entirely by the client, not the server. This decouples the consumer and forces the server to obey Postel's law. Client implementations are now available in many programming languages, but we have seen a flurry of interest of Facebook's Relay, a JavaScript framework that was designed to support the React.js stateless component model.","blip_selector":"graphql","name":"GraphQL","display_name":"GraphQL","url":"/radar/languages-and-frameworks/graphql","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911075,"quadrant":"tools","volume_date":"2019-11","description":"Twistlock is a commercial product with build-time and run-time security vulnerability detection and prevention capabilities. These capabilities span protecting VMs, container schedulers and containers to various registries and repositories that applications rely on. Twistlock has helped our teams accelerate development of regulated applications, where application infrastructure and architecture require compliance with, for example, Payment Card Industry (PCI) standards and the Health Insurance Portability and Accountability Act (HIPAA). Our teams have enjoyed the developer experience that Twistlock provides: the ability to run provisioning as code, the easy integration with other common observability platforms, and the out-of-the-box benchmarks to measure the infrastructure against industry-consensus best practices. We run Twistlock with regular runtime scans over our cloud-native applications, particularly when regulatory compliance is required.","blip_selector":"twistlock","name":"Twistlock","display_name":"Twistlock","url":"/radar/tools/twistlock","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911066,"quadrant":"tools","volume_date":"2019-11","description":"It's often very difficult to get a handle on our software estates as they grow ever more complex. Aplas is a new software mapping tool that can be used to create visualizations of our software landscapes in the form of maps. The tool works by ingesting metadata about your existing systems and then displaying a map over which various views can be projected. Ingestion is either a manual process or one that can be automated via APIs. We're pretty excited to see this product evolve and to see what's possible with the automated collection of metadata. It should be possible, for example, to expose architectural fitness functions such as run cost to create visualizations of how much is being spent on cloud infrastructure. Understanding which systems talk to other systems via which technology is another problem we often face and Aplas can visualize it for us.","blip_selector":"aplas","name":"Aplas","display_name":"Aplas","url":"/radar/tools/aplas","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911043,"quadrant":"techniques","volume_date":"2019-11","description":"Today, many organizations' answer to unlocking data for analytical usage is to build a labyrinth of data pipelines. Pipelines retrieve data from one or multiple sources, cleanse it and then transform and move it to another location for consumption. This approach to data management often leaves the consuming pipelines with the difficult task of verifying the inbound data's integrity and building complex logic to cleanse the data to meet its required level of quality. The fundamental problem is that the source of the data has no incentive and accountability for providing quality data to its consumers. For this reason, we strongly advocate for data integrity at the origin, by which we mean, any source that provides consumable data must describe its measures of data quality explicitly and guarantee those measures. The main reason behind this is that the originating systems and teams are most intimately familiar with their data and best positioned to fix it at the source. Data mesh architecture takes this one step further, comparing consumable data to a product, where data quality and its objectives are integral attributes of every shared data set.","blip_selector":"data-integrity-at-the-origin","name":"Data integrity at the origin","display_name":"Data integrity at the origin","url":"/radar/techniques/data-integrity-at-the-origin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911110,"quadrant":"techniques","volume_date":"2019-11","description":"The trend that started as backend as a service for native mobile apps many years ago is now becoming popular with web applications. We're seeing frameworks such as Gatsby.js that combine static site generation and client-side rendering with third-party APIs. Referred to as JAMstack (the JAM stands for JavaScript, API, and Markup), this approach can provide rich user experiences to web applications that rely mostly on APIs and SaaS offerings. Because the HTML is rendered either in the web browser or at build time, the deployment model is the same as fully statically generated sites, with all its benefits: the attack surface on the server is small and great performance can be achieved with low resource usage. Such deployments are also ideal for a content delivery network. In fact, we toyed with the idea of labelling this technique as CDN first applications.","blip_selector":"jamstack","name":"JAMstack","display_name":"JAMstack","url":"/radar/techniques/jamstack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1246,"quadrant":"languages-and-frameworks","volume_date":"2019-11","description":"Several of our teams use Flutter and really like it. It's a cross-platform framework that enables you to write native mobile apps in Dart. It benefits from Dart and can be compiled into native code and communicates with the target platform without bridge and context switching. Flutter's hot-reload feature is still impressive and provides superfast visual feedback when editing code. We're confident in recommending that you try Flutter on one of your projects. | Flutter is a cross-platform framework that enables you to write native mobile apps in Dart. It benefits from Dart and can be compiled into native code and communicates with the target platform without bridge and context switching—something that can cause performance bottlenecks in frameworks such as React Native or Weex. Flutter's hot-reload feature is impressive and provides superfast visual feedback when editing code. Currently, Flutter is still in beta, but we'll continue keeping an eye on it to see how its ecosystem matures. | Flutter is a cross-platform framework that enables you to write native mobile apps in Dart. It benefits from Dart and can be compiled into native code and communicates with the target platform without bridge and context switching — something that can cause performance bottlenecks in frameworks such as React Native or Weex. Flutter’s hot-reload feature is impressive and provides superfast visual feedback when editing code. Currently Flutter is still in beta, but we’ll continue keeping an eye on it to see how its ecosystem matures.","blip_selector":"flutter","name":"Flutter","display_name":"Flutter","url":"/radar/languages-and-frameworks/flutter","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201911038,"quadrant":"techniques","volume_date":"2019-11","description":"Zhong Tai has been a buzzword in the Chinese IT industry for years, but it has yet to catch on in the West. At its core, Zhong Tai is an approach to delivering encapsulated business models. It's designed to help a new breed of small businesses deliver first-rate services without the costs of traditional enterprise infrastructure and enabling existing organizations to bring innovative services to market at breakneck speeds. The Zhong Tai strategy was originally proposed by Alibaba and soon followed by many Chinese Internet companies, because their business model is digital native, making it suitable to replicate for new markets and sectors. Nowadays, more Chinese firms are using Zhong Tai as a lever for digital transformation.","blip_selector":"zhong-tai","name":"Zhong Tai","display_name":"Zhong Tai","url":"/radar/techniques/zhong-tai","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911028,"quadrant":"languages-and-frameworks","volume_date":"2019-11","description":"Fairseq is a sequence-to-sequence modelling toolkit by Facebook AI Research that allows researchers and developers to train custom models for translation, summarization, language modeling and other NLP tasks. For users of PyTorch, this is a good choice. It provides reference implementations of various sequence-to-sequence models; supports distributed training across multiple GPUs and machines; is very extensible; and has a bunch of pretrained models, including RoBERTa which is an optimization on top of BERT.","blip_selector":"fairseq","name":"Fairseq","display_name":"Fairseq","url":"/radar/languages-and-frameworks/fairseq","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911024,"quadrant":"languages-and-frameworks","volume_date":"2019-11","description":"When using HTML and related technologies to produce books and other print output, the question of pagination must be considered. This includes page counters, repeated elements in headers and footers, as well as mechanisms to avoid awkward page breaks. Paged.js is an open-source library that implements a series of polyfills for the Paged Media and Generated Content for Paged Media CSS modules. It is still experimental but fills an important gap in the \"write once, publish everywhere\" story for HTML.","blip_selector":"paged-js","name":"Paged.js","display_name":"Paged.js","url":"/radar/languages-and-frameworks/paged-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201911068,"quadrant":"tools","volume_date":"2019-11","description":"Docker Notary is an OSS tool that enables signing of assets such as images, files and containers. This means that the provenance of assets can be asserted which is superuseful in regulated environments and better practice everywhere. As an example, when a container is created, it's signed by a private key and a hash, tied to the publisher's identity, stored as metadata. Once published, the provenance of the container (or other asset) can be checked using the image hash and the publisher's public key. There are publicly available, trusted registries such as the Docker Trusted Registry, but it's also possible to run your own. Our teams have reported some spiky edges running local Notary servers and suggest using a registry that includes Notary where possible.","blip_selector":"docker-notary","name":"Docker Notary","display_name":"Docker Notary","url":"/radar/tools/docker-notary","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201911031,"quadrant":"techniques","volume_date":"2019-11","description":"When adopting continuous delivery (CD) successfully, teams strive to make the various test environments look as close to production as possible. This allows them to avoid bugs that would otherwise only show themselves in the production environment. This remains just as valid for embedded and Internet of Things software; if we don't run our tests in realistic environments we can expect to find some bugs for the first time in production. Testing using real devices helps avoid this issue by making sure the right devices are available in the CD pipeline.","blip_selector":"testing-using-real-device","name":"Testing using real device","display_name":"Testing using real device","url":"/radar/techniques/testing-using-real-device","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904018,"quadrant":"tools","volume_date":"2019-04","description":"One of the challenges of search is ensuring the most relevant results for the user appear at the top of the list. This is where learning to rank (LTR) can help. LTR is the process of applying machine learning to rank documents retrieved by a search engine. If you're using Elasticsearch, you can achieve search-relevant ranking with the Elasticsearch LTR plugin. The plugin uses RankLib for generating the models during the training phase. Then, when querying Elasticsearch, you can use this plugin to \"rescore\" the top results. We've used it in a few projects and have been happy with the results. There's also an equivalent LTR solution for Solr users.","blip_selector":"elasticsearch-ltr","name":"Elasticsearch LTR","display_name":"Elasticsearch LTR","url":"/radar/tools/elasticsearch-ltr","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904022,"quadrant":"tools","volume_date":"2019-04","description":"Cage is an open-source wrapper around Docker Compose that lets you configure and run multiple dependent components as a larger application. It lets you orchestrate the execution of components such as Docker images, service source code from repo, scripts to load datastores and pods, which are containers that run together as a unit. Cage uses the Docker Compose v2 configuration file format. It addresses some of the Docker Compose gaps such as supporting multiple environments, including the dev environment for running a distributed application on the local developer machine and the test environment for running integration tests and production.","blip_selector":"cage","name":"Cage","display_name":"Cage","url":"/radar/tools/cage","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201904058,"quadrant":"techniques","volume_date":"2019-04","description":"Change data capture (CDC) is a very powerful technique for pulling database changes out of a system and performing some actions on that data. One of the most popular ways of doing this is to use the database's transaction log to identify changes and then publish those changes directly onto an event bus that can be consumed by other services. This works very well for use cases such as breaking monoliths into microservices but when used for first-class integration between microservices, this leads to puncturing encapsulation and leaking the source service's data layer into the event contract. We've talked about domain scoped events and other techniques that emphasize the importance of having our events model our domain properly. We're seeing some projects use CDC for publishing row-level change events and directly consuming these events in other services. This puncturing of encapsulation with change data capture can be a slippery slope leading to fragile integrations and we would like to call this out with this blip.","blip_selector":"puncturing-encapsulation-with-change-data-capture","name":"Puncturing encapsulation with change data capture","display_name":"Puncturing encapsulation with change data capture","url":"/radar/techniques/puncturing-encapsulation-with-change-data-capture","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904029,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Aeron is an efficient and reliable peer-to-peer message transport. It provides a replicated persistent log of messages via a number of media drivers, including HTTP, UDP and TCP. It also supports persistent storage of message streams for later replay. For many applications, Aeron may be overkill because it operates at a pretty low level (OSI Layer 4 conceptually), but it's peer-to-peer design and low (and predictable) latency are useful in a number of use cases. Indeed, we've found it to be useful in certain machine learning applications as well as playing a part in event-driven architectures. We think it's worth pointing out that alternative messaging protocols exist that don't require additional services such as Apache Kafka to be run.","blip_selector":"aeron","name":"Aeron","display_name":"Aeron","url":"/radar/languages-and-frameworks/aeron","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1324,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Much like Cypress and TestCafe, Puppeteer is one of the web UI testing tools garnering praise from our teams. Puppeteer can have fine-grained control over headless browsers, obtain time-trace for performance diagnostics and more. Our teams have found Puppeteer to be stable as well as faster and more flexible than alternatives based on WebDriver. | In the previous Radar we mentioned Headless Chrome for front-end test. With the adoption of Chrome DevTools Protocol (CDP) by other browsers a new set of libraries is emerging for browser automation and testing. CDP allows for fine-grained control over the browser even in headless mode. New high-level libraries are being created using CDP for testing and automation. Puppeteer is one of these new libraries. It can drive headless Chrome through a single-page application, obtain time-trace for performance diagnostics and more. Our teams found it faster and also more flexible than alternatives based on WebDriver.","blip_selector":"puppeteer","name":"Puppeteer","display_name":"Puppeteer","url":"/radar/languages-and-frameworks/puppeteer","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904042,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"joi is a schema description language and validator for JavaScript objects. We like that joi is independent of any web application framework, so our teams can use the same schemas across different stacks. You can also use companion libraries to generate Swagger documentation for APIs that validate requests with joi schemas.","blip_selector":"joi","name":"joi","display_name":"joi","url":"/radar/languages-and-frameworks/joi","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904008,"quadrant":"tools","volume_date":"2019-04","description":"Gremlin is a SaaS solution for organizations to conduct chaos experiments and help test the resilience of their systems. It comes with a series of failure attacks — including resource, network and state failures — that can be run ad hoc or on schedule and require minimal setup (especially for Kubernetes users, who can run Helm to install Gremlin). The Gremlin client also has a nice web-based user interface, which makes it easy to execute and manage chaos experiments.","blip_selector":"gremlin","name":"Gremlin","display_name":"Gremlin","url":"/radar/tools/gremlin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9215,"quadrant":"techniques","volume_date":"2019-04","description":"We put polyglot programming on Trial in one of our first Radars to suggest that choosing the right language for the job could significantly boost productivity, and there were new language entrants that were worthy of consideration. We want to reraise this suggestion because we're seeing a new push to standardize language stacks by both developers and enterprises. While we acknowledge that placing no restrictions on language uses can create more problems than it solves, promoting a few languages that support different ecosystems or language features is important for both enterprises to accelerate processes and go live more quickly and developers to have the right tools to solve the problem at hand. | This technique was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This technique was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"polyglot-programming","name":"Polyglot programming","display_name":"Polyglot programming","url":"/radar/techniques/polyglot-programming","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1239,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Our teams report that Apollo has become the library of choice when building a React application that uses GraphQL to access data from a back-end service. Although the Apollo project also provides a server framework and a GraphQL gateway, the Apollo client gets our attention because it simplifies the problem of binding UI components to data served by any GraphQL backend. Put simply, this means less code needs to be written than using REST backends and redux. | Since it was first introduced in the Radar, we’ve seen a steady adoption of GraphQL, particularly as a remote interface for a Backend for Frontend (BFF). As they gain more experience, our teams have reached consensus on Apollo, a GraphQL client, as the preferred way to access GraphQL data from a React application. Although the Apollo project also provides a server framework and a GraphQL gateway, the Apollo client simplifies the problem of binding UI components to data served by any GraphQL backend. Notably, Apollo is used by Amazon AWS in their recent launch of the new AWS AppSync service.","blip_selector":"apollo","name":"Apollo","display_name":"Apollo","url":"/radar/languages-and-frameworks/apollo","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904005,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Pose is a simple CSS-like animation library for React.js, React Native and Vue.js frameworks. It is a declarative motion system that combines the simplicity of CSS syntax with the power and flexibility of JavaScript animations and interactions.","blip_selector":"pose","name":"Pose","display_name":"Pose","url":"/radar/languages-and-frameworks/pose","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":983,"quadrant":"platforms","volume_date":"2019-04","description":"HashiCorp continues to release interesting software. We've featured HashiCorp Vault in March 2017, and tools related to Terraform are all over this edition of the Radar. We've moved Nomad to Trial because we've had positive experiences using it. While Kubernetes continues to gain traction, we like Nomad's general applicability. It's not just limited to running containerized workloads but can be used to schedule just about anything. Java and Golang are supported natively as well as batch and distributed cron jobs. We like its focus on multi- and hybrid-cloud operations, something likely to become more important to avoid sticky clouds and the fact that it does scheduling well. | HashiCorp continues to turn out interesting software. The latest to catch our attention is Nomad, which is competing in the ever-more-populated scheduler arena. Major selling points include not just being limited to containerized workloads, and operating in multi–data center / multiregion deployments.","blip_selector":"nomad","name":"Nomad","display_name":"Nomad","url":"/radar/platforms/nomad","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904028,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"The Chaos Toolkit is one of a number of Chaos Engineering tools that made this edition of the Radar. You use the toolkit to describe and then run repeatable experiments on your infrastructure to understand its resilience in the event of failure. Many of our teams have been using homegrown tools to do this, so it's great to see an open-source project dedicated to the practice. The toolkit already has drivers for AWS, Azure Service Fabric and GCE (among others) and plays nicely with build tools which lets you experiment with automation. The usual caveats apply though, Chaos Engineering is a very powerful technique that is best used on resilience-aware systems, that is, systems that have been built to cope with failure. For that reason, we recommend starting using Chaos Toolkit in your nonproduction environments first.","blip_selector":"chaos-toolkit","name":"Chaos Toolkit","display_name":"Chaos Toolkit","url":"/radar/languages-and-frameworks/chaos-toolkit","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904054,"quadrant":"platforms","volume_date":"2019-04","description":"Time series databases (TSDBs) have been around for some time now. But increasingly they're becoming more mainstream as more use cases naturally fit the time series model. InfluxDB continues to remain a good choice for TSDBs with monitoring being one of its key use cases. TICK Stack is an example of a monitoring solution that has InfluxDB at its heart. Influx 2.0 alpha recently introduced Flux — a scripting language for querying and processing time series data. It's still early days for Flux and the jury's out on its broader adoption beyond InfluxDB, but it promises to be more powerful and expressive than InfluxQL and enables pushing time series analytic workloads to the database. However, clustering support for InfluxDB is only available with the enterprise version which has limited its adoption on some of our projects.","blip_selector":"influxdb","name":"InfluxDB","display_name":"InfluxDB","url":"/radar/platforms/influxdb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1320,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Apache Beam is an open-source unified programming model for defining and executing both batch and streaming data parallel processing pipelines. The Beam model is based on the Dataflow model which allows us to express logic in an elegant way so that we can easily switch between batch, windowed batch or streaming. The big data-processing ecosystem has been evolving quite a lot which can make it difficult to choose the right data-processing engine. One of the key reasons to choose Beam is that it allows us to switch between different runners — a few months ago Apache Samza was added to the other runners it already supports, which include Apache Spark, Apache Flink and Google Cloud Dataflow. Different runners have different capabilities and providing a portable API is a difficult task. Beam tries to strike a delicate balance by actively pulling innovations from these runners into the Beam model and also working with the community to influence the roadmap of these runners. Beam has SDKs in multiple languages including Java, Python and Golang. We've also had success using Scio which provides a Scala wrapper around Beam. | Apache Beam is an open source unified programming model for defining and executing both batch and streaming data-parallel processing pipelines. Beam provides a portable API layer for describing these pipelines independent of execution engines (or runners) such as Apache Spark, Apache Flink or Google Cloud Dataflow. Different runners have different capabilities and providing a portable API is a difficult task. Beam tries to strike a delicate balance by actively pulling innovations from these runners into the Beam model and also working with the community to influence the roadmap of these runners. Beam has a rich set of built-in I/O transformations that cover most of the data pipeline needs and it also provides a mechanism to implement custom transformations for specific use cases. The portable API and extensible IO transformations make a compelling case for assessing Apache Beam for data pipeline needs.","blip_selector":"apache-beam","name":"Apache Beam","display_name":"Apache Beam","url":"/radar/languages-and-frameworks/apache-beam","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":201904060,"quadrant":"techniques","volume_date":"2019-04","description":"For as long as we can remember, what style to use for formatting code has been a matter of personal taste, company policy and heated debate. Finally, the industry appears to be tiring of this endless argument and teams are freeing up surprisingly large amounts of time by forgoing these discussions and just adopting opinionated and automated code formatting tools. Even if you don't agree 100% with the opinions of the various tools, the benefits of focusing on what your code does rather than how it looks is something most teams should be able to get behind. Prettier has been getting our vote for JavaScript, but similar tools, such as Black for Python, are available for many other languages and are increasingly being built-in as we see with Golang and Elixir. The key here is not to spend hours discussing which rules to enforce, but instead pick a tool that is opinionated, minimally configurable and automated — ideally as a pre-commit hook.","blip_selector":"opinionated-and-automated-code-formatting","name":"Opinionated and automated code formatting","display_name":"Opinionated and automated code formatting","url":"/radar/techniques/opinionated-and-automated-code-formatting","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904052,"quadrant":"platforms","volume_date":"2019-04","description":"As a group we have mixed feelings about programming in JavaScript on the server side, especially when the rationale for doing so is simply to avoid polyglot programming. That said, if you decide to use JavaScript or TypeScript on the server, have a look at Deno. Written by Ryan Dahl, the inventor of Node.js, Deno aims to avoid what Ryan considers mistakes that were made in Node.js. It brings a strict sandbox system and built-in dependency and package management, and it supports TypeScript out of the box. Deno is built using Rust and V8.","blip_selector":"deno","name":"Deno","display_name":"Deno","url":"/radar/platforms/deno","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904100,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Laconia is a framework for developing AWS Lambda functions in JavaScript. As interest and use of serverless tech has grown so has the complexity of the applications being built. Laconia is a small, lightweight framework that takes away some of the rough edges we often encounter. It uses dependency injection to isolate your application code from lower-level AWS APIs and provides adaptors for the different events that your application can respond too. It also plays nicely with the Serverless Framework at deploy time. We like small and simple frameworks and Laconia is just that.","blip_selector":"laconia","name":"Laconia","display_name":"Laconia","url":"/radar/languages-and-frameworks/laconia","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904024,"quadrant":"tools","volume_date":"2019-04","description":"Good UI animation could greatly improve user experience. However, to reproduce a designer's delicate animation on an app is usually a challenging task for developers. Lottie is a library for Android, iOS, web, and Windows that parses Adobe After Effects animations exported as JSON with Bodymovin and renders them natively on mobile and on the web. Both designers and developers can continue to use their familiar tools and have a fluent collaboration.","blip_selector":"lottie","name":"Lottie","display_name":"Lottie","url":"/radar/tools/lottie","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1243,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"We've talked about Reactor in the previous Radars. It has continued to gain traction in many of our projects. With the Spring ecosystem embracing Reactor, it has become the dominant implementation of Reactive Streams. Reactive systems come with improved scalability and resilience but with increased cost of debugging and a steeper learning curve. For those projects where this tradeoff is acceptable, Reactor has proven to be a good choice. Some of our projects have observed significant improvements in scalability once they moved to Reactor and the rest of the Reactive stack. With R2DBC we are starting to get reactive support for RDBMS drivers which addresses one of the weaknesses of reactive services. | Reactor is a library for building non-blocking applications on the JVM — version 8 and above — based on the Reactive Streams specification. Reactive programming emphasizes moving from imperative logic to asynchronous, non-blocking and functional style code, especially when dealing with external resources. Reactor implements the reactive stream specification and provides two publisher APIs — Flux (0 to N elements) and Mono (0 or 1 element) — to effectively model push-based stream processing. Reactor project is well suited for microservices architecture and offers back pressure–ready network engines for HTTP, WebSockets, TCP and UDP traffic.","blip_selector":"reactor","name":"Reactor","display_name":"Reactor","url":"/radar/languages-and-frameworks/reactor","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904053,"quadrant":"platforms","volume_date":"2019-04","description":"Ethereum Virtual Machine (EVM) was originally designed for the Ethereum main network. Nowadays, however, most teams no longer want to reinvent blockchain from scratch; instead, they'd like to take EVM beyond Ethereum. We've seen a lot of blockchain teams choose to fork Ethereum (e.g., Quorum) or implement the EVM spec (e.g., Burrow, Pantheon), adding their own designs. The intention is to not only reuse the Ethereum design but also leverage its ecosystem and developer community. To many developers, the concept of \"smart contract\" is almost equivalent to a smart contract written in Solidity. Although Ethereum itself has some constraints, the technology around the EVM ecosystem is booming.","blip_selector":"evm-beyond-ethereum","name":"EVM beyond Ethereum","display_name":"EVM beyond Ethereum","url":"/radar/platforms/evm-beyond-ethereum","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1271,"quadrant":"tools","volume_date":"2019-04","description":"One of the challenges of using cloud services is being able to develop and test locally. LocalStack solves this problem for AWS by providing local test double implementations of a wide range of AWS services, including S3, Kinesis, DynamoDB and Lambda. It builds on top of best-of-breed tools such as Kinesalite, dynalite and Moto and adds isolated processes and error injection functionality. LocalStack is very easy to use, ships with a simple JUnit runner and a JUnit 5 extension and can also run inside a docker container. For many teams, it has become the default for testing services that are deployed on AWS. | One of the challenges of using cloud services is being able to develop and test locally using those services. LocalStack solves this problem for AWS by providing local test double implementations of a wide range of AWS services, including S3, Kinesis, DynamoDB and Lambda. It builds on top of existing best-of-breed tools such as Kinesalite, Dynalite and Moto and adds isolated processes and error injection functionality. LocalStack is very easy to use and ships with a simple JUnit runner and a JUnit 5 extension. We're using it in a few of our projects and have been impressed with it.","blip_selector":"localstack","name":"LocalStack","display_name":"LocalStack","url":"/radar/tools/localstack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":681,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"TypeScript, a statically typed language and superset of JavaScript, has become our sensible default. Large-scale projects benefit most from the type safety. Our developers favor its minimal configuration management, well-integrated IDE support and its ability to refactor code safely and gradually adopt types. With its good repository of TypeScript-type definitions at hand, we benefit from all the rich JavaScript libraries while gaining type safety. | TypeScript is a carefully considered language and its consistently improving tools and IDE support continues to impress us. With a good repository of TypeScript-type definitions, we benefit from all the rich JavaScript libraries while gaining type safety. This is particularly important as our browser-based code base continues to grow. The type safety in TypeScript lets you use IDEs and other tools to provide deeper context into your code and make changes and refactor code with safety. TypeScript, being a superset of JavaScript, and documentation and the community has helped ease the learning curve. | TypeScript is an interesting approach to bringing a new programming language to the browser. With TypeScript, the new language features compile down to normal JavaScript, and yet as a superset of JavaScript it does not feel like a completely new language. It does not represent an either-or proposition and it does not relegate JavaScript to an intermediate execution platform. Many of the language features are based on planned future extensions of JavaScript.","blip_selector":"typescript","name":"TypeScript","display_name":"TypeScript","url":"/radar/languages-and-frameworks/typescript","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904017,"quadrant":"tools","volume_date":"2019-04","description":"One of the challenges in adopting an open-source alternative to popular commercial packages is sorting through the complicated landscape of projects to understand which components you need, which ones play nicely together and exactly which part of a total solution each component covers. This is particularly difficult in the world of observability, where the standard practice is to purchase one comprehensive but pricey package to do it all. OpenAPM makes the open-source selection process for observability tools easier. It displays the current crop of open-source packages classified by component roles, so you can interactively select compatible components. As long as you keep the tool up to date, it should help you navigate through the confusing array of possible tools.","blip_selector":"openapm","name":"OpenAPM","display_name":"OpenAPM","url":"/radar/tools/openapm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1300,"quadrant":"techniques","volume_date":"2019-04","description":"Maintaining proper control over sensitive data is difficult, especially when it's copied outside of a master system of record for backup and recovery purposes. Crypto shredding is the practice of rendering sensitive data unreadable by deliberately overwriting or deleting encryption keys used to secure that data. Considering there are systems, such as audit application or blockchain, that should not or could not delete historical records, this technique is quite useful for privacy protection and GDPR compliance. | Maintaining proper control over sensitive data is difficult, especially when—for backup and recovery purposes—data is copied outside of a master system of record. Crypto shredding is the practice of rendering sensitive data unreadable by deliberately overwriting or deleting encryption keys used to secure that data. For example, an entire table of customer personal details could be encrypted using random keys for each record, with a different table storing the keys. If a customer exercised their \"right to be forgotten,\" we can simply delete the appropriate key, effectively \"shredding\" the encrypted data. This technique can be useful where we're confident of maintaining appropriate control of a smaller set of encryption keys but less confident about control over a larger data set.","blip_selector":"crypto-shredding","name":"Crypto shredding","display_name":"Crypto shredding","url":"/radar/techniques/crypto-shredding","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1272,"quadrant":"tools","volume_date":"2019-04","description":"As more and more teams embrace DesignOps, practices and tooling in this space mature. UI dev environments provide a comprehensive environment for quickly iterating on UI components, focusing on collaboration between user experience designers and developers. We now have a few options in this space: Storybook, React Styleguidist, Compositor and MDX. You can use these tools standalone in component library or design system development as well as embedded in a web application project. Many teams were able to decrease their UI feedback cycles and improve timing of UI work in preparation for development work, which has made using UI dev environments a reasonable default for us. | As more and more teams embrace DesignOps, practices and tooling in this space mature, too. Many of our teams now work with what could be called UI dev environments , which provide a comprehensive environment for quickly iterating on UI components, focusing on collaboration between user experience designers and developers. We now have a few options in this space: Storybook, react-styleguidist, Compositor and MDX. You can use these tools standalone in component library or design system development as well as embedded in a web application project. Rather than spinning up the app, plus a BFF, plus services simply to add a feature to a component, you can start up the Storybook dev server instead.","blip_selector":"ui-dev-environments","name":"UI dev environments","display_name":"UI dev environments","url":"/radar/tools/ui-dev-environments","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904026,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Data scientists and engineers often use libraries such as pandas to perform ad hoc data analysis. Although expressive and powerful, these libraries have one critical limitation: they only work on a single CPU and don't provide horizontal scalability for large data sets. Dask, however, includes a lightweight, high-performance scheduler that can scale from a laptop to a cluster of machines. And because it works with NumPy, pandas and Scikit-learn, Dask looks promising for further assessment.","blip_selector":"dask","name":"Dask","display_name":"Dask","url":"/radar/languages-and-frameworks/dask","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1315,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"MockK is our go-to tool for mocks when writing tests for Kotlin applications. We like to use this library because of its first-class support for Kotlin language features such as coroutines or lambda blocks. As a native library, it helps our teams to write clean and concise code on testing Kotlin applications instead of using the inconvenient wrappers of Mockito or PowerMock. | MockK is a library for mocking written in Kotlin. Its main philosophy is to provide first-class support for Kotlin language features such as Coroutines or lambda blocks. As a native library, it helps our teams to write clean and concise code on testing Kotlin applications instead of using incommodious wrappers of Mockito or PowerMock.","blip_selector":"mockk","name":"MockK","display_name":"MockK","url":"/radar/languages-and-frameworks/mockk","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1331,"quadrant":"platforms","volume_date":"2019-04","description":"SPIFFE standardization of service identity has been an important step in enabling turnkey solutions for end-to-end encryption and mutual authentication between services. The SPIFFE standards are backed by the OSS SPIFFE Runtime Environment (SPIRE), which automatically delivers cryptographically provable identities to services. Istio also uses SPIFFE by default. SPIFFE enables many use cases, including identity translation, OAuth client authentication, mTLS \"encryption everywhere\" and workload observability. ThoughtWorks is actively working with the Istio and SPIFFE communities to bridge the gap between legacy service identity providers and SPIFFE-based identities so that mTLS can be used everywhere between services, inside a service mesh and outside. | Making key elements of Google's groundbreaking, high-scale platform available as open source offerings appears to have become a trend. In the same way that HBASE drew on BigTable and Kubernetes drew on Borg, SPIFFE is now drawing upon Google's LOAS to bring to life a critical cloud-native concept called workload identity. The SPIFFE standards are backed by the OSS SPIFFE Runtime Environment (SPIRE), which automatically delivers cryptographically provable identities to software workloads. Although SPIRE isn't quite ready for production use, we see tremendous value in a platform-agnostic way to make strong identity assertions between workloads in modern, distributed IT infrastructures. SPIRE supports many use cases, including identity translation, OAuth client authentication, mTLS \"encryption everywhere,\" and workload observability. Istio uses SPIFFE by default.","blip_selector":"spiffe","name":"SPIFFE","display_name":"SPIFFE","url":"/radar/platforms/spiffe","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904012,"quadrant":"platforms","volume_date":"2019-04","description":"Byzantine fault tolerance (BFT) is one of the fundamental problems in cryptocurrency and blockchain systems. It requires overall system agreement on a single data value in the presence of a number of arbitrary faulty processes, which includes malicious fraud. Tendermint is a BFT state machine replication engine that lets you implement your own blockchain systems. The consensus engine, Tendermint Core, takes over the peer-to-peer communication and consensus part, you just need to implement the rest of the application (e.g., construct transaction and verify cryptographic signature) and communicate with Tendermint Core through ABCI. Some blockchain implementations have already chosen Tendermint as their consensus engine.","blip_selector":"tendermint","name":"Tendermint","display_name":"Tendermint","url":"/radar/platforms/tendermint","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904045,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Room is a persistence library to access SQLite on Android. It makes database access code much simpler, with minimal boilerplate code, and more robust, with compile-time verification of SQL queries. Our developers like its complete integration with observable queries, using LiveData. Room is one of the Android Jetpack components that were created to make application development on Android easier.","blip_selector":"room","name":"Room","display_name":"Room","url":"/radar/languages-and-frameworks/room","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904036,"quadrant":"tools","volume_date":"2019-04","description":"AVA is a test runner for Node.js. Even though JavaScript is single-threaded, IO in Node.js can happen in parallel because of its asynchronous nature. AVA takes advantage of this and runs your tests concurrently, which is especially beneficial for IO-heavy tests. In addition, test files are run in parallel as separate processes, giving you even better performance and an isolated environment for each test file. AVA is a lightweight option, when compared to full-featured frameworks such as Jest. It is opinionated and forces you to write atomic test cases.","blip_selector":"ava","name":"AVA","display_name":"AVA","url":"/radar/tools/ava","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904023,"quadrant":"tools","volume_date":"2019-04","description":"Setting up highly available PostgreSQL instances can be tricky, which is why we like Patroni — it helps us speed up the setup of PostgreSQL clusters. Stolon is another tool that we've used successfully to run high-availability (HA) clusters of PostgreSQL instances in production using Kubernetes. Although PostgreSQL supports streaming replication out of the box, the challenge in an HA setup is to assure that the clients always connect to the current master. We like that Stolon enforces the connection to the right PostgreSQL master by actively closing connections to unelected masters and routing requests to the active one.","blip_selector":"stolon","name":"Stolon","display_name":"Stolon","url":"/radar/tools/stolon","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1278,"quadrant":"tools","volume_date":"2019-04","description":"We have good experience using \"post-Selenium\" web UI testing tools such as Cypress, TestCafe and Puppeteer. TestCafe lets you write tests in JavaScript or TypeScript and runs in-browser tests. TestCafe has several useful features that include out-of-the-box parallel execution and HTTP request mocking. TestCafe uses an asynchronous execution model with no explicit wait times, which results in much more stable test suites. Its selector API makes it easier to implement PageObject patterns. TestCafe recently released version 1.0.x, which improved stability and functionality. | Our teams are reporting good success with TestCafe, a JavaScript-based browser test automation tool. TestCafe allows you to write tests in JavaScript or TypeScript and runs tests in any browser that supports JavaScript. TestCafe has several useful features including out-of-the-box parallel execution and HTTP request mocking. TestCafe uses an asynchronous execution model with no explicit wait times, which results in much more stable test suites.","blip_selector":"testcafe","name":"TestCafe","display_name":"TestCafe","url":"/radar/tools/testcafe","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904041,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"fastai is an open-source Python library that simplifies training fast and accurate neural nets. It is built on top of PyTorch and has become a popular tool for our data scientists. fastai simplifies painful aspects of model training such as preprocessing and loading data down to a few lines of code. It's built on deep learning best practices and has out-of-the-box support for computer vision, natural language processing (NLP) and more. The founders' motivation has been to create an easy-to-use library for deep learning and an improved successor to Keras. GCP, AWS and Azure all have already included fastai in their machine images. The creators of fastai, acknowledging the speed and safety limitations of Python, have announced embracing Swift as an alternative language for deep learning. We'll be closely watching this space.","blip_selector":"fastai","name":"fastai","display_name":"fastai","url":"/radar/languages-and-frameworks/fastai","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904050,"quadrant":"platforms","volume_date":"2019-04","description":"In previous Radars we've discussed PostgreSQL for NoSQL. PostgreSQL's maturity and extensibility have led to a steady stream of innovative persistence stores built on the Postgres engine. One that caught our attention is TimescaleDB, a database that allows fast writes and optimized queries over time-series data. Albeit not (yet) as full-featured as InfluxDB, TimescaleDB offers an alternative data model and querying capability. You should evaluate TimescaleDB if you have modest scalability needs, prefer to use SQL and appreciate the stability and familiar administrative interface that PostgreSQL offers.","blip_selector":"timescaledb","name":"TimescaleDB","display_name":"TimescaleDB","url":"/radar/platforms/timescaledb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1154,"quadrant":"tools","volume_date":"2019-04","description":"Over the past couple of years, we've noticed a steady rise in the popularity of analytics notebooks. These are Mathematica-inspired applications that combine text, visualization and code in a living, computational document. Jupyter Notebooks are widely used by our teams for prototyping and exploration in analytics and machine learning. We've moved Jupyter to Adopt for this issue of the Radar to reflect that it has emerged as the current default for Python notebooks. However, we caution to use Jupyter Notebooks in production. | Over the last couple of years, we've noticed a steady rise in the popularity of analytics notebooks. These are Mathematica-inspired applications that combine text, visualization and code in a living, computational document. Increased interest in machine learning — along with the emergence of Python as the programming language of choice for practitioners in this field — has focused particular attention on Python notebooks, of which Jupyter seems to be gaining the most traction among ThoughtWorks teams. People seem to keep finding creative uses for Jupyter beyond a simple analytics tool. For example, see Jupyter for automated testing. | Over the last couple of years, we've noticed a steady rise in the popularity of analytics notebooks. These are Mathematica-inspired applications that combine text, visualization and code in a living, computational document. In a previous edition, we mentioned GorillaREPL, a Clojure variant of these. But increased interest in machine learning — along with the emergence of Python as the programming language of choice for practitioners in this field — has focused particular attention on Python notebooks, of which Jupyter seems to be gaining the most traction among ThoughtWorks teams.","blip_selector":"jupyter","name":"Jupyter","display_name":"Jupyter","url":"/radar/tools/jupyter","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904014,"quadrant":"tools","volume_date":"2019-04","description":"Humio is a fairly new player in the log management space. It's been built from the ground up to be super fast at both log ingestion and query using its built-in query language on top of a custom-designed time series database. It integrates with just about everything out there from an ingestion, visualization and alerting perspective. The log management space has been dominated by Splunk and the ELK Stack, so having alternatives is a good thing. We'll be watching Humio's development with interest.","blip_selector":"humio","name":"Humio","display_name":"Humio","url":"/radar/tools/humio","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904048,"quadrant":"platforms","volume_date":"2019-04","description":"Object storage is a popular choice for storing unstructured data and in a few cases structured data in the cloud. We do discourage the use of generic cloud but if you want to minimize the risk of cloud stickiness for object storage, we've found MinIO quite helpful. With an S3-compatible API layer, MinIO abstracts object storage across cloud providers, including AWS, Azure and Google Cloud Platform (GCP), and we've used it successfully in products with flexible target infrastructures from data centers to cloud providers.","blip_selector":"minio","name":"MinIO","display_name":"MinIO","url":"/radar/platforms/minio","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904067,"quadrant":"platforms","volume_date":"2019-04","description":"Even in the era of deep learning, statistical models still play a role in business decision support. Time series models are widely used to forecast inventories, demand, customer traffic, and so on. Hand-crafting these models so that they're robust and flexible has typically been the role of either specialized statisticians or large commercial software vendors. Prophet is an open-source alternative to commercial forecasting packages that can be programmed in R or Python. Facebook claims to use Prophet internally for business forecasting at scale and has made it available as an open-source package for anyone to use. We like that Prophet removes some of the tedium of model construction, maintenance and data manipulation so that human analysts and subject matter experts can focus on doing what they do best.","blip_selector":"prophet","name":"Prophet","display_name":"Prophet","url":"/radar/platforms/prophet","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1138,"quadrant":"techniques","volume_date":"2019-04","description":"Service mesh is an approach to operating a secure, fast and reliable microservices ecosystem. It has been an important stepping stone in making it easier to adopt microservices at scale. It offers discovery, security, tracing, monitoring and failure handling. It provides these cross-functional capabilities without the need for a shared asset such as an API gateway or baking libraries into each service. A typical implementation involves lightweight reverse-proxy processes, aka sidecars, deployed alongside each service process in a separate container. Sidecars intercept the inbound and outbound traffic of each service and provide cross-functional capabilities mentioned above. This approach has relieved the distributed service teams from building and updating the capabilities that the mesh offers as code in their services. This has lead to an even easier adoption of polyglot programming in a microservices ecosystem. Our teams have been successfully using this approach with open source projects such as Istio and we will continue to monitor other open service mesh implementations such as Linkerd closely. | As large organizations transition to more autonomous teams owning and operating their own microservices, how can they ensure the necessary consistency and compatibility between those services without relying on a centralized hosting infrastructure? To work together efficiently, even autonomous microservices need to align with some organizational standards. A service mesh offers consistent discovery, security, tracing, monitoring and failure handling without the need for a shared asset such as an API gateway or ESB. A typical implementation involves lightweight reverse-proxy processes deployed alongside each service process, perhaps in a separate container. These proxies communicate with service registries, identity providers, log aggregators and other services. Service interoperability and observability are gained through a shared implementation of this proxy but not a shared runtime instance. We've advocated for a decentralized approach to microservices management for some time and are happy to see this consistent pattern emerge. Open source projects such as Linkerd and Istio will continue to mature and make service meshes even easier to implement. | As large organizations transition to more autonomous teams owning and operating their own microservices, how can they ensure the necessary consistency and compatibility between those services without relying on a centralized hosting infrastructure? To work together efficiently, even autonomous microservices need to align with some organizational standards. A service mesh offers consistent discovery, security, tracing, monitoring and failure handling without the need for a shared asset such as an API gateway or ESB. A typical implementation involves lightweight reverse-proxy processes deployed alongside each service process, perhaps in a separate container. These proxies communicate with service registries, identity providers, log aggregators, and so on. Service interoperability and observability are gained through a shared implementation of this proxy but not a shared runtime instance. We've advocated for a decentralized approach to microservice management for some time and are happy to see this consistent pattern emerge. Open source projects such as linkerd and Istio will continue to mature and make service meshes even easier to implement. | As large organizations transition to more autonomous teams owning and operating their own microservices, how can they ensure the necessary consistency and compatibility between those services without relying on a centralized hosting infrastructure? To work together efficiently, even autonomous microservices need to align with some organizational standards. A service mesh offers consistent discovery, security, tracing, monitoring and failure handling without the need for a shared asset such as an API gateway or ESB. A typical implementation involves lightweight reverse-proxy processes deployed alongside each service process, perhaps in a separate container. These proxies communicate with service registries, identity providers, log aggregators, and so on. Service interoperability and observability are gained through a shared implementation of this proxy but not a shared runtime instance. We've advocated for a decentralized approach to microservice management for some time and are happy to see this consistent pattern emerge. Open source projects such as linkerd and Istio will continue to mature and make service meshes even easier to implement.","blip_selector":"service-mesh","name":"Service mesh","display_name":"Service mesh","url":"/radar/techniques/service-mesh","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201904003,"quadrant":"tools","volume_date":"2019-04","description":"AWS CloudFormation is a proprietary declarative language to provision AWS infrastructure as code. Handwriting CloudFormation files is often a default approach to bootstrap AWS infrastructure automation. Although this might be a sensible way to start a small project, our teams, and the industry at large, have found that handwritten CloudFormation simply does not scale as the infrastructure grows. Noticeable pitfalls of handwritten CloudFormation files for large projects include poor readability, lack of imperative constructs, limited parameter definition and usage, and lack of type checking. Addressing these shortfalls has led to a rich ecosystem of both open-source and custom tooling. We find Terraform a sensible default that not only addresses shortfalls of CloudFormation but also has an active community to add the latest AWS features and fix bugs. In addition to Terraform, you can choose from many other tools and languages, including troposphere, sceptre, Stack Deployment Tool and Pulumi.","blip_selector":"handwritten-cloudformation","name":"Handwritten CloudFormation","display_name":"Handwritten CloudFormation","url":"/radar/tools/handwritten-cloudformation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1206,"quadrant":"techniques","volume_date":"2019-04","description":"In the last year we've seen Chaos Engineering move from a much talked-about idea to an accepted, mainstream approach to improving and assuring distributed system resilience. As organizations large and small begin to implement Chaos Engineering as an operational process, we're learning how to apply these techniques safely at scale. The approach is definitely not for everyone, and to be effective and safe, it requires organizational support at scale. Industry acceptance and available expertise will definitely increase with the appearance of commercial services such as Gremlin and deployment tools such as Spinnaker implementing some Chaos Engineering tools. | In previous editions of the Radar, we've talked about using Chaos Monkey from Netflix to test how a running system is able to cope with outages in production by randomly disabling instances and measuring the results. Chaos Engineering is the nascent term for the wider application of this technique. By running experiments on distributed systems in production, we're able to build confidence that those systems work as expected under turbulent conditions. A good place to start understanding this technique is the Principles of Chaos Engineering website. | In previous editions of the Radar, we've talked about using Chaos Monkey from Netflix to test how a running system is able to cope with outages in production by randomly disabling instances and measuring the results. Chaos Engineering is the nascent term for the wider application of this technique. By running experiments on distributed systems in production, we're able to build confidence that those systems work as expected under turbulent conditions. A good place to start understanding this technique is the Principles of Chaos Engineering website.","blip_selector":"chaos-engineering","name":"Chaos Engineering","display_name":"Chaos Engineering","url":"/radar/techniques/chaos-engineering","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1303,"quadrant":"techniques","volume_date":"2019-04","description":"Humans and machines use secrets throughout the value stream of building and operating software. The build pipelines need secrets to interface with secure infrastructures such as container registries, the applications use API keys as secrets to get access to business capabilities, and the service-to-service communications are secured using certificates and keys as secrets. You can set and retrieve these secrets in different ways. We've long cautioned developers about using source code management for storing secrets. We've recommended decoupling secret management from source code and using tools such as git-secrets and Talisman to avoid storing secrets in the source code. We've been using secrets as a service as a default technique for storing and accessing secrets. With this technique you can use tools such as Vault or AWS Key Management Service (KMS) to read/write secrets over an HTTPS endpoint with fine-grained levels of access control. Secrets as a service uses external identity providers such as AWS IAM to identify the actors who request access to secrets. Actors authenticate themselves with the secrets service. For this process to work, it's important to automate bootstrapping the identity of the actors, services and applications. Platforms based on SPIFFE have improved the automation of assigning identities to services. | We've long cautioned people about the temptation to check secrets into their source code repositories. Previously, we've recommended decoupling secret management from source code. However, now we're seeing a set of good tools emerge that offer secrets as a service. With this approach, rather than hardwiring secrets or configuring them as part of the environment, applications retrieve them from a separate process. Tools such as Vault by HashiCorp let you manage secrets separately from the application and enforce policies such as frequent rotation externally.","blip_selector":"secrets-as-a-service","name":"Secrets as a service","display_name":"Secrets as a service","url":"/radar/techniques/secrets-as-a-service","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904006,"quadrant":"tools","volume_date":"2019-04","description":"As developers used to pushing many small commits daily, we rely on monitors to notify us when builds go green. AnyStatus is a lightweight Windows desktop app that rolls up metrics and events from various sources into one place. Examples include build results and releases, health checks for different services and OS metrics. Think of it as CCTray on steroids. It's also available as a Visual Studio plugin.","blip_selector":"anystatus","name":"AnyStatus","display_name":"AnyStatus","url":"/radar/tools/anystatus","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1321,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"WebFlux is the Spring Framework implementation of Reactive Streams. We see a rise in reactive programming models across our teams in general and the use of WebFlux in teams who are working in the Spring ecosystem. It's best used in large microservices ecosystems where the high performance of the requests is a major concern. It allows overlapping request processing asynchronously without the complications of using multiple threads. WebFlux uses Reactor as its reactive library but it is interoperable with other reactive libraries via Reactive Streams. It uses Netty as its underlying high-performance communications engine. Although we encourage using Reactive Streams, adopting this programming model requires a significant shift in thinking. | Spring Framework 5, released over a year ago, embraces reactive streams, a standard for asynchronous stream processing with non-blocking backpressure. The WebFlux module introduces a reactive alternative to the traditional Spring MVC module for writing web applications in the Spring ecosystem. After working with it on a number of applications, our teams have come away impressed and report that the reactive (functional) approach improves code readability and system throughput. They do note, though, that adopting WebFlux requires a significant shift in thinking and recommend to factor this into the decision to choose WebFlux over Spring MVC.","blip_selector":"webflux","name":"WebFlux","display_name":"WebFlux","url":"/radar/languages-and-frameworks/webflux","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201904097,"quadrant":"techniques","volume_date":"2019-04","description":"We've seen organizations successfully move from very infrequent releases to a higher cadence by using the release train concept. The release train is a technique for coordinating releases across multiple teams or components that have runtime dependencies. All releases happen on a fixed and reliable schedule regardless of whether all expected features are ready (the train doesn't wait for you — if you miss it you wait for the next one). Although we wholeheartedly endorse discipline around regularly releasing and demoing working software, we've experienced serious drawbacks with the approach over the medium to long term as it reinforces temporal coupling around sequencing of changes and can degrade quality as teams rush to complete features. We prefer to focus on the architectural and organizational approaches necessary to support independent releases. Although the train can be a useful forcing function for speeding up slower teams, we've also seen it as imposing an upper limit on how quickly faster-moving teams can move. We believe that it is a technique that should be approached with a good degree of caution, if at all.","blip_selector":"release-train","name":"Release train","display_name":"Release train","url":"/radar/techniques/release-train","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1255,"quadrant":"platforms","volume_date":"2019-04","description":"AWS Fargate, the docker-as-a-service option on AWS, is now widely available across regions. It's a great solution for situations in which teams want to run Docker containers, because AWS Lambda functions aren't powerful enough, without having to manage EC2 instances or Kubernetes clusters. Our teams report generally positive experiences with Fargate; however, the convenience of this managed service can come at a cost, in financial terms. | AWS Fargate is a recent entry into the docker-as-a-service space, currently limited to the US-East-1 region. For teams using AWS Elastic Container Service (ECS), AWS Fargate is a good alternative without having to manage, provision and configure any underlying EC2 instances or clusters. Fargate allows defining (ECS or EKS – ECS for Kubernetes) tasks as a Fargate type, and they will run on the AWS Fargate infrastructure. If you like the focus on business functionality that AWS Lambda gives you, Fargate is the closest you can get when applications can't be deployed as single functions.","blip_selector":"aws-fargate","name":"AWS Fargate","display_name":"AWS Fargate","url":"/radar/platforms/aws-fargate","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904064,"quadrant":"techniques","volume_date":"2019-04","description":"As developers at ThoughtWorks we're acutely aware of the ethics of the work we do. As society becomes ever more reliant on technology, it's important that we consider ethics when making decisions as software development teams. Several toolkits have emerged that can help us think through some of the future implications of the software we're building. They include Tarot Cards of Tech and Ethical OS, which we've had good feedback on. Ethical OS is a thinking framework and a set of tools that drive discussions around the ethics of building software. The framework is a collaboration between the Institute for the Future and the Tech and Society Solutions Lab. It's based on a practical set of risk zones, such as addiction and the dopamine economy, plus a number of scenarios to drive conversation and discussion.","blip_selector":"ethical-os","name":"Ethical OS","display_name":"Ethical OS","url":"/radar/techniques/ethical-os","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904038,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"ReasonML is an interesting new language based on OCaml with a sprinkling of C-like syntax and uses JavaScript as the default compilation target. Created by Facebook, it allows embedded JavaScript snippets and JSX templating with good React integration. It aims to be approachable for JavaScript developers and leverages that ecosystem, while providing type safety in a functional language.","blip_selector":"reasonml","name":"ReasonML","display_name":"ReasonML","url":"/radar/languages-and-frameworks/reasonml","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904043,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Formik is a useful higher-order component for making the surprisingly verbose and complex job of handling forms in React much easier. It localizes state management, assists with submission and optionally uses Yup to simplify data validation.","blip_selector":"formik","name":"Formik","display_name":"Formik","url":"/radar/languages-and-frameworks/formik","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":815,"quadrant":"tools","volume_date":"2019-04","description":"Terraform, is rapidly becoming a de facto choice for creating and managing cloud infrastructures by writing declarative definitions. The configuration of the servers instantiated by Terraform is usually left to Puppet, Chef or Ansible. We like Terraform because the syntax of its files is quite readable and because it supports a number of cloud providers while making no attempt to provide an artificial abstraction across those providers. The active community will add support for the latest features from most cloud providers. Following our first, more cautious, mention of Terraform almost two years ago, it has seen continued development and has evolved into a stable product with a good ecosystem that has proven its value in our projects. The issue with state file management can now be sidestepped by using what Terraform calls a \"remote state backend.\" We've successfully used AWS S3 for that purpose. | With Terraform, you can manage cloud infrastructure by writing declarative definitions. The configuration of the servers instantiated by Terraform is usually left to tools like Puppet, Chef or Ansible. We like Terraform because the syntax of its files is quite readable and because it supports a number of cloud providers while making no attempt to provide an artificial abstraction across those providers. Following our first, more cautious, mention of Terraform almost two years ago, it has seen continued development and has evolved into a stable product that has proven its value in our projects. The issue with state file management can now be sidestepped by using what Terraform calls a \"remote state backend.\" We've successfully used Consul for that purpose. | With Terraform, you can manage cloud infrastructure by writing declarative definitions. The configuration of the servers instantiated by Terraform is usually left to tools like Puppet, Chef or Ansible. We like Terraform because the syntax of its files is quite readable and because it supports a number of cloud providers while making no attempt to provide an artificial abstraction across those providers. Following our first, more cautious, mention of Terraform almost two years ago, it has seen continued development and has evolved into a stable product that has proven its value in our projects. The issue with state file management can now be sidestepped by using what Terraform calls a \"remote state backend.\" We’ve successfully used Consul for that purpose. | With Terraform, cloud infrastructure can be managed by writing declarative definitions. The configuration of the servers instantiated by Terraform is usually left to tools like Puppet, Chef, or Ansible. We like Terraform because the syntax of its files is quite readable and because it supports multiple cloud providers while making no attempt to provide an artificial abstraction across these providers. At this stage, Terraform is new and not everything is implemented yet. We have also found its state management to be fragile, often needing awkward manual work to untangle.","blip_selector":"terraform","name":"Terraform","display_name":"Terraform","url":"/radar/tools/terraform","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904013,"quadrant":"tools","volume_date":"2019-04","description":"Feature toggles are an important technique in continuous deployment scenarios. We've come across a number of good home-grown solutions, but we do like the approach Flagr takes: a complete feature toggle as a service, distributed as a Docker container. It comes with SDKs for all major languages, has a simple and well-documented REST API and provides a convenient frontend.","blip_selector":"flagr","name":"Flagr","display_name":"Flagr","url":"/radar/tools/flagr","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1337,"quadrant":"platforms","volume_date":"2019-04","description":"The serverless architecture has popularized a FaaS style of programming among developers; it helps developers focus on solving core business problems with independently built and deployed functions that react to an event, run a business process, produce other events in the process and scale down to zero. Historically, proprietary serverless platforms such as AWS Lambda or Microsoft Azure Functions have enabled this programming paradigm. Knative is an open-source Kubernetes-based platform to run FaaS workloads. There are few things that stand out about Knative: it's open source and provider agnostic; it implements the serverless workflow as described in the CNCF Serverless Working Group whitepaper; it ensures cross-service interoperability by implementing its eventing interface consistent with CNCF CloudEvents specification; and, most importantly, it addresses a common challenge of operating a harmonized and yet hybrid FaaS and long-running container-based architecture. It easily integrates with both Istio and Kubernetes. For example, developers can take advantage of roll-out strategies that Istio implements by traffic splitting between different revisions of the functions. Developers can take benefit of Istio-provided observability not only for long-running container services but also for FaaS programs in the same Kubernetes environment. We anticipate that Knative open-source eventing interface will continue to enable new underlying source and destination event integrations. | As application developers, we love to focus on solving core business problems and let the underlying platform handle the boring but difficult tasks of deploying, scaling and managing applications. Although serverless architecture is a step in that direction, most of the popular offerings are tied to a proprietary implementation, which means vendor lock-in. Knative tries to address this by being an open source serverless platform that integrates well with the popular Kubernetes ecosystem. With Knative you can model computations on request in a supported framework of your choice (including Ruby on Rails, Django and Spring among others); subscribe, deliver and manage events; integrate with familiar CI and CD tools; and build containers from source. By providing a set of middleware components for building source-centric and container-based applications that can be elastically scaled, Knative is an attractive platform that deserves to be assessed for your serverless needs.","blip_selector":"knative","name":"Knative","display_name":"Knative","url":"/radar/platforms/knative","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1113,"quadrant":"tools","volume_date":"2019-04","description":"How does an organization give autonomy to delivery teams while still making sure their deployed solutions are safe and compliant? How do you ensure that servers, once deployed, maintain a consistent configuration without drift? InSpec is positioned as a solution for continuous compliance and security, but you can also use it for general infrastructure testing. InSpec allows the creation of declarative infrastructure tests, which can then be continuously run against provisioned environments including production. Our teams particularly praise its extensible design with resources and matchers for multiple platforms. We recommend trialling InSpec as a solution to the problem of assuring compliance and security. | How does a business hand autonomy to delivery teams while still making sure their deployed solutions are safe and compliant? How do you ensure that servers, once deployed, remain secure and compliant over their operational lifetime? These are the problems that InSpec tries to address. InSpec is an infrastructure testing tool inspired by Serverspec, but with modifications that make the tool more useful for security professionals who need to ensure compliance across thousands of servers. Individual tests can be combined into complete security profiles and run remotely from a command line. InSpec is useful for developers but extends to testing deployed production infrastructure continuously, moving toward QA in production. | How does a business hand autonomy to delivery teams while still making sure their deployed solutions are safe and compliant? How do you ensure that servers, once deployed, remain secure and compliant over their operational lifetime? These are the problems that InSpec tries to address. InSpec is an infrastructure testing tool inspired by Serverspec, but with modifications that make the tool more useful for security professionals who need to ensure compliance across thousands of servers. Individual tests can be combined into complete security profiles and run remotely from a command line. InSpec is useful for developers but extends to testing deployed production infrastructure continuously, moving toward QA in production.","blip_selector":"inspec","name":"InSpec","display_name":"InSpec","url":"/radar/tools/inspec","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904033,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"http4k is an HTTP toolkit written in pure Kotlin for serving and consuming HTTP services. One of the key ideas behind http4k is that HTTP apps are modelled by composing two simple functions — HttpHandler and Filter. They derive inspiration from Twitter's \"Your Server as a Function\" paper. It's very lightweight with the core module having no dependencies apart from Kotlin StdLib. Apart from its elegance and simplicity, we also like its emphasis on testability — given that the entities in the libraries are immutable and the routes in the app, as well as the app itself, are just functions, they're super easy to test. One of the things to be aware of, though, is that we don't have nonblocking or coroutines support in http4k yet.","blip_selector":"http4k","name":"http4k","display_name":"http4k","url":"/radar/languages-and-frameworks/http4k","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904049,"quadrant":"platforms","volume_date":"2019-04","description":"Outside the function code itself, applications written as serverless functions are tightly coupled to the cloud platform on which they're hosted. Although events are a common FaaS-triggering mechanism, and every cloud provider supports them in some form, the current proprietary specifications prevent interoperability across clouds. The CloudEvents specification is a burgeoning standard that has been accepted into the CNCF Sandbox. The standard is still in active development but several language bindings exist and Microsoft has announced first-class support in Azure. We're hoping other cloud providers will follow suit.","blip_selector":"cloudevents","name":"CloudEvents","display_name":"CloudEvents","url":"/radar/platforms/cloudevents","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1295,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Kotlin has demonstrated its value beyond mobile app development. When building microservices and shipping software to production, our teams have had good experiences with Ktor. Ktor is a framework that, unlike other web frameworks that support Kotlin, is written in Kotlin, using language features such as coroutines which allow for an asynchronous nonblocking implementation. The flexibility to incorporate different tools for logging, DI or a template engine — in addition to its lightweight architecture — makes Ktor an interesting option for creating RESTful services. | Kotlin is no longer just a great fit for mobile app development. New tools and frameworks have emerged that demonstrate the value of the language for web application development as well. Ktor is one such framework. In contrast to other web frameworks that support Kotlin, Ktor is written in Kotlin, using language features such as coroutines which allows for an asynchronous non-blocking implementation. The flexibility to incorporate different tools for logging, DI or a templates engine—in addition to its lightweight architecture—makes Ktor an interesting option for our teams for creating RESTful services.","blip_selector":"ktor","name":"Ktor","display_name":"Ktor","url":"/radar/languages-and-frameworks/ktor","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904016,"quadrant":"tools","volume_date":"2019-04","description":"We're excited about the impact Kubernetes has had on our industry but also concerned about the operational complexity that comes with it. Keeping a Kubernetes cluster up and running and then managing packages deployed on it requires special skills and time. Operational processes such as upgrades, migrations, backups, among others, can be a full-time job. We think that Kubernetes Operators will play a key role in reducing this complexity. The framework provides a standard mechanism to describe automated operational processes for packages running in a Kubernetes cluster. Although Operators were spearheaded and promoted by RedHat, several community-developed Operators for common open-source packages such as Jaeger, MongoDB and Redis have begun to emerge.","blip_selector":"kubernetes-operators","name":"Kubernetes Operators","display_name":"Kubernetes Operators","url":"/radar/tools/kubernetes-operators","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":201904059,"quadrant":"techniques","volume_date":"2019-04","description":"As infrastructures grow in complexity, so do the configuration files that define them. Tools such as AWS CloudFormation, Kubernetes and Helm expect configuration files in JSON or YAML syntax, presumably in an attempt to make them easy to write and process. However, in most cases, teams quickly reach the point where they have some parts that are similar but not quite the same, for example, when the same service must be deployed in different regions with a slightly different setup. For such cases tools offer templating in YAML (or JSON), which has caused a huge amount of frustration with practitioners. The problem is that the syntax of JSON and YAML requires all sorts of awkward compromises to graft templating features such as conditionals and loops into the files. We recommend using an API from a programming language instead or, when this is not an option, a templating system in a programming language, either a general-purpose language such as Python or something specialized such as Jsonnet.","blip_selector":"templating-in-yaml","name":"Templating in YAML","display_name":"Templating in YAML","url":"/radar/techniques/templating-in-yaml","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1297,"quadrant":"platforms","volume_date":"2019-04","description":"Quorum is \"an enterprise-focused version of Ethereum\" that aims to provide network permissioning and transaction privacy as well as higher performance. One of our teams has worked deeply with Quorum; however, their experience so far hasn't been great. Some challenges result from complex smart contract programming and some come from Quorum itself. For example, it doesn't work well with load balancers and only has partial database support, which will lead to significant deployment burden. We faced some stability and compatibility issues especially on private transactions. Quorum recently attracted a lot of attention because of JPM Coin. However, from a tech perspective, we recommend being cautious when implementing Quorum while keeping an eye on its development. | Ethereum is the leading developer ecosystem in blockchain tech. We've seen emerging solutions that aim to spread this technology into enterprise environments that usually require network permissioning and transaction privacy as well as higher throughput and lower latency. Quorum is one of these solutions. Originally developed by J.P. Morgan, Quorum positions itself as \"an enterprise-focused version of Ethereum.\" Unlike the Hyperledger Burrow node, which creates a new Ethereum virtual machine (EVM), Quorum forks code from Ethereum's official client so that it can evolve alongside Ethereum. Although it keeps most features of the Ethereum ledger, Quorum changes the consensus protocol from PoW to more efficient ones and adds private transaction support. With Quorum, developers can use their Ethereum knowledge of using, for example, Solidity and Truffle contracts to build enterprise blockchain applications. However, based on our experience, Quorum is not yet enterprise ready; for example, it lacks access control for private contracts, doesn't work well with load balancers and only has partial database support, all of which will lead to significant deployment and design burden. We recommend that you're cautious in implementing Quorum while keeping an eye on its development.","blip_selector":"quorum","name":"Quorum","display_name":"Quorum","url":"/radar/platforms/quorum","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904004,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"We've recommended Truffle for decentralized application (dapp) development in the past. Embark too can make your work easier. Embark provides features such as scaffolding, building, testing and debugging and integrates with decentralized storages such as IPFS. Through its declarative configuration, you can manage smart contract configuration, dependencies, artifact and deployment quite easily. Embark's interactive CLI dashboard is also impressive. We keep seeing people use Remix to write smart contracts and manually deploy their apps without automated testing, source control management or artifact management. We'd like to draw people's attention to dapp engineering practice by promoting tools such as Truffle and Embark.","blip_selector":"embark","name":"Embark","display_name":"Embark","url":"/radar/languages-and-frameworks/embark","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904065,"quadrant":"techniques","volume_date":"2019-04","description":"We're usually wary of covering diagrammatic techniques, but Wardley mapping is an interesting approach to start conversations around the evolution of an organization's software estate. At their simplest, they're used to visualize the value chains that exist within an organization, starting with customers' needs and progressively plotting the different capabilities and systems used to deliver on those needs along with the evolution of those capabilities and systems. The value of this technique is the process of collaborating to create the maps rather than the artefact itself. We recommend getting the right people in the room to produce them, and then treat them as living, evolving things rather than a complete artefact.","blip_selector":"wardley-mapping","name":"Wardley mapping","display_name":"Wardley mapping","url":"/radar/techniques/wardley-mapping","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1231,"quadrant":"techniques","volume_date":"2019-04","description":"For some time now we've recommended that delivery teams take ownership of their entire stack, including infrastructure. This means increased responsibility in the delivery team itself for configuring the infrastructure in a safe, secure and compliant way. When adopting cloud strategies, most organizations default to a tightly locked-down and centrally managed configuration to reduce risk, but this also creates substantial productivity bottlenecks. An alternative approach is to allow teams to manage their own configuration and use an infrastructure configuration scanner to ensure the configuration is safe and secure. Options include open-source scanners such as prowler for AWS and kube-bench for Kubernetes installations. For more continuous detection, take a look at cloud platforms such as AWS Config Rules among other commercial services. | For some time now we've recommended increased delivery team ownership of their entire stack, including infrastructure. This means increased responsibility in the delivery team itself for configuring infrastructure in a safe, secure, and compliant way. When adopting cloud strategies, most organizations default to a tightly locked-down and centrally managed configuration to reduce risk, but this also creates substantial productivity bottlenecks. An alternative approach is to allow teams to manage their own configuration, and use an Infrastructure configuration scanner to ensure the configuration is set in a safe and secure way. Watchmen is an interesting tool, built to provide rule-driven assurance of AWS account configurations that are owned and operated independently by delivery teams. Scout2 is another example of configuration scanning to support secure compliance. | For some time now we've recommended increased delivery team ownership of their entire stack, including infrastructure. This means increased responsibility in the delivery team itself for configuring infrastructure in a safe, secure, and compliant way. When adopting cloud strategies, most organizations default to a tightly locked-down and centrally managed configuration to reduce risk, but this also creates substantial productivity bottlenecks. An alternative approach is to allow teams to manage their own configuration, and use an Infrastructure configuration scanner to ensure the configuration is set in a safe and secure way. Watchmen is an interesting tool, built to provide rule-driven assurance of AWS account configurations that are owned and operated independently by delivery teams. Scout2 is another example of configuration scanning to support secure compliance.","blip_selector":"infrastructure-configuration-scanner","name":"Infrastructure configuration scanner","display_name":"Infrastructure configuration scanner","url":"/radar/techniques/infrastructure-configuration-scanner","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1058,"quadrant":"platforms","volume_date":"2019-04","description":"Kafka Streams is a lightweight library to build streaming applications. It supports basic streaming APIs such as join, filter, map and aggregate as well as local storage for common use cases such as windowing and sessions. Unlike other stream-processing platforms such as Apache Spark and Alpakka Kafka, Kafka Streams has been a good fit for scenarios that don't require large-scale distribution and parallel processing; hence we could get away without yet another piece of infrastructure such as cluster schedulers. Naturally, Kafka Streams has been a good choice when operating in the Kafka ecosystem. Kafka Streams is particularly useful when we have to process data strictly in order and exactly once. One particular use case of Kafka Streams is to build a change data capture (CDC) platform. | Kafka Streams is a lightweight library for building streaming applications. It's been designed with the goal of simplifying stream processing enough to make it easily accessible as a mainstream application programming model for asynchronous services. It can be a good alternative in scenarios where you want to apply a stream processing model to your problem, without embracing the complexity of running a cluster (usually introduced by full-fledged stream processing frameworks). New developments include ‘exactly once’ stream processing in a Kafka cluster. This was achieved by introducing idempotency in Kafka producers and allowing atomic writes across multiple partitions using the new Transactions API. | Kafka Streams is a lightweight library for building streaming applications. It's been designed with the goal of simplifying stream processing enough to make it easily accessible as a mainstream application programming model for asynchronous services. It can be a good alternative in scenarios where you want to apply a stream processing model to your problem without embracing the complexity of running a cluster (usually introduced by full-fledged stream processing frameworks).","blip_selector":"kafka-streams","name":"Kafka Streams","display_name":"Kafka Streams","url":"/radar/platforms/kafka-streams","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904025,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Taiko is a node.js library with a clear and concise API to assist with chrome or chromium browser automation. You can leverage Taiko's smart selectors and write reliable tests as the structure of the web application evolves. There's no need for ID, CSS or XPath selectors or adding explicit waits (for XHR requests) in test scripts. The interactive REPL recorder comes in handy when you want to develop the tests side by side as you explore the functionality. Although you could use Taiko independently, we've had good success using it with Gauge.","blip_selector":"taiko","name":"Taiko","display_name":"Taiko","url":"/radar/languages-and-frameworks/taiko","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1153,"quadrant":"tools","volume_date":"2019-04","description":"Traefik is an open-source reverse proxy and load balancer. If you're looking for an edge proxy that provides simple routing without all the features of NGINX and HAProxy, Traefik is a good choice. The router provides a reload-less reconfiguration, metrics, monitoring and circuit breakers that are essential when running microservices. It also integrates nicely with Let's Encrypt to provide SSL termination as well as infrastructure components such as Kubernetes, Docker Swarm or Amazon ECS to automatically pick up new services or instances to include in its load balancing. | Traefik is an open-source reverse proxy and load balancer. If you're looking for an edge proxy that provides simple routing without all the features of NGINX and HAProxy, Traefik is a good choice. The router provides a reload-less reconfiguration, metrics, monitoring and circuit breakers that are essential when running microservices. It also integrates nicely with Let's Encrypt to provide SSL termination. When compared to Traefik, tools such as NGINX and HAProxy may require additional tooling to templatize configuration in response to scaling, adding or removing microservices and may, at times, require a restart which can be annoying in production environments.","blip_selector":"traefik","name":"Traefik","display_name":"Traefik","url":"/radar/tools/traefik","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904009,"quadrant":"tools","volume_date":"2019-04","description":"Traditional Linux network security approaches, such as iptables, filter on IP address and TCP/UDP ports. However, these IP addresses frequently churn in dynamic microservices environments. By leveraging Linux eBPF, Cilium provides API-aware networking and security by transparently inserting security in a way that is based on service, pod or container identity in contrast to IP address identification. By decoupling security from addressing, Cilium could play a significant role as a new network protection layer and we recommend you to check it out.","blip_selector":"cilium","name":"Cilium","display_name":"Cilium","url":"/radar/tools/cilium","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904044,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"HiveRunner is an open-source unit test framework for Apache Hadoop Hive queries based on JUnit4. When writing nontrivial analytics or data pipelines in Hive SQL, we found HiveRunner to be a good enabler for writing tests and even TDDing out some moderately complicated SQL. HiveRunner enables you to write Hive SQL as releasable tested artifacts.","blip_selector":"hiverunner","name":"HiveRunner","display_name":"HiveRunner","url":"/radar/languages-and-frameworks/hiverunner","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904039,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"We're strong proponents of polyglot programming but recognize that in some cases it can make sense to focus on a single programming language. If you're heavily invested in Swift, most likely because of iOS development, and you find yourself looking for a technology to write server-side services, have a look at Vapor, a modern web framework for Swift that has gained a fair amount of popularity.","blip_selector":"vapor","name":"Vapor","display_name":"Vapor","url":"/radar/languages-and-frameworks/vapor","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904020,"quadrant":"tools","volume_date":"2019-04","description":"Taurus is a handy application and service performance testing tool written in Python. It wraps many performance testing executors, including Gatling and Locust. You can run it from the command line and easily integrate it with continuous delivery pipelines to run performance tests at different stages of the pipeline. Taurus also has great reporting either as console text-based output or integrated with an interactive web UI. Our teams have found that configuring Taurus YAML files is easy because you can use multiple files to describe each test scenario and refer to underlying executer's scenario definitions.","blip_selector":"taurus","name":"Taurus","display_name":"Taurus","url":"/radar/tools/taurus","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904055,"quadrant":"techniques","volume_date":"2019-04","description":"The more experience we gain with using distributed ledger technologies (DLTs), the more we encounter the rough edges around the current state of smart contracts. Committing automated, irrefutable, irreversible contracts on ledger sounds great in theory. The problems arise when you consider how to use modern software delivery techniques to developing them, as well as the differences between implementations. Immutable data is one thing, but immutable business logic is something else entirely! It's really important to think about whether to include logic in a smart contract. We've also found very different operational characteristics between different implementations. For example, even though contracts can evolve, different platforms support this evolution to a greater or lesser extent. Our advice is to think long and hard before committing business logic to a smart contract and to weigh the merits of the different platforms before you do.","blip_selector":"smart-contracts","name":"Smart contracts","display_name":"Smart contracts","url":"/radar/techniques/smart-contracts","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904051,"quadrant":"platforms","volume_date":"2019-04","description":"Most modern server-side or serverless code execution platforms are centered around containers or VMs. Cloudflare Workers, however, takes a different approach to hosting a serverless computing offering. It uses V8 Isolates, the open source JavaScript engine developed for Chrome, to run functions as a service (FaaS) on their extensive CDN network. Code can be written in JavaScript or anything that compiles to WebAssembly and data can be accessed from Cloudflare's cache or key-value store. The major benefit for developers is performance: by being on the edge network, close to end users, cold-starts take only five milliseconds. For the provider the benefits include both the ability to densely pack isolates because of their lower memory overhead and faster performance through reduced process context switching. This is definitely an intriguing approach to monitor and assess.","blip_selector":"cloudflare-workers","name":"Cloudflare Workers","display_name":"Cloudflare Workers","url":"/radar/platforms/cloudflare-workers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904015,"quadrant":"tools","volume_date":"2019-04","description":"Terraform provider GoCD lets you build pipelines using Terraform, a mature and widely used tool in the infrastructure as code space. With this provider, you can write pipelines in the HashiCorp Configuration Language (HCL) that use all of the functionality Terraform provides, including workspaces, modules and remote state. This approach is an excellent alternative to Gomatic, which we highlighted in the Pipelines as code blip before. The Golang SDK used in this provider has automatic regression tests for the GoCD API which should minimize issues while upgrading.","blip_selector":"terraform-provider-gocd","name":"Terraform provider GoCD","display_name":"Terraform provider GoCD","url":"/radar/tools/terraform-provider-gocd","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":201904031,"quadrant":"languages-and-frameworks","volume_date":"2019-04","description":"Resilience4j is a lightweight fault tolerance library inspired by Netflix Hystrix. We like its lightweight and modular structure where we pull in specific modules for specific capabilities such as circuit-breaking, rate-limiting, retry, and bulkhead. While service meshes are taking on some of the fault tolerance capabilities, fault tolerance libraries continue to remain a key component of our systems for more nuanced domain-specific fault tolerance behavior and for non-containerized services. With Hystrix going into maintenance mode, Resilience4j becomes a default choice in the Java ecosystem. It can work with synchronous APIs as well as reactive ones. It also surfaces metrics to dropwizard metrics, Prometheus and others using additional modules.","blip_selector":"resilience4j","name":"Resilience4j","display_name":"Resilience4j","url":"/radar/languages-and-frameworks/resilience4j","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":201904019,"quadrant":"tools","volume_date":"2019-04","description":"It's easy to think of many of the processes we work within as linear chains of cause and effect. Most of the time we are working within more complex systems where positive and negative feedback loops influence outcomes. Systems is a set of tools for describing, executing and visualizing systems diagrams. Using a compact DSL and running either standalone or within a Jupyter Notebook, it's super easy to describe fairly complex processes and the flow of information through them. It's pretty much a niche tool; but an interesting and fun one.","blip_selector":"systems","name":"Systems","display_name":"Systems","url":"/radar/tools/systems","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1318,"quadrant":"languages-and-frameworks","volume_date":"2018-11","description":"One insight we gained after talking with our teams is that Python is making a comeback across many technology domains. In fact, it's well on its way to become the most-used programming language. In part, this is driven by its adoption by data scientists and in machine learning, but we also see teams adopting it to build microservices. Nameko is a super-lightweight microservices framework and an alternative to Flask for writing services. Unlike Flask, Nameko only has a limited set of features that includes WebSocket, HTTP and AMQP support. We also like its focus on testability. If you don't need features such as templating that Flask provides, then Nameko is worth a look.","blip_selector":"nameko","name":"Nameko","display_name":"Nameko","url":"/radar/languages-and-frameworks/nameko","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1305,"quadrant":"tools","volume_date":"2018-11","description":"kube-bench is an example of an infrastructure configuration scanner that automates checking your Kubernetes configuration against the CIS benchmark for K8s. It covers user authentication, permissions and secure data among other areas. Our teams have found kube-bench valuable in the identification of vulnerable configurations.","blip_selector":"kube-bench","name":"kube-bench","display_name":"kube-bench","url":"/radar/tools/kube-bench","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1325,"quadrant":"languages-and-frameworks","volume_date":"2018-11","description":"PredictionIO is an open source machine-learning server. Developers and data scientists can use it to build intelligent applications for prediction. Like all intelligent applications, PredictionIO has three parts: data collection and storage, model training, and model deployment and expose service. Developers could focus on implementing data-processing logic, model algorithm and prediction logic based on the corresponding interfaces and liberate themselves from data storage and model training deployment. In our experience, PredictionIO can support both small and large volumes of data with low concurrency. We mostly use PredictionIO to build predictive services for small and medium-sized enterprises or as a proof of concept when building more complex, customized prediction engines.","blip_selector":"predictionio","name":"PredictionIO","display_name":"PredictionIO","url":"/radar/languages-and-frameworks/predictionio","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1280,"quadrant":"tools","volume_date":"2018-11","description":"We're continually on the lookout for tools and techniques that allow delivery teams to work independently from the rest of a larger organization while staying within its security and risk guardrails. Grafeas is such a tool. It lets organizations publish authoritative metadata about software artifacts—Docker images, libraries, packages—that is then accessible from build scripts or other automated compliance controls. The access control mechanisms allow for a separation of responsibility between the teams that publish approvals or vulnerabilities and the teams that build and deploy software. Although several organizations, including Google and JFrog, use Grafeas in their workflows, note that the tool is still in alpha.","blip_selector":"grafeas","name":"Grafeas","display_name":"Grafeas","url":"/radar/tools/grafeas","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1304,"quadrant":"techniques","volume_date":"2018-11","description":"When organizations choose a vanilla Hadoop or Spark distribution instead of one of the vendor distributions, they have to decide how they want to provision and manage the cluster. Occasionally, we see \"handcranking\" of Hadoop clusters using config management tools such as Ansible, Chef and others. Although these tools are great at provisioning immutable infrastructure components, they're not very useful when you have to manage stateful systems and can often lead to significant effort trying to manage and evolve clusters using these tools. We instead recommend using tools such as Ambari to provision and manage your stateful Hadoop or Spark clusters.","blip_selector":"handcranking-of-hadoop-clusters-using-config-management-tools","name":"\"Handcranking\" of Hadoop clusters using config management tools","display_name":"\"Handcranking\" of Hadoop clusters using config management tools","url":"/radar/techniques/handcranking-of-hadoop-clusters-using-config-management-tools","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1322,"quadrant":"languages-and-frameworks","volume_date":"2018-11","description":"Quantum computing currently exists in a twilight zone of being available for testing without having arrived yet. While we're still waiting for the hardware to arrive, we can experiment with and learn from languages and simulators. Although IBM and others have been making good progress, we've paid particular attention to Microsoft's efforts based around the Q# language and its simulator (32 qubits locally and 40 on Azure). If you want to start wrapping your head around the potential future of programming, check out their set of samples on GitHub.","blip_selector":"q","name":"Q#","display_name":"Q#","url":"/radar/languages-and-frameworks/q","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1291,"quadrant":"tools","volume_date":"2018-11","description":"Codefresh is a hosted CI server similar to CircleCI or Buildkite. It's container-centric, making Dockerfiles and container-hosting clusters first-class entities. We like that the tool encourages a pipelined delivery approach and supports branching and merging. Early reports from our teams are positive, but we've yet to see how it works for larger projects and complex pipelines.","blip_selector":"codefresh","name":"Codefresh","display_name":"Codefresh","url":"/radar/tools/codefresh","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1281,"quadrant":"tools","volume_date":"2018-11","description":"Heptio Ark is a tool for managing disaster recovery for Kubernetes clusters and persistent volumes. Ark is easy to use and configure and lets you back up and restore your clusters through a series of checkpoints. With Ark you can significantly reduce recovery time in case of an infrastructure failure, easily migrate Kubernetes resources from one cluster to another and replicate the production environment for testing and troubleshooting. Ark supports key backup storage providers (including AWS, Azure and Google Cloud) and, as of version 0.6.0, a plugin system that adds compatibility for additional backup and volume storage platforms. Managed Kubernetes environments, such as GKE, provide these services out of the box. However, if you're operating Kubernetes either on premise or in the cloud, take a closer look at Heptio Ark for disaster recovery.","blip_selector":"heptio-ark","name":"Heptio Ark","display_name":"Heptio Ark","url":"/radar/tools/heptio-ark","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1319,"quadrant":"languages-and-frameworks","volume_date":"2018-11","description":"Polly.js is a simple tool that helps teams test JavaScript websites and applications. Our teams particularly like that it enables them to intercept and stub HTTP interactions which allows for easier and faster testing of JavaScript code without having to spin up dependent services or components.","blip_selector":"polly-js","name":"Polly.js","display_name":"Polly.js","url":"/radar/languages-and-frameworks/polly-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1339,"quadrant":"platforms","volume_date":"2018-11","description":"We've been intrigued by Glitch, which is a collaborative online development environment that lets you easily copy and adapt (or \"remix\") existing web apps or create your own. Rooted in the \"tinkerer\" ethos, it's ideal for people learning to code but it has the capability to support more complex applications. The main focus is on JavaScript and Node.js, but it also has limited support for other languages. With integrated live editing, hosting, sharing and automatic source versioning, Glitch offers a refreshing and distinctive take on collaborative programming.","blip_selector":"glitch","name":"Glitch","display_name":"Glitch","url":"/radar/platforms/glitch","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1316,"quadrant":"platforms","volume_date":"2018-11","description":"With the growing and diverse data needs of enterprises comes a growing need for metadata management. Apache Atlas is a metadata management framework that fits the data governance needs of enterprises. Atlas provides capabilities to model types for metadata, classify data assets, track the data lineage and enable data discovery. However, when building a metadata management platform, we need to be careful not to repeat the mistakes of master data management.","blip_selector":"apache-atlas","name":"Apache Atlas","display_name":"Apache Atlas","url":"/radar/platforms/apache-atlas","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":909,"quadrant":"tools","volume_date":"2018-11","description":"Visual Studio Code is Microsoft's free IDE editor, available across platforms. We've had good experience using this for front-end development using React and TypeScript, and back-end languages such as GoLang, without having to switch between different editors. The tooling, language support and extensions for Visual Studio Code continue to soar and get better. We'd particularly like to call out Visual Studio Live Share for real-time collaboration and remote pairing. While complex projects in statically typed languages, such as Java, .NET or C++, will likely find better support from the more mature IDEs from Microsoft or Jetbrains, we find that Visual Studio Code is increasingly becoming a tool of choice among infrastructure and front-end development teams. | Visual Studio Code is Microsoft’s free IDE editor, available across platforms. We find the version-control integration with Git very beneficial to promoting continuous integration practices. Visual Studio Code also provides a means of integrating with external tools via tasks, with autodetection of grunt/gulp tasks eliminating the need for running grunt/gulp tasks via terminals and simply using the editor. With the growth of the Docker ecosystem, this IDE offers support for the dockerfile with snippets and definitions of valid commands.","blip_selector":"visual-studio-code","name":"Visual Studio Code","display_name":"Visual Studio Code","url":"/radar/tools/visual-studio-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1263,"quadrant":"tools","volume_date":"2018-11","description":"Snyk helps you find, fix and monitor known vulnerabilities in npm, Ruby, Python, Scala, Golang, .NET, PHP, Java and Docker dependency trees. When added to your build pipeline, Snyk continuously monitors and tests the library dependency tree against a hosted vulnerability database and suggests the minimal direct dependency version upgrade needed for remediation.","blip_selector":"snyk","name":"Snyk","display_name":"Snyk","url":"/radar/tools/snyk","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1317,"quadrant":"techniques","volume_date":"2018-11","description":"Most organizations that don't have the resources to custom-build their software will select out-of-the-box or SaaS solutions to meet their requirements. All too often, however, these solutions tend to aggressively expand their scope to entangle themselves into every part of your business. This blurs integration boundaries and makes change less predictable and slow. To mitigate this risk, we recommend that organizations develop a clear target capability model and then employ a strategy we call Bounded Buy —that is, only select vendor products that are modular and decoupled and can be contained within the Bounded Context of a single business capability. This modularity and independent deliverability should be included in the acceptance criteria for a vendor selection process.","blip_selector":"bounded-buy","name":"Bounded Buy","display_name":"Bounded Buy","url":"/radar/techniques/bounded-buy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1289,"quadrant":"tools","volume_date":"2018-11","description":"We have more and more projects that require unstructured data processing. To extract meaningful business information from text data is a key technique. Stanford CoreNLP is a Java-based set of natural language processing tools. It supports named-entity recognition, relationship extraction, sentiment analysis and text classification as well as multiple languages, including English, Chinese and Arabic. We also find tools usable to label corpus and training models for our scenario. With Stanford CoreNLP, we were able to use the latest research in the field of NLP to solve various business problems.","blip_selector":"stanford-corenlp","name":"Stanford CoreNLP","display_name":"Stanford CoreNLP","url":"/radar/tools/stanford-corenlp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":473,"quadrant":"platforms","volume_date":"2018-11","description":"Microsoft has steadily improved Azure and today not much separates the core cloud experience provided by the major cloud providers—Amazon, Google and Microsoft. The cloud providers seem to agree and seek to differentiate themselves in other areas such as features, services and cost structure. Microsoft is the provider who shows real interest in the legal requirements of European companies. They’ve a nuanced and plausible strategy, including unique offerings such as Azure Germany and Azure Stack, which gives some certainty to European companies in anticipation of the GDPR and possible legislative changes in the United States. | Microsoft has steadily improved Azure and today not much separates the core cloud experience provided by the major cloud providers – Amazon, Google and Microsoft. The cloud providers seem to agree and seek to differentiate themselves in other areas such as features, services and cost structure. Microsoft is the provider who shows real interest in the legal requirements of European companies. They’ve a nuanced and plausible strategy, including unique offerings such as Azure Germany and Azure Stack, which gives some certainty to European companies in anticipation of the GDPR and possible legislative changes in the United States. | Microsoft’s Azure cloud platform continues to play catchup with more mature clouds such as AWS, but we’ve been impressed with how Microsoft has responded to market demands. As with most Microsoft solutions it continues to be a contender and worth evaluating. | The Cloud continues to be of interest to us, with Software as a Service the most mature cloud component. Platform and Infrastructure as service offerings have reached different levels of maturity, and we reflect that in our placement of EC2, Google App Engine and Azure.","blip_selector":"azure","name":"Azure","display_name":"Azure","url":"/radar/platforms/azure","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1308,"quadrant":"techniques","volume_date":"2018-11","description":"At ThoughtWorks, as early adopters and leaders in the agile space, we've been proponents of the practice of incremental delivery. We've also advised many clients to look at off-the-shelf software through a \"Can this be released incrementally?\" lens. This has often been difficult because of the big-bang approach of most vendors which usually involves migrating large amounts of data. Recently, however, we've also had success using incremental delivery with COTS (commercial off-the-shelf), launching specific business processes incrementally to smaller subsets of users. We recommend you assess whether you can apply this practice to the vendor software of your choice, to help reduce the risks involved in big-bang deliveries.","blip_selector":"incremental-delivery-with-cots","name":"Incremental delivery with COTS","display_name":"Incremental delivery with COTS","url":"/radar/techniques/incremental-delivery-with-cots","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1270,"quadrant":"tools","volume_date":"2018-11","description":"Security continues to be paramount, and inadvertently checking credentials and other secrets into source control is a major attack vector. git-secrets is a simple tool that prevents you from committing passwords and other sensitive information to a git repository. It can also scan all historical revisions before making a repository public, if you want to ensure you've never accidentally checked in a credential. git-secrets comes with built-in support for common AWS keys and credentials and can be set up quickly for other providers too.","blip_selector":"git-secrets","name":"git-secrets","display_name":"git-secrets","url":"/radar/tools/git-secrets","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1260,"quadrant":"platforms","volume_date":"2018-11","description":"TICK Stack is a collection of open source components that combine to deliver a platform for easily storing, visualizing and monitoring time series data such as metrics and events. The components are: Telegraf, a server agent for collecting and reporting metrics; InfluxDB, a high-performance time series database; Chronograf, a user interface for the platform; and Kapacitor, a data-processing engine that can process, stream and batch data from InfluxDB. Unlike Prometheus, which is based on the pull model, TICK Stack is based on the push model of collecting data. The heart of the system is the InfluxDB component, which is one of the best time series databases. The stack is backed by InfluxData and although you need the enterprise version for features such as database clustering, it's still a fairly good choice for monitoring. We're using it in a few places in production and have had good experiences with it. | TICK Stack is a platform composed of open source components which makes collection, storage, graphing and alerting on-time series data such as metrics and events easy. The components of the TICK Stack are: Telegraf, a server agent for collecting and reporting metrics; InfluxDB, a high-performance time series database; Chronograf, a user interface for the platform; and Kapacitor, a data-processing engine that can process, stream and batch data from InfluxDB. Unlike Prometheus, which is based on the Pull model, the TICK Stack is based on the Push model of collecting data. The heart of the system is the InfluxDB component which is one of the best time series databases. This stack is backed by InfluxData and needs the enterprise version for features such as DB clustering, but it’s still a fairly good choice for monitoring. We’re using it in a few places in production and have had good experiences with it.","blip_selector":"tick-stack","name":"TICK Stack","display_name":"TICK Stack","url":"/radar/platforms/tick-stack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1334,"quadrant":"platforms","volume_date":"2018-11","description":"Rook is an open source cloud native storage orchestrator for Kubernetes. Rook integrates with Ceph and brings File, Block and Object storage systems into the Kubernetes cluster, running them seamlessly alongside other applications and services that are consuming the storage. By using Kubernetes operators, Rook orchestrates Ceph at the control plane and stays clear of the data path between applications and Ceph. Storage is one of the important components of cloud-native computing and we believe that Rook, though still an incubating-level project at CNCF, takes us a step closer to self-sufficiency and portability across public cloud and on-premise deployments.","blip_selector":"rook","name":"Rook","display_name":"Rook","url":"/radar/platforms/rook","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1296,"quadrant":"languages-and-frameworks","volume_date":"2018-11","description":"An open source framework developed by WeChat, MMKV provides fast key-value storage for mobile apps. It uses iOS memory-mapping features to avoid the need to explicitly save changes and is extremely fast and performant. In the event of an unexpected crash, MMKV allows the app to restore the data quickly.","blip_selector":"mmkv","name":"MMKV","display_name":"MMKV","url":"/radar/languages-and-frameworks/mmkv","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1332,"quadrant":"platforms","volume_date":"2018-11","description":"Resin.io is an Internet of Things (IoT) platform that does one thing and does it well: it deploys containers onto devices. Developers use a software as a service (SaaS) portal to manage devices and assign applications, defined by Dockerfiles, to them. The platform can build containers for various hardware types and deploys the images over the air. For the containers, Resin.io uses balena, an engine based on the Moby framework created by Docker. The platform is still under development, has some rough edges and lacks some features (e.g., working with private registries), but the current feature set, including the option to ssh into a container on a device from the web portal, points toward a promising future.","blip_selector":"resin-io","name":"Resin.io","display_name":"Resin.io","url":"/radar/platforms/resin-io","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1269,"quadrant":"tools","volume_date":"2018-11","description":"Azure Container Service Engine (acs-engine) is an Azure Resource Manager (ARM) template generator. The required configurations of the cluster are defined in a JSON file; acs-engine reads these cluster definitions and generates a number of files that can be consumed by ARM. The tool also provides flexibility to choose different orchestrators—including Kubernetes, DC/OS, OpenShift, Swarm mode and Swarm—and to configure features and agents of the cluster. We’ve been using acs-engine in a number of projects and would recommend it for managing clusters in Azure Container Service.","blip_selector":"acs-engine","name":"acs-engine","display_name":"acs-engine","url":"/radar/tools/acs-engine","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1313,"quadrant":"techniques","volume_date":"2018-11","description":"On a number of occasions we have seen system designs that use request-response events in user-facing workflows. In these cases, the UI is blocked or the user has to wait for a new page to load until a corresponding response message to a request message is received. The main reasons cited for designs like this are performance or a unified approach to communication between backends for synchronous and asynchronous use cases. We feel that the increased complexity—in development, testing and operations—far outweighs the benefit of having a unified approach, and we strongly suggest to use synchronous HTTP requests when synchronous communication between backend services is needed. When implemented well, communication using HTTP rarely is a bottleneck in a distributed system.","blip_selector":"request-response-events-in-user-facing-workflows","name":"Request-response events in user-facing workflows","display_name":"Request-response events in user-facing workflows","url":"/radar/techniques/request-response-events-in-user-facing-workflows","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1309,"quadrant":"techniques","volume_date":"2018-11","description":"Chaos Katas is a technique that our teams have developed to train and upskill infrastructure and platform engineers. It combines Chaos Engineering techniques—that is, creating failures and outages in a controlled environment—with the systematic teaching and training approach of Kata. Here, Kata refers to code patterns that trigger controlled failures, allowing engineers to discover the problem, recover from the failure, run postmortem and find the root cause. Repeated execution of Katas helps engineers to internalize their new skills.","blip_selector":"chaos-katas","name":"Chaos Katas","display_name":"Chaos Katas","url":"/radar/techniques/chaos-katas","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1267,"quadrant":"tools","volume_date":"2018-11","description":"Prettier is an opinionated, automated code formatter for JavaScript (with growing support for other languages). By enforcing its own opinionated formatting style it increases consistency and readability and reduces developer effort both on formatting and engaging in wasteful team debates about code style. Even though you may disagree with the stylistic choices enforced by Prettier, we find that the benefits to the team generally outweigh small style issues. Prettier can be used with a precommit hook or an IDE plugin. As with any formatter, a one-time reformatting of your codebase can confuse your version control history, but we feel that's a minor drawback. We particularly like the way Prettier flips the linter-based approach and, borrowing from gofmt, instead of validating your code, it ensures that your code will always be valid.","blip_selector":"prettier","name":"Prettier","display_name":"Prettier","url":"/radar/tools/prettier","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":883,"quadrant":"techniques","volume_date":"2018-11","description":"When organizations move toward microservices, one of the main drivers is the hope for faster time to market. However, this aspiration only tends to be realized when services (and their supporting teams) are cleanly sliced along long-lived business domain boundaries. Otherwise meaningful features will naturally require tight coordination between multiple teams and services, introducing natural friction in competing roadmap prioritization. The solution to this problem is good domain modeling, and event storming has rapidly become one of our favorite methods for rapidly identifying the key concepts in a problem space and aligning a variety of stakeholders in the best way to slice potential solutions. | Event Storming is a useful way to do rapid \"outside-in\" domain modeling: starting with the events that occur in the domain rather than a static data model. Run as a facilitated workshop, it focuses on discovering key domain events, placing them along a timeline, identifying their triggers and then exploring their relationships. This approach is particularly useful for people taking an Event Sourced approach. Getting the right people in the room is important - a blend of business and technical people who bring both the questions and the answers. Ensuring that you have enough wall space for modeling is the second key to success. Look to discover the big picture, with the goal of collectively understanding the domain in all of its complexity, before diving into solutions. | Event Storming is a useful way to do rapid “outside-in” domain modeling: starting with the events that occur in the domain rather than a static data model. Run as a facilitated workshop, it focuses on discovering key domain events, placing them along a timeline, identifying their triggers and then exploring their relationships. This approach is particularly useful for people taking a CQRS or Event Sourcing approach. Getting the right people in the room is important - a blend of business and technical people who bring both the questions and the answers. Ensuring that you have enough wall space for modeling is the second key to success. Look to discover the big picture, with the goal of collectively understanding the domain in all of its complexity, before diving into solutions.","blip_selector":"event-storming","name":"Event Storming","display_name":"Event Storming","url":"/radar/techniques/event-storming","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1274,"quadrant":"tools","volume_date":"2018-11","description":"We're seeing significant advances in security tooling integration with modern software delivery processes. Archery is an open source tool with an active community that's doing a good job of pulling together a collection of other tools, including Zap. Designed primarily for web applications, Archery makes it easy to integrate security tooling into your build and deployment systems. Its dashboards also let you track vulnerabilities as well as application and network scans.","blip_selector":"archery","name":"Archery","display_name":"Archery","url":"/radar/tools/archery","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1335,"quadrant":"platforms","volume_date":"2018-11","description":"gVisor is a user-space kernel for containers. It limits the host kernel surface accessible to the application without taking away access to all the features it expects. Unlike existing sandbox technologies, such as virtualized hardware (KVM and Xen) or rule-based execution (seccomp, SELinux and AppArmor), gVisor takes a distinct approach to container sandboxing by intercepting application system calls and acting as the guest kernel without the need for translation through virtualized hardware. gVisor includes an Open Container Initiative (OCI) runtime called runsc that integrates with Docker and provides experimental support for Kubernetes. gVisor is a relatively new project and we recommend assessing it for your container security landscape.","blip_selector":"gvisor","name":"gVisor","display_name":"gVisor","url":"/radar/platforms/gvisor","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1299,"quadrant":"techniques","volume_date":"2018-11","description":"On-demand self-service is a key characteristic (and benefit) of cloud computing. When large-scale service landscapes are deployed using a single account, rules and processes around usage of that account become necessary, often involving approval steps that increase turnaround time. A better approach is a multi-account cloud setup where several accounts are used, in the extreme one account per team. This does add overhead in other places, for example, ensuring shared billing, enabling communication between VPCs and managing the relationship with the cloud provider. However, it often accelerates development and it usually improves security, because single-service accounts are easier to audit and, in the case of a breach, the impact is greatly reduced. Having multiple accounts also reduces stickiness, because an account provides a good boundary for services that can be moved en bloc to another cloud provider.","blip_selector":"multi-account-cloud-setup","name":"Multi-account cloud setup","display_name":"Multi-account cloud setup","url":"/radar/techniques/multi-account-cloud-setup","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1345,"quadrant":"techniques","volume_date":"2018-11","description":"Often, in an effort to outsource risk to their suppliers, businesses look for \"one throat to choke\" on their most critical and risky system implementations. Unfortunately, this gives them fewer solution choices and less flexibility. Instead, businesses should look to maintain the greatest vendor independence where the business risk exposure is highest. We see a new risk-commensurate vendor strategy emerging that encourages investment to maintain vendor independence for highly critical business systems. Less critical business functions can take advantage of the streamlined delivery of a vendor-native solution because it allows them to absorb more easily the impact of losing that vendor. This trade-off has become apparent as the major cloud providers have expanded their range of service offerings. For example, using AWS Secret Management Service can speed up initial development and has the benefit of ecosystem integration, but it will also add more inertia if you ever need to migrate to a different cloud provider than it would if you had implemented, for example, Vault.","blip_selector":"risk-commensurate-vendor-strategy","name":"Risk-commensurate vendor strategy","display_name":"Risk-commensurate vendor strategy","url":"/radar/techniques/risk-commensurate-vendor-strategy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1228,"quadrant":"tools","volume_date":"2018-11","description":"ArchUnit is a Java testing library for checking architecture characteristics such as package and class dependencies, annotation verification and even layer consistency. We like that it runs as unit tests within your existing test setup, even though it supports only Java-based architectures. The ArchUnit test suite can be incorporated into a CI environment or a deployment pipeline, making it easier to implement fitness functions in an evolutionary architecture way. | ArchUnit is a Java testing library for checking architecture characteristics such as package and class dependencies, annotation verification and even layer consistency. The fact that it runs as unit tests, within your existing test setup, pleases us, even though it's available for Java architectures only. The ArchUnit test suite can be incorporated into a CI environment or a deployment pipeline, making it easier to implement fitness functions in an evolutionary architecture way.","blip_selector":"archunit","name":"ArchUnit","display_name":"ArchUnit","url":"/radar/tools/archunit","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1333,"quadrant":"platforms","volume_date":"2018-11","description":"In most cases, blockchain is not the right place to store a blob file (e.g., image or audio). When developing DApp, one option is to put blob files in some off-chain centralized data storage, which usually signals lack of trust. Another option is to store them on InterPlanetary File System (IPFS), which is a content-addressed, versioned, peer-to-peer file system. It’s designed to distribute high volumes of data with high efficiency and removed from any centralized authority. Files are stored on peers that don’t need to trust each other. IPFS keeps every version of a file so you never lose important files. We see IPFS as a good complement to blockchain technology. Beyond its blockchain application, IPFS has an ambitious goal to decentralize the Internet infrastructure.","blip_selector":"ipfs","name":"IPFS","display_name":"IPFS","url":"/radar/platforms/ipfs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1217,"quadrant":"techniques","volume_date":"2018-11","description":"The major cloud providers have become increasingly competitive in their pricing and the rapid pace of releasing new features. This leaves consumers in a difficult place when choosing and committing to a provider. Increasingly, we're seeing organizations prepare to use \"any cloud\" and to avoid vendor lock-in at all costs. This, of course, leads to generic cloud usage. We see organizations limiting their use of the cloud to only those features common across all cloud providers—thereby missing out on the providers' unique benefits. We see organizations making large investments in home-grown abstraction layers that are too complex to build and too costly to maintain to stay cloud agnostic. The problem of lock-in is real. We recommend approaching this problem with a multicloud strategy that evaluates the migration cost and effort of capabilities from one cloud to another, against the benefits of using cloud-specific features. We recommend increasing the portability of the workloads by shipping the applications as widely adopted Docker containers: use open source security and identity protocols to easily migrate the identity of the workloads, a risk-commensurate vendor strategy to maintain cloud independence only where necessary and Polycloud to mix and match services from different providers where it makes sense. In short, shift your approach from a generic cloud usage to a sensible multicloud strategy. | The major cloud providers continue to add new features to their clouds at a rapid pace, and under the banner of Polycloud we've suggested using multiple clouds in parallel, to mix and match services based on the strengths of each provider’s offerings. Increasingly, we're seeing organizations prepare to use multiple clouds — not to benefit from individual provider’s strengths, though, but to avoid vendor \"lock-in\" at all costs. This, of course, leads to generic cloud usage , only using features that are present across all providers, which reminds us of the lowest common denominator scenario we saw 10 years ago when companies avoided many advanced features in relational databases in an effort to remain vendor neutral. The problem of lock-in is real. However, instead of treating it with a sledgehammer approach, we recommend looking at this problem from the perspective of exit costs and relate those to the benefits of using cloud-specific features.","blip_selector":"generic-cloud-usage","name":"Generic cloud usage","display_name":"Generic cloud usage","url":"/radar/techniques/generic-cloud-usage","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9022,"quadrant":"platforms","volume_date":"2018-11","description":"We first placed AWS in Adopt seven years ago, and the breadth, depth and reliability of its services have improved in leaps and bounds since then. However, we're now moving AWS back into Trial, not because of any deficiencies in its offering, but because its competitors, GCP and Azure, have matured considerably and selecting a cloud provider has become increasingly complex. We reserve Adopt for when we see a clear winner in a field. For many years, AWS was the default choice, but we now feel that organizations should make a balanced selection across cloud providers that takes into account their geographic and regulatory footprint, their strategic alignment (or lack thereof) with the providers, and, of course, the fit between their most important needs and the cloud providers' differentiating products. | While it can be all too easy to ignore geographical location of cloud-based services, for legal and technical reasons it can be a serious constraint when considering appropriate platforms. With the recently announced Brazil and Singapore regions, Amazon has made AWSbased systems more viable for people in areas previously poorly served by IaaS providers. In addition, they continue to add features to existing services, such as VPC. We remain confident in recommending AWS for those situations where flexibility in provisioning resources is key. | Amazon continues to evolve the AWS cloud with services such as RDB, making it even easier to engineer and deploy cloud-based applications. Not every AWS feature is as mature as EC2 and S3, so you should carefully evaluate which AWS components to use. We feel comfortable recommending AWS where elasticity or on-demand computing are required.","blip_selector":"aws","name":"AWS","display_name":"AWS","url":"/radar/platforms/aws","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1290,"quadrant":"tools","volume_date":"2018-11","description":"Ocelot is a .NET API gateway. After three years of development, Ocelot has built a relatively complete feature set and an active community. Although there is no dearth of excellent API gateways (e.g., Kong), the .NET community appears to prefer Ocelot when building microservices. Part of the reason is that Ocelot integrates well with the .NET ecosystem (e.g., with IdentityServer). Another reason may be that the .NET community has extended Ocelot to support communication protocols such as gRPC, Orleans and WebSocket.","blip_selector":"ocelot","name":"Ocelot","display_name":"Ocelot","url":"/radar/tools/ocelot","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1328,"quadrant":"platforms","volume_date":"2018-11","description":"As we've gained more experience with the public cloud across organizations large and small, certain patterns have emerged. One of those patterns is a virtual private cloud network managed at the organizational level and divided into smaller subnets under the control of each delivery team. This is closely related to the idea of multiaccount cloud setup and helps to partition an infrastructure along team bounds. After configuring this setup many times using VPCs, subnets, security groups and NACLs, we really like Google's notion of the shared VPC. Shared VPC makes organizations, projects, VPCs and subnets first-class entities in network configurations. VPCs can be managed by an organization's administrators who can delegate subnet administration to projects. Projects can then be explicitly associated with subnets in the VPC. This simplifies configuration and makes security and access control more transparent.","blip_selector":"shared-vpc","name":"Shared VPC","display_name":"Shared VPC","url":"/radar/platforms/shared-vpc","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1343,"quadrant":"platforms","volume_date":"2018-11","description":"Low-code platforms use graphical user interfaces and configuration in order to create applications. Unfortunately, low-code environments are promoted with the idea that this means you no longer need skilled development teams. Such suggestions ignore the fact that writing code is just a small part of what needs to happen to create high-quality software—practices such as source control, testing and careful design of solutions are just as important. Although these platforms have their uses, we suggest approaching them with caution, especially when they come with extravagant claims for lower cost and higher productivity.","blip_selector":"low-code-platforms","name":"Low-code platforms","display_name":"Low-code platforms","url":"/radar/platforms/low-code-platforms","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1307,"quadrant":"techniques","volume_date":"2018-11","description":"In more complex architectures and deployments, it may not be immediately obvious that a build that depends on the code currently being checked in is broken. Developers trying to fix a broken build could find themselves working against a moving target, as the build is continually triggered by upstream dependencies. Pre-commit downstream build checks is a very simple technique: have a pre-commit or pre-push script check the status of these downstream builds and alert the developer beforehand that a build is broken.","blip_selector":"pre-commit-downstream-build-checks","name":"Pre-commit downstream build checks","display_name":"Pre-commit downstream build checks","url":"/radar/techniques/pre-commit-downstream-build-checks","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1340,"quadrant":"platforms","volume_date":"2018-11","description":"CockroachDB is an open source distributed database inspired by the white paper Spanner: Google's distributed database. In CockroachDB, data is automatically divided into ranges, usually 64MB, and distributed across nodes in the cluster. Each range has a consensus group and, because it uses the Raft consensus algorithm, the data is always kept in sync. With its unique design, CockroachDB provides distributed transactions and geo-partitioning while still supporting SQL. Unlike Spanner, which relies on TrueTime with atomic clock for linearizability, CockroachDB uses NTP for clock synchronization and provides serializability as the default isolation level. If you're working with structured data that fits in a single node, then choose a traditional relational database. However, if your data needs to scale across nodes, be consistent and survive failures, then we recommend you take a closer look at CockroachDB.","blip_selector":"cockroachdb","name":"CockroachDB","display_name":"CockroachDB","url":"/radar/platforms/cockroachdb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1312,"quadrant":"techniques","volume_date":"2018-11","description":"A defining characteristic of a microservices architecture is that system components and services are organized around business capabilities. Regardless of size, microservices encapsulate a meaningful grouping of functionality and information to allow for the independent delivery of business value. This is in contrast to earlier approaches in service architecture which organized services according to technical characteristics. We've observed a number of organizations who've adopted a layered microservices architecture , which in some ways is a contradiction in terms. These organizations have fallen back to arranging services primarily according to a technical role, for example, experience APIs, process APIs or system APIs. It's too easy for technology teams to be assigned by layer, so delivering any valuable business change requires slow and expensive coordination between multiple teams. We caution against the effects of this layering and recommend arranging services and teams primarily according to business capability.","blip_selector":"layered-microservices-architecture","name":"Layered microservices architecture","display_name":"Layered microservices architecture","url":"/radar/techniques/layered-microservices-architecture","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1342,"quadrant":"platforms","volume_date":"2018-11","description":"Data-hungry packages are solutions that require absorption of data into themselves in order to function. In some cases they may even require that they become the \"master\" for that data. Once the data is owned by the package, that software becomes the only way to update, change or access the data. The data-hungry package might solve a particular business problem such as ERP. However, inventory or finance \"data demands\" placed upon an organization will often require complex integration and changes to systems that lie well outside of the original scope.","blip_selector":"data-hungry-packages","name":"Data-hungry packages","display_name":"Data-hungry packages","url":"/radar/platforms/data-hungry-packages","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1302,"quadrant":"techniques","volume_date":"2018-11","description":"Fast feedback is one of our core values for building software. For many years, we've used the canary release approach to encourage early feedback on new software versions, while reducing the risk through incremental rollout to selected users. One of the questions regarding this technique is how to segment users. Canary releases to a very small segment (say 1%) of users can be a catalyst for change. While starting with a very small segment of users enables teams to get comfortable with the technique, capturing fast user feedback enables diverse teams to observe the impact of new releases and learn and adjust course as necessary—a priceless change in engineering culture. We call this, the mighty 1% canary.","blip_selector":"1-canary","name":"1% canary","display_name":"1% canary","url":"/radar/techniques/1-canary","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1215,"quadrant":"techniques","volume_date":"2018-11","description":"Although we've had mostly new blips in this edition of the Radar, we think it's worth continuing to call out the usefulness of Security Chaos Engineering. We've moved it to Trial because the teams using this technique are confident that the security policies they have in place are robust enough to handle common security failure modes. Still, proceed with caution when using this technique—we don't want our teams to become desensitized to these issues. | We’ve previously talked about the technique of Chaos Engineering in the Radar and the Simian Army suite of tools from Netflix that we’ve used to run experiments to test the resilience of production infrastructure. Security Chaos Engineering broadens the scope of this technique to the realm of security. We deliberately introduce false positives into production networks and other infrastructure — build-time dependencies, for example — to check whether procedures in place are capable of identifying security failures under controlled conditions. Although useful, this technique should be used with care to avoid desensitizing teams to security problems.","blip_selector":"security-chaos-engineering","name":"Security Chaos Engineering","display_name":"Security Chaos Engineering","url":"/radar/techniques/security-chaos-engineering","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":791,"quadrant":"techniques","volume_date":"2018-11","description":"Microservices has emerged as a leading architectural technique in modern cloud-based systems, but we still think teams should proceed carefully when making this choice. Microservice envy tempts teams to complicate their architecture by having lots of services simply because it's a fashionable architecture choice. Platforms such as Kubernetes make it much easier to deploy complex sets of microservices, and vendors are pushing their solutions to managing microservices, potentially leading teams further down this path. It's important to remember that microservices trade development complexity for operational complexity and require a solid foundation of automated testing, continuous delivery and DevOps culture. | We remain convinced that microservices can offer significant advantages to organizations, in terms of improving team autonomy and faster frequency of change. The additional complexity that comes from distributed systems requires an additional level of maturity and investment. We are concerned that some teams are rushing in to adopting microservices without understanding the changes to development, test, and operations that are required to do them well. Our general advice remains simple. Avoid microservice envy  and start with one or two services before rushing headlong into developing more, to allow your teams time to adjust and understand the right level of granularity.","blip_selector":"microservice-envy","name":"Microservice envy","display_name":"Microservice envy","url":"/radar/techniques/microservice-envy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1233,"quadrant":"tools","volume_date":"2018-11","description":"Headless Firefox has the same maturity as that of Headless Chrome for front-end test. Similar to Headless Chrome, with Firefox in headless mode we now get to enjoy browser tests without the visible UI components, executing the UI tests suite much faster. | When developing front-end applications, we've mentioned Headless Chrome as a better alternative to PhantomJS for front-end testing in a previous edition of the Radar. Now we suggest assessing Headless Firefox as a viable option in this area. In the same way as Headless Chrome, Firefox in a headless mode runs the browser without the visible UI components, executing the UI tests suite much faster.","blip_selector":"headless-firefox","name":"Headless Firefox","display_name":"Headless Firefox","url":"/radar/tools/headless-firefox","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1268,"quadrant":"tools","volume_date":"2018-11","description":"Mermaid lets you generate diagrams from a markdown-like markup language. Born out of need to simplify documentation, Mermaid has grown into a larger ecosystem with plugins for Confluence, Visual Studio Code and Jekyll to name a few. To see how it works, you can use the Live Editor on GitHub. Mermaid also has a convenient command line interface that lets you generate SVG, PNG and PDF files as output from definition files. We've been using Mermaid in many projects and we like the simplicity of describing graphs and flowcharts with markdown and checking in the definition files with the code repository.","blip_selector":"mermaid","name":"Mermaid","display_name":"Mermaid","url":"/radar/tools/mermaid","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1326,"quadrant":"languages-and-frameworks","volume_date":"2018-11","description":"The adoption of a new language typically spawns the emergence of new tools that support mature engineering practices such as test automation. Kotlin is no exception. Spek is a testing framework—inspired by well-known tools such as Cucumber, RSpec and Jasmine—that writes tests in Gherkin and Specification, allowing teams to bring mature practices such as behaviour-driven development into the Kotlin space.","blip_selector":"spek","name":"Spek","display_name":"Spek","url":"/radar/languages-and-frameworks/spek","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1192,"quadrant":"platforms","volume_date":"2018-11","description":"As Google Cloud Platform (GCP) has expanded in terms of available geographic regions and maturity of services, customers globally can now seriously consider it for their cloud strategy. In some areas, GCP has reached feature parity with its main competitor, Amazon Web Services, while in other areas it has differentiated itself—notably with accessible machine learning platforms, data engineering tools, and a workable Kubernetes as a service solution (GKE). In practice, our teams have nothing but praise for the developer experience working with the GCP tools and APIs. | As Google Cloud Platform (GCP) has expanded in terms of available geographic regions and maturity of services, customers globally can now seriously consider it for their cloud strategy. In some areas, GCP has reached feature parity with its main competitor, Amazon Web Services, while in other areas it has differentiated itself — notably with accessible machine learning platforms, data engineering tools, and a workable Kubernetes as a service solution (GKE). In practice, our teams have nothing but praise for the developer experience working with the GCP tools and APIs. | As Google Cloud Platform (GCP) has expanded in terms of available geographic regions and maturity of services, customers globally can now seriously consider it for their cloud strategy. In some areas, GCP has reached feature parity with its main competitor, Amazon Web Services, while in other areas it has differentiated itself — notably with accessible machine learning platforms, data engineering tools, and a workable Kubernetes as a service solution (GKE). In practice, our teams have nothing but praise for the developer experience working with the GCP tools and APIs.","blip_selector":"google-cloud-platform","name":"Google Cloud Platform","display_name":"Google Cloud Platform","url":"/radar/platforms/google-cloud-platform","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1310,"quadrant":"techniques","volume_date":"2018-11","description":"Robotic process automation ( RPA ) is a key part of many digital transformation initiatives, as it promises to deliver cost savings without having to modernize the underlying architecture and systems. The problem with this approach of focusing only on automating business processes, without addressing the underlying software systems or capabilities, is that this can make it even harder to change the underlying systems by introducing additional coupling. This makes any future attempts to address the legacy IT landscape even more difficult. Very few systems can afford to ignore change and hence RPA efforts need to be coupled with appropriate legacy modernization strategies.","blip_selector":"rpa","name":"RPA","display_name":"RPA","url":"/radar/techniques/rpa","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1275,"quadrant":"tools","volume_date":"2018-11","description":"We've covered Visual Studio Code in the Radar since 2015, but it isn't the only cross-platform .NET Core IDE kid on the block anymore. Recently, Rider, which is part of the IDEA platform developed by JetBrains, has gained adoption, especially by those used to the speed and dexterity provided by ReSharper, which drives the refactoring in Rider. Rider, however, does more than ReSharper to bring the full IDEA platform to .NET and increase developer productivity. Regardless of your preferred platform, it's worth exploring Rider as it currently has the productivity edge on Visual Studio Code. It's also great to see the ecosystem alive and well, as competition ensures these tools continue to improve.","blip_selector":"rider","name":"Rider","display_name":"Rider","url":"/radar/tools/rider","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1242,"quadrant":"languages-and-frameworks","volume_date":"2018-11","description":"We're trying out troposphere as a way of defining the infrastructure as code on AWS for our projects that use AWS CloudFormation instead of Terraform. troposphere is a Python library that allows us to write Python code to generate CloudFormation JSON descriptions. What we like about troposphere is that it facilitates catching JSON errors early, applying type checking, and unit testing and DRY composition of AWS resources. | We’re trying out troposphere as a way of defining the infrastructure as code on AWS for our projects where AWS CloudFormation is used instead of Terraform. troposphere is a Python library that allows us to write Python code to generate CloudFormation JSON descriptions. What we like about troposphere is that it facilitates catching JSON errors early, applying type checking, and unit testing and DRY composition of AWS resources.","blip_selector":"troposphere","name":"troposphere","display_name":"troposphere","url":"/radar/languages-and-frameworks/troposphere","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":950,"quadrant":"languages-and-frameworks","volume_date":"2018-11","description":"With the increased adoption of a microservices architecture, we're building more distributed applications than before. Although there are many benefits of a decoupled architecture, the complexity and the effort involved in proving the correctness of the overall system has dramatically increased. Jepsen provides much needed tooling to verify correctness in coordination of task schedulers, test eventual consistency, linearizability and serializability characteristics of distributed databases. We've used Jepsen in a few projects and we like the fact that we can test drive configurations, inject and correct faults, and verify the state of the system after recovery. | With the growth in usage of NoSQL data stores, and the growth in popularity of polyglot approaches to persistence, teams now have many choices when it comes to storing their data. While this has brought many advantages, product behavior with flaky networks can introduce subtle (and not so subtle) issues that are often not well understood, even in some cases by the product developers themselves. The Jepsen toolkit and accompanying blog have become the de-facto reference for anyone looking to understand how different database and queuing technologies react under adverse conditions. Crucially, the approach to testing, which includes clients in the transactions, shines a spotlight on possible failure modes for many teams building microservices.","blip_selector":"jepsen","name":"Jepsen","display_name":"Jepsen","url":"/radar/languages-and-frameworks/jepsen","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1301,"quadrant":"techniques","volume_date":"2018-11","description":"The observability is an integral part of operating a distributed and microservices architecture. We rely on different system outputs such as distributed tracing, aggregate logs and metrics to infer the internal state of the distributed components, diagnose where the problems are and get to the root cause. An important aspect of an observability ecosystem is monitoring—visualizing and analyzing the system's output—and alerting when unexpected conditions are detected. Traditionally, configuration of monitoring dashboards and setting up alerts is done through GUI-based point-and-click systems. This approach leads to nonrepeatable dashboard configurations, no ability to continuously test and adjust alerts to avoid alert fatigue or missing out on important alerts, and drift from organizational best practices. We highly recommend treating your observability ecosystem configurations as code, called observability as code , and adopt infrastructure as code for your monitoring and alerting infrastructure. Choose observability products that support configuration through version-controlled code and execution of APIs or commands via infrastructure CD pipelines. Observability as code is an often-forgotten aspect of infrastructure as code and, we believe, crucial enough to be called out.","blip_selector":"observability-as-code","name":"Observability as code","display_name":"Observability as code","url":"/radar/techniques/observability-as-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1314,"quadrant":"techniques","volume_date":"2018-11","description":"When it comes to large-scale data analysis or machine intelligence problems, being able to reproduce different versions of analysis done on different data sets and parameters is immensely valuable. To achieve reproducible analysis, both the data and the model (including algorithm choice, parameters and hyperparameters) need to be version controlled. Versioning data for reproducible analytics is a relatively trickier problem than versioning models because of the data size. Tools such as DVC help in versioning data by allowing users to commit and push data files to a remote cloud storage bucket using a git-like workflow. This makes it easy for collaborators to pull a specific version of data to reproduce an analysis.","blip_selector":"versioning-data-for-reproducible-analytics","name":"Versioning data for reproducible analytics","display_name":"Versioning data for reproducible analytics","url":"/radar/techniques/versioning-data-for-reproducible-analytics","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1284,"quadrant":"tools","volume_date":"2018-11","description":"We all obsess about fast feedback during test-driven development and we're always looking for new ways to make this even faster. Wallaby.js is a commercial extension for popular editors that provides continuous execution of JavaScript unit tests, highlighting the results in line next to your code. The tool identifies and runs the minimum set of tests affected by each code change and lets you run tests continuously as you type.","blip_selector":"wallaby-js","name":"Wallaby.js","display_name":"Wallaby.js","url":"/radar/tools/wallaby-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1341,"quadrant":"techniques","volume_date":"2018-11","description":"Master data management (MDM) is a classic example of the enterprise \"silver bullet\" solution: it promises to solve an apparently related class of problems in one go. Through creating a centralized single point of change, coordination, test and deployment, MDM solutions negatively impact an organization's ability to respond to business change. Implementations tend to be long and complex, as organizations try to capture and map all \"master\" data into the MDM while integrating the MDM solution into all consuming or producing systems.","blip_selector":"master-data-management","name":"Master data management","display_name":"Master data management","url":"/radar/techniques/master-data-management","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1323,"quadrant":"languages-and-frameworks","volume_date":"2018-11","description":"The SAFE stack —short for Suave, Azure, Fable, and Elmish—brings a number of technologies into a coherent stack for web development. It's built around the F# programming language, both on the server side and in the browser, and therefore has a focus on functional, type-safe programming with an asynchronous approach. It offers productivity features such as hot reloading and lets you substitute parts of the stack, for example, the server-side web framework or the cloud provider.","blip_selector":"safe-stack","name":"SAFE stack","display_name":"SAFE stack","url":"/radar/languages-and-frameworks/safe-stack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1202,"quadrant":"platforms","volume_date":"2018-05","description":"After thorough exploration, R3, an important player in the blockchain space, realized that blockchain doesn't fit their purpose well, so they created Corda. Corda is a distributed ledger technology (DLT) platform focused on the financial field. R3 have a very clear value proposition and know that their problem requires a pragmatic technology approach. This matches our own experience; current blockchain solutions may not be the reasonable choice for some business cases, due to mining costs and operational inefficiency. Although the development experience we have on Corda thus far has not been the smoothest, APIs are still unstable after v1.0 release, we expect to see the DLT space mature further. | After thorough exploration, R3, an important player in the blockchain space, realized that blockchain doesn't fit their purpose well, so they created Corda. Corda is a distributed ledger technology (DLT) platform focused on the financial field. R3 have a very clear value proposition and know that their problem requires a pragmatic technology approach. This matches our own experience; current blockchain solutions may not be the reasonable choice for some business cases, due to mining costs and operational inefficiency. Although the development experience we have on Corda thus far has not been the smoothest, APIs are still unstable after v1.0 release, we expect to see the DLT space mature further.","blip_selector":"corda","name":"Corda","display_name":"Corda","url":"/radar/platforms/corda","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1234,"quadrant":"tools","volume_date":"2018-05","description":"Parcel is a web application bundler similar to Webpack or Browserify. We’ve featured Webpack previously in our Radar and it continues to be a great tool. Parcel distinguishes itself from its rivals through developer experience and speed. It has all the standard bundling features and provides true zero-configuration experience, making it really easy to get started with and use. It has fast bundle times and beats its competitors in many benchmarks. Parcel has gained a lot of community interest and is worth keeping an eye on.","blip_selector":"parcel","name":"Parcel","display_name":"Parcel","url":"/radar/tools/parcel","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1237,"quadrant":"tools","volume_date":"2018-05","description":"We've been enjoying BackstopJS for visual regression testing of web applications. The configurable viewports and ability to adjust tolerances are particularly useful, as is the visual comparison tool, which makes it easier to spot minor variations. It has good scriptability and the option to run in Headless Chrome, PhantomJS and SlimerJS. We find it particularly helpful when running it against living component style guides.","blip_selector":"backstopjs","name":"BackstopJS","display_name":"BackstopJS","url":"/radar/tools/backstopjs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1195,"quadrant":"platforms","volume_date":"2018-05","description":"Much of the power of sophisticated IDEs comes from their ability to parse a program into an abstract syntax tree (AST) and then use that AST for program analysis and manipulation. This supports features such as autocomplete, finding callers and refactoring. Language servers pull this capability into a process that allows any text editor to access an API to work with the AST. Microsoft has led the creation of the Language Server Protocol (LSP), harvested from their OmniSharp and TypeScript Server projects.Any editor that uses this protocol can work with any language that has an LSP-compliant server. This means we can keep using our favorite editors without forgoing the rich text editing modes of many languages — much to the delight of our Emacs addicts. | Much of the power of sophisticated IDEs comes from their ability to parse a program into an abstract syntax tree (AST) and then use that AST for program analysis and manipulation. This supports features such as autocomplete, finding callers and refactoring. Language servers pull this capability into a process that allows any text editor to access an API to work with the AST. Microsoft has led the creation of the Language Server Protocol (LSP), harvested from their OmniSharp and TypeScript Server projects.Any editor that uses this protocol can work with any language that has an LSP-compliant server. This means we can keep using our favorite editors without forgoing the rich text editing modes of many languages — much to the delight of our Emacs addicts.","blip_selector":"language-server-protocol","name":"Language Server Protocol","display_name":"Language Server Protocol","url":"/radar/platforms/language-server-protocol","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1165,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"AssertJ is a Java library that provides a fluent interface for assertions, which makes it easy to convey intent within test code. AssertJ gives readable error messages, soft assertions and improved collections and exception support. Many of our teams choose AssertJ as their default assertion library instead of JUnit combined with Java Hamcrest. | AssertJ is a Java library that provides a fluent interface for assertions, which makes it easy to convey intent within test code. AssertJ gives readable error messages, soft assertions, and improved collections and exception support. We're seeing some teams default to its use instead of JUnit combined with Hamcrest.","blip_selector":"assertj","name":"AssertJ","display_name":"AssertJ","url":"/radar/languages-and-frameworks/assertj","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1188,"quadrant":"techniques","volume_date":"2018-05","description":"Blockchains have been widely hyped as the panacea for all things fintech, from banking to digital currency to supply chain transparency. We’ve previously featured Ethereum because of its feature set, which includes smart contracts. Now, we're seeing more development using Ethereum for decentralized applications in other areas. Although this is still a very young technology, we're encouraged to see it being used to build decentralized applications beyond cryptocurrency and banking. | Blockchains have been widely hyped as the panacea for all things fintech, from banking to digital currency to supply chain transparency. We’ve previously featured Ethereum because of its feature set, which includes smart contracts. Now, we're seeing more development using Ethereum for decentralized applications in other areas. Although this is still a very young technology, we're encouraged to see it being used to build decentralized applications beyond cryptocurrency and banking.","blip_selector":"ethereum-for-decentralized-applications","name":"Ethereum for decentralized applications","display_name":"Ethereum for decentralized applications","url":"/radar/techniques/ethereum-for-decentralized-applications","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1219,"quadrant":"tools","volume_date":"2018-05","description":"It’s surprising how many problems can be expressed as mathematical optimization problems and often convex problems that can be efficiently solved. CVXPY is an open source Python-embedded modeling language for convex optimization problems. It’s maintained by academics at Stanford University and offers a batteries-included install for several open source and commercial solvers. The documentation includes many examples which should inspire developers to use it. It’s particularly useful for prototyping solutions even though commercially licensed solvers, such Gurobi or IBM CPLEX, may be required. In most cases though, it suffices by itself. However, the same group has written many extension packages such as DCCP and related software such as CVXOPT based on recent advances in optimization.","blip_selector":"cvxpy","name":"CVXPY","display_name":"CVXPY","url":"/radar/tools/cvxpy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1240,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"The Hyperledger project has grown into a broader collaboration and now contains a series of subprojects. It supports Blockchain implementations for different purposes; for example, Burrow is dedicated to build a permissioned Ethereum and Indy is more focused on digital identity. Among these platforms, Fabric is the most mature one. Most of time when people talk about adopting Hyperledger they are actually thinking about Hyperledger Fabric. However, the programming abstraction of chaincode is relatively low level given it manipulates the state of the ledger directly. Moreover, it always takes a lot of time to set up infrastructure before writing the first line of blockchain code. Hyperledger Composer, which builds on top of Fabric, accelerates the process of turning ideas into software. Composer provides DSLs to model business assets, define access control and build a business network. By using Composer you could quickly validate your idea through a browser without setting up any infrastructure. Just remember that the Composer itself isn't Blockchain — you still need to deploy it on Fabric.","blip_selector":"hyperledger-composer","name":"Hyperledger Composer","display_name":"Hyperledger Composer","url":"/radar/languages-and-frameworks/hyperledger-composer","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1259,"quadrant":"platforms","volume_date":"2018-05","description":"Web Bluetooth allows us to control any Bluetooth Low Energy device directly from the browser. This allows us to target scenarios that previously could only be solved with a native app. The specification is published by the Web Bluetooth Community Group and describes an API to discover and communicate with devices over the Bluetooth 4 wireless standard. Right now, Chrome is the only major browser which currently supports this specification. With Physical Web and Web Bluetooth, we now have other avenues for getting users to interact with devices without them having to install yet another app on their phone. This is an exciting space which is worth keeping an eye on.","blip_selector":"web-bluetooth","name":"Web Bluetooth","display_name":"Web Bluetooth","url":"/radar/platforms/web-bluetooth","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1216,"quadrant":"techniques","volume_date":"2018-05","description":"It’s important to remember that encapsulation applies to events and event-driven architectures just as it applies to other areas of software. In particular, think about the scope of an event and whether we expect it to be consumed within the same application, the same domain or across an entire organization. A domain-scoped event will be consumed within the same domain as it’s published, as such we expect the consumer to have access to a certain context, resources or references in order to act on the event. If the consumption is happening more widely within an organization, the contents of the event might well need to be different, and we need to take care not to \"leak\" implementation details that other domains then come to depend upon.","blip_selector":"domain-scoped-events","name":"Domain-scoped events","display_name":"Domain-scoped events","url":"/radar/techniques/domain-scoped-events","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1258,"quadrant":"platforms","volume_date":"2018-05","description":"Cloud computing brings significant benefits over self-hosted virtualized solutions but sometimes data simply cannot leave an organization’s premises, usually for latency or regulatory reasons. For European companies, the current political climate also raises more concerns about placing data in the hands of US-based entities. With Azure Stack, Microsoft adds an interesting offering as a middle ground between full-featured public clouds and simple on-premises virtualization: a slimmed-down version of the software that runs Microsoft’s Azure Global cloud is combined with a rack of preconfigured commodity hardware from the usual suspects like HP and Lenovo, providing an organization with the core Azure experience on premises. By default, support is split between Microsoft and the hardware vendors (and they promise to cooperate), but system integrators can offer complete Azure Stack solutions, too.","blip_selector":"azure-stack","name":"Azure Stack","display_name":"Azure Stack","url":"/radar/platforms/azure-stack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1221,"quadrant":"tools","volume_date":"2018-05","description":"We've featured Appium in the Radar in the past. It's one of the most popular mobile test automation frameworks. As we scale our test suite, being able to run our tests in parallel against an array of devices is key in having short feedback loops. Appium Test Distribution solves this problem very effectively with its ability to run tests in parallel as well as run the same tests on multiple devices. Among other things, it distinguishes itself by its ability to add and remove devices in which tests run without any manual setup required and with its ability to run tests on remote devices. We've used it in a few projects at ThoughtWorks over the last couple of years and it worked very well for us.","blip_selector":"appium-test-distribution","name":"Appium Test Distribution","display_name":"Appium Test Distribution","url":"/radar/tools/appium-test-distribution","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1249,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"Rasa is a new entrant in the area of chatbots. Instead of using a simple decision tree it uses neural networks to map intent and internal state to a response. Rasa integrates with natural language processing solutions such as spaCy; and, unlike other solutions we've featured in the Radar, Rasa is open source software and can be self-hosted, which makes it a viable solution when ownership of data is of concern. Our experiences with using Rasa Stack for an internal application have been positive.","blip_selector":"rasa","name":"Rasa","display_name":"Rasa","url":"/radar/languages-and-frameworks/rasa","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1128,"quadrant":"techniques","volume_date":"2018-05","description":"Many organizations we work with are trying hard to use modern engineering approaches to build new capabilities and features, while also having to coexist with a long tail of legacy systems. An old strategy that, based on our experience, has turned out to be increasingly helpful in these scenarios is Eric Evans's Autonomous bubble pattern. This approach involves creating a fresh context for new application development that is shielded from the entanglements of the legacy world. This is a step beyond just using an anticorruption layer. It gives the new bubble context full control over its backing data, which is then asynchronously kept up-to-date with the legacy systems. It requires some work to protect the boundaries of the bubble and keep both worlds consistent, but the resulting autonomy and reduction in development friction is a first bold step toward a modernized future architecture. | Many organizations we work with are trying hard to use modern engineering approaches to build new capabilities and features, while also having to coexist with a long tail of legacy systems. An old strategy that, based on our experience, has turned out to be increasingly helpful in these scenarios is Eric Evans's Autonomous bubble pattern. This approach involves creating a fresh context for new application development that is shielded from the entanglements of the legacy world. This is a step beyond just using an anticorruption layer. It gives the new bubble context full control over its backing data, which is then asynchronously kept up-to-date with the legacy systems. It requires some work to protect the boundaries of the bubble and keep both worlds consistent, but the resulting autonomy and reduction in development friction is a first bold step toward a modernized future architecture.","blip_selector":"autonomous-bubble-pattern","name":"Autonomous bubble pattern","display_name":"Autonomous bubble pattern","url":"/radar/techniques/autonomous-bubble-pattern","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1196,"quadrant":"platforms","volume_date":"2018-05","description":"Cloud Spanner is a fully managed relational database service offering high availability and strong consistency without compromising latency. Google has been working on a globally distributed database called Spanner for quite some time. It has recently released the service to the outside world as Cloud Spanner. You can scale your database instance from one to thousands of nodes across the globe without worrying about data consistency. By levering TrueTime, a highly available and distributed clock, Cloud Spanner provides strong consistency for reads and snapshots. You can use standard SQL to read data from Cloud Spanner, but for write operations you have to use their RPC API. Although not all services would require a global-scale distributed database, the general availability of Cloud Spanner is a big shift in the way we think about databases. And its design is influencing open source products such as CockroachDB. | Cloud Spanner is a fully managed relational database service offering high availability and strong consistency without compromising latency. Google has been working on a globally distributed database called Spanner for quite some time. It has recently released the service to the outside world as Cloud Spanner. You can scale your database instance from one to thousands of nodes across the globe without worrying about data consistency. By levering TrueTime, a highly available and distributed clock, Cloud Spanner provides strong consistency for reads and snapshots. You can use standard SQL to read data from Cloud Spanner, but for write operations you have to use their RPC API. Although not all services would require a global-scale distributed database, the general availability of Cloud Spanner is a big shift in the way we think about databases. And its design is influencing open source products such as CockroachDB.","blip_selector":"cloud-spanner","name":"Cloud Spanner","display_name":"Cloud Spanner","url":"/radar/platforms/cloud-spanner","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1162,"quadrant":"tools","volume_date":"2018-05","description":"Sonobuoy is a diagnostic tool for running end-to-end conformance tests on any Kubernetes cluster in a nondestructive way. The team at Heptio, which was founded by two creators of the Kubernetes projects, built this tool to ensure that the wide array of Kubernetes distributions and configurations conform to the best practices, while following the open source standardization for interoperability of clusters. We're experimenting with Sonobuoy to run as part of our infrastructure as code build pipeline, as well as continuous monitoring of our Kubernetes installations, to validate the behavior and health of the whole cluster. | Sonobuoy is a diagnostic tool for running end-to-end conformance tests on any Kubernetes cluster in a nondestructive way. The team at Heptio, which was founded by two creators of the Kubernetes projects, built this tool to ensure that the wide array of Kubernetes distributions and configurations conform to the best practices, while following the open source standardization for interoperability of clusters. We're experimenting with Sonobuoy to run as part of our infrastructure as code build pipeline, as well as continuous monitoring of our Kubernetes installations, to validate the behavior and health of the whole cluster.","blip_selector":"sonobuoy","name":"Sonobuoy","display_name":"Sonobuoy","url":"/radar/tools/sonobuoy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1207,"quadrant":"techniques","volume_date":"2018-05","description":"Organizations are becoming more comfortable with the Polycloud strategy — rather than going \"all-in\" with one provider, they are passing different types of workloads to different providers based on their own strategy. Some of them apply the best-of-breed approach, for example: putting standard services on AWS, but using Google for machine learning and data-oriented applications and Azure for Microsoft Windows applications. For some organizations this is a cultural and business decision. Retail businesses, for example, often refuse to store their data on Amazon and they distribute load to different providers based on their data. This is different to a cloud-agnostic strategy of aiming for portability across providers, which is costly and forces lowest-common-denominator thinking. Polycloud instead focuses on using the best match that each cloud provider offers. | The major cloud providers (Amazon, Microsoft and Google) are locked in an aggressive race to maintain parity on core capabilities while their products are differentiated only marginally. This is causing a few organizations to adopt a Polycloud strategy — rather than going ‘all-in’ with one provider, they are passing different types of workloads to different providers in a best-of-breed approach. This may involve, for example, putting standard services on AWS, but using Google for machine learning, Azure for .NET applications that use SQLServer, or potentially using the Ethereum Consortium Blockchain solution. This is different than a cloud-agnostic strategy of aiming for portability across providers, which is costly and forces lowest-common-denominator thinking. Polycloud instead focuses on using the best that each cloud offers.","blip_selector":"polycloud","name":"Polycloud","display_name":"Polycloud","url":"/radar/techniques/polycloud","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1166,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"CSS is the preferred choice for laying out web pages, even when it did not provide much explicit support for creating layouts. Flexbox helped with simpler, one-dimensional layouts, but developers usually reached for libraries and toolkits for more complex layouts. CSS Grid Layout is a two-dimensional grid-based layout system that provides a mechanism to divide available space for layout into columns and rows using a set of predictable sizing behaviors. Grid does not require any libraries and plays well with Flexbox and other CSS display elements. However, since IE11 is only partially supported, it ignores users who still depend on a Microsoft browser on Windows 7. | CSS is the preferred choice for laying out web pages, even when it did not provide much explicit support for creating layouts. Flexbox helped with simpler, one-dimensional layouts, but developers usually reached for libraries and toolkits for more complex layouts. CSS Grid Layout is a two-dimensional grid-based layout system that provides a mechanism to divide available space for layout into columns and rows using a set of predictable sizing behaviors. Grid does not require any libraries and plays well with Flexbox and other CSS display elements. However, since IE11 is only partially supported, it ignores users who still depend on a Microsoft browser on Windows 7.","blip_selector":"css-grid-layout","name":"CSS Grid Layout","display_name":"CSS Grid Layout","url":"/radar/languages-and-frameworks/css-grid-layout","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":925,"quadrant":"platforms","volume_date":"2018-05","description":"Since we last mentioned Kubernetes in the Radar, it has become the default solution for most of our clients when deploying containers into a cluster of machines. The alternatives didn’t capture as much mindshare, and in some cases our clients are even changing their ‘engine’ to Kubernetes. Kubernetes has become the container orchestration platform of choice for major public cloud platforms, including Microsoft's Azure Container Service and Google Cloud (see the GKE blip). And there are many useful products enriching the fast-growing Kubernetes ecosystem. Platforms that try to hide Kubernetes under an abstraction layer, however, have yet to prove themselves. | Since we last mentioned Kubernetes in the Radar, it has become the default solution for most of our clients when deploying containers into a cluster of machines. The alternatives didn’t capture as much mindshare, and in some cases our clients are even changing their ‘engine’ to Kubernetes. Kubernetes has become the container orchestration platform of choice for major public cloud platforms, including Microsoft's Azure Container Service and Google Cloud (see the GKE blip). And there are many useful products enriching the fast-growing Kubernetes ecosystem. Platforms that try to hide Kubernetes under an abstraction layer, however, have yet to prove themselves. | Kubernetes is Google's answer to the problem of deploying containers into a cluster of machines, which is becoming an increasingly common scenario. It is not the solution used by Google internally but an open source project that originated at Google and has seen a fair number of external contributions. Since we mentioned Kubernetes on the previous Radar, our initial positive impressions have been confirmed, and we are seeing successful use of Kubernetes in production at our clients. | Kubernetes is Google's answer to the problem of deploying containers into a cluster of machines, which is becoming an increasingly common scenario. It is not the solution used by Google internally but an open source project that originated at Google and has seen a fair number of external contributions. Docker and Rocket are supported as container formats, and services offered include health management, replication and discovery. A similar solution in this space is Rancher.","blip_selector":"kubernetes","name":"Kubernetes","display_name":"Kubernetes","url":"/radar/platforms/kubernetes","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1198,"quadrant":"platforms","volume_date":"2018-05","description":"Machine-learning models are starting to creep into everyday business applications. When enough training data is available, these algorithms can address problems that might have previously required complex statistical models or heuristics. As we move from experimental use to production, we need a reliable way to host and deploy the models that can be accessed remotely and scale with the number of consumers. TensorFlow Serving addresses part of that problem by exposing a remote gRPC interface to an exported model; this allows a trained model to be deployed in a variety of ways. TensorFlow Serving also accepts a stream of models to incorporate continuous training updates, and its authors maintain a Dockerfile to ease the deployment process. Presumably, the choice of gRPC is to be consistent with the TensorFlow execution model; however, we’re generally wary of protocols that require code generation and native bindings. | Machine-learning models are starting to creep into everyday business applications. When enough training data is available, these algorithms can address problems that might have previously required complex statistical models or heuristics. As we move from experimental use to production, we need a reliable way to host and deploy the models that can be accessed remotely and scale with the number of consumers. TensorFlow Serving addresses part of that problem by exposing a remote gRPC interface to an exported model; this allows a trained model to be deployed in a variety of ways. TensorFlow Serving also accepts a stream of models to incorporate continuous training updates, and its authors maintain a Dockerfile to ease the deployment process. Presumably, the choice of gRPC is to be consistent with the TensorFlow execution model; however, we’re generally wary of protocols that require code generation and native bindings.","blip_selector":"tensorflow-serving","name":"TensorFlow Serving","display_name":"TensorFlow Serving","url":"/radar/platforms/tensorflow-serving","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1163,"quadrant":"tools","volume_date":"2018-05","description":"Flow is a static type checker for JavaScript that allows you to add type checking across the codebase incrementally. Unlike Typescript, which is a different language, Flow can be added incrementally to an existing JavaScript codebase supporting the 5th, 6th and 7th editions of ECMAScript. We suggest adding Flow to your continuous integration pipeline, starting with the code that concerns you most. Flow adds to the clarity of the code, increases the reliability of refactoring and catches type-related bugs early during the build. | Flow is a static type checker for JavaScript that allows you to add type checking across the codebase incrementally. Unlike Typescript, which is a different language, Flow can be added incrementally to an existing JavaScript codebase supporting the 5th, 6th and 7th editions of ECMAScript. We suggest adding Flow to your continuous integration pipeline, starting with the code that concerns you most. Flow adds to the clarity of the code, increases the reliability of refactoring and catches type-related bugs early during the build.","blip_selector":"flow","name":"Flow","display_name":"Flow","url":"/radar/tools/flow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1211,"quadrant":"techniques","volume_date":"2018-05","description":"Previously in the Radar, we’ve discussed the rise of the perimeterless enterprise. Now, some organizations are doing away with implicitly trusted intranets altogether and treating all communication as if it was being transmitted through the public internet. A set of practices, collectively labeled BeyondCorp, have been described by Google engineers in a set of publications. Collectively, these practices — including managed devices, 802.1x networking and standard access proxies protecting individual services — make this a viable approach to network security in large enterprises.","blip_selector":"beyondcorp","name":"BeyondCorp","display_name":"BeyondCorp","url":"/radar/techniques/beyondcorp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1152,"quadrant":"tools","volume_date":"2018-05","description":"kops is a command line tool for creating and managing high-availability production Kubernetes clusters. kops has become our go-to tool to self-manage Kubernetes clusters on AWS, not the least because of its rapidly growing open source community. It also supports installing, upgrading and managing Kubernetes clusters on Google Cloud. Our experience with kops on Google, however, is very limited because of our preference for GKE, the managed Kubernetes offering. We recommend using kops in reusable scripts to create infrastructure as code. We're interested to see how kops continues to evolve to support managed Kubernetes clusters such as EKS, Amazon's own managed Kubernetes service. | kops is a command line tool for creating and managing high-availability production Kubernetes clusters. Initially targeting AWS, it now has experimental support for other providers. It can get you up and running fast, and even though a few features (such as rolling upgrades) have yet to be fully developed, we've been impressed by the community.","blip_selector":"kops","name":"kops","display_name":"kops","url":"/radar/tools/kops","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1150,"quadrant":"tools","volume_date":"2018-05","description":"Apex is a tool to build, deploy and manage AWS Lambda functions with ease. With Apex, you can write functions in languages that are not yet natively supported in AWS, including Golang, Rust and others. This is made possible by a Node.js shim, which creates a child process and processes events through stdin and stdout. Apex has a lot of nice features that improve the developer experience, and we particularly like the ability to test functions locally and perform a dry run of the changes before they're applied to AWS resources. | Apex is a tool to build, deploy and manage AWS Lambda functions with ease. With Apex, you can write functions in languages that are not yet natively supported in AWS, including Golang, Rust and others. This is made possible by a Node.js shim, which creates a child process and processes events through stdin and stdout. Apex has a lot of nice features that improve the developer experience, and we particularly like the ability to test functions locally and perform a dry run of the changes before they're applied to AWS resources.","blip_selector":"apex","name":"Apex","display_name":"Apex","url":"/radar/tools/apex","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1214,"quadrant":"techniques","volume_date":"2018-05","description":"We're seeing some interesting reports of using Jupyter for automated testing. The ability to mix code, comments and output in the same document reminds us of FIT, FitNesse and Concordion. This flexible approach is particularly useful if your tests are data heavy or rely on some statistical analysis such as performance testing. Python provides all the power you need, but as tests grow in complexity, a way to manage suites of notebooks would be helpful.","blip_selector":"jupyter-for-automated-testing","name":"Jupyter for automated testing","display_name":"Jupyter for automated testing","url":"/radar/techniques/jupyter-for-automated-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1253,"quadrant":"platforms","volume_date":"2018-05","description":"EMQ is a scalable open source multiplatform MQTT broker. It’s written in Erlang/OTP for higher performance, handling millions of concurrent connections. It supports multiple protocols including MQTT, MQTT Sensor Networks, CoAP as well as WebSockets, making it suitable for both IoT and mobile devices. We’ve started using EMQ in our projects and have enjoyed its ease of installation and use, its ability to route messages to different destinations including Kafka and PostgreSQL, as well as its API-driven approach for its monitoring and configuration.","blip_selector":"emq","name":"EMQ","display_name":"EMQ","url":"/radar/platforms/emq","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1236,"quadrant":"tools","volume_date":"2018-05","description":"In the current state of technology services, exposing RESTFul APIs is increasingly adopted and API documentation is very important for consumers. In this space, Swagger has been largely used across teams and we would like to highlight Swashbuckle for .NET Core. Swashbuckle for .NET Core is a tool that generates living API documentation in Swagger, based on the code for .NET Core projects. When using it, you can also explore and test operations of APIs through its UI.","blip_selector":"swashbuckle-for-net-core","name":"Swashbuckle for .NET Core","display_name":"Swashbuckle for .NET Core","url":"/radar/tools/swashbuckle-for-net-core","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1220,"quadrant":"tools","volume_date":"2018-05","description":"A key driver for architectures based on microservices is independent evolvability of services. For example, when two services depend on each other, the testing process for one usually involves stubs and mocks for the other one. These can be written by hand, but as with mocking in unit tests, a framework helps developers focus on the actual test scenario. We have known of WireMock for a while but we’ve preferred running tests with mountebank. Over the past year, though, WireMock has really caught up and we now recommend it as a good alternative.","blip_selector":"wiremock","name":"WireMock","display_name":"WireMock","url":"/radar/tools/wiremock","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1190,"quadrant":"platforms","volume_date":"2018-05","description":"Load testing became easier with the maturity of tools such as Gatling and Locust. At the same time, elastic cloud infrastructures make it possible to simulate a large number of client instances. We're delighted to see Flood and other cloud platforms go further by leveraging these technologies. Flood IO is an SaaS load-testing service that helps to distribute and execute testing scripts across hundreds of servers in the cloud. Our teams find it simple to migrate performance testing to Flood by reusing existing Gatling scripts. | Load testing became easier with the maturity of tools such as Gatling and Locust. At the same time, elastic cloud infrastructures make it possible to simulate a large number of client instances. We're delighted to see Flood and other cloud platforms go further by leveraging these technologies. Flood IO is an SaaS load-testing service that helps to distribute and execute testing scripts across hundreds of servers in the cloud. Our teams find it simple to migrate performance testing to Flood by reusing existing Gatling scripts.","blip_selector":"flood-io","name":"Flood IO","display_name":"Flood IO","url":"/radar/platforms/flood-io","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1189,"quadrant":"platforms","volume_date":"2018-05","description":"WeChat , often seen as a WhatsApp equivalent, is becoming the de facto business platform in China. Many people may not know but WeChat is also one of the most popular online payment platforms. With the app's built-in CMS and membership management, small businesses are now conducting their commerce entirely on WeChat. Through the Service Account feature, large organizations can interface their internal system to their employees. Given that more than 70 percent of Chinese people are using WeChat, it's an important consideration for businesses that want to expand into the China market. | WeChat , often seen as a WhatsApp equivalent, is becoming the de facto business platform in China. Many people may not know but WeChat is also one of the most popular online payment platforms. With the app's built-in CMS and membership management, small businesses are now conducting their commerce entirely on WeChat. Through the Service Account feature, large organizations can interface their internal system to their employees. Given that more than 70 percent of Chinese people are using WeChat, it's an important consideration for businesses that want to expand into the China market.","blip_selector":"wechat","name":"WeChat","display_name":"WeChat","url":"/radar/platforms/wechat","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1210,"quadrant":"techniques","volume_date":"2018-05","description":"One problem with observability in a highly distributed microservices architecture is the choice between logging everything — and taking up huge amounts of storage space — or randomly sampling logs and potentially missing important events. Recently, we’ve noticed a technique that offers a compromise between these two solutions. Set the log level per request via a parameter passed in through the tracing header. Using a tracing framework, possibly based on the OpenTracing standard, you can pass a correlation id from service to service in a single transaction. You can even inject other data, such as the desired log level, at the initiating transaction and pass it along with the tracing information. This ensures that the additional data collected corresponds to a single user transaction as it flows through the system. This is also a useful technique for debugging, since services might be paused or otherwise modified on a transaction-by-transaction basis.","blip_selector":"log-level-per-request","name":"Log level per request","display_name":"Log level per request","url":"/radar/techniques/log-level-per-request","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1187,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"Most large CSS codebases require complex naming schemes to help avoid naming conflicts in the global namespace. CSS Modules address these problems by creating a local scope for all class names in a single CSS file. This file is imported to a JavaScript module, where CSS classes are referenced as strings. Then, in the build pipeline (Webpack, Browserify, etc.), the class names are replaced with generated unique strings. This is a significant change in responsibilities. Previously, a human had to manage the global namespace, to avoid class naming conflicts; now that responsibility rests with the build tooling. A small downside we've encountered with CSS Modules: functional tests are usually out of the local scope and can therefore not reference classes by the name defined in the CSS file. We recommend using IDs or data attributes instead. | Most large CSS codebases require complex naming schemes to help avoid naming conflicts in the global namespace. CSS Modules address these problems by creating a local scope for all class names in a single CSS file. This file is imported to a JavaScript module, where CSS classes are referenced as strings. Then, in the build pipeline (Webpack, Browserify, etc.), the class names are replaced with generated unique strings. This is a significant change in responsibilities. Previously, a human had to manage the global namespace, to avoid class naming conflicts; now that responsibility rests with the build tooling. A small downside we've encountered with CSS Modules: functional tests are usually out of the local scope and can therefore not reference classes by the name defined in the CSS file. We recommend using IDs or data attributes instead.","blip_selector":"css-modules","name":"CSS Modules","display_name":"CSS Modules","url":"/radar/languages-and-frameworks/css-modules","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1218,"quadrant":"tools","volume_date":"2018-05","description":"Patroni is a template for PostgreSQL high availability. Born out of the need to provide automatic failure for PostgreSQL, Patroni is a Python-based PostgreSQL controller that leverages a distributed configuration store (such as etcd, ZooKeeper, or Consul) to manage the state of the PostgreSQL cluster. Patroni supports both streaming and synchronous replication models and provides a rich set of REST APIs for dynamic configuration of the PostgreSQL cluster. If you want to achieve high availability in a distributed PostgreSQL setup, you have to consider many edge cases, and we like the fact that Patroni provides a template to achieve most of the common use cases.","blip_selector":"patroni","name":"Patroni","display_name":"Patroni","url":"/radar/tools/patroni","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1034,"quadrant":"techniques","volume_date":"2018-05","description":"Much documentation can be replaced with highly readable code and tests. In a world of evolutionary architecture, however, it's important to record certain design decisions for the benefit of future team members as well as for external oversight. Lightweight Architecture Decision Records is a technique for capturing important architectural decisions along with their context and consequences. We recommend storing these details in source control, instead of a wiki or website, as then they can provide a record that remains in sync with the code itself. For most projects, we see no reason why you wouldn't want to use this technique. | Much documentation can be replaced with highly readable code and tests. In a world of evolutionary architecture, however, it's important to record certain design decisions for the benefit of future team members as well as for external oversight. Lightweight Architecture Decision Records is a technique for capturing important architectural decisions along with their context and consequences. We recommend storing these details in source control, instead of a wiki or website, as then they can provide a record that remains in sync with the code itself. For most projects, we see no reason why you wouldn't want to use this technique. | Although much documentation can be replaced with highly readable code and tests, in a world of evolutionary architecture it's important to record certain design decisions for the benefit of future team members and for external oversight. Lightweight Architecture Decision Records is a technique for capturing important architectural decisions along with their context and consequences. Although these items are often stored in a wiki or collaboration tool, we generally prefer storing them in source control with simple markup. | Although much documentation can be replaced with highly readable code and tests, in a world of evolutionary architecture it's important to record certain design decisions for the benefit of future team members and for external oversight. Lightweight Architecture Decision Records is a technique for capturing important architectural decisions along with their context and consequences. Although these items are often stored in a wiki or collaboration tool, we generally prefer storing them in source control with simple markup.","blip_selector":"lightweight-architecture-decision-records","name":"Lightweight Architecture Decision Records","display_name":"Lightweight Architecture Decision Records","url":"/radar/techniques/lightweight-architecture-decision-records","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1201,"quadrant":"platforms","volume_date":"2018-05","description":"We like simple tools that solve one problem really well, and Netlify fits this description nicely. You can create static website content, check it into GitHub and then quickly and easily get your site live and available. There is a CLI available to control the process; content delivery networks (CDNs) are supported; it can work alongside tools such as Grunt; and, most importantly, Netlify supports HTTPS. | We like simple tools that solve one problem really well, and Netlify fits this description nicely. You can create static website content, check it into GitHub and then quickly and easily get your site live and available. There is a CLI available to control the process; content delivery networks (CDNs) are supported; it can work alongside tools such as Grunt; and, most importantly, Netlify supports HTTPS.","blip_selector":"netlify","name":"Netlify","display_name":"Netlify","url":"/radar/platforms/netlify","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1181,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"Truffle is a development framework that brings a modern web development experience to the Ethereum platform. It takes over the job of smart contract compiling, library linking and deployment, as well as dealing with artifacts in different blockchain networks. One of the reasons we love Truffle is that it encourages people to write tests for their smart contracts. You need to take tests really seriously as smart contract programming is often related to money. With its built-in testing framework and integration with TestRPC, Truffle makes it possible to write the contract in a TDD way. We expect to see more technologies similar to Truffle to promote continuous integration in the blockchain area. | Truffle is a development framework that brings a modern web development experience to the Ethereum platform. It takes over the job of smart contract compiling, library linking and deployment, as well as dealing with artifacts in different blockchain networks. One of the reasons we love Truffle is that it encourages people to write tests for their smart contracts. You need to take tests really seriously as smart contract programming is often related to money. With its built-in testing framework and integration with TestRPC, Truffle makes it possible to write the contract in a TDD way. We expect to see more technologies similar to Truffle to promote continuous integration in the blockchain area.","blip_selector":"truffle","name":"Truffle","display_name":"Truffle","url":"/radar/languages-and-frameworks/truffle","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1191,"quadrant":"platforms","volume_date":"2018-05","description":"Cosmos DB is Microsoft's globally distributed, multimodel database service, which became generally available earlier this year. While most modern NoSQL databases offer tunable consistency, Cosmos DB makes it a first-class citizen and offers five different consistency models. It's worth highlighting that it also supports multiple models — key value, document, column family and graph — all of which map to its internal data model, called atom-record-sequence (ARS). One interesting aspect of Cosmos DB is that it offers service level agreements (SLAs) on its latency, throughput, consistency and availability. With its wide range of applicability, it has set a high standard for other cloud vendors to match. | Cosmos DB is Microsoft's globally distributed, multimodel database service, which became generally available earlier this year. While most modern NoSQL databases offer tunable consistency, Cosmos DB makes it a first-class citizen and offers five different consistency models. It's worth highlighting that it also supports multiple models — key value, document, column family and graph — all of which map to its internal data model, called atom-record-sequence (ARS). One interesting aspect of Cosmos DB is that it offers service level agreements (SLAs) on its latency, throughput, consistency and availability. With its wide range of applicability, it has set a high standard for other cloud vendors to match.","blip_selector":"cosmos-db","name":"Cosmos DB","display_name":"Cosmos DB","url":"/radar/platforms/cosmos-db","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1262,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"Given the number of JavaScript application frameworks we’ve featured in the Radar over the years we asked ourselves, do we really need to call out another one? We decided that Hyperapp is worth a look because of its minimalist approach. It has a very small footprint, less than 1KB, and yet covers all the essential functionality for writing a web application. This is only possible with an elegant design that reduces everything to the absolute minimum, which in turn makes it easier to understand and use the framework. Despite being relatively new, it has attracted a good-size community and we recommend to at least consider it when picking a framework for a new application.","blip_selector":"hyperapp","name":"Hyperapp","display_name":"Hyperapp","url":"/radar/languages-and-frameworks/hyperapp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1077,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"Kotlin has experienced an accelerated rate of adoption and rapid growth of tooling support. Some of the reasons behind its popularity are its concise syntax, null safety, ease of transition from Java and interoperability with other JVM-based languages in general, and that it doubles as a great introductory language to functional programming. With JetBrains adding the ability to compile Kotlin to native binaries on multiple platforms, as well as transpile to JavaScript, we believe it has the potential of much wider use by the larger community of mobile and native application developers. Although at the time of writing, some of the tooling such as static and coverage code analysis have yet to mature, given our experience of using Kotlin in many production applications, we believe Kotlin is ready for general adoption. | The announcement of first-class Android support has given an extra boost to the rapidly progressing Kotlin language, and we're closely following the progress of Kotlin/Native — the LLVM-backed ability to compile to native executables. Null safety, data classes and the ease of creating DSLs are some of the benefits we've enjoyed, along with the Anko library for Android development. Despite the downsides of slow initial compilation and reliance on IntelliJ for first-class IDE support, we recommend giving this fresh and concise modern language a try. | The Kotlin programming language is on many of our developers' bucket lists to assess this year, and some have already used it successfully in production. It is an open source JVM language from JetBrains. Our Swift mobile developers like it as it is syntactically closer to Swift and equally concise. Our Java developers have enjoyed its seamless interoperability with the Java language and tools and found it easier to learn than Scala. Kotlin supports functional programming concepts but with less features than Scala. Developers on our teams who like static typing with the compiler catching null pointer defects found themselves writing fewer boilerplate tests.","blip_selector":"kotlin","name":"Kotlin","display_name":"Kotlin","url":"/radar/languages-and-frameworks/kotlin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1146,"quadrant":"tools","volume_date":"2018-05","description":"Our teams very much like the hosted CI/CD tool Buildkite for its simplicity and quick setup. With Buildkite, you provide your own machines to execute builds — on premise or in the cloud — and install a lightweight agent application to connect the build agent to the hosted service. In many cases, having this level of control over the configuration of your build agents is a plus when compared to using hosted agents. | Our teams very much like the hosted CI/CD tool Buildkite for its simplicity and quick setup. With Buildkite, you provide your own machines to execute builds — on premise or in the cloud — and install a lightweight agent application to connect the build agent to the hosted service. In many cases, having this level of control over the configuration of your build agents is a plus when compared to using hosted agents.","blip_selector":"buildkite","name":"Buildkite","display_name":"Buildkite","url":"/radar/tools/buildkite","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1193,"quadrant":"platforms","volume_date":"2018-05","description":"While the software development ecosystem is converging on Kubernetes as the orchestration platform for containers, running Kubernetes clusters remains operationally complex. Google Kubernetes Engine ( GKE ) is a managed Kubernetes solution for deploying containerized applications that alleviates the operational overhead of running and maintaining Kubernetes clusters. Our teams have had a good experience using GKE, with the platform doing the heavy lifting of applying security patches, monitoring and auto-repairing the nodes, and managing multicluster and multiregion networking. In our experience, Google's API-first approach in exposing platform capabilities, as well as using industry standards such as OAuth for service authorisation, improve the developer experience. It's important to consider that GKE is under rapid development with many of its APIs in beta release which, despite the developers' best efforts to abstract consumers from underlying changes, can impact you. We're expecting continuous improvement around maturity of infrastructure as code with Terraform on GKE and similar tools. | While the software development ecosystem is converging on Kubernetes as the orchestration platform for containers, running Kubernetes clusters remains operationally complex. GKE (Google Kubernetes Engine) is a managed Kubernetes solution for deploying containerized applications that alleviates the operational overhead of running and maintaining Kubernetes clusters. Our teams have had a good experience using GKE, with the platform doing the heavy lifting of applying security patches, monitoring and auto-repairing the nodes, and managing multicluster and multiregion networking. In our experience, Google's API-first approach in exposing platform capabilities, as well as using industry standards such as OAuth for service authorization, improve the developer experience. It's important to consider that GKE is under rapid development which, despite the developers' best efforts to abstract consumers from underlying changes, has impacted us temporarily in the past. We're expecting continuous improvement around maturity of Infrastructure as code with Terraform on GKE and similar tools.","blip_selector":"gke","name":"GKE","display_name":"GKE","url":"/radar/platforms/gke","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1248,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"In the last issue we featured PyTorch, a deep-learning modeling framework that allows an imperative programming style. Now TensorFlow Eager Execution provides this imperative style in TensorFlow by enabling execution of modeling statements outside of the context of a session. This improvement could provide the ease of debugging and finer-grained model control of PyTorch with the widespread popularity and performance of TensorFlow models. The feature is still quite new so we’re anxious to see how it performs and how it’ll be received by the TensorFlow community.","blip_selector":"tensorflow-eager-execution","name":"Tensorflow Eager Execution","display_name":"Tensorflow Eager Execution","url":"/radar/languages-and-frameworks/tensorflow-eager-execution","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1209,"quadrant":"techniques","volume_date":"2018-05","description":"Identity management is a critical platform component. External users on mobile apps need to be authenticated, developers need to be given access to delivery infrastructure components, and microservices may need to identify themselves to other microservices. You should ask yourself whether identity management should be “self-hosted”. In our experience, a hosted identity management as a service (SaaS) solution is preferable. We believe that top-tier hosted providers such as Auth0 and Okta can provide better uptime and security SLAs. That said, sometimes self-hosting the solution is a realistic decision, especially for enterprises that have the operational discipline and resources to do so safely. Large enterprise identity solutions typically offer a much more expansive range of capabilities such as centralized entitlements, governance reporting and separation of duties management among others. However, these concerns are typically more relevant for employee identities, especially in regulated enterprises with legacy systems.","blip_selector":"hosted-identity-management-as-a-service","name":"Hosted identity management as a service","display_name":"Hosted identity management as a service","url":"/radar/techniques/hosted-identity-management-as-a-service","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1168,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"The ability to compile the Go programming language to bare metal targets has raised interest among developers in using the language for embedded systems. Gobot is a framework for robotics, physical computing, and the Internet of Things, written in the Go programming language and supporting a variety of platforms. We've used the framework for experimental robotic projects where real-time response hasn't been a requirement, and we’ve created open source software drivers with Gobot. Gobot HTTP APIs enable simple hardware integration with mobile devices to create richer applications. | The ability to compile the Go programming language to bare metal targets has raised interest among developers in using the language for embedded systems. Gobot is a framework for robotics, physical computing, and the Internet of Things, written in the Go programming language and supporting a variety of platforms. We've used the framework for experimental robotic projects where real-time response hasn't been a requirement, and we’ve created open source software drivers with Gobot. Gobot HTTP APIs enable simple hardware integration with mobile devices to create richer applications.","blip_selector":"gobot","name":"Gobot","display_name":"Gobot","url":"/radar/languages-and-frameworks/gobot","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1245,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"We’re in favor of asynchronous and reactive styles of programming especially for network I/O-bound distributed systems. Reactive libraries often sit on top of a lower level nonblocking communication framework such as Netty. Recently SwiftNIO, an open source nonblocking networking framework from Apple, has grabbed our attention. SwiftNIO is similar to Netty but written in Swift. It’s currently supported on MacOS and Ubuntu and implements HTTP as a higher-level protocol. We’re excited to see the usage of this framework and integration of it into higher-level application frameworks and other protocols.","blip_selector":"swiftnio","name":"SwiftNIO","display_name":"SwiftNIO","url":"/radar/languages-and-frameworks/swiftnio","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1179,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"A multi-app strategy is really controversial, particularly at a time when fewer and fewer users are downloading new apps. Instead of introducing a new app and struggling with the download numbers, multiteams have to deliver functionality via a single app that is already widely installed, which creates an architectural challenge. Atlas and BeeHive are modularization solutions for Android and iOS apps, respectively. Atlas and BeeHive enable multiteams working on physically isolated modules to reassemble or dynamically load these modules from a facade app. Both are Alibaba open source projects, since Alibaba encountered the same problem of dwindling downloads and single-app architectural challenges. | A multi-app strategy is really controversial, particularly at a time when fewer and fewer users are downloading new apps. Instead of introducing a new app and struggling with the download numbers, multiteams have to deliver functionality via a single app that is already widely installed, which creates an architectural challenge. Atlas and BeeHive are modularization solutions for Android and iOS apps, respectively. Atlas and BeeHive enable multiteams working on physically isolated modules to reassemble or dynamically load these modules from a facade app. Both are Alibaba open source projects, since Alibaba encountered the same problem of dwindling downloads and single-app architectural challenges.","blip_selector":"atlas-and-beehive","name":"Atlas and BeeHive","display_name":"Atlas and BeeHive","url":"/radar/languages-and-frameworks/atlas-and-beehive","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1142,"quadrant":"techniques","volume_date":"2018-05","description":"Kafka is becoming very popular as a messaging solution, and along with it, Kafka Streams is at the forefront of the wave of interest in streaming architectures. Unfortunately, as they start to embed Kafka at the heart of their data and application platforms, we're seeing some organizations recreating ESB antipatterns with Kafka by centralizing the Kafka ecosystem components — such as connectors and stream processors — instead of allowing these components to live with product or service teams. This reminds us of seriously problematic ESB antipatterns, where more and more logic, orchestration and transformation were thrust into a centrally managed ESB, creating a significant dependency on a centralized team. We're calling this out to dissuade further implementations of this flawed pattern. | Kafka is becoming very popular as a messaging solution, and along with it, Kafka Streams is at the forefront of the wave of interest in streaming architectures. Unfortunately, as they start to embed Kafka at the heart of their data and application platforms, we're seeing some organizations recreating ESB antipatterns with Kafka by centralizing the Kafka ecosystem components — such as connectors and stream processors — instead of allowing these components to live with product or service teams. This reminds us of seriously problematic ESB antipatterns, where more and more logic, orchestration and transformation were thrust into a centrally managed ESB, creating a significant dependency on a centralized team. We're calling this out to dissuade further implementations of this flawed pattern.","blip_selector":"recreating-esb-antipatterns-with-kafka","name":"Recreating ESB antipatterns with Kafka","display_name":"Recreating ESB antipatterns with Kafka","url":"/radar/techniques/recreating-esb-antipatterns-with-kafka","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1241,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"RIBs — which is short for router, interactor and builder — is a cross-platform architecture mobile framework from Uber. The key idea of RIBs is to decouple business logic from the view tree, and thus ensure the app is driven by business logic. By applying consistent architecture patterns across native Android and iOS, RIBs provides clear statement management and good testability. We advise putting business logic in the back-end service rather than leak it into the view, so if you do have a complicated mobile application, RIBs can help manage this complexity.","blip_selector":"ribs","name":"RIBs","display_name":"RIBs","url":"/radar/languages-and-frameworks/ribs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1229,"quadrant":"tools","volume_date":"2018-05","description":"The cloud and continuous delivery had a dramatic effect on infrastructure security. When following infrastructure as code, the entire infrastructure — which includes networks, firewalls and accounts — is defined in scripts and configuration files, and with Phoenix Servers and Environments, the infrastructure is recreated in each deployment, often many times a day. In such a scenario, testing the infrastructure after it's created is neither sufficient nor feasible. A tool that helps address this problem is cfn_nag. It scans the CloudFormation templates used with AWS for patterns that may indicate insecure infrastructure, and it does so before the infrastructure is created. Running a tool such as cfn_nag in a build pipeline is fast and it can detect a number of problems before they even reach a cloud environment.","blip_selector":"cfn-nag","name":"cfn_nag","display_name":"cfn_nag","url":"/radar/tools/cfn-nag","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":931,"quadrant":"platforms","volume_date":"2018-05","description":"We remain concerned about business logic and process orchestration implemented in middleware, especially where it requires expert skills and tooling while creating single points of scaling and control. Vendors in the highly competitive API gateway market are continuing this trend by adding features through which they attempt to differentiate their products. This results in overambitious API gateway products whose functionality — on top of what is essentially a reverse proxy — encourages designs that continue to be difficult to test and deploy. API gateways do provide utility in dealing with some specific concerns — such as authentication and rate limiting — but any domain smarts should live in applications or services. | We remain concerned about business logic and process orchestration implemented in middleware, especially where it requires expert skills and tooling while creating single points of scaling and control. Vendors in the highly competitive API gateway market are continuing this trend by adding features through which they attempt to differentiate their products. This results in overambitious API gateway products whose functionality — on top of what is essentially a reverse proxy — encourages designs that continue to be difficult to test and deploy. API gateways do provide utility in dealing with some specific concerns — such as authentication and rate limiting — but any domain smarts should live in applications or services. | One of our regular complaints is about business smarts implemented in middleware, resulting in transport software with ambitions to run critical application logic. Vendors in the highly competitive API gateway market continue to add features that differentiate their products. This results in overambitious API gateway products whose functionality—on top of what is essentially a reverse proxy—encourages designs that are difficult to test and deploy. API gateways can provide utility in dealing with some generic concerns—for example, authentication and rate-limiting—but any domain smarts such as data transformation or rule processing should live in applications or services where they can be controlled by product teams working closely with the domains they support. | One of our regular complaints is about business smarts implemented in middleware, resulting in transport software with ambitions to run critical application logic. Vendors in the highly competitive API gateway market continue to add features that differentiate their products. This results in overambitious API gateway products whose functionality—on top of what is essentially a reverse proxy—encourages designs that are difficult to test and deploy. API gateways can provide utility in dealing with some generic concerns—for example, authentication and rate-limiting—but any domain smarts such as data transformation or rule processing should live in applications or services where they can be controlled by product teams working closely with the domains they support. | One of our common complaints is the pushing of business smarts into middleware, resulting in application servers and enterprise service buses with ambitions to run critical application logic. These require complex programming in environments not well suited to the purpose. We're seeing a worrying re-emergence of this disease with overambitious API Gateway products. API Gateways can provide utility in dealing with some generic concerns - for example, authentication and rate-limiting - but any domain smarts such as data transformation or rule processing should live in applications or services where they can be controlled by product teams working closely with the domains they support.","blip_selector":"overambitious-api-gateways","name":"Overambitious API gateways","display_name":"Overambitious API gateways","url":"/radar/platforms/overambitious-api-gateways","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1132,"quadrant":"techniques","volume_date":"2018-05","description":"Borrowed from evolutionary computing, a fitness function is used to summarize how close a given design solution is to achieving the set aims. When defining an evolutionary algorithm, the designer seeks a ‘better’ algorithm; the fitness function defines what ‘better’ means in this context. An architectural fitness function , as defined in Building Evolutionary Architectures, provides an objective integrity assessment of some architectural characteristics, which may encompass existing verification criteria, such as unit testing, metrics, monitors, and so on. We believe architects can communicate, validate and preserve architectural characteristics in an automated, continual manner, which is the key to building evolutionary architectures. | Borrowed from evolutionary computing, a fitness function is used to summarize how close a given design solution is to achieving the set aims. When defining an evolutionary algorithm, the designer seeks a ‘better’ algorithm; the fitness function defines what ‘better’ means in this context. An architectural fitness function , as defined in Building Evolutionary Architectures, provides an objective integrity assessment of some architectural characteristics, which may encompass existing verification criteria, such as unit testing, metrics, monitors, and so on. We believe architects can communicate, validate and preserve architectural characteristics in an automated, continual manner, which is the key to building evolutionary architectures.","blip_selector":"architectural-fitness-function","name":"Architectural fitness function","display_name":"Architectural fitness function","url":"/radar/techniques/architectural-fitness-function","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1159,"quadrant":"platforms","volume_date":"2018-05","description":"Kong is an open source API gateway which also comes as an enterprise product integrating with proprietary API analytics and a developer portal. Kong can be deployed, in a variety of configurations, as an edge API gateway, as an internal API proxy, or even as a sidecar in a service mesh configuration. OpenResty, through its Nginx modules, provides a strong and performant foundation, with Lua plugins for extensions. Kong can either use PostgreSQL for single-region deployments or Cassandra for multiregion configurations. Our developers have enjoyed Kong's high performance, its API-first approach (which enables automation of its configuration) and its ease of deployment as a container. Kong API Gateway , unlike overambitious API gateways, has a smaller set of features but it implements the essential set of API gateway capabilities such as traffic control, security, logging, monitoring and authentication. | Kong is an open source API gateway built and sponsored by Mashape, who also provide an enterprise offering integrating Kong with their proprietary API analytics and developer portal tools. They can be deployed in a variety of configurations, as an edge API gateway or an internal API proxy. OpenResty, through its Nginx modules, provides a strong and performant foundation, with Lua plugins for extensions. Kong can either use PostgreSQL for single region deployments or Cassandra for multiregion configurations. Our developers have enjoyed Kong's high performance, its API-first approach (which enables automation of its configuration) and its ease of deployment as a container. Kong API Gateway , unlike overambitious API gateways, has a smaller set of features but it implements the essential set of API gateway capabilities such as traffic control, security, logging, monitoring and authentication. We're excited to assess Kong in a sidecar configuration in the near future.","blip_selector":"kong-api-gateway","name":"Kong API Gateway","display_name":"Kong API Gateway","url":"/radar/platforms/kong-api-gateway","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1185,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"Historically, Google's Android documentation examples lacked architecture and structure. This changes with the release of Android Architecture Components , a set of opinionated libraries that help developers create Android applications with better architecture. They address longstanding pain points of Android development: handling lifecycles; pagination; SQLite databases; and data persistence over configuration changes. The libraries don't need to be used together — you can pick the ones you need most and integrate them into your existing project. | Historically, Google's Android documentation examples lacked architecture and structure. This changes with the release of Android Architecture Components , a set of opinionated libraries that help developers create Android applications with better architecture. They address longstanding pain points of Android development: handling lifecycles; pagination; SQLite databases; and data persistence over configuration changes. The libraries don't need to be used together — you can pick the ones you need most and integrate them into your existing project.","blip_selector":"android-architecture-components","name":"Android Architecture Components","display_name":"Android Architecture Components","url":"/radar/languages-and-frameworks/android-architecture-components","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1261,"quadrant":"platforms","volume_date":"2018-05","description":"As AR and VR continue to gain traction, we continue to explore tools with which we can create immersive virtual worlds. Our positive experience with Unity, one of the two major gaming engines, led us to feature it in previous Radars. We still like Unity but are also excited about Godot, a relatively new entrant to the field. Godot is open source software and although not as fully featured as the big commercial engines, it comes with a more modern software design and less clutter. Offering C# and Python further lowers the barrier to entry for developers outside the gaming industry. Godot version 3, released earlier this year, adds support for VR and support for AR is on the horizon.","blip_selector":"godot","name":"Godot","display_name":"Godot","url":"/radar/platforms/godot","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1197,"quadrant":"platforms","volume_date":"2018-05","description":"LoRaWAN is a low-power wide-area network, designed for low-power consumption and communication over long distances using low bitrates. It provides for communication between devices and gateways, which can then forward the data to, for example, applications or servers. A typical usage is for a distributed set of sensors, or for Internet of Things (IoT) devices, for which long battery life and long-range communication is a must. LoRaWAN addresses two of the key problems with attempting to use normal Wi-Fi for such applications: range and power consumption. There are several implementations, a notable one being The Things Network, a free, open source implementation. | LoRaWAN is a low-power wide-area network, designed for low-power consumption and communication over long distances using low bitrates. It provides for communication between devices and gateways, which can then forward the data to, for example, applications or servers. A typical usage is for a distributed set of sensors, or for Internet of Things (IoT) devices, for which long battery life and long-range communication is a must. LoRaWAN addresses two of the key problems with attempting to use normal Wi-Fi for such applications: range and power consumption. There are several implementations, a notable one being The Things Network, a free, open source implementation.","blip_selector":"lorawan","name":"LoRaWAN","display_name":"LoRaWAN","url":"/radar/platforms/lorawan","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1203,"quadrant":"platforms","volume_date":"2018-05","description":"Azure Service Fabric is a distributed systems platform built for microservices and containers. It can act as a PaaS with its reliable services, or like a container orchestrator with its ability to manage containers. What distinguishes Service Fabric though are programming models such as Reliable Actors built on top of reliable services. When it comes to IoT use cases, for example, Reliable Actors offers some compelling advantages — in addition to the reliability and platform benefits of being on Service Fabric, you also get its state management and replication capabilities. In keeping with continued focus on open source software (OSS), Microsoft will be transitioning Service Fabric to an open development process on Github. All this makes Azure Service Fabric worth trialling — particularly for organizations who are invested in the .NET framework. | Azure Service Fabric is a distributed systems platform built for microservices and containers. It’s comparable to container orchestrators such as Kubernetes, but also works with plain old services. It can be used in a bewildering array of ways, starting from simple services in your language of choice to Docker containers or services built using an SDK. Since its release a couple of years ago, it has steadily added more features, including Linux container support. Kubernetes has been the poster child of container orchestration tools, but Service Fabric is the default choice for .NET applications. We're using it in a few projects at ThoughtWorks and we like what we’ve seen so far.","blip_selector":"azure-service-fabric","name":"Azure Service Fabric","display_name":"Azure Service Fabric","url":"/radar/platforms/azure-service-fabric","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1212,"quadrant":"techniques","volume_date":"2018-05","description":"When developing mobile applications, our teams often find themselves without an external server for testing apps. Setting up an over-the-wire mock may be a good fit for this particular problem. Developing the HTTP mocks and compiling them into the mobile binary for testing — embedded mobile mocks — enables teams to test their mobile apps when disconnected and with no external dependencies. This technique may require creating an opinionated library based on both the networking library used by the mobile app and your usage of the underlying library.","blip_selector":"embedded-mobile-mocks","name":"Embedded mobile mocks","display_name":"Embedded mobile mocks","url":"/radar/techniques/embedded-mobile-mocks","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1139,"quadrant":"techniques","volume_date":"2018-05","description":"As event streaming platforms, such as Apache Kafka, rise in popularity, many consider them as an advanced form of message queuing, used solely to transmit events. Even when used in this way, event streaming has its benefits over traditional message queuing. However, we're more interested in how people use event streaming as the source of truth with platforms (Kafka in particular) as the primary store for data as immutable events. A service with an Event Sourcing design, for example, can use Kafka as its event store; those events are then available for other services to consume. This technique has the potential to reduce duplicating efforts between local persistence and integration. | As event streaming platforms, such as Apache Kafka, rise in popularity, many consider them as an advanced form of message queuing, used solely to transmit events. Even when used in this way, event streaming has its benefits over traditional message queuing. However, we're more interested in how people use event streaming as the source of truth with platforms (Kafka in particular) as the primary store for data as immutable events. A service with an Event Sourcing design, for example, can use Kafka as its event store; those events are then available for other services to consume. This technique has the potential to reduce duplicating efforts between local persistence and integration.","blip_selector":"event-streaming-as-the-source-of-truth","name":"Event streaming as the source of truth","display_name":"Event streaming as the source of truth","url":"/radar/techniques/event-streaming-as-the-source-of-truth","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1148,"quadrant":"tools","volume_date":"2018-05","description":"Since mid-2017, Chrome users have had the option of running the browser in headless mode. This feature is ideally suited to running front-end browser tests without the overhead of displaying actions on a screen. Previously, this was largely the province of PhantomJS but Headless Chrome is rapidly replacing the JavaScript-driven WebKit approach. Tests in Headless Chrome should run much faster, and behave more like a real browser, but our teams have found that it does use more memory than PhantomJS. With all these advantages, Headless Chrome for front-end test is likely to become the de facto standard. | Since mid-2017, Chrome users have had the option of running the browser in headless mode. This feature is ideally suited to running front-end browser tests without the overhead of displaying actions on a screen. Previously, this was largely the province of PhantomJS but Headless Chrome is rapidly replacing the JavaScript-driven WebKit approach. Tests in Headless Chrome should run much faster, and behave more like a real browser, but our teams have found that it does use more memory than PhantomJS. With all these advantages, Headless Chrome for front-end test is likely to become the de facto standard.","blip_selector":"headless-chrome-for-front-end-test","name":"Headless Chrome for front-end test","display_name":"Headless Chrome for front-end test","url":"/radar/tools/headless-chrome-for-front-end-test","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1180,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"Our first rule of thumb in selecting a rules engine is normally: you don't need a rules engine. We've seen too many people tying themselves to a hard-to-test black-box rules engine for spurious reasons, when custom code would have been a better solution. That said, we've had success using Clara rules for scenarios where a rules engine does make sense. We like that it uses simple Clojure code to express and evaluate the rules, which means they are amenable to refactoring, testing and source control. Rather than chasing the illusion that business people should directly manipulate the rules, it drives collaboration between the business experts and developers. | Our first rule of thumb in selecting a rules engine is normally: you don't need a rules engine. We've seen too many people tying themselves to a hard-to-test black-box rules engine for spurious reasons, when custom code would have been a better solution. That said, we've had success using Clara rules for scenarios where a rules engine does make sense. We like that it uses simple Clojure code to express and evaluate the rules, which means they are amenable to refactoring, testing and source control. Rather than chasing the illusion that business people should directly manipulate the rules, it drives collaboration between the business experts and developers.","blip_selector":"clara-rules","name":"Clara rules","display_name":"Clara rules","url":"/radar/languages-and-frameworks/clara-rules","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1183,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"Programming for smart contracts requires a more expressive language than a scripting system for transactions. Solidity is the most popular among the new programming languages designed for smart contracts. Solidity is a contract-oriented, statically typed language whose syntax is similar to JavaScript. It provides abstractions for writing self-enforcing business logic in smart contracts. The toolchain around Solidity is growing fast. Nowadays, Solidity is the primary choice on the Ethereum platform. Given the immutable nature of deployed smart contracts, it should go without saying that rigorous testing and audit of dependencies is vital. | Programming for smart contracts requires a more expressive language than a scripting system for transactions. Solidity is the most popular among the new programming languages designed for smart contracts. Solidity is a contract-oriented, statically typed language whose syntax is similar to JavaScript. It provides abstractions for writing self-enforcing business logic in smart contracts. The toolchain around Solidity is growing fast. Nowadays, Solidity is the primary choice on the Ethereum platform. Given the immutable nature of deployed smart contracts, it should go without saying that rigorous testing and audit of dependencies is vital.","blip_selector":"solidity","name":"Solidity","display_name":"Solidity","url":"/radar/languages-and-frameworks/solidity","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1086,"quadrant":"platforms","volume_date":"2018-05","description":"In a microservice, or any other distributed architecture, one of the most common needs is to secure the services or APIs through authentication and authorization features. This is where Keycloak comes in. Keycloak is an open source identity and access management solution that makes it easy to secure applications or microservices with little to no code. It supports single sign-on, social login and standard protocols such as OpenID Connect, OAuth 2.0 and SAML out of the box. Our teams have been using this tool and plan to keep using it for the foreseeable future. But it requires a little work to set up. Because configuration happens both at initialization and at runtime through APIs, it's necessary to write scripts to ensure deployments are repeatable. | In a microservice, or any other distributed architecture, one of the most common needs is to secure the services or APIs through authentication and authorization features. This is where Keycloak comes in. Keycloak is an open source identity and access management solution that makes it easy to secure applications or microservices with little to no code. It supports single sign-on, social login and standard protocols such as OpenID Connect, OAuth 2.0 and SAML out of the box. Our teams have been using this tool and plan to keep using it for the foreseeable future. But it requires a little work to set up. Because configuration happens both at initialization and at runtime through APIs, it's necessary to write scripts to ensure deployments are repeatable. | In a microservices or any other distributed architecture, one of the most common needs is to secure the services or APIs through authentication and authorization features. This is where Keycloak comes in. Keycloak is an open source identity and access management solution that makes it easy to secure applications or microservices with little to no code. Out of the box, it supports single sign-on, social login, and standard protocols such as OpenID Connect, OAuth2 and SAML.","blip_selector":"keycloak","name":"Keycloak","display_name":"Keycloak","url":"/radar/platforms/keycloak","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1235,"quadrant":"tools","volume_date":"2018-05","description":"nsp is a command line tool to identify known vulnerabilities in Node.js applications. By running the check command on the root of a Node.js project, nsp generates the vulnerabilities report by checking against the published advisories. nsp provides a way to customize the check command to hide all vulnerabilities below the given CVSS score or exit with an error code if at least one finding has a CVSS score above the given value. Once the advisories are saved through the gather command, nsp can also be used in offline mode.","blip_selector":"nsp","name":"nsp","display_name":"nsp","url":"/radar/tools/nsp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1238,"quadrant":"languages-and-frameworks","volume_date":"2018-05","description":"Security is the cornerstone of the blockchain economy. In the last issue of the Radar, we highlighted the importance of testing and auditing smart contracts dependencies. OpenZeppelin is a framework to help build secure smart contracts in Solidity. The team behind OpenZeppelin summed up a series of pitfalls and best practices around smart contracts' security and embedded these experiences into the source code. The framework is well reviewed and validated by the open source community. We recommend the use of OpenZeppelin instead of writing your own implementation of the ERC20/ERC721 token. OpenZeppelin is also integrated with Truffle.","blip_selector":"openzeppelin","name":"OpenZeppelin","display_name":"OpenZeppelin","url":"/radar/languages-and-frameworks/openzeppelin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1200,"quadrant":"platforms","volume_date":"2018-05","description":"Microsoft is catching up in the container space with Windows Containers enabling running Windows applications as containers on Windows-based environments. At the time of writing, Microsoft provides two Windows OS images as Docker containers — Windows Server 2016 Server Core and Windows Server 2016 Nano Server — that can run as a Windows Server Container with Docker. Our teams have started using Windows containers in scenarios where build agents and similar containers have been working successfully. Microsoft is aware that there’s room for improvements such as decreasing the large image sizes and enriching ecosystem support and documentation. | Microsoft is catching up in the container space with Windows Containers. At the time of writing, Microsoft provides two Windows OS images as Docker containers, Windows Server 2016 Server Core and Windows Server 2016 Nano Server. Although there is room for improvement for Windows Containers, for instance, decreasing the large image sizes, and enriching ecosystem support and documentation, our teams have started using them in scenarios where other containers have been working successfully, such as build agents.","blip_selector":"windows-containers","name":"Windows Containers","display_name":"Windows Containers","url":"/radar/platforms/windows-containers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1225,"quadrant":"tools","volume_date":"2018-05","description":"Conduit is a lightweight service mesh for Kubernetes. Conduit embraces the out-of-process architecture with data plane proxy written in Rust and a control plane in Go. The data plane proxy runs as a sidecar for all TCP traffic in the Kubernetes cluster and the control plane runs in a separate namespace in Kubernetes exposing REST APIs to control the behavior of the data plane proxy. By proxying all requests, Conduit provides a wealth of metrics for monitoring and observability of interactions in the service mesh for HTTP, HTTP/2 and gRPC traffic. Even though Conduit is relatively new to this space, we recommend it because it’s simple to install and operate.","blip_selector":"conduit","name":"Conduit","display_name":"Conduit","url":"/radar/tools/conduit","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1257,"quadrant":"platforms","volume_date":"2018-05","description":"Most people may know the \"Internet of money\" through Bitcoin. In fact, this idea can be traced to the early stages of the Web. HTTP even reserved a status code for digital payment. The challenging part of this idea is to transfer value between different ledgers in different entities. Blockchain technology promotes this idea through building a distributed shared ledger. The current challenge is how to achieve interoperability between different blockchain ledgers and interoperability with traditional centralized ledgers. Interledger is a protocol to connect different ledgers. This protocol uses connectors and a cryptographic mechanism such as HTLC to route secure payments across ledgers. It’s not hard to join the payment network through its suites. Interledger was first initiated by Ripple and is now steadily developed by a W3C community group.","blip_selector":"interledger","name":"Interledger","display_name":"Interledger","url":"/radar/platforms/interledger","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1141,"quadrant":"techniques","volume_date":"2018-05","description":"Traditional approaches to enterprise security often emphasize locking things down and slowing the pace of change. However, we know that the more time an attacker has to compromise a system, the greater the potential damage. The three Rs of enterprise security — rotate, repair and repave — take advantage of infrastructure automation and continuous delivery to eliminate opportunities for attack. Rotating credentials, applying patches as soon as they're available and rebuilding systems from a known, secure state — all within a matter of minutes or hours — makes it harder for attackers to succeed. The three Rs of security technique is made feasible with the advent of modern cloud-native architectures. When applications are deployed as containers, and built and tested via a completely automated pipeline, a security patch is just another small release that can be sent through the pipeline with one click. Of course, in keeping with best distributed systems practices, developers need to design their applications to be resilient to unexpected server outages. This is similar to the impact of implementing Chaos Monkey within your environment. | Traditional approaches to enterprise security often emphasize locking things down and slowing the pace of change. However, we know that the more time an attacker has to compromise a system, the greater the potential damage. The three Rs of enterprise security — rotate, repair and repave — take advantage of infrastructure automation and continuous delivery to eliminate opportunities for attack. Rotating credentials, applying patches as soon as they're available and rebuilding systems from a known, secure state — all within a matter of minutes or hours — makes it harder for attackers to succeed. The three Rs of security technique is made feasible with the advent of modern cloud-native architectures. When applications are deployed as containers, and built and tested via a completely automated pipeline, a security patch is just another small release that can be sent through the pipeline with one click. Of course, in keeping with best distributed systems practices, developers need to design their applications to be resilient to unexpected server outages. This is similar to the impact of implementing Chaos Monkey within your environment.","blip_selector":"the-three-rs-of-security","name":"The three Rs of security","display_name":"The three Rs of security","url":"/radar/techniques/the-three-rs-of-security","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1182,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"We've seen a flurry of activity in mobile augmented reality much of it fueled by ARKit and ARCore , the native AR libraries used by Apple and Google, respectively. These libraries are bringing mobile AR technologies to the mainstream. However, the challenge will be for companies to find use cases that go beyond gimmicky and provide genuine solutions that actually enhance the user experience.","blip_selector":"arkit-arcore","name":"ARKit/ARCore","display_name":"ARKit/ARCore","url":"/radar/languages-and-frameworks/arkit-arcore","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1120,"quadrant":"tools","volume_date":"2017-11","description":"The popular Serverless Framework provides tooling for scaffolding and deployment of serverless applications, primarily using AWS Lambda and other AWS offerings. Serverless Framework provides template support for JavaScript, Python, Java and C#, and has an active community that contributes plugins that extend the framework. The framework also supports the Apache incubator project OpenWhisk as an alternative to AWS Lambda. | The popular Serverless Framework provides tooling for scaffolding and deployment of serverless applications, primarily using AWS Lambda and other AWS offerings. Serverless Framework provides template support for JavaScript, Python, Java and C#, and has an active community that contributes plugins that extend the framework. The framework also supports the Apache incubator project OpenWhisk as an alternative to AWS Lambda.","blip_selector":"serverless-framework","name":"Serverless Framework","display_name":"Serverless Framework","url":"/radar/tools/serverless-framework","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1176,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Digdag is a tool for building, running, scheduling and monitoring complex data pipelines in the cloud. You can define these pipelines in YAML, using either the rich set of out-of-the-box operators or building your own through the API. Digdag has most of the common features in a data pipeline solution such as dependency management, modular workflow to promote reuse, secured secret management and multilingual support. The feature we're most excited about is polycloud support, which lets you move and join data across AWS RedShift, S3, and Google BigQuery. As more and more cloud providers offer competing data-processing solutions, we think Digdag (and similar tools) will be useful in leveraging the best option for the task.","blip_selector":"digdag","name":"Digdag","display_name":"Digdag","url":"/radar/languages-and-frameworks/digdag","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1127,"quadrant":"techniques","volume_date":"2017-11","description":"Back in the days when SOAP held sway in the enterprise software industry, the practice of generating client code from WSDL specs was an accepted—even encouraged—practice. Unfortunately, the resulting code was often complex, untestable, difficult to modify and frequently didn't work across implementation platforms. With the advent of REST, we found it better to evolve API clients that use the tolerant reader pattern for extracting and processing only the fields needed. Recently we have observed a disturbing return to old habits with developers generating code from API specifications written in Swagger or RAML—a practice that we refer to as spec-based codegen. Although such tools are very useful for driving the design of APIs and for extracting documentation, we caution against the tempting shortcut of simply generating client code directly from these specifications. The chances are that such code will be difficult to test and maintain. | Back in the days when SOAP held sway in the enterprise software industry, the practice of generating client code from WSDL specs was an accepted—even encouraged—practice. Unfortunately, the resulting code was often complex, untestable, difficult to modify and frequently didn't work across implementation platforms. With the advent of REST, we found it better to evolve API clients that use the tolerant reader pattern for extracting and processing only the fields needed. Recently we have observed a disturbing return to old habits with developers generating code from API specifications written in Swagger or RAML—a practice that we refer to as spec-based codegen. Although such tools are very useful for driving the design of APIs and for extracting documentation, we caution against the tempting shortcut of simply generating client code directly from these specifications. The chances are that such code will be difficult to test and maintain.","blip_selector":"spec-based-codegen","name":"Spec-based codegen","display_name":"Spec-based codegen","url":"/radar/techniques/spec-based-codegen","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1082,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Avro is a framework for data serialization. By storing schema along with the message content, it encourages schema evolution. Producers can edit field names, add new fields or delete existing fields and Avro guarantees that the clients continue to consume the messages. Having a schema allows data to be written without overhead which results in compact data encoding and faster data processing. Although the exchange of structure-less messages between producer and consumer is flexible, we've seen teams facing issues with incompatible unprocessed messages in the queue during deployments. We've used Avro in a number of projects and would recommend using it over just sending unstructured messages. | Avro is a framework for data serialization. By storing schema along with the message content, it encourages schema evolution. Producers can edit field names, add new fields or delete existing fields and Avro guarantees that the clients continue to consume the messages. Having a schema allows data to be written without overhead which results in compact data encoding and faster data processing. Although the exchange of structure-less messages between producer and consumer is flexible, we've seen teams facing issues with incompatible unprocessed messages in the queue during deployments. We've used Avro in a number of projects and would recommend using it over just sending unstructured messages.","blip_selector":"avro","name":"Avro","display_name":"Avro","url":"/radar/languages-and-frameworks/avro","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1140,"quadrant":"techniques","volume_date":"2017-11","description":"The amount of data collected by IT operations has been increasing for years. For example, the trend toward microservices means that more applications are generating their own operational data, and tools such as Splunk, Prometheus, or the ELK stack make it easier to store and process data later on, to gain operational insights. When combined with increasingly democratized machine learning tools, it’s inevitable that operators will start to incorporate statistical models and trained classification algorithms into their toolsets. Although these algorithms have been available for years, and various attempts have been made to automate service management, we're only just starting to understand how machines and humans can collaborate to identify outages earlier or pinpoint the source of failures. Although there is a risk of overhyping Algorithmic IT operations , steady improvement in machine learning algorithms will inevitably change the role of humans in operating tomorrow's data centers.","blip_selector":"algorithmic-it-operations","name":"Algorithmic IT operations","display_name":"Algorithmic IT operations","url":"/radar/techniques/algorithmic-it-operations","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1085,"quadrant":"platforms","volume_date":"2017-11","description":"In parallel with the recent surge of chatbots and voice platforms, we've seen a proliferation of tools and platforms that provide a service to extract intent from text and management of conversational flows that you can hook into. DialogFlow (formerly API.ai), which was acquired by Google, is one such ‘natural-language-understanding as a service’ offering that competes with wit.ai and Amazon Lex among other players in this space. | In parallel with the recent surge of chatbots and voice platforms, we've seen a proliferation of tools and platforms such as api.ai that provide a service to extract intent from text and management of conversational flow that you can hook into. Recently acquired by Google, this \"natural-language-understanding as a service\" offering competes with other players in this space such as wit.ai and Amazon's Lex.","blip_selector":"dialogflow","name":"DialogFlow","display_name":"DialogFlow","url":"/radar/platforms/dialogflow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1170,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"ECharts is a lightweight charting library with rich support for different types of charts and interactions. Since ECharts is entirely based on the Canvas API, it has incredible performance even when dealing with over 100k data points, and it's also been optimized for mobile usage. Together with its sibling project, ECharts-X, it can support 3D plotting. ECharts is a Baidu open source project.","blip_selector":"echarts","name":"ECharts","display_name":"ECharts","url":"/radar/languages-and-frameworks/echarts","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1169,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Druid is a JDBC connection pool with rich monitoring features. It has a built-in SQL parser, which provides semantic monitoring of the SQL statements executing in the database. Injections or suspicious SQL statements will be blocked and logged directly from the JDBC layer. What’s more, queries can be merged based on their semantics. This is an Alibaba open source project, and reflects the lessons Alibaba learnt from operating their own database systems.","blip_selector":"druid","name":"Druid","display_name":"Druid","url":"/radar/languages-and-frameworks/druid","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1134,"quadrant":"techniques","volume_date":"2017-11","description":"Many development teams have adopted test-driven development practices for writing application code because of their benefits. Others have turned to containers to package and deploy their software, and it's accepted practice to use automated scripts to build the containers. What we’ve seen few teams do so far is combine the two trends and drive the writing of the container scripts using tests. With frameworks such as Serverspec and Goss, you can express the intended functionality for either isolated or orchestrated containers, with short feedback loops. This means that it’s possible to use the same principles we’ve championed for code by TDD'ing containers. Our initial experience doing so has been very positive.","blip_selector":"tdd-ing-containers","name":"TDD'ing containers","display_name":"TDD'ing containers","url":"/radar/techniques/tdd-ing-containers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1078,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Instana is yet another entrant into the crowded application performance management space. The fact that it's built from the ground up for cloud native architectures differentiates Instana from many of its competitors. Features include dynamic discovery, distributed tracing and service health plus the ability to \"time shift\" your view of your infrastructure to the moment an incident occurred. It remains to be seen whether this product can gain traction over the combination of open source projects—such as Consul, Prometheus and the implementations of OpenTracing—that do the same thing; however it's worth taking a look if you need an out-of-the-box solution. | Instana is yet another entrant into the crowded application performance management space. The fact that it's built from the ground up for cloud native architectures differentiates Instana from many of its competitors. Features include dynamic discovery, distributed tracing and service health plus the ability to \"time shift\" your view of your infrastructure to the moment an incident occurred. It remains to be seen whether this product can gain traction over the combination of open source projects—such as Consul, Prometheus and the implementations of OpenTracing—that do the same thing; however it's worth taking a look if you need an out-of-the-box solution.","blip_selector":"instana","name":"Instana","display_name":"Instana","url":"/radar/languages-and-frameworks/instana","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1073,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Keras is a high-level interface in Python for building neural networks. Created by a Google engineer, Keras is open source and runs on top of either TensorFlow or Theano. It provides an amazingly simple interface for creating powerful deep-learning algorithms to train on CPUs or GPUs. Keras is well designed with modularity, simplicity, and extensibility in mind. Unlike a library such as Caffe, Keras supports more general network architectures such as recurrent nets, making it overall more useful for text analysis, NLP and general machine learning. If computer vision, or any other specialized branch of machine learning, is your primary concern, Caffe may be a more appropriate choice. However, if you’re looking to learn a simple yet powerful framework, Keras should be your first choice. | Keras is a high-level interface in Python for building neural networks. Created by a Google engineer, Keras is open source and runs on top of either TensorFlow or Theano. It provides an amazingly simple interface for creating powerful deep-learning algorithms to train on CPUs or GPUs. Keras is well designed with modularity, simplicity, and extensibility in mind. Unlike a library such as Caffe, Keras supports more general network architectures such as recurrent nets, making it overall more useful for text analysis, NLP and general machine learning. If computer vision, or any other specialized branch of machine learning, is your primary concern, Caffe may be a more appropriate choice. However, if you're looking to learn a simple yet powerful framework, Keras should be your first choice.","blip_selector":"keras","name":"Keras","display_name":"Keras","url":"/radar/languages-and-frameworks/keras","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1167,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"TensorFlow Mobile makes it possible for developers to incorporate a wide range of comprehension and classification techniques into their iOS or Android applications. This is particularly useful given the range of sensor data available on mobile phones. Pretrained TensorFlow models can be loaded into a mobile application and applied to inputs such as live video frames, text or speech. Mobile phones present a surprisingly opportune platform for implementing these computational models. TensorFlow models are exported and loaded as protobuf files, which can present some problems for implementers. Protobuf's binary format can make it hard to examine models and requires that you link the correct protobuf library version to your mobile app. But local model execution offers an attractive alternative to TensorFlow Serving without the communication overhead of remote execution.","blip_selector":"tensorflow-mobile","name":"TensorFlow Mobile","display_name":"TensorFlow Mobile","url":"/radar/languages-and-frameworks/tensorflow-mobile","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1164,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Our teams are delighted with the results of using Jest for front-end testing. It provides a ‘zero-configuration’ experience and has out-of-the-box features such as mocking and code coverage. You can apply this testing framework not only to React applications, but also to other JavaScript frameworks. One of Jest's often hyped features is UI snapshot testing. Snapshot testing would be a good addition to the upper layer of the test pyramid, but remember, unit testing is still the solid foundation.","blip_selector":"jest","name":"Jest","display_name":"Jest","url":"/radar/languages-and-frameworks/jest","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":679,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Python 3 introduced many useful features that are not backward compatible with Python 2.x. It also removed numerous Python 2.x features that were maintained for backward compatibility, making Python 3 easier to learn and use and more consistent with the rest of the language. Our experience using Python 3 in domains such as machine learning and web application development shows that both the language and most of its supporting libraries have matured for adoption. We were able to fork and patch minor issues of existing libraries or avoided using incompatible Python 2.x libraries that had been abandoned. If you are developing in Python we strongly encourage you to use Python 3. | Python 3 introduced many useful features that are not backward compatible with Python 2.x. It also removed numerous Python 2.x features that were maintained for backward compatibility, making Python 3 easier to learn and use and more consistent with the rest of the language. Our experience using Python 3 in domains such as machine learning and web application development shows that both the language and most of its supporting libraries have matured for adoption. We were able to fork and patch minor issues of existing libraries or avoided using incompatible Python 2.x libraries that had been abandoned. If you are developing in Python we strongly encourage you to use Python 3. | Python 3 was a major change from the previous Python 2.x that introduced backwards incompatible changes. It was notable for actually removing languages features, making Python 3 easier to use and more consistent without reducing its power. This has led to problems in adoption as some widely used supporting libraries have not been ported, and Python developers often have to find new ways of doing things. Nonetheless the drive towards making a language simpler is to be applauded, and if you are actively developing in Python, then give Python 3 another look.","blip_selector":"python-3","name":"Python 3","display_name":"Python 3","url":"/radar/languages-and-frameworks/python-3","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1144,"quadrant":"tools","volume_date":"2017-11","description":"If you're looking for a JSON encoder/decoder with high performance in Go and Java, check out the open source jsoniter library. The library is compatible with the standard JSON encoding package in Go.","blip_selector":"jsoniter","name":"jsoniter","display_name":"jsoniter","url":"/radar/tools/jsoniter","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1075,"quadrant":"platforms","volume_date":"2017-11","description":"Hyperledger is a platform built around blockchain technologies. It consists of a blockchain implementation named Fabric and other associated tools. Disregarding the hype surrounding blockchain, our teams have found it easy to get started with these tools. The fact that it is an open source platform supported by the Linux Foundation also adds to our excitement about Hyperledger. | Hyperledger is a platform built around blockchain technologies. It consists of a blockchain implementation named Fabric and other associated tools. Disregarding the hype surrounding blockchain, our teams have found it easy to get started with these tools. The fact that it is an open source platform supported by the Linux Foundation also adds to our excitement about Hyperledger.","blip_selector":"hyperledger","name":"Hyperledger","display_name":"Hyperledger","url":"/radar/platforms/hyperledger","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1091,"quadrant":"platforms","volume_date":"2017-11","description":"Voice platforms such as Amazon Alexa and Google Home are riding high on the hype cycle; some even herald the ubiquity of the conversational voice interface. We're already integrating conversational UIs into products and seeing the impact of this new interaction in how we design interfaces. Alexa specifically was built from the ground up without a screen and treats the conversational UI as first-class. But it's still too early to believe the hype, and we expect more big players to get in the game. | Voice platforms such as Amazon Alexa and Google Home are riding high on the hype cycle; some even herald the ubiquity of the conversational voice interface. We're already integrating conversational UIs into products and seeing the impact of this new interaction in how we design interfaces. Alexa specifically was built from the ground up without a screen and treats the conversational UI as first-class. But it's still too early to believe the hype, and we expect more big players to get in the game.","blip_selector":"voice-platforms","name":"Voice platforms","display_name":"Voice platforms","url":"/radar/platforms/voice-platforms","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1157,"quadrant":"tools","volume_date":"2017-11","description":"An AssertJ library, assertj-swagger enables you to validate an API implementation's compliance with its contract specification. Our teams use assertj-swagger to catch problems when the API endpoint implementation changes without updating its Swagger specification, or fails to publish the updated documentation.","blip_selector":"assertj-swagger","name":"assertj-swagger","display_name":"assertj-swagger","url":"/radar/tools/assertj-swagger","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1155,"quadrant":"tools","volume_date":"2017-11","description":"If you're implementing Java services using the Spring framework, you may want to consider Spring Cloud Contract for consumer-driven contract testing. The current ecosystem of this tool supports verification of the client calls and the server implementation against the contract. In comparison to Pact, an open source consumer-driven contract testing tool set, it lacks the brokering of the contracts and the support for other programming languages. However, it integrates well with the Spring ecosystem, for instance message routing with Spring Integration.","blip_selector":"spring-cloud-contract","name":"Spring Cloud Contract","display_name":"Spring Cloud Contract","url":"/radar/tools/spring-cloud-contract","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1097,"quadrant":"platforms","volume_date":"2017-11","description":"The huge number of mobile devices makes it almost impossible for companies to test their mobile apps on all of them. Enter AWS Device Farm, an app-testing service that enables you to run and interact with your Android, iOS and web apps on a wide variety of physical devices that are hosted in the cloud simultaneously. Detailed logs, performance graphs and screenshots are generated during each run to provide general and device-specific feedback. The service offers a lot of flexibility by allowing the state and configuration of each device to be altered in order to reproduce very specific test scenarios. Our teams are using AWS Device Farm to run end-to-end tests on devices with the largest install base for their apps. | The huge number of mobile devices makes it almost impossible for companies to test their mobile apps on all of them. Enter AWS Device Farm, an app-testing service that enables you to run and interact with your Android, iOS and web apps on a wide variety of physical devices that are hosted in the cloud simultaneously. Detailed logs, performance graphs and screenshots are generated during each run to provide general and device-specific feedback. The service offers a lot of flexibility by allowing the state and configuration of each device to be altered in order to reproduce very specific test scenarios. Our teams are using AWS Device Farm to run end-to-end tests on devices with the largest install base for their apps.","blip_selector":"aws-device-farm","name":"AWS Device Farm","display_name":"AWS Device Farm","url":"/radar/platforms/aws-device-farm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1110,"quadrant":"tools","volume_date":"2017-11","description":"Netflix has open sourced Spinnaker, its microservices continuous delivery (CD) platform. Compared to other CI/CD platforms, Spinnaker implements cluster management and deployment of baked images to the cloud as first-class features. It supports out-of-the-box deployment and cluster management for multiple cloud providers such as Google Cloud Platform, AWS and Pivotal Cloud Foundry. You can integrate Spinnaker with Jenkins to run a Jenkins job build. We like Spinnaker's opinionated approach for deploying microservices to the cloud—with the exception that Spinnaker's pipelines are created via a user interface (UI) and cannot be configured as code. | Netflix has open sourced Spinnaker, its microservices continuous delivery (CD) platform. Compared to other CI/CD platforms, Spinnaker implements cluster management and deployment of baked images to the cloud as first-class features. It supports out-of-the-box deployment and cluster management for multiple cloud providers such as Google Cloud Platform, AWS and Pivotal Cloud Foundry. You can integrate Spinnaker with Jenkins to run a Jenkins job build. We like Spinnaker's opinionated approach for deploying microservices to the cloud—with the exception that Spinnaker's pipelines are created via a user interface (UI) and cannot be configured as code.","blip_selector":"spinnaker","name":"Spinnaker","display_name":"Spinnaker","url":"/radar/tools/spinnaker","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1090,"quadrant":"platforms","volume_date":"2017-11","description":"In our experience—for Internet of Things (IoT) solutions where a lot of devices communicate with each other and/or a central data hub—the MQTT connectivity protocol has proven itself. We've also come to like the Mosquitto MQTT broker. It might not satisfy all demands, particularly with regard to scalability, but its compact nature and easy setup makes it ideal for development and testing purposes. | In our experience—for Internet of Things (IoT) solutions where a lot of devices communicate with each other and/or a central data hub—the MQTT connectivity protocol has proven itself. We've also come to like the Mosquitto MQTT broker. It might not satisfy all demands, particularly with regard to scalability, but its compact nature and easy setup makes it ideal for development and testing purposes.","blip_selector":"mosquitto","name":"Mosquitto","display_name":"Mosquitto","url":"/radar/platforms/mosquitto","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1204,"quadrant":"platforms","volume_date":"2017-11","description":"MapD is an in-memory columnar analytic database with SQL support that's built to run on GPU. We debated whether the database workload is actually I/O or computationally bound but there are instances where the parallelism of the GPU, combined with the large bandwidth of VRAM, can be quite useful. MapD transparently manages the most frequently used data in VRAM (such as columns involved in group-by, filters, calculations and join conditions) and stores the rest of the data in the main memory. With this memory management setup, MapD achieves significant query performance without the need of indexes. Although there are other GPU database vendors, MapD is leading this segment with the recent open source release of its core database and through the GPU Open Analytics Initiative. If your analytical workload is computationally heavy, can exploit GPU parallelism and can fit in the main memory, we recommend assessing MapD.","blip_selector":"mapd","name":"MapD","display_name":"MapD","url":"/radar/platforms/mapd","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1171,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Weex is a framework for building cross-platform mobile apps by using the Vue.js component syntax. For those who prefer the simplicity of Vue.js, Weex is a viable option for native mobile apps, but it also works very well for more complicated apps. We see many successes for fairly complicated mobile apps built on this framework, including TMall and Taobao, two of the most popular mobile apps in China. Weex was developed by Alibaba, and is now an Apache incubator project.","blip_selector":"weex","name":"Weex","display_name":"Weex","url":"/radar/languages-and-frameworks/weex","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1083,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Spring Cloud continues to evolve and add interesting new features. Support for binding to Kafka Streams, for example, in the spring-cloud-streams project makes it relatively easy to build message driven applications with connectors for Kafka and RabbitMQ. The teams we have using it appreciate the simplicity it brings to using sometimes complex infrastructure, such as ZooKeeper, and support for common problems that we need to address when building distributed systems, tracing with the spring-cloud-sleuth for example. The usual caveats apply but we're successfully using it on multiple projects. | Teams building systems composed of microservices need to think about coordination techniques such as service discovery, load balancing, circuit breaking and health checking. Many of these techniques require teams to set up tooling, which is not always trivial. The Spring Cloud project provides tools for developers so they can use these coordination techniques in the familiar Spring environment. These tools support Consul, ZooKeeper and the Netflix OSS full stack, all tools that we like. Simply put, it makes it easy to do the right thing with these tool sets. Although our usual concerns with Spring still stand, namely that it hides too much of the complexity, you should consider Spring Cloud if you are in the ecosystem and need to solve these problems.","blip_selector":"spring-cloud","name":"Spring Cloud","display_name":"Spring Cloud","url":"/radar/languages-and-frameworks/spring-cloud","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1156,"quadrant":"tools","volume_date":"2017-11","description":"A perennial problem for JavaScript-heavy web applications is how to make the dynamic portion of those pages available to search engines. Historically, developers have resorted to a variety of tricks, including server-side rendering with React, external services or prerendering content. Now Google Chrome's new headless mode adds a new ‘trick’ to the toolbox — Rendertron, a headless Chrome rendering solution. Rendertron wraps an instance of headless Chrome in a Docker container, ready to deploy as a standalone HTTP server. Bots that don't render JavaScript can be routed to this server to do the rendering for them. Although developers can always deploy their own headless Chrome proxy and associated routing machinery, Rendertron simplifies the configuration and deployment process, and provides example middleware code for detecting and routing bots.","blip_selector":"rendertron","name":"Rendertron","display_name":"Rendertron","url":"/radar/tools/rendertron","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1089,"quadrant":"platforms","volume_date":"2017-11","description":"PlatformIO provides a rich ecosystem for IoT development by providing cross-platform builds, library management and good integration with existing IDEs. The intelligent code completion and Smart Code Linter with built-in terminal and serial port monitor greatly enhances the developer experience. It also organizes and maintains thousands of libraries and provides a clean dependency manager with semantic versioning to ease IoT development. We've started using PlatformIO in a few IoT projects and we really like it for its simplicity and wide support of platforms and boards. | PlatformIO provides a rich ecosystem for IoT development by providing cross-platform builds, library management and good integration with existing IDEs. The intelligent code completion and Smart Code Linter with built-in terminal and serial port monitor greatly enhances the developer experience. It also organizes and maintains thousands of libraries and provides a clean dependency manager with semantic versioning to ease IoT development. We've started using PlatformIO in a few IoT projects and we really like it for its simplicity and wide support of platforms and boards.","blip_selector":"platformio","name":"PlatformIO","display_name":"PlatformIO","url":"/radar/platforms/platformio","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1072,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"Caffe is an open source library for deep learning created by the Berkeley Vision and Learning Center. It mostly focusses on convolutional networks for computer vision applications. Caffe is a solid and popular choice for computer vision-related tasks and you can download many successful models made by Caffe users from the Caffe Model Zoo for out-of-the-box use. Like Keras, Caffe is a Python-based API. In Keras, however, models and components are objects created directly in Python code, whereas Caffe models are described by Protobuf configuration files. Either approach has its pros and cons, and converting between the two is also possible. | Caffe is an open source library for deep learning created by the Berkeley Vision and Learning Center. It mostly focusses on convolutional networks for computer vision applications. Caffe is a solid and popular choice for computer vision-related tasks and you can download many successful models made by Caffe users from the Caffe Model Zoo for out-of-the-box use. Like Keras, Caffe is a Python-based API. In Keras, however, models and components are objects created directly in Python code, whereas Caffe models are described by Protobuf configuration files. Either approach has its pros and cons, and converting between the two is also possible.","blip_selector":"caffe","name":"Caffe","display_name":"Caffe","url":"/radar/languages-and-frameworks/caffe","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1106,"quadrant":"techniques","volume_date":"2017-11","description":"Working with legacy code, especially large monoliths, is one of the most unsatisfying, high-friction experiences for developers. Although we caution against extending and actively maintaining legacy monoliths, they continue to be dependencies in our environments, and developers often underestimate the cost and time required to develop against these dependencies. To help reduce the friction, developers have used virtualized machine images or container images with Docker containers to create immutable images of legacy systems and their configurations. The intent is to contain the legacy in a box for developers to run locally and remove the need for rebuilding, reconfiguring or sharing environments. In an ideal scenario, teams that own legacy systems generate the corresponding boxed legacy images through their build pipelines, and developers can then run and orchestrate these images in their allocated sandbox more reliably. Although this approach has reduced the overall time spent by each developer, it has had limited success when the teams owning the downstream dependencies have been reluctant to create container images for others to use. | Working with legacy code, especially large monoliths, is one of the most unsatisfying, high-friction experiences for developers. Although we caution against extending and actively maintaining legacy monoliths, they continue to be dependencies in our environments, and developers often underestimate the cost and time required to develop against these dependencies. To help reduce the friction, developers have used virtualized machine images or container images with Docker containers to create immutable images of legacy systems and their configurations. The intent is to contain the legacy in a box for developers to run locally and remove the need for rebuilding, reconfiguring or sharing environments. In an ideal scenario, teams that own legacy systems generate the corresponding boxed legacy images through their build pipelines, and developers can then run and orchestrate these images in their allocated sandbox more reliably. Although this approach has reduced the overall time spent by each developer, it has had limited success when the teams owning the downstream dependencies have been reluctant to create container images for others to use.","blip_selector":"legacy-in-a-box","name":"Legacy in a box","display_name":"Legacy in a box","url":"/radar/techniques/legacy-in-a-box","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":999,"quadrant":"techniques","volume_date":"2017-11","description":"The use of serverless architecture has very quickly become an accepted approach for organizations deploying cloud applications, with a plethora of choices available for deployment. Even traditionally conservative organizations are making partial use of some serverless technologies. Most of the discussion goes to Functions as a Service (e.g., AWS Lambda, Google Cloud Functions, Azure Functions) while the appropriate patterns for use are still emerging. Deploying serverless functions undeniably removes the nontrivial effort that traditionally goes into server and OS configuration and orchestration. Serverless functions, however, are not a fit for every requirement. At this stage, you must be prepared to fall back to deploying containers or even server instances for specific requirements. Meanwhile, the other components of a serverless architecture, such as Backend as a Service, have become almost a default choice. | A serverless architecture approach replaces long-running virtual machines with ephemeral compute power that comes into existence on request and disappears immediately after use. Our teams like the serverless approach; it's working well for us and we consider it a valid architectural choice. Note that serverless doesn't have to be an all-or-nothing approach: some of our teams have deployed a new chunk of their systems using serverless while sticking to a traditional architectural approach for other pieces. Although AWS Lambda is almost synonymous with serverless, the other major cloud providers all have similar offerings, and we also recommend assessing niche players such as webtask. | Serverless architecture is an approach that replaces long-running virtual machines with ephemeral compute power that comes into existence on request and disappears immediately after use. Since the last Radar, we have had several teams put applications into production using a \"serverless\" style. Our teams like the approach, it’s working well for them and we consider it a valid architectural choice. Note that serverless doesn’t have to be an all-or-nothing approach: some of our teams have deployed a new chunk of their systems using serverless while sticking to a traditional architectural approach for other pieces. | Serverless architecture replaces long-running virtual machines with ephemeral compute power that comes into existence on request and disappears immediately after use. Examples include Firebase and AWS Lambda. Use of this architecture can mitigate some security concerns such as security patching and SSH access control, and can make much more efficient use of compute resources. These systems cost very little to operate and can have inbuilt scaling features (this is especially true for AWS Lambda). An example architecture could be a JavaScript app with static assets served by a CDN or S3 coupled with AJAX calls served by the API Gateway and Lambda. While serverless architectures have significant benefits, there are drawbacks too: Deploying, managing and sharing code across services is more complex, and local or offline testing is more difficult if not impossible.","blip_selector":"serverless-architecture","name":"Serverless architecture","display_name":"Serverless architecture","url":"/radar/techniques/serverless-architecture","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1104,"quadrant":"techniques","volume_date":"2017-11","description":"When the enterprise-wide quarterly or monthly releases were considered best practice, it was necessary to maintain a complete environment for performing testing cycles prior to deployment to production. These enterprise-wide integration test environments (often referred to as SIT or Staging) are a common bottleneck for continuous delivery today. The environments themselves are fragile and expensive to maintain, often with components that need manual configuration by a separate environment management team. Testing in the staging environment provides unreliable and slow feedback, and testing effort is duplicated with what can be performed on components in isolation. We recommend that organizations incrementally create an independent path to production for key components. Important techniques include contract testing, decoupling deployment from release, focus on mean time to recovery and testing in production. | When the enterprise-wide quarterly or monthly releases were considered best practice, it was necessary to maintain a complete environment for performing testing cycles prior to deployment to production. These enterprise-wide integration test environments (often referred to as SIT or Staging) are a common bottleneck for continuous delivery today. The environments themselves are fragile and expensive to maintain, often with components that need manual configuration by a separate environment management team. Testing in the staging environment provides unreliable and slow feedback, and testing effort is duplicated with what can be performed on components in isolation. We recommend that organizations incrementally create an independent path to production for key components. Important techniques include contract testing, decoupling deployment from release, focus on mean time to recovery and testing in production.","blip_selector":"enterprise-wide-integration-test-environments","name":"Enterprise-wide integration test environments","display_name":"Enterprise-wide integration test environments","url":"/radar/techniques/enterprise-wide-integration-test-environments","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1130,"quadrant":"techniques","volume_date":"2017-11","description":"Inspired by the DevOps movement, DesignOps is a cultural shift and a set of practices that allows people across an organization to continuously redesign products without compromising quality, service coherency or team autonomy. DesignOps advocates for the creation and evolution of a design infrastructure that minimizes the effort necessary to create new UI concepts and variations, and to establish a rapid and reliable feedback loop with end users. With tools such as Storybook promoting close collaboration, the need for upfront analysis and specification handoffs is reduced to the absolute minimum. With DesignOps, design is shifting from being a specific practice to being a part of everyone's job.","blip_selector":"designops","name":"DesignOps","display_name":"DesignOps","url":"/radar/techniques/designops","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1004,"quadrant":"techniques","volume_date":"2017-11","description":"We're compelled to caution, again, against creating a single CI instance for all teams. While it's a nice idea in theory to consolidate and centralize Continuous Integration (CI) infrastructure, in reality we do not see enough maturity in the tools and products in this space to achieve the desired outcome. Software delivery teams which must use the centralized CI offering regularly have long delays depending on a central team to perform minor configuration tasks, or to troubleshoot problems in the shared infrastructure and tooling. At this stage, we continue to recommend that organizations limit their centralized investment to establishing patterns, guidelines and support for delivery teams to operate their own CI infrastructure. | We're compelled to caution, again, against creating a single CI instance for all teams. While it's a nice idea in theory to consolidate and centralize Continuous Integration (CI) infrastructure, in reality we do not see enough maturity in the tools and products in this space to achieve the desired outcome. Software delivery teams which must use the centralized CI offering regularly have long delays depending on a central team to perform minor configuration tasks, or to troubleshoot problems in the shared infrastructure and tooling. At this stage, we continue to recommend that organizations limit their centralized investment to establishing patterns, guidelines and support for delivery teams to operate their own CI infrastructure. | There might be the impression that it's easier to manage a single CI (Continuous Integration) instance for all teams because it gives them a single configuration and monitoring point. But a bloated instance that is shared by every team in an organization can cause a lot of damage. We have found that problems like build timeouts, configuration conflicts and gigantic build queues appear more frequently. Having this single point of failure can interrupt the work of many teams. Carefully consider the trade-off between these pitfalls and having a single point of configuration. In organizations with multiple teams, we recommend having CI instances distributed by teams, with enterprise decisions based not on the single CI installation but on defining guidelines about the instances' selection and configuration.","blip_selector":"a-single-ci-instance-for-all-teams","name":"A single CI instance for all teams","display_name":"A single CI instance for all teams","url":"/radar/techniques/a-single-ci-instance-for-all-teams","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1111,"quadrant":"tools","volume_date":"2017-11","description":"spaCy is a Natural Language Processing (NLP) library written in Python. It is a high-performance library, intended for use by developers in production, and applies NLP models suited for processing text that often mixes in emoticons and inconsistent punctuation marks. Unlike other NLP frameworks, spaCy is a pluggable library and not a platform; it is aimed at production applications rather than model training for research. It plays well with TensorFlow and the rest of the Python AI ecosystem. We've used spaCy in the enterprise context to build a search engine that takes human language queries and helps users make business decisions. | spaCy is a Natural Language Processing (NLP) library written in Python. It is a high-performance library, intended for use by developers in production, and applies NLP models suited for processing text that often mixes in emoticons and inconsistent punctuation marks. Unlike other NLP frameworks, spaCy is a pluggable library and not a platform; it is aimed at production applications rather than model training for research. It plays well with TensorFlow and the rest of the Python AI ecosystem. We've used spaCy in the enterprise context to build a search engine that takes human language queries and helps users make business decisions.","blip_selector":"spacy","name":"spaCy","display_name":"spaCy","url":"/radar/tools/spacy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1105,"quadrant":"techniques","volume_date":"2017-11","description":"In previous Radars issues we mentioned tools such as git-crypt and Blackbox that allow us to keep secrets safe inside the source code. Decoupling secret management from source code is our way to remind technologists that there are other options for storing secrets. For example, HashiCorp vault, CI servers and configuration management tools provide mechanisms for storing secrets that are not linked to the source code of an application. Both approaches are viable and we recommend you use at least one of them in your projects. | In previous Radars issues we mentioned tools such as git-crypt and Blackbox that allow us to keep secrets safe inside the source code. Decoupling secret management from source code is our way to remind technologists that there are other options for storing secrets. For example, HashiCorp vault, CI servers and configuration management tools provide mechanisms for storing secrets that are not linked to the source code of an application. Both approaches are viable and we recommend you use at least one of them in your projects.","blip_selector":"decoupling-secret-management-from-source-code","name":"Decoupling secret management from source code","display_name":"Decoupling secret management from source code","url":"/radar/techniques/decoupling-secret-management-from-source-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1070,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"In previous Radar editions, we've been hesitant to give Angular a strong recommendation because it was essentially a new, and on the whole unexciting, framework, sharing only its name with AngularJS, an older framework we loved in days past. In the meantime, Angular, now in version 5, has improved steadily while providing backward compatibility along the way. Several of our teams have Angular applications in production and reportedly, they like what they see. For this reason, we're moving Angular into the Trial ring in this Radar, to signify that some of our teams now consider it a solid choice. Most of our teams, however, still prefer React, Vue or Ember over Angular. | In the previous Radar, we moved AngularJS into the Hold ring (where it remains in this edition). When it comes to Angular 2 , we're seeing mixed messages. Over the past year some teams at ThoughtWorks have used Angular 2 successfully and consider it a solid choice. However, Angular 2 is a rewrite, not an evolution, of AngularJS, and switching from AngularJS to Angular 2 is not much different than switching from AngularJS to another framework. Given the, in our experience, superior contenders such as React.js, Ember.js and Vue.js, we're still hesitant to give Angular 2 a strong recommendation. We do want to highlight, though, that it is not a bad choice, especially if you bought into TypeScript.","blip_selector":"angular","name":"Angular","display_name":"Angular","url":"/radar/languages-and-frameworks/angular","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1102,"quadrant":"techniques","volume_date":"2017-11","description":"We've long been advocates of continuous integration (CI), and we were pioneers in building CI server programs to automatically build projects on check-ins. Used well, these programs run as a daemon process on a shared project mainline that developers commit to daily. The CI server builds the project and runs comprehensive tests to ensure the whole software system is integrated and is in an always-releasable state, thus satisfying the principles of continuous delivery. Sadly, many developers simply set up a CI server and falsely assume they are \"doing CI\" when in reality they miss out on all the benefits. Common failure modes include: running CI against a shared mainline but with infrequent commits, so integration isn't really continuous; running a build with poor test coverage; allowing the build to stay red for long periods; or running CI against feature branches which results in continuous isolation. The ensuing \" CI theatre\" might make people feel good, but would fail any credible CI certification test. | We've long been advocates of continuous integration (CI), and we were pioneers in building CI server programs to automatically build projects on check-ins. Used well, these programs run as a daemon process on a shared project mainline that developers commit to daily. The CI server builds the project and runs comprehensive tests to ensure the whole software system is integrated and is in an always-releasable state, thus satisfying the principles of continuous delivery. Sadly, many developers simply set up a CI server and falsely assume they are \"doing CI\" when in reality they miss out on all the benefits. Common failure modes include: running CI against a shared mainline but with infrequent commits, so integration isn't really continuous; running a build with poor test coverage; allowing the build to stay red for long periods; or running CI against feature branches which results in continuous isolation. The ensuing \" CI theatre\" might make people feel good, but would fail any credible CI certification test.","blip_selector":"ci-theatre","name":"CI theatre","display_name":"CI theatre","url":"/radar/techniques/ci-theatre","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1061,"quadrant":"platforms","volume_date":"2017-11","description":"In previous Radars, we mentioned that Unity has become the platform of choice for VR and AR application development because it provides the abstractions and tooling of a mature platform, while being more accessible than its main alternative, the Unreal Engine. With the recent introductions of ARKit for iOS and ARCore for Android, the two main mobile platforms now have powerful native SDKs for building augmented reality applications. Yet, we feel that many teams, especially those without deep experience in building games, will benefit from using an abstraction such as Unity, which is why we're calling out Unity beyond gaming. This allows developers unfamiliar with the technology to focus on one SDK. It also offers a solution for the huge number of devices, especially on the Android side, that are not supported by the native SDKs. | After experiencing years of growth as a platform for game development, Unity has recently become the platform of choice for VR and AR application development. Whether you're creating a fully immersive world for the Oculus or HTC Vive headsets, a holographic layer for your newly spatial enterprise application or an AR feature set for your mobile app, Unity likely provides what you need to both prototype it and get it ready for prime time. Many of us at ThoughtWorks believe that VR and AR represent the next significant shift in the computing platform, and for now, Unity is the single most important tool in the toolbox we use to develop for this change. We've used Unity to develop all our VR prototypes, as well as AR functionality for headsets and phone/tablet applications. | After experiencing years of growth as a platform for game development, Unity has recently become the platform of choice for VR and AR application development. Whether you’re creating a fully immersive world for the Oculus or HTC Vive headsets, a holographic layer for your newly spatial enterprise application or an AR feature set for your mobile app, Unity likely provides what you need to both prototype it and get it ready for prime time. Many of us at ThoughtWorks believe that VR and AR represent the next significant shift in the computing platform, and for now, Unity is the single most important tool in the toolbox we use to develop for this change. We’ve used Unity to develop all our VR prototypes, as well as AR functionality for headsets and phone/tablet applications.","blip_selector":"unity-beyond-gaming","name":"Unity beyond gaming","display_name":"Unity beyond gaming","url":"/radar/platforms/unity-beyond-gaming","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1033,"quadrant":"tools","volume_date":"2017-11","description":"Scikit-learn is not a new tool (it is approaching its tenth birthday); what is new is the rate of adoption of machine-learning tools and techniques outside of academia and major tech companies. Providing a robust set of models and a rich set of functionality, Scikit-learn plays an important role in making machine-learning concepts and capabilities more accessible to a broader (and often non-expert) audience. | Scikit-learn is not a new tool (it is approaching its tenth birthday); what is new is the rate of adoption of machine-learning tools and techniques outside of academia and major tech companies. Providing a robust set of models and a rich set of functionality, Scikit-learn plays an important role in making machine-learning concepts and capabilities more accessible to a broader (and often non-expert) audience. | Scikit-learn is an increasingly popular machine-learning library written in Python. It provides a robust set of machine-learning models such as clustering, classification, regression and dimensionality reduction, and a rich set of functionality for companion tasks like model selection, model evaluation and data preparation. Since it is designed to be simple, reusable in various contexts and well documented, we see this tool accessible even to nonexperts to explore the machine-learning space.","blip_selector":"scikit-learn","name":"Scikit-learn","display_name":"Scikit-learn","url":"/radar/tools/scikit-learn","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":849,"quadrant":"tools","volume_date":"2017-11","description":"We've seen both continuing improvements in and an uptick in adoption of Prometheus, the monitoring and time series database tool originally developed by Soundcloud. Prometheus primarily supports a pull-based HTTP model but it also supports alerts, making it an active part of your operational toolset. As of this writing, Prometheus 2.0 is in prerelease, and continues to evolve. Prometheus developers have focused their efforts on core time series databases and the variety of metrics available. Grafana has become the dashboard visualization tool of choice for Prometheus users and support for Grafana ships with the tool. Our teams also find that Prometheus monitoring nicely complements the indexing and search capabilities of an Elastic Stack. | SoundCloud has recently open sourced its monitoring and alerting toolkit, Prometheus. Developed in reaction to difficulties with Graphite in its production systems, Prometheus primarily supports a pull-based HTTP model (although a more Graphite-like push model is also supported). It also goes further by supporting alerts, making it an active part of your operational toolset. As of this writing, Prometheus is still only in release 0.15.1 but is evolving rapidly. We’re glad to see the recent product focus on core time-series DB and multidimensional indexing capabilities while allowing for export to a wider variety of front-end graphing tools. | SoundCloud have recently open sourced a Graphite replacement, Prometheus. Developed as a reaction to difficulties with Graphite in their production systems, Prometheus works differently to Graphite, by primarily supporting a pull-based HTTP model (although a more Graphite-like push model is also supported). It also goes beyond Graphite by being built to support alerting based on captured metrics, so it becomes a much more active part of your operational toolset. Some caution should be used in adopting new technology in the production monitoring space, but early reports are that SoundCloud are happy using it in production, and Docker are also contributing to ongoing development.","blip_selector":"prometheus","name":"Prometheus","display_name":"Prometheus","url":"/radar/tools/prometheus","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1071,"quadrant":"languages-and-frameworks","volume_date":"2017-11","description":"PostCSS is a Node.js-based JavaScript framework for operating on an abstract syntax tree-based representation of CSS documents with a rich ecosystem of plugins. Often incorrectly thought of as a preprocessor (such as SASS or Less), we find that the real power of PostCSS comes from the number of things that can be done with the rich set of plugins which includes linting (the stylelint plugin), cross-compilation (the sugarss plugin), name-mangling to avoid selector collision (the modules plugin), boilerplate CSS code generation (the autoprefixer plugin), minification and many others. The different maturity levels of the plugins notwithstanding, PostCSS itself remains a simple and powerful framework for treating CSS like a full-fledged language for front-end development. | PostCSS is a Node.js-based JavaScript framework for operating on an abstract syntax tree-based representation of CSS documents with a rich ecosystem of plugins. Often incorrectly thought of as a preprocessor (such as SASS or Less), we find that the real power of PostCSS comes from the number of things that can be done with the rich set of plugins which includes linting (the stylelint plugin), cross-compilation (the sugarss plugin), name-mangling to avoid selector collision (the modules plugin), boilerplate CSS code generation (the autoprefixer plugin), minification and many others. The different maturity levels of the plugins notwithstanding, PostCSS itself remains a simple and powerful framework for treating CSS like a full-fledged language for front-end development.","blip_selector":"postcss","name":"PostCSS","display_name":"PostCSS","url":"/radar/languages-and-frameworks/postcss","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1001,"quadrant":"techniques","volume_date":"2017-03","description":"In a number of countries, we see government agencies seeking broad access to private, personally identifiable information (PII). The increased use of public cloud solutions makes it more difficult for organizations to protect the data entrusted to them by their users while also respecting all relevant laws. The European Union has some of the most progressive privacy laws, and all the major cloud providers—Amazon, Google and Microsoft—offer multiple data centers and regions within the European Union. Therefore, we recommend that companies, especially those with a global user base, assess the feasibility of a safe haven for their users' data by hosting PII data in the EU. Since we wrote about this technique in the last Radar, we have rolled out a new internal system that handles sensitive information relating to all our employees, and we have chosen to host it in a data center located in the European Union. | In a number of countries, we see government agencies seeking broad access to private, personally identifiable information (PII). The increased use of public cloud solutions makes it more difficult for organizations to protect the data entrusted to them by their users while also respecting all relevant laws. The European Union has some of the most progressive privacy laws, and all the major cloud providers—Amazon, Google and Microsoft—offer multiple data centers and regions within the European Union. Therefore, we recommend that companies, especially those with a global user base, assess the feasibility of a safe haven for their users' data by hosting PII data in the EU. Since we wrote about this technique in the last Radar, we have rolled out a new internal system that handles sensitive information relating to all our employees, and we have chosen to host it in a data center located in the European Union. | In a number of countries around the world, we see government agencies seeking broad access to private, personally identifiable information (PII). In the EU, the highest court has invalidated the Safe Harbor framework, and Privacy Shield, its successor, is expected to be challenged too. At the same time, the use of cloud computing is increasing, and all the major cloud providers—Amazon, Google and Microsoft—offer multiple data centers and regions within the European Union. Therefore, we recommend that companies, especially those with a global user base, assess the feasibility of a safe haven for their users' data, protected by the most progressive privacy laws, by Hosting PII in the EU.","blip_selector":"hosting-pii-data-in-the-eu","name":"Hosting PII data in the EU","display_name":"Hosting PII data in the EU","url":"/radar/techniques/hosting-pii-data-in-the-eu","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1017,"quadrant":"platforms","volume_date":"2017-03","description":"Nuance Mix is a framework for natural language processing from the company that created the speech-to-text technology behind Dragon Speaking and the first roll-out of Siri. This framework supports the creation of grammars that allow for free-form user interaction via voice. The developer defines a domain-specific grammar that the framework can train itself to understand. The outcomes are responses to user input that identify the user's intents and interaction concepts. At first, it is limited to phrases close to the ones used to train it, but over time it can start to identify meaning from more divergent phrasing. Though it is still in beta, the accuracy from early exploration has been compelling, and the eventual product is one to watch for application forms that could benefit from hands-free user interaction—including mobile, IoT, AR, VR and interactive spaces. | Nuance Mix is a framework for natural language processing from the company that created the speech-to-text technology behind Dragon Speaking and the first roll-out of Siri. This framework supports the creation of grammars that allow for free-form user interaction via voice. The developer defines a domain-specific grammar that the framework can train itself to understand. The outcomes are responses to user input that identify the user's intents and interaction concepts. At first, it is limited to phrases close to the ones used to train it, but over time it can start to identify meaning from more divergent phrasing. Though it is still in beta, the accuracy from early exploration has been compelling, and the eventual product is one to watch for application forms that could benefit from hands-free user interaction—including mobile, IoT, AR, VR and interactive spaces.","blip_selector":"nuance-mix","name":"Nuance Mix","display_name":"Nuance Mix","url":"/radar/platforms/nuance-mix","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1121,"quadrant":"techniques","volume_date":"2017-03","description":"The combined use of InVision and Sketch has changed the way some people approach web application development. Although these are tools, it is really the technique of prototyping with InVision and Sketch that makes this blip significant. Creating rich, clickable prototypes as the starting point for implementing front-end and back-end behavior helps speed up the development and eliminates churn in the implementation details. This combined use of these tools strikes the right balance between premature elaboration of visual detail and capturing early user feedback on the interactive experience.","blip_selector":"prototyping-with-invision-and-sketch","name":"Prototyping with InVision and Sketch","display_name":"Prototyping with InVision and Sketch","url":"/radar/techniques/prototyping-with-invision-and-sketch","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":989,"quadrant":"platforms","volume_date":"2017-03","description":"Amazon API Gateway enables developers to expose API services to Internet clients. It offers the usual API gateway features including traffic management, monitoring, authentication and authorization. Our teams have had positive experiences using this service to front AWS Lambda as part of serverless architectures. On the other hand, we have had more challenges using it as a more general purpose gateway to front HTTP/HTTPS endpoints running on EC2—where we have been stymied by a lack of interoperability with VPCs and difficulty in establishing client cert authentication with the gateway. Due to this mixed experience, we would like to advise teams to trial using AWS API Gateway with Lambda but assess suitability when using it in a more general setting. | Amazon API Gateway is Amazon's offering enabling developers to expose API services to Internet clients. It offers the usual API gateway features like traffic management, monitoring, authentication and authorization. Our teams have been using this service to front other AWS capabilities like AWS Lambda as part of serverless architectures. We continue to monitor for the challenges presented by overambitious API gateways, but at this stage Amazon's offering appears to be lightweight enough to avoid those problems. | Amazon API Gateway is Amazon's offering enabling developers to expose API services to Internet clients, offering the usual API gateway features like traffic management, monitoring, authentication and authorization. Our teams have been using this service to front other AWS capabilities like AWS Lambda as part of serverless architectures. We continue to monitor for the challenges presented by overambitious API gateways, but at this stage Amazon's offering appears to be lightweight enough to avoid those problems.","blip_selector":"amazon-api-gateway","name":"Amazon API Gateway","display_name":"Amazon API Gateway","url":"/radar/platforms/amazon-api-gateway","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1066,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"ECMAScript 2017 —not to be confused with ES7 (a.k.a. ECMAScript 2016)—brings several noteworthy improvements to the language. Browsers are expected to implement this standard fully in the summer of 2017, but the Babel JavaScript compiler already supports a number of the features today. If you make extensive use of JavaScript and your codebase is under active development, we recommend that you add Babel to your build pipeline and begin using the supported features. | ECMAScript 2017 —not to be confused with ES7 (a.k.a. ECMAScript 2016)—brings several noteworthy improvements to the language. Browsers are expected to implement this standard fully in the summer of 2017, but the Babel JavaScript compiler already supports a number of the features today. If you make extensive use of JavaScript and your codebase is under active development, we recommend that you add Babel to your build pipeline and begin using the supported features\n\n.","blip_selector":"ecmascript-2017","name":"ECMAScript 2017","display_name":"ECMAScript 2017","url":"/radar/languages-and-frameworks/ecmascript-2017","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1117,"quadrant":"tools","volume_date":"2017-03","description":"Given the wide use of infrastructure tools today, it should come as no surprise that infrastructure as code has increased in current projects. With this tendency comes the need for testing this code. With Testinfra you can test the actual state of your servers configured manually or by tools such as Ansible, Puppet and Docker. Testinfra aims to be a Serverspec equivalent in Python and is written as a plugin to the Pytest test engine.","blip_selector":"testinfra","name":"Testinfra","display_name":"Testinfra","url":"/radar/tools/testinfra","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":795,"quadrant":"platforms","volume_date":"2017-03","description":"The Principle of Least Privilege encourages us to restrict software components to access only the resources that they need. By default, however, a Linux process can do anything its running user can do—from binding to arbitrary ports to spawning new shells. The Linux Security Modules (LSM) framework, which allows for security extensions to be plugged into the kernel, has been used to implement MAC on Linux. SELinux and AppArmor are the predominant and best-known LSM-compatible implementations that ship with the kernel. We recommend that teams learn to use one of these security frameworks (which is why we placed it in the Adopt ring). They help teams assess questions about who has access to what resources on shared hosts, including contained services. This conservative approach to access management will help teams build security into their SDLC processes. | Application whitelisting has proven to be one of the most effective ways to mitigate cyber intrusion attacks. A convenient way to implement this widely recommended practice is through Linux security modules. With SELinux or AppArmor included by default in most Linux distributions, and with more comprehensive tools such as Grsecurity readily available, we have moved this technology into the Adopt ring in this edition. These tools help teams assess questions about who has access to what resources on shared hosts, including contained services. This conservative approach to access management will help teams build security into their SDLC processes. | In earlier versions of the Radar, we have highlighted the value of Linux security modules , talking about how they enable people to think about server hardening as a part of their development workflow. More recently, with LXC and Docker containers now shipping with default AppArmor profiles on certain Linux distributions, it has forced the hand of many teams to understand how these tools work. In the event that teams use container images to run any process that they did not themselves create, these tools help them assess questions about who has access to what resources on the shared host and the capabilities that these contained services have, and be conservative in managing levels of access. | While server hardening is an old technique that is considered fairly commonplace by sysadmins who have had to manage production systems, it has not become commonplace among the developer community. However, the rise in the DevOps culture has resulted in renewed focus on tools like SELinux, AppArmor and Grsecurity that aim to make this simpler, at least on the Linux ecosystem. Each of these tools comes with their own strengths and weaknesses and it is currently hard to pick one as being the only one you will need. That said, we highly recommend that all teams at least assess which Linux security modules would be the right one for them and make security and server hardening a part of their development workflow.","blip_selector":"linux-security-modules","name":"Linux Security Modules","display_name":"Linux Security Modules","url":"/radar/platforms/linux-security-modules","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1107,"quadrant":"techniques","volume_date":"2017-03","description":"The increase in Progressive Web Applications (PWAs) is the latest attempt to bring back the mobile web in response to users' \"app fatigue\". Originally proposed by Google in 2015, PWAs are web applications that take advantage of the latest technologies to combine the best of web and native mobile applications. Using a set of open standard technologies such as, service workers, the app manifest, and cache and push APIs, we can create applications that are platform independent and deliver app-like user experiences. This brings parity to web and native applications and helps mobile developers reach users beyond the walled garden of the app stores. Think of PWAs as websites that act and feel like native apps.","blip_selector":"progressive-web-applications","name":"Progressive Web Applications","display_name":"Progressive Web Applications","url":"/radar/techniques/progressive-web-applications","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":686,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"Distributed systems often utilize multithreading, event-based communication and nonblocking I/O to improve the overall system efficiency. These programming techniques impose challenges such as low-level threading, synchronization, thread safety, concurrent data structures, and non-blocking I/O. The open source ReactiveX library beautifully abstracts away these concerns, provides the required application plumbing, and extends the observable pattern on streams of asynchronous events. ReactiveX also has an active developer community and supports a growing list of languages, the most recent addition being RxSwift. It also implements binding to mobile and desktop platforms. | The reactive architecture keeps spreading across platforms and paradigms simply because it solves a common problem in an elegant way, hiding inevitable application plumbing in a nice encapsulation. | Reactive Programming deals with streams or values that change over time. Using elements of data flow, implicit concurrency and transparent event propagation, these techniques enable efficient handling of events on a large scale with a high degree of efficiency and low latency. In the previous radar, we mentioned Reactive Extensions in .NET due to the extensive work done by Microsoft in making Rx a core part of the .NET framework. Since then, with the introduction of the Reactive Cocoa library for Objective C, the Java port of Reactive Extensions, the React JavaScript library, the Elm language based on Haskell & the Flapjax JavaScript library, we are extending this blip to include Reactive Extensions across languages.","blip_selector":"reactivex","name":"ReactiveX","display_name":"ReactiveX","url":"/radar/languages-and-frameworks/reactivex","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1006,"quadrant":"platforms","volume_date":"2017-03","description":"We have a growing belief that for most scenarios it is rarely worth rolling your own authentication code. Outsourced identity management speeds up delivery, reduces mistakes and tends to enable a faster response to newly discovered vulnerabilities. Auth0 has particularly impressed us in this field for its ease of integration, range of protocols and connectors supported, and rich management API. | We have a growing belief that for most scenarios it is rarely worth rolling your own authentication code. Outsourced identity management speeds up delivery, reduces mistakes and tends to enable a faster response to newly discovered vulnerabilities. Auth0 has particularly impressed us in this field for its ease of integration, range of protocols and connectors supported, and rich management API.","blip_selector":"auth0","name":"Auth0","display_name":"Auth0","url":"/radar/platforms/auth0","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1079,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"Knet.jl is the Koç University deep-learning framework implemented in Julia by Deniz Yuret and collaborators. Unlike gradient-generating compilers such as Theano and TensorFlow which force users into a restricted mini-language, Knet allows the definition and training of machine-learning models using the full power and expressiveness of Julia. Knet uses dynamic computational graphs generated at runtime for the automatic differentiation of almost any Julia code. We really like the support of GPU operations through the KnetArray type, and in case you don't have access to a GPU machine, the team behind Knet also maintains a preconfigured Amazon Machine Image (AMI) so you can evaluate it in the cloud.","blip_selector":"knet-jl","name":"Knet.jl","display_name":"Knet.jl","url":"/radar/languages-and-frameworks/knet-jl","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1040,"quadrant":"techniques","volume_date":"2017-03","description":"It has long been known that \"anonymized\" bulk data sets can reveal information about individuals, especially when multiple data sets are cross-referenced together. With increasing concern over personal privacy, some companies—including Apple and Google—are turning to differential privacy techniques in order to improve individual privacy while retaining the ability to perform useful analytics on large numbers of users. Differential privacy is a cryptographic technique that attempts to maximize the accuracy of statistical queries from a database while minimizing the chances of identifying its records. These results can be achieved by introducing a low amount of \"noise\" to the data, but it's important to note that this is an ongoing research area. Apple has announced plans to incorporate differential privacy into its products—and we wholeheartedly applaud its commitment to customers' privacy—but the usual Apple secrecy has left some security experts scratching their heads. We continue to recommend Datensparsamkeit as an alternative approach: simply storing the minimum data you actually need will achieve better privacy results in most cases. | It has long been known that \"anonymized\" bulk data sets can reveal information about individuals, especially when multiple data sets are cross-referenced together. With increasing concern over personal privacy, some companies—including Apple and Google—are turning to differential privacy techniques in order to improve individual privacy while retaining the ability to perform useful analytics on large numbers of users. Differential privacy is a cryptographic technique that attempts to maximize the accuracy of statistical queries from a database while minimizing the chances of identifying its records. These results can be achieved by introducing a low amount of \"noise\" to the data, but it’s important to note that this is an ongoing research area. Apple has announced plans to incorporate differential privacy into its products—and we wholeheartedly applaud its commitment to customers' privacy—but the usual Apple secrecy has left some security experts scratching their heads. We continue to recommend Datensparsamkeit as an alternative approach: simply storing the minimum data you actually need will achieve better privacy results in most cases.","blip_selector":"differential-privacy","name":"Differential privacy","display_name":"Differential privacy","url":"/radar/techniques/differential-privacy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":885,"quadrant":"platforms","volume_date":"2017-03","description":"HTTP Strict Transport Security ( HSTS ) is a now widely supported policy that allows websites to protect themselves from downgrade attacks. A downgrade attack in the context of HTTPS is one that can cause users of your site to fall back to HTTP rather than HTTPS, allowing for further attacks such as man-in-the-middle attacks. With HSTS, the server sends a header that informs the browser that it should only use HTTPS to access the website. Browser support is now widespread enough that this easy-to-implement feature should be added to any site using HTTPS. Mozilla's Observatory can help identify this and other useful headers and configuration options that improve security and privacy. When implementing HSTS, it is critical to verify that all resources load properly over HTTPS, because once HSTS is turned on, there is (almost) no turning back until the expiry time. The directive to include subdomains should be added but, again, a thorough verification that all subdomains support secure transport is required. | HTTP Strict Transport Security ( HSTS ) is a now widely supported policy that allows websites to protect themselves from downgrade attacks. A downgrade attack in the context of HTTPS is one that can cause users of your site to fall back to HTTP rather than HTTPS, allowing for further attacks such as man-in-the-middle attacks. With HSTS, the server sends a header that informs the browser that it should only use HTTPS to access the website. Browser support is now widespread enough that this easy-to-implement feature should be added to any site using HTTPS. Mozilla’s Observatory can help identify this and other useful headers and configuration options that improve security and privacy. When implementing HSTS, it is critical to verify that all resources load properly over HTTPS, because once HSTS is turned on, there is (almost) no turning back until the expiry time. The directive to include subdomains should be added but, again, a thorough verification that all subdomains support secure transport is required. | HTTP Strict Transport Security ( HSTS ) is a now widely supported policy that allows websites to protect themselves from downgrade attacks. A downgrade attack in the context of HTTPS is one that can cause users of your site to fall back to HTTP rather than HTTPS, allowing for further attacks such as man-in-the-middle attacks. By using the server header, you inform browsers that they should only use HTTPS to access your website, and should ignore downgrade attempts to contact the site via HTTP. Browser support is now widespread enough that this easy-to-implement feature should be considered for any site using HTTPS.","blip_selector":"hsts","name":"HSTS","display_name":"HSTS","url":"/radar/platforms/hsts","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1022,"quadrant":"tools","volume_date":"2017-03","description":"With the maturity of tools such as Vault, there is no longer an excuse for storing secrets in code repositories, particularly since this often ends up being the soft underbelly of important systems. We've previously mentioned repository-scanning tools such as Gitrob, but we are now pushing proactive tools such as (the ThoughtWorks-created) Talisman, which is a prepush hook for Git that scans commits for secrets matching predefined patterns. | With the maturity of tools such as Vault, there is no longer an excuse for storing secrets in code repositories, particularly since this often ends up being the soft underbelly of important systems. We’ve previously mentioned repository-scanning tools such as Gitrob, but we are now pushing proactive tools such as (the ThoughtWorks-created) Talisman, which is a prepush hook for Git that scans commits for secrets matching predefined patterns.","blip_selector":"talisman","name":"Talisman","display_name":"Talisman","url":"/radar/tools/talisman","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1080,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"One common problem in application development is how to schedule tasks that run outside the main process periodically or when certain conditions are met. The problem gets more complicated when unexpected events, such as application shutdowns, occur. The Hangfire framework, as our teams discovered, can do this and much more in the .NET environment. Hangfire is both easy to use and flexible, and it embraces a functional style. Particularly interesting is its ability to save a task's state so it can resume when an application restarts after a crash or shutdown.","blip_selector":"hangfire","name":"Hangfire","display_name":"Hangfire","url":"/radar/languages-and-frameworks/hangfire","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1063,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"Widespread adoption of AR/VR as a collaboration and communication medium requires a modern and readily available video streaming platform. WebRTC is an emerging standard for real-time communication between browsers that enables video streaming within commonly available web technologies. The range of browsers that support this standard is increasing, but Microsoft and Apple have been slow to adopt WebRTC in their proprietary browsers. If momentum continues to build, WebRTC could form the future foundation for AR/VR collaboration on the web. | Widespread adoption of AR/VR as a collaboration and communication medium requires a modern and readily available video streaming platform. WebRTC is an emerging standard for real-time communication between browsers that enables video streaming within commonly available web technologies. The range of browsers that support this standard is increasing, but Microsoft and Apple have been slow to adopt WebRTC in their proprietary browsers. If momentum continues to build, WebRTC could form the future foundation for AR/VR collaboration on the web.","blip_selector":"webrtc","name":"WebRTC","display_name":"WebRTC","url":"/radar/languages-and-frameworks/webrtc","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1039,"quadrant":"techniques","volume_date":"2017-03","description":"Although many problems that people encounter with RESTful approaches to APIs can be attributed to the anemic REST antipattern, some use cases warrant exploration of other approaches. In particular, organizations that have to support a long tail of client applications (and thus a likely proliferation of API versions even if they employ consumer-driven contracts)—and have a large portion of their APIs supporting the endless-list style of activity feeds—may hit some limits in RESTful architectures. These can sometimes be mitigated by employing the client-directed query approach to client-server interaction. We see this approach being successfully used in both GraphQL and Falcor, where clients have more control over both the contents and the granularity of the data returned to them. This does put more responsibility onto the service layer and can still lead to tight coupling to the underlying data model, but the benefits may be worth exploring if well-modeled RESTful APIs aren't working for you. | Although many problems that people encounter with RESTful approaches to APIs can be attributed to the anemic REST antipattern, some use cases warrant exploration of other approaches. In particular, organizations that have to support a long tail of client applications (and thus a likely proliferation of API versions even if they employ consumer-driven contracts)—and have a large portion of their APIs supporting the endless-list style of activity feeds—may hit some limits in RESTful architectures. These can sometimes be mitigated by employing the client-directed query approach to client-server interaction. We see this approach being successfully used in both GraphQL and Falcor, where clients have more control over both the contents and the granularity of the data returned to them. This does put more responsibility onto the service layer and can still lead to tight coupling to the underlying data model, but the benefits may be worth exploring if well-modeled RESTful APIs aren’t working for you.","blip_selector":"client-directed-query","name":"Client-directed query","display_name":"Client-directed query","url":"/radar/techniques/client-directed-query","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1068,"quadrant":"tools","volume_date":"2017-03","description":"Testing that layout and styling of responsive websites is working as expected across various form factors can be a slow and often manual process. Galen helps ease this problem by providing a simple language, running on top of Selenium, that allows you to specify expectations for the appearance of your website in various screen sizes. Although Galen suffers from the typical brittleness and speed issues of any end-to-end testing approach, we have found benefit in the early feedback on design issues. | Testing that layout and styling of responsive websites is working as expected across various form factors can be a slow and often manual process. Galen helps ease this problem by providing a simple language, running on top of Selenium, that allows you to specify expectations for the appearance of your website in various screen sizes. Although Galen suffers from the typical brittleness and speed issues of any end-to-end testing approach, we have found benefit in the early feedback on design issues.","blip_selector":"galen","name":"Galen","display_name":"Galen","url":"/radar/tools/galen","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1094,"quadrant":"platforms","volume_date":"2017-03","description":"WebVR is an experimental JavaScript API that enables you to access VR devices through your browser. It has garnered support from the community and is available through nightly builds as well as in some release versions. If you are looking to build VR experiences in your browser, then this is a great place to start. This technology alongside complementary tools such Three.js, A-Frame, ReactVR, Argon.js and Awe.js brings AR experiences to the browser. The flurry of tools in this space, alongside Internet commission standards, could help promote stronger adoption of AR and VR.","blip_selector":"webvr","name":"WebVR","display_name":"WebVR","url":"/radar/platforms/webvr","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1087,"quadrant":"platforms","volume_date":"2017-03","description":"We've had some early successes with DataStax Enterprise Graph (DSE Graph) for handling large graph databases. Built on top of Cassandra, DSE Graph targets the type of large data sets where our longtime favorite Neo4j begins to show some limitations. This scale has its trade-offs; for example, you lose the ACID transactions and run-time schema-free nature of Neo4j, but access to the underlying Cassandra tables, the integration of Spark for analytical workloads, and the powerful TinkerPop/Gremlin query language make this an option worth considering.","blip_selector":"datastax-enterprise-graph","name":"DataStax Enterprise Graph","display_name":"DataStax Enterprise Graph","url":"/radar/platforms/datastax-enterprise-graph","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1093,"quadrant":"platforms","volume_date":"2017-03","description":"Image comprehension used to be a dark art and required a team of onsite data scientists. In recent years, however, we've come closer to solving problems such as image and facial classification/categorization, facial comparisons, facial landmark identification, and facial recognition. Cloud-based image comprehension provides access to machine-learning capabilities through services such as AmazonRekognition, Microsoft Computer Vision API and Google Cloud Vision API which can supplement AR applications and anything involving photo tagging and classification.","blip_selector":"cloud-based-image-comprehension","name":"Cloud-based image comprehension","display_name":"Cloud-based image comprehension","url":"/radar/platforms/cloud-based-image-comprehension","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":889,"quadrant":"platforms","volume_date":"2017-03","description":"Mesosphere DCOS is a platform built on top of Mesos that abstracts away your underlying infrastructure for containerized applications as well as for applications you don't want to run inside Docker. This may be overkill for more modest deployments, but we're beginning to see successes with both the commercial and open source versions. We particularly like that it facilitates portability between different cloud providers as well as dedicated hardware, and that for containerized workloads you're not tied into one container orchestration framework. Although upgrades can be a little more complex than we would like, the overall stack is stabilizing nicely. | Mesosphere DCOS is a platform built on top of Mesos. It provides an abstraction over underling machines, giving you a pool of storage and compute that allows services built for DCOS to operate at massive scale (Support is already there for Hadoop, Spark and Cassandra, among others). This is probably overkill for more modest workloads at the moment (where plain old Mesos could still be a good fit), but it will be interesting to see if Mesosphere starts trying to position DCOS as a general-purpose system.","blip_selector":"mesosphere-dcos","name":"Mesosphere DCOS","display_name":"Mesosphere DCOS","url":"/radar/platforms/mesosphere-dcos","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1108,"quadrant":"tools","volume_date":"2017-03","description":"The combination of AWS Lambda with Amazon API Gateway has had a big impact on how we deploy services and APIs. However, even in this serverless configuration, the amount of configuration required to wire things together is not trivial. Claudia is a tool which automates deployment of AWS Lambda functions written in JavaScript and associated API Gateway configurations. It provides reasonable defaults, and our teams have found it allows them to get started quickly with Lambda-based microservices.","blip_selector":"claudia","name":"Claudia","display_name":"Claudia","url":"/radar/tools/claudia","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":969,"quadrant":"tools","volume_date":"2017-03","description":"Having a way to securely manage secrets is increasingly becoming a huge project issue. The old practice of keeping secrets in a file or in environment variables is becoming hard to manage, especially in environments with multiple applications and large numbers of microservices. HashiCorp Vault addresses the problem by providing mechanisms for securely accessing secrets through a unified interface. It has served us well on a number of projects, and our teams liked how easy it was to integrate Vault with their services. Storing and updating secrets is a bit cumbersome, because it relies on a command-line tool and a fair amount of discipline from the team. | Having a way to securely manage secrets is increasingly becoming a huge project issue. The old practice of keeping secrets in a file or in environment variables is becoming hard to manage, especially in environments with multiple applications and large numbers of microservices. HashiCorp Vault addresses the problem by providing mechanisms for securely accessing secrets through a unified interface. It has served us well on a number of projects, and our teams liked how easy it was to integrate Vault with their services. Storing and updating secrets is a bit cumbersome, because it relies on a command-line tool and a fair amount of discipline from the team. | Having a way to securely manage secrets is increasingly becoming a huge project issue. The old idea of just having a file with secrets or environment variables is becoming hard to manage, especially in environments with multiple applications like microservices or microcontainer environments, where the applications need to access a multitude of secrets. HashiCorp Vault is a promising tool that tries to solve the problem by providing mechanisms for securely accessing secrets through an unified interface. It has some features that make life easier, such as encryption and automatically generating secrets for known tools, among others.","blip_selector":"hashicorp-vault","name":"HashiCorp Vault","display_name":"HashiCorp Vault","url":"/radar/tools/hashicorp-vault","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1000,"quadrant":"techniques","volume_date":"2017-03","description":"The idea of virtual reality has been around for more than 50 years, and with successive advancements in computing technology many ideas have been hyped and explored. We believe that we've reached a tipping point. Reasonably affordable consumer-oriented VR headsets were shipped to the market last year, and modern graphics cards provide sufficient power to create immersive experiences with them. The headsets are mainly targeted at video game enthusiasts, but we're convinced that they'll open the doors to many possibilities for VR beyond gaming. Teams without experience in building video games should not underestimate the effort and skill required to create good 3-D models and convincing textures. | The idea of virtual reality has been around for more than 50 years, and with successive improvements of computing technology many ideas have been hyped and explored. We believe that we're reaching a tipping point now. Modern graphics cards provide sufficient compute power to render detailed, realistic scenes in high resolutions, and at the same time at least two consumer-oriented VR headsets (the HTC Vive and Facebook's Oculus Rift) are coming to market. These headsets are affordable, they have high-resolution displays, and they eliminate perceivable motion-tracking lag, which was causing issues such as headaches and nausea before. The headsets are mainly targeted at enthusiast video gaming, but we are convinced that they will open many possibilities for VR beyond gaming , particularly as the low-fi approaches, such as Google Cardboard, are driving greater awareness.","blip_selector":"vr-beyond-gaming","name":"VR beyond gaming","display_name":"VR beyond gaming","url":"/radar/techniques/vr-beyond-gaming","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1069,"quadrant":"platforms","volume_date":"2017-03","description":"Hype surrounding machine intelligence has reached a crescendo, but as with Big Data, useful frameworks and tools are waiting to be discovered among all the hot air. One such tool is wit.ai, a SaaS platform that allows developers to create conversational interfaces using natural language processing (NLP). Wit works with either text or speech inputs, helps developers manage conversational intent and allows custom business logic to be implemented using JavaScript. The system is free for commercial and noncommercial use and encourages the creation of open applications. Be aware that you must agree to let Wit use your data in order to improve the service and for its own analysis, so read the terms and conditions carefully. Another contender in this space is the Microsoft Bot Framework, but it's available only in limited preview form as of this writing. As with most things Microsoft, we expect the Bot Framework to evolve quickly, so it's worth keeping an eye on. | Hype surrounding machine intelligence has reached a crescendo, but as with Big Data, useful frameworks and tools are waiting to be discovered among all the hot air. One such tool is wit.ai, a SaaS platform that allows developers to create conversational interfaces using natural language processing (NLP). Wit works with either text or speech inputs, helps developers manage conversational intent and allows custom business logic to be implemented using JavaScript. The system is free for commercial and noncommercial use and encourages the creation of open applications. Be aware that you must agree to let Wit use your data in order to improve the service and for its own analysis, so read the terms and conditions carefully. Another contender in this space is the Microsoft Bot Framework, but it’s available only in limited preview form as of this writing. As with most things Microsoft, we expect the Bot Framework to evolve quickly, so it’s worth keeping an eye on.","blip_selector":"wit-ai","name":"wit.ai","display_name":"wit.ai","url":"/radar/platforms/wit-ai","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1019,"quadrant":"tools","volume_date":"2017-03","description":"When combining modern techniques and architecture styles, such as microservices, DevOps and QA in production, development teams need increasingly sophisticated monitoring. Simply looking a graphs of disk usage and CPU utilization is not sufficient anymore, and many teams collect application and business-specific metrics using tools such a Graphite and Kibana. Grafana makes it easy to create useful and elegant dashboards for data from a number of sources. A particularly useful feature allows timescales of different graphs to be synchronized, which helps with spotting correlations in the underlying data. The templating system that is being added shows a lot promise and will likely make managing sets of similar services even easier. Based on its strengths, Grafana has become our default choice in this category. | When combining modern techniques and architecture styles, such as microservices, DevOps and QA in production, development teams need increasingly sophisticated monitoring. Simply looking a graphs of disk usage and CPU utilization is not sufficient anymore, and many teams collect application and business-specific metrics using tools such a Graphite and Kibana. Grafana makes it easy to create useful and elegant dashboards for data from a number of sources. A particularly useful feature allows timescales of different graphs to be synchronized, which helps with spotting correlations in the underlying data. The templating system that is being added shows a lot promise and will likely make managing sets of similar services even easier. Based on its strengths, Grafana has become our default choice in this category.","blip_selector":"grafana","name":"Grafana","display_name":"Grafana","url":"/radar/tools/grafana","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1059,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"Most of our iOS teams are now using the Quick and Nimble pairing for their unit tests. In the RSpec family of behavior-driven development (BDD) testing tools, it provides very readable tests (with describe blocks) across Swift and Objective-C and has good support for asynchronous testing. | Most of our iOS teams are now using the Quick and Nimble pairing for their unit tests. In the RSpec family of behavior-driven development (BDD) testing tools, it provides very readable tests (with describe blocks) across Swift and Objective-C and has good support for asynchronous testing.","blip_selector":"quick-and-nimble","name":"Quick and Nimble","display_name":"Quick and Nimble","url":"/radar/languages-and-frameworks/quick-and-nimble","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1014,"quadrant":"platforms","volume_date":"2017-03","description":"The hype seems to have peaked for blockchains and cryptocurrencies, as evidenced by the slowdown of previous firehose-scale announcements in this area, and we expect some of the more speculative efforts to die out over time. One of the blockchains, Ethereum, while not universally popular among diehard blockchain aficionados, appears in increasing numbers in new initiatives. Ethereum is a public blockchain with a built-in programming language allowing developers to build \"smart contracts\", which are algorithmic movements of ether (the Ethereum cryptocurrency) in response to activity happening on the blockchain. R3CEV, the consortium building blockchain tech for banks, built its first proofs of concept on Ethereum. Ethereum has been used to build a distributed autonomous organization (DAO)—one of the first \"algorithmic corporations\"—although a recent heist of $150 million in the ether demonstrates that the blockchains and cryptocurrencies are still the Wild West of the technology world. | The hype seems to have peaked for blockchain and cryptocurrencies, as evidenced by the previous firehose-scale announcements in this area slowing to a trickle, and we expect some of the more speculative efforts to die out over time. One of the blockchains, Ethereum, is making good progress and is worth watching. Ethereum is a public blockchain with a built-in programming language that allows \"smart contracts\" to be built into it. These are algorithmic movements of \"ether\" (the Ethereum cryptocurrency) in response to activity happening on the blockchain. R3Cev, the consortium building blockchain tech for banks, built its first proofs of concept on Ethereum. Ethereum has been used to build a Distributed Autonomous Organization (DAO)—one of the first \"algorithmic corporations\"—although a recent heist of $150m worth of Ether demonstrates that the blockchain and cryptocurrencies are still the Wild West of the technology world.","blip_selector":"ethereum","name":"Ethereum","display_name":"Ethereum","url":"/radar/platforms/ethereum","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1098,"quadrant":"techniques","volume_date":"2017-03","description":"Technologies such as Amazon Alexa, Google Voice and Siri have dramatically lowered the bar for voice-based interaction with software. However, a more conversational style of input (voice or text) can be hard to build on top of many existing APIs, especially when it comes to a more stateful style of interaction where a follow-up interaction needs to be aware of the overall conversational context. In this style of interaction, for example, we'd like to inquire about trains from Manchester to Glasgow and then being able to ask \"What time is the first departure?\" without having to give the context of the conversation again. Normally this context would be present in the initial response we send back to a browser, but in the case of voice interfaces we need to handle this context elsewhere. Conversationally aware APIs can be an example of the backend for front-end pattern where the front-end is a voice or chat platform. This type of API can handle the specifics of this style of interaction by managing conversation states while calling underlying services on behalf of the voice front-end.","blip_selector":"conversationally-aware-apis","name":"Conversationally aware APIs","display_name":"Conversationally aware APIs","url":"/radar/techniques/conversationally-aware-apis","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1031,"quadrant":"tools","volume_date":"2017-03","description":"One of those perpetual developer debates involves language typing: How much is just right? Clojure, the dynamically typed functional Lisp on the JVM, added a new entry into this discussion that blurs the lines. Clojure.spec is a new facility built into Clojure that allows developers to wrap type and other verification criteria around data structures, such as allowable value ranges. Once they are established, Clojure uses these specifications to provide a slew of benefits: generated tests, validation, destructuring of data structures and others. Clojure.spec is a promising way to have the benefits of types and ranges where developers need them but not everywhere. | One of those perpetual developer debates involves language typing: How much is just right? Clojure, the dynamically typed functional Lisp on the JVM, added a new entry into this discussion that blurs the lines. Clojure.spec is a new facility built into Clojure that allows developers to wrap type and other verification criteria around data structures, such as allowable value ranges. Once they are established, Clojure uses these specifications to provide a slew of benefits: generated tests, validation, destructuring of data structures and others. Clojure.spec is a promising way to have the benefits of types and ranges where developers need them but not everywhere.","blip_selector":"clojure-spec","name":"Clojure.spec","display_name":"Clojure.spec","url":"/radar/tools/clojure-spec","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1054,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"We have been intrigued by the Physical Web standard created by Google. The idea of Physical Web is simple—beacons broadcast a URL—but the possibilities are broad. Basically, this is a way to annotate the physical world, tying objects and locations into the digital realm. The current transport mechanism is Eddystone URLs over Bluetooth LE, and sample clients are available. Although there are obvious security concerns with following randomly discovered links, we are most interested in use cases with customized clients where you can filter or proxy the URLs as required. | We have been intrigued by the Physical Web standard created by Google. The idea of Physical Web is simple—beacons broadcast a URL—but the possibilities are broad. Basically, this is a way to annotate the physical world, tying objects and locations into the digital realm. The current transport mechanism is Eddystone URLs over Bluetooth LE, and sample clients are available. Although there are obvious security concerns with following randomly discovered links, we are most interested in use cases with customized clients where you can filter or proxy the URLs as required.","blip_selector":"physical-web","name":"Physical Web","display_name":"Physical Web","url":"/radar/languages-and-frameworks/physical-web","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":727,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"AngularJS helped revolutionize the world of single-page JavaScript applications, and we have delivered many projects successfully with it over the years. However, we are no longer recommending it (v1) for teams starting fresh projects. We prefer the ramp-up speed and more maintainable codebases we are seeing with Ember and React, particularly in conjunction with Redux. | AngularJS helped revolutionize the world of single-page JavaScript applications, and we have delivered many projects successfully with it over the years. However, we are no longer recommending it (v1) for teams starting fresh projects. We prefer the ramp-up speed and more maintainable codebases we are seeing with Ember and React, particularly in conjunction with Redux. | While we have delivered many successful projects using AngularJS and are seeing an acceleration of adoption in corporate settings, we have decided to move Angular back to Assess on this edition of the Radar. This move is intended as a note of caution: React.js and Ember offer strong alternatives; the migration path from Angular version 1 to version 2 is causing uncertainty; and we see some organizations adopting the framework without really thinking through whether a single-page application fits their needs. We have passionate internal debates about this topic but have certainly seen codebases become overly complex from a combination of two-way binding and inconsistent state-management patterns. We believe that rather than requiring that a solid framework be jettisoned, these issues can be solved through careful design and use of Redux or Flux from the outset. | We continue to see JavaScript frameworks as a useful way to structure code and bring better coding techniques to JavaScript. AngularJS is used widely by Thoughtworks projects. However, we are a bit concerned about the future of the framework.  The 2.0 version currently under development represents a ground-up redesign that some might not be happy with.  Without an evolutionary migration path, maintainers of existing AngularJS applications will be forced to either live with an unsupported version or undertake a large rewrite.  We advise teams to first assess their requirements to understand if a single-page JavaScript application is really necessary.  In many cases, a traditional page-model app is simpler to write and easier to maintain.  Remember that there are other good alternatives to AngularJS, such as Ember.js, Knockout.js, and React.js. | We continue to see JavaScript frameworks as a useful way to structure code and bring better coding techniques to JavaScript. AngularJS is used widely by Thoughtworks projects. However we do advise teams to assess other good alternatives such as Ember.js and Knockout.js.","blip_selector":"angularjs","name":"AngularJS","display_name":"AngularJS","url":"/radar/languages-and-frameworks/angularjs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1045,"quadrant":"techniques","volume_date":"2017-03","description":"With the increasing popularity of the BFF - Backend for frontends pattern and use of one-way data-binding frameworks like React.js, we've noticed a backlash against REST-style architectures. Critics accuse REST of causing chatty, inefficient interactions among systems and failing to adapt as client needs evolve. They offer frameworks such as GraphQL or Falcor as alternative data-fetch mechanisms that let the client specify the format of the data returned. But in our experience, it isn't REST that causes these problems. Rather, they stem from a failure to properly model the domain as a set of resources. Naively developing services that simply expose static, hierarchical data models via templated URLs result in an anemic REST implementation. In a richly modeled domain, REST should enable more than simple repetitive data fetching. In a fully evolved RESTful architecture, business events and abstract concepts are also modeled as resources, and the implementation should make effective use of hypertext, link relations and media types to maximize decoupling between services. This antipattern is closely related to the Anemic Domain Model pattern and results in services that rank low in Richardson Maturity Model. We have more advice for designing effective REST APIs in our Insights article\n\n. | With the increasing popularity of the BFF - Backend for frontends pattern and use of one-way data-binding frameworks like React.js, we’ve noticed a backlash against REST-style architectures. Critics accuse REST of causing chatty, inefficient interactions among systems and failing to adapt as client needs evolve. They offer frameworks such as GraphQL or Falcor as alternative data-fetch mechanisms that let the client specify the format of the data returned. But in our experience, it isn’t REST that causes these problems. Rather, they stem from a failure to properly model the domain as a set of resources. Naively developing services that simply expose static, hierarchical data models via templated URLs result in an anemic REST implementation. In a richly modeled domain, REST should enable more than simple repetitive data fetching. In a fully evolved RESTful architecture, business events and abstract concepts are also modeled as resources, and the implementation should make effective use of hypertext, link relations and media types to maximize decoupling between services. This antipattern is closely related to the Anemic Domain Model pattern and results in services that rank low in Richardson Maturity Model. We have more advice for designing effective REST APIs in our Insights article.","blip_selector":"anemic-rest","name":"Anemic REST","display_name":"Anemic REST","url":"/radar/techniques/anemic-rest","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":1003,"quadrant":"techniques","volume_date":"2017-03","description":"We continue to see organizations chasing \"cool\" technologies, taking on unnecessary complexity and risk when a simpler choice would be better. One particular theme is using distributed, Big Data systems for relatively small data sets. This behavior prompts us to put Big Data envy on hold once more, with some additional data points from our recent experience. The Apache Cassandra database promises massive scalability on commodity hardware, but we have seen teams overwhelmed by its architectural and operational complexity. Unless you have data volumes that require a 100+ node cluster, we recommend against using Cassandra. The operational team you'll need to keep the thing running just isn't worth it. While creating this edition of the Radar, we discussed several new database technologies, many offering \"10x\" performance improvements over existing systems. We're always skeptical until new technology—especially something as critical as a database—has been properly proven. Jepsen provides analysis of database performance under difficult conditions and has found numerous bugs in various NoSQL databases. We recommend maintaining a healthy dose of skepticism and keeping an eye on sites such as Jepsen when you evaluate database tech. | We continue to see organizations chasing \"cool\" technologies, taking on unnecessary complexity and risk when a simpler choice would be better. One particular theme is using distributed, Big Data systems for relatively small data sets. This behavior prompts us to put Big Data envy on hold once more, with some additional data points from our recent experience. The Apache Cassandra database promises massive scalability on commodity hardware, but we have seen teams overwhelmed by its architectural and operational complexity. Unless you have data volumes that require a 100+ node cluster, we recommend against using Cassandra. The operational team you’ll need to keep the thing running just isn’t worth it. While creating this edition of the Radar, we discussed several new database technologies, many offering \"10x\" performance improvements over existing systems. We’re always skeptical until new technology—especially something as critical as a database—has been properly proven. Jepsen provides analysis of database performance under difficult conditions and has found numerous bugs in various NoSQL databases. We recommend maintaining a healthy dose of skepticism and keeping an eye on sites such as Jepsen when you evaluate database tech. | While we've long understood the value of Big Data to better understand how people interact with us, we've noticed an alarming trend of Big Data envy : organizations using complex tools to handle \"not-really-that-big” Data. Distributed map-reduce algorithms are a handy technique for large data sets, but many data sets we see could easily fit in a single-node relational or graph database. Even if you do have more data than that, usually the best thing to do is to first pick out the data you need, which can often then be processed on such a single node. So we urge that before you spin up your clusters, take a realistic assessment of what you need to process, and if it fits—maybe in RAM—use the simple option.","blip_selector":"big-data-envy","name":"Big Data envy","display_name":"Big Data envy","url":"/radar/techniques/big-data-envy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1012,"quadrant":"platforms","volume_date":"2017-03","description":"Electron is a solid framework for building native desktop clients using web technologies such as HTML, CSS and JavaScript. Teams can leverage their web know-how to deliver polished cross-platform desktop clients without spending time learning another set of technologies. | Electron is a solid framework for building native desktop clients using web technologies such as HTML, CSS and JavaScript. Teams can leverage their web know-how to deliver polished cross-platform desktop clients without spending time learning another set of technologies.","blip_selector":"electron","name":"Electron","display_name":"Electron","url":"/radar/platforms/electron","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1052,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"JuMP is a domain-specific language for mathematical optimizations in Julia. JuMP defines a common API called MathProgBase and enables users to write solver-agnostic code in Julia. Currently supported solvers include Artelys Knitro, Bonmin, Cbc, Clp, Couenne, CPLEX, ECOS, FICO Xpress, GLPK, Gurobi, Ipopt, MOSEK, NLopt and SCS. One other benefit is the implementation of automatic differentiation technique in reverse mode to compute derivatives so users are not limited to the standard operators like sin, cos, log and sqrt but can also implement their own custom objective functions in Julia. | JuMP is a domain-specific language for mathematical optimizations in Julia. JuMP defines a common API called MathProgBase and enables users to write solver-agnostic code in Julia. Currently supported solvers include Artelys Knitro, Bonmin, Cbc, Clp, Couenne, CPLEX, ECOS, FICO Xpress, GLPK, Gurobi, Ipopt, MOSEK, NLopt and SCS. One other benefit is the implementation of automatic differentiation technique in reverse mode to compute derivatives so users are not limited to the standard operators like sin, cos, log and sqrt but can also implement their own custom objective functions in Julia.","blip_selector":"jump","name":"JuMP","display_name":"JuMP","url":"/radar/languages-and-frameworks/jump","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1011,"quadrant":"platforms","volume_date":"2017-03","description":"Apache's Cassandra database is a powerful, scalable Big Data solution for storing and processing large amounts of data, often using hundreds of nodes split over multiple worldwide locations. It's a great tool and we like it, but too often we see teams run into trouble using it. We recommend using Cassandra carefully. Teams often misunderstand the use case for Cassandra, attempting to use it as a general-purpose data store when in fact it is optimized for fast reads on large data sets based on predefined keys or indexes. Its dependence on the storage schema can also make it difficult to evolve over time. Cassandra also has significant operational complexity and some rough edges, so unless you absolutely need the scaling it provides, a simpler solution is usually better. If you don't need Cassandra's specific use-case and scaling characteristics, you might just be choosing it out of Big Data envy. Careful use of Cassandra will include extensive automated testing, and we're happy to recommend CassandraUnit as part of your testing strategy. | Apache’s Cassandra database is a powerful, scalable Big Data solution for storing and processing large amounts of data, often using hundreds of nodes split over multiple worldwide locations. It’s a great tool and we like it, but too often we see teams run into trouble using it. We recommend using Cassandra carefully. Teams often misunderstand the use case for Cassandra, attempting to use it as a general-purpose data store when in fact it is optimized for fast reads on large data sets based on predefined keys or indexes. Its dependence on the storage schema can also make it difficult to evolve over time. Cassandra also has significant operational complexity and some rough edges, so unless you absolutely need the scaling it provides, a simpler solution is usually better. If you don’t need Cassandra’s specific use-case and scaling characteristics, you might just be choosing it out of Big Data envy. Careful use of Cassandra will include extensive automated testing, and we’re happy to recommend CassandraUnit as part of your testing strategy.","blip_selector":"cassandra-carefully","name":"Cassandra carefully","display_name":"Cassandra carefully","url":"/radar/platforms/cassandra-carefully","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1123,"quadrant":"tools","volume_date":"2017-03","description":"MSBuild has been the primary build system in the .NET ecosystem since its introduction in 2005; however, it suffers from many of the same weaknesses we've previously called out in Maven. The .NET community has started to develop alternatives to MSBuild which are easier to maintain and more flexible, and evolve more fluidly as a project grows. Two of these alternatives are Cake and Fake. Cake uses a DSL built in C#, while Fake uses F#. Each has seen significant growth over the last year and has proven to be a viable alternative to MSBuild for orchestrating common build tasks in .NET projects.","blip_selector":"cake-and-fake","name":"Cake and Fake","display_name":"Cake and Fake","url":"/radar/tools/cake-and-fake","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":879,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"If you are faced with building a single-page application (SPA) and trying to choose a framework to build with, Ember.js has emerged as a leading choice. Our teams praise Ember for its highly productive developer experience, with far fewer surprises than other frameworks such as AngularJS. The Ember CLI build tooling is a haven in the storm of JavaScript build tools, and the Ember core team and community are highly active and responsive. | If you are faced with building a single-page application (SPA) and trying to choose a framework to build with, Ember.js has emerged as a leading choice. Our teams praise Ember for its highly productive developer experience, with far fewer surprises than other frameworks such as AngularJS. The Ember CLI build tooling is a haven in the storm of JavaScript build tools, and the Ember core team and community are highly active and responsive. | Ember.js has developed further support based on project experiences and is clearly a strong contender in the field of JavaScript application frameworks. Ember is praised for its developer experience, with far fewer surprises than other frameworks such as AngularJS. The Ember CLI build tooling, convention-over-configuration approach and ES6 support also gain positive feedback. | Widespread usage of AngularJS continues on ThoughtWorks projects, although not every experience is positive. We continue to advise teams to assess whether the additional complexity of a single-page JavaScript application is necessary to meet their requirements.  We also recommend assessing alternative frameworks, and in this radar edition we highlight Ember.js which is growing in popularity within ThoughtWorks.  Ember is praised for its approach of opinionated convention over configuration, responsive core team of committers, performance, and build tooling support via Ember CLI.","blip_selector":"ember-js","name":"Ember.js","display_name":"Ember.js","url":"/radar/languages-and-frameworks/ember-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":796,"quadrant":"platforms","volume_date":"2017-03","description":"We've continued to have positive experiences deploying the Apache Mesos platform to manage cluster resources for highly distributed systems. Mesos abstracts out underlying computing resources such as CPU and storage, aiming to provide efficient utilization while maintaining isolation. Mesos includes Chronos for distributed and fault-tolerant execution of scheduled jobs, and Marathon for orchestrating long-running processes in containers. | We've continued to have positive experiences deploying the Apache Mesos platform to manage cluster resources for highly distributed systems. Mesos abstracts out underlying computing resources such as CPU and storage, aiming to provide efficient utilization while maintaining isolation. Mesos includes Chronos for distributed and fault-tolerant execution of scheduled jobs, and Marathon for orchestrating long-running processes in containers. | Mesos is a platform that abstracts out underlying computing resources to make it easier to build massively scalable distributed systems. It can be used to provide a scheduling layer for Docker, or to act as an abstraction layer to things like AWS. Twitter has used it to great effect to help it scale its infrastructure. Tools built on top of Mesos are starting to appear, such as Chronos, which is a distributed, fault-tolerant cron replacement. Prominent success stories are appearing, such as Apple's Siri rearchitecting to use Mesos. | Mesos is a platform that abstracts out underlying computing resources to make it easier to build massively scalable distributed systems. It can be used to provide a scheduling layer for Docker, or to act as an abstraction layer to things like AWS. Twitter has used it to great effect to help them scale their infrastructure. Tools build on top of Mesos are starting to appear such as Chronos, which is a distributed, fault tolerant cron replacement.","blip_selector":"apache-mesos","name":"Apache Mesos","display_name":"Apache Mesos","url":"/radar/platforms/apache-mesos","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1050,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"Some of our ThoughtWorks teams have had very positive experiences with Phoenix , a server-side web MVC framework written in Elixir. In addition to being streamlined and easy to use, Phoenix takes advantage of Elixir to be extremely fast. For some developers, Phoenix evokes the joy they experienced when first discovering Ruby and Rails. Although the ecosystem of libraries for Phoenix is not as extensive as for some more mature frameworks, it should benefit from the continuing success and growth of support for Elixir. | Some of our ThoughtWorks teams have had very positive experiences with Phoenix , a server-side web MVC framework written in Elixir. In addition to being streamlined and easy to use, Phoenix takes advantage of Elixir to be extremely fast. For some developers, Phoenix evokes the joy they experienced when first discovering Ruby and Rails. Although the ecosystem of libraries for Phoenix is not as extensive as for some more mature frameworks, it should benefit from the continuing success and growth of support for Elixir.","blip_selector":"phoenix","name":"Phoenix","display_name":"Phoenix","url":"/radar/languages-and-frameworks/phoenix","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1088,"quadrant":"platforms","volume_date":"2017-03","description":"Alongside virtual reality (VR), which has a relatively high bar to entry due to hardware requirements and the effort to create virtual worlds, alternate reality (AR) and mixed reality (MR) also entered into the mainstream last year. Pokémon Go provided evidence that regular smartphones are sufficient to create compelling AR/MR experiences. Tango is a new hardware sensor technology for mobile phones that further enhances the possibilities for AR/MR on phones. It allows apps to acquire detailed 3-D measurements of the user's surroundings so that virtual objects can be placed and rendered more convincingly on the camera feed. The first phones with Tango technology are now available.","blip_selector":"tango","name":"Tango","display_name":"Tango","url":"/radar/platforms/tango","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1119,"quadrant":"tools","volume_date":"2017-03","description":"As any Emacs fan will tell you, Emacs is more than a text editor; it is a platform for character-mapped applications. Over the past few years, there has been an explosion of new developments on this platform, but we think Spacemacs deserves particular attention. Spacemacs provides an introduction to the Emacs platform, with a new keyboard user-interface, simplified customization layers, and a curated distribution of Emacs packages. One of the project's aims is to be the best of worlds by combining the Vim UI with the internal reprogrammability of Emacs. We consider developer productivity tools to be a vital part of effective software development, and if you haven't considered Emacs for a while, we suggest you take a look at how Spacemacs rethinks this classic development platform.","blip_selector":"spacemacs","name":"Spacemacs","display_name":"Spacemacs","url":"/radar/tools/spacemacs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":919,"quadrant":"platforms","volume_date":"2017-03","description":"Our teams continue to enjoy using AWS Lambda and are beginning to use it to experiment with serverless architectures, combining Lambda with the API Gateway. We do recommend that Lambda functions contain only a moderate amount of code. Ensuring the quality of a solution based on a tangle of many large Lambda functions is difficult, and such a solution may not be cost-effective. For more complex needs, deployments based on containers or VMs are still preferable. In addition, we have run into significant problems using Java for Lambda functions, with erratic latencies up to several seconds as the Lambda container is started. Of course, you can sidestep this issue by using JavaScript or Python, and if Lambda functions do not contain a lot of code, the choice of programming language should not matter too much. | Our teams continue to enjoy using AWS Lambda and are beginning to use it to experiment with serverless architectures, combining Lambda with the API Gateway. We do recommend that Lambda functions contain only a moderate amount of code. Ensuring the quality of a solution based on a tangle of many large Lambda functions is difficult, and such a solution may not be cost-effective. For more complex needs, deployments based on containers or VMs are still preferable. In addition, we have run into significant problems using Java for Lambda functions, with erratic latencies up to several seconds as the Lambda container is started. Of course, you can sidestep this issue by using JavaScript or Python, and if Lambda functions do not contain a lot of code, the choice of programming language should not matter too much. | Our teams continue to enjoy using AWS Lambda and are beginning to use it to experiment with Serverless architectures, combining Lambda with the API Gateway to produce highly scalable systems with invisible infrastructure. We have run into significant problems using Java for Lambda functions, with erratic latencies up to several seconds as the Lambda container is started. We recommend sticking with JavaScript or Python for the time being. | AWS releases a huge number of new features on what seems like a monthly basis, so it can sometimes be hard for any new service offering to rise above the noise, but Lambda certainly manages to attract notice. Initially just supporting JavaScript, but now adding support for JVM-based applications (with more no doubt to follow), Lambda allows you to fire up very short-lived processes either in reaction to an event, or via a call from the related API Gateway. For stateless services, this means you don’t need to worry about running any long-lived machines, potentially reducing costs and improving security. Despite other forays into the PaaS space by AWS, Lambda looks the closest to getting this right.","blip_selector":"aws-lambda","name":"AWS Lambda","display_name":"AWS Lambda","url":"/radar/platforms/aws-lambda","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1081,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"Nightwatch is a framework that allows automated acceptance tests for browser-based apps to be created in JavaScript and run in Node.js. Nightwatch allows tests to be defined using a fluent API which can then be executed against a Selenium/WebDriver server. In the case of single page apps or other JavaScript-heavy pages, this allows the automated tests to be created and run within the same language and environment as the bulk of the code.","blip_selector":"nightwatch","name":"Nightwatch","display_name":"Nightwatch","url":"/radar/languages-and-frameworks/nightwatch","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1032,"quadrant":"tools","volume_date":"2017-03","description":"With the growth of interest in streaming data architectures and the downstream data lakes they feed, we have seen an increased reliance on \"change data capture\" tooling to connect transactional data stores to stream-processing systems. Bottled Water is a welcome addition to this field, converting changes in PostgreSQL's write-ahead log into Kafka events. One downside of this approach, however, is that you are tied to low-level database events rather than the higher-level business events we recommend as the foundation for an event-oriented architecture. | With the growth of interest in streaming data architectures and the downstream data lakes they feed, we have seen an increased reliance on \"change data capture\" tooling to connect transactional data stores to stream-processing systems. Bottled Water is a welcome addition to this field, converting changes in PostgreSQL’s write-ahead log into Kafka events. One downside of this approach, however, is that you are tied to low-level database events rather than the higher-level business events we recommend as the foundation for an event-oriented architecture.","blip_selector":"bottled-water","name":"Bottled Water","display_name":"Bottled Water","url":"/radar/tools/bottled-water","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":676,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"Interest in the Elixir programming language continues to build. Increasingly, we see it used in serious projects and hear feedback from developers who find its Actor model to be robust and very fast. Elixir, which is built on top of the Erlang virtual machine, is showing promise for creating highly concurrent and fault-tolerant systems. Elixir has distinctive features such as the Pipe operator, which allows developers to build a pipeline of functions as you would in the UNIX command shell. The shared byte code allows Elixir to interoperate with Erlang and leverage existing libraries while supporting tools such as the Mix build tool, the IEx interactive shell and the ExUnit unit-testing framework. | Interest in the Elixir programming language continues to build. Increasingly, we see it used in serious projects and hear feedback from developers who find its Actor model to be robust and very fast. Elixir, which is built on top of the Erlang virtual machine, is showing promise for creating highly concurrent and fault-tolerant systems. Elixir has distinctive features such as the Pipe operator, which allows developers to build a pipeline of functions as you would in the UNIX command shell. The shared byte code allows Elixir to interoperate with Erlang and leverage existing libraries while supporting tools such as the Mix build tool, the IEx interactive shell and the ExUnit unit-testing framework. | We continue to see a lot of excitement from people using the Elixir programming language. Elixir, which is built on top of the Erlang virtual machine, is showing promise for creating highly concurrent and fault-tolerant systems. Elixir has distinctive features such as the Pipe operator, which allows developers to build a pipeline of functions as you would in the UNIX command shell. The shared byte code allows Elixir to interoperate with Erlang and leverage existing libraries while supporting tools such as the Mix build tool, the Iex interactive shell and the ExUnit unit testing framework. | Elixir is a dynamic, functional, homoiconic programming language built on top of the Erlang virtual machine with a powerful macro system that makes it ideal for building Domain Specific Languages. Elixir has distinctive features such as the Pipe operator that allows developers to build a pipeline of functions like you would in the UNIX command shell. The shared byte code allows Elixir to interoperate with Erlang and leverage existing libraries while supporting tools such as the Mix build tool, the Iex interactive shell and the ExUnit unit testing framework. It is a practical alternative to Erlang for building DSLs.","blip_selector":"elixir","name":"Elixir","display_name":"Elixir","url":"/radar/languages-and-frameworks/elixir","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1053,"quadrant":"platforms","volume_date":"2017-03","description":"OpenVR is the underlying SDK in making many of the VR head-mounted displays (HMDs) work with Unity and will likely keep growing in importance. Much of the VR work at ThoughtWorks was built on top of OpenVR, because it will run on any HMD, unlike the other SDKs. Though it is not open source, it is free via the license. The Oculus SDK is more restrictive in its licensing and only works on Oculus devices. OSVR, while truly open source, doesn't seem to have as much adoption yet. If you're going to develop a VR application and target as many devices as possible—and not use Unity or Unreal to develop them—OpenVR is the most concrete and pragmatic solution right now. | OpenVR is the underlying SDK in making many of the VR head-mounted displays (HMDs) work with Unity and will likely keep growing in importance. Much of the VR work at ThoughtWorks was built on top of OpenVR, because it will run on any HMD, unlike the other SDKs. Though it is not open source, it is free via the license. The Oculus SDK is more restrictive in its licensing and only works on Oculus devices. OSVR, while truly open source, doesn't seem to have as much adoption yet. If you're going to develop a VR application and target as many devices as possible—and not use Unity or Unreal to develop them—OpenVR is the most concrete and pragmatic solution right now.","blip_selector":"openvr","name":"OpenVR","display_name":"OpenVR","url":"/radar/platforms/openvr","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1030,"quadrant":"tools","volume_date":"2017-03","description":"Android-x86 is a port of the Android open source project to x86 platforms. The project started by hosting various patches from the community for x86 support but then later created its own codebase to provide support for different x86 platforms. We have seen significant time savings by utilizing Android-x86 in our CI servers instead of emulators for hermetic UI testing. However, for UI-specific tests targeting a particular device resolution—simulating low memory, bandwidth and battery—it is better to stick with emulators. | Android-x86 is a port of the Android open source project to x86 platforms. The project started by hosting various patches from the community for x86 support but then later created its own codebase to provide support for different x86 platforms. We have seen significant time savings by utilizing Android-x86 in our CI servers instead of emulators for hermetic UI testing. However, for UI-specific tests targeting a particular device resolution—simulating low memory, bandwidth and battery—it is better to stick with emulators.","blip_selector":"android-x86","name":"Android-x86","display_name":"Android-x86","url":"/radar/tools/android-x86","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1100,"quadrant":"techniques","volume_date":"2017-03","description":"Social code analysis enriches our understanding of the code quality by overlaying a developer's behavior with the structural analysis of the code. It uses data from version control systems, such as frequency and time of the change as well as the person making the change. You can choose to write your own scripts to analyze such data or use tools such as CodeScene. CodeScene can help you gain a better understanding of your software systems by identifying hotspots and complex, hard-to-maintain subsystems, coupling between distributed subsystems through temporal coupling, as well as the view of Conway's law in your organization. We believe that with technology trends such as distributed systems, microservices and distributed teams the social dimension of our code is vital in our holistic understanding of our systems' health.","blip_selector":"social-code-analysis","name":"Social code analysis","display_name":"Social code analysis","url":"/radar/techniques/social-code-analysis","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1025,"quadrant":"tools","volume_date":"2017-03","description":"Pa11y is an automatic accessibility tester that can run from the command line and be embedded into a build pipeline. Our teams have had success using Pa11y on a highly dynamic site by first creating a static HTML version, then running the accessibility tests against that. For many systems—especially government websites—accessibility testing is a requirement, and Pa11y makes it all a lot easier. | Pa11y is an automatic accessibility tester that can run from the command line and be embedded into a build pipeline. Our teams have had success using Pa11y on a highly dynamic site by first creating a static HTML version, then running the accessibility tests against that. For many systems—especially government websites—accessibility testing is a requirement, and Pa11y makes it all a lot easier.","blip_selector":"pa11y","name":"Pa11y","display_name":"Pa11y","url":"/radar/tools/pa11y","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1074,"quadrant":"languages-and-frameworks","volume_date":"2017-03","description":"DeepLearning.scala is an open source deep-learning toolkit in Scala created by our colleagues at ThoughtWorks. We're excited about this project because it uses differentiable functional programming to create and compose neural networks; a developer simply writes code in Scala with static typing. DeepLearning.scala currently supports basic types such as float, double, GPU-accelerated N-dimensional arrays as well as algebraic data types. We're looking forward to future releases of the toolkit which are said to support higher order functions and distributed training on Spark.","blip_selector":"deeplearning-scala","name":"DeepLearning.scala","display_name":"DeepLearning.scala","url":"/radar/languages-and-frameworks/deeplearning-scala","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1010,"quadrant":"platforms","volume_date":"2017-03","description":"IndiaStack is a set of Open APIs designed with the goal of transforming India from a data-poor to a data-rich country. The stack emphasizes layered innovation by specifying a minimal set of APIs and encourages the rest of the ecosystem to build custom applications on top of these APIs. Aadhaar serves as one of the foundation layers, providing authentication services for more than a billion Indian citizens. In addition, there are services to provide paperless transactions through digital signatures (eSign), unified online payment (UPI) and an electronic consent layer ((e-KYC)[https://uidai.gov.in/ecosystem/authentication-devices-documents/about-aadhaar-paperless-offline-e-kyc.html]) to securely provide Aadhaar details to service providers. We believe in the Open API–driven initiative to bring digital innovation, and the design principles behind IndiaStack could be used as a change agent for other regions/countries. | IndiaStack is a set of Open APIs designed with the goal of transforming India from a data-poor to a data-rich country. The stack emphasizes layered innovation by specifying a minimal set of APIs and encourages the rest of the ecosystem to build custom applications on top of these APIs. Aadhaar serves as one of the foundation layers, providing authentication services for more than a billion Indian citizens. In addition, there are services to provide paperless transactions through digital signatures (eSign), unified online payment (UPI) and an electronic consent layer ((e-KYC)[https://uidai.gov.in/ecosystem/authentication-devices-documents/about-aadhaar-paperless-offline-e-kyc.html]) to securely provide Aadhaar details to service providers. We believe in the Open API–driven initiative to bring digital innovation, and the design principles behind IndiaStack could be used as a change agent for other regions/countries.","blip_selector":"indiastack","name":"IndiaStack","display_name":"IndiaStack","url":"/radar/platforms/indiastack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":696,"quadrant":"platforms","volume_date":"2017-03","description":"We are seeing too many organizations run into trouble as they attempt to use their CMS as a platform for delivering large and complex digital applications. This is often driven by the vendor-fueled hope of bypassing unresponsive IT organizations and enabling the business to drag and drop changes directly to production. While we are very supportive of providing content producers with the right tools and workflows, for applications with complex business logic we tend to recommend treating your CMS as a component of your platform (often in a hybrid or headless mode) cooperating cleanly with other services, rather than attempting to implement all of your functionality in the CMS itself. | We are seeing too many organizations run into trouble as they attempt to use their CMS as a platform for delivering large and complex digital applications. This is often driven by the vendor-fueled hope of bypassing unresponsive IT organizations and enabling the business to drag and drop changes directly to production. While we are very supportive of providing content producers with the right tools and workflows, for applications with complex business logic we tend to recommend treating your CMS as a component of your platform (often in a hybrid or headless mode) cooperating cleanly with other services, rather than attempting to implement all of your functionality in the CMS itself. | In previous editions of the radar, we have written about the pitfalls of trying to use a CMS as a platform and we continue to see this problematic approach “in the wild.” CMS as an editing, collaboration and workflow platform can work well, and we certainly do not discount these features. We have had success using Two Stack CMS, an approach that separates the concerns of editing and publishing content. | Content Management Systems (CMS) have their place. In many cases it is unreasonable to write editing and workflow functionality from scratch. However, we have experienced serious problems when CMS as a platform becomes an IT solution that grows beyond managing simple content.","blip_selector":"cms-as-a-platform","name":"CMS as a platform","display_name":"CMS as a platform","url":"/radar/platforms/cms-as-a-platform","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1036,"quadrant":"techniques","volume_date":"2017-03","description":"Companies have wholeheartedly embraced APIs as a way to expose business capabilities to both external and internal developers. APIs promise the ability to experiment quickly with new business ideas by recombining core capabilities. But what differentiates an API from an ordinary enterprise integration service? One difference lies in treating APIs as a product , even when the consumer is an internal system or fellow developer. Teams that build APIs should understand the needs of their customers and make the product compelling to them. Usability testing and UX research can lead to a better design and understanding of the API usage patterns and help bring a product mindset to APIs. APIs, like products, should be actively maintained and supported, and, easy to use. They should have an owner who advocates for the customer and strives for continual improvement. In our experience, product orientation is the missing ingredient that makes the difference between ordinary enterprise integration and an agile business built on a platform of APIs. | Businesses have wholeheartedly embraced APIs as a way to expose business capabilities to both external and internal developers. APIs promise the ability to experiment quickly with new business ideas by recombining core capabilities. But what differentiates an API from an ordinary enterprise integration service? One difference lies in treating APIs as a product , even when the consumer is an internal system. Teams that build APIs should understand the needs of their customers and make the product compelling to them. Products are also improved, maintained and supported over the long term. They should have an owner who advocates for the customer and strives for continual improvement. Products are actively maintained and supported, easy to find and easy to use. In our experience, a product orientation is the missing ingredient that makes the difference between ordinary enterprise integration and an agile business built on a platform of APIs.","blip_selector":"apis-as-a-product","name":"APIs as a product","display_name":"APIs as a product","url":"/radar/techniques/apis-as-a-product","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1114,"quadrant":"tools","volume_date":"2017-03","description":"Amazon Rekognition is one of the cloud-based image comprehension tools we've mentioned elsewhere in this Radar. What we like about it is that Amazon has taken a somewhat novel approach to making faces anonymous (using GUIDs) from AWS to accommodate some of the privacy concerns that come with facial recognition.","blip_selector":"amazon-rekognition","name":"Amazon Rekognition","display_name":"Amazon Rekognition","url":"/radar/tools/amazon-rekognition","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1116,"quadrant":"tools","volume_date":"2017-03","description":"Molecule is designed to aid in the development and testing of Ansible roles. By building the scaffolding for running Ansible role tests on a virtual machine or container of choice, we don't have to setup our testing environment manually. Molecule leverages Vagrant, Docker, and OpenStack to manage virtual machines or containers, and supports Serverspec, Testinfra, or Goss to run the tests. The default steps in the sequence facility model include: virtual machine management, Ansible linting, idempotence testing and convergence testing. Although it is a fairly young project, we see a great potential for its usage.","blip_selector":"molecule","name":"Molecule","display_name":"Molecule","url":"/radar/tools/molecule","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":979,"quadrant":"platforms","volume_date":"2016-11","description":"Realm is a database designed for use on mobile devices, with its own persistence engine to achieve high performance. Realm is marketed as a replacement for SQLite and Core Data. Note that migrations are not quite as straightforward as the Realm documentation would have you believe. However, more and more teams are choosing Realm as the persistence mechanism in production environments for mobile applications. | Realm is a database designed for use on mobile devices, with its own persistence engine to achieve high performance. Realm is marketed as a replacement for SQLite and Core Data, and our teams have enjoyed using it. Note that migrations are not quite as straightforward as the Realm documentation would have you believe. Still, Realm has us excited, and we suggest you take a look.","blip_selector":"realm","name":"Realm","display_name":"Realm","url":"/radar/platforms/realm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":968,"quadrant":"tools","volume_date":"2016-11","description":"LambdaCD provides teams with a way to define Continuous Delivery pipelines in Clojure. This brings the benefits of Infrastructure as code to the configuration of CD servers: source-control management, unit testing, refactoring and code reuse. In the \"pipelines as code\" space, LambdaCD stands out for being lightweight, self-contained and fully programmable, allowing teams to work with their pipelines in the same way that they do with their code.","blip_selector":"lambdacd","name":"LambdaCD","display_name":"LambdaCD","url":"/radar/tools/lambdacd","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":967,"quadrant":"tools","volume_date":"2016-11","description":"Teams using the Phoenix Server or Phoenix Environment techniques have found little in the way of support from Application Performance Management (APM) tools. Their licensing models, based on long-running, limited amounts of tin, and their difficulty in dealing with ephemeral hardware, have meant that they are often more trouble than they are worth. However, distributed systems need monitoring, and at some point many teams recognize the need for an APM tool. We think Pinpoint, an open source tool in this space, is worth investigating as an alternative to AppDynamics and Dynatrace. Pinpoint is written in Java, with plugins available for many servers, databases and frameworks. While we think you can go a long way using a combination of other lightweight open source tools—Zipkin, for example—if you are in the market for an APM, Pinpoint is worth considering.","blip_selector":"pinpoint","name":"Pinpoint","display_name":"Pinpoint","url":"/radar/tools/pinpoint","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1067,"quadrant":"tools","volume_date":"2016-11","description":"Our teams have had success with axios, a promises-based HTTP client in JavaScript that they describe as \"better than Fetch.\" The project has lots of endorsements and activity on GitHub, and it gets a thumbs-up from us.","blip_selector":"axios","name":"axios","display_name":"axios","url":"/radar/tools/axios","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":962,"quadrant":"tools","volume_date":"2016-11","description":"In the past we have included automated Provisioning Testing as a recommended technique, and in this issue we highlight Serverspec as a popular tool for implementing those tests. Although this tool is not new, we are seeing its use become more common as more cross-functional delivery teams take on responsibility for infrastructure provisioning. Serverspec is built on the Ruby library RSpec and comes with a comprehensive set of helpers for asserting that server configuration is correct.","blip_selector":"serverspec","name":"Serverspec","display_name":"Serverspec","url":"/radar/tools/serverspec","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":927,"quadrant":"platforms","volume_date":"2016-11","description":"The emerging Containers as a Service (CaaS) space is seeing a lot of movement and provides a useful option between basic IaaS (Infrastructure as a Service) and more opinionated PaaS (Platform as a Service). While Rancher creates less noise than some other players, we have enjoyed the simplicity that it brings to running Docker containers in production. It can run stand-alone as a full solution or in conjunction with tools like Kubernetes. | Rancher is an open source solution that allows deployment of containers into a cluster of machines, which is becoming an increasingly common scenario. It provides services such lifecycle management, monitoring, health checks and discovery. Also included is a completely containerized operating system based on Docker. The broad focus on containerization and very small footprint are key advantages for Rancher. A similar solution in this space is Kubernetes.","blip_selector":"rancher","name":"Rancher","display_name":"Rancher","url":"/radar/platforms/rancher","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":932,"quadrant":"platforms","volume_date":"2016-11","description":"We've seen the indisputable productivity gains that come from deployment of applications and services into mature cloud providers. Much of that gain comes from the ability of teams to deploy and operate their own services with a high degree of autonomy and responsibility. We are now regularly coming across Superficial Private Cloud offerings within organizations, where basic virtualization platforms are being given the “cloud” label. Often teams can self-provision a restricted set of fixed service types with limited access and little ability to customize the centrally governed “enterprise blueprints,” leading to kludge solutions. Deployment pace regularly remains constrained by manually provisioned infrastructure such as network, firewall and storage. We encourage organizations to more fully consider the costs of mandating the use of an inadequate private cloud offering.","blip_selector":"superficial-private-cloud","name":"Superficial private cloud","display_name":"Superficial private cloud","url":"/radar/platforms/superficial-private-cloud","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":964,"quadrant":"tools","volume_date":"2016-11","description":"Webpack has solidified itself as our go-to JavaScript module bundler. With its ever-growing list of loaders, it provides a single dependency tree for all your static assets, allowing flexible manipulation of JavaScript, CSS, etc. and minimizing what needs to be sent to the browser and when. Of particular relevance is the smooth integration among AMD, CommonJS and ES6 modules and how it has enabled teams to work in ES6 and seamlessly transpile (using Babel) to earlier versions for browser compatibility. Many of our teams also value Browserify, which covers a similar space but is more focused on making Node.js modules available for client-side use.","blip_selector":"webpack","name":"Webpack","display_name":"Webpack","url":"/radar/tools/webpack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":980,"quadrant":"tools","volume_date":"2016-11","description":"Attacks on web properties using bots are becoming more sophisticated. Identifying these bad actors and their behaviors is the goal of the Repsheet project. It's a plugin for either Apache or NGINX that records user activity, fingerprints actors using predefined and user-defined rules, and then allows action to be taken, including the ability to block offensive actors. It includes a utility that visualizes current actors; this puts the ability to manage bot-based threats in the hands of team members, increasing security awareness for teams. We like this since it's a good example of a simple tool solving a very real but often invisible problem—bot-based attacks.","blip_selector":"repsheet","name":"Repsheet","display_name":"Repsheet","url":"/radar/tools/repsheet","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":905,"quadrant":"tools","volume_date":"2016-11","description":"Load Impact is a SaaS load-testing tool that can generate highly realistic loads of up to 1.2 million concurrent users. Record and playback web interactions using a Chrome plugin simulate network connections for mobile or desktop users and generate load from up to 10 different locations around the world. While not the only on-demand load-testing tool we've used—we also like BlazeMeter—our teams were very enthusiastic about Load Impact.","blip_selector":"load-impact","name":"Load Impact","display_name":"Load Impact","url":"/radar/tools/load-impact","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":937,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"We are seeing continued success with React Native for rapid cross-platform mobile development. Despite some churn as it undergoes continuing development, the advantages of trivial integration between native and nonnative code and views, the rapid development cycle (instant reload, chrome debugging, Flexbox layout) and general growth of the React style is winning us over. As with many frameworks, care needs to be taken to keep your code well structured, but diligent use of a tool like Redux really helps here. | Yet another entrant into the cross-platform mobile development world, Facebook’s React Native brings the React.js programming model to iOS and Android developers. React Native programs are written in JavaScript, but unlike a hybrid framework such as Ionic, React Native gives developers access to native UI components on the target platform. This is an approach we’ve seen before (e.g., Calatrava), but React Native has already inspired a substantial developer community and builds on the momentum generated by React.js. This framework could play a significant role in the future of mobile app development.","blip_selector":"react-native","name":"React Native","display_name":"React Native","url":"/radar/languages-and-frameworks/react-native","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1005,"quadrant":"platforms","volume_date":"2016-11","description":"Amazon recently launched the AWS Application Load Balancer (ALB), a direct replacement for Elastic Load Balancers introduced back in 2009. ALB supports Layer 7 traffic inspection and is built to support modern cloud architecture. If you’re building a microservices-based system using ECS, the new load balancers will directly understand container hosting and scaling, with multiple containers and ports per EC2 instance. Content-based routing allows segmentation of requests onto groups of target servers, along with independent scaling of those groups. Health checks performed by the load balancers are much improved, with the ability to capture detailed metrics about application performance. We like everything that we see here, and teams have begun to report successful usage of ALB.","blip_selector":"aws-application-load-balancer","name":"AWS Application Load Balancer","display_name":"AWS Application Load Balancer","url":"/radar/platforms/aws-application-load-balancer","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":956,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"Immutability is often emphasized in the functional programming paradigm, and most languages have the ability to create immutable objects—objects that can't be changed once created. Immutable.js is a library for JavaScript that provides many persistent immutable data structures, which are highly efficient on modern JavaScript virtual machines. Immutable.js objects are, however, not normal JavaScript objects, so references to JavaScript objects from immutable objects should be avoided. More teams are using this library for tracking mutation and maintaining state in production. We recommend that developers investigate this library, especially when it's combined with the rest of the Facebook stack. | Immutability is often emphasized in the functional programming paradigm, and most languages have the ability to create immutable objects, which cannot be changed once created. Immutable.js is a library for JavaScript that provides many persistent immutable data structures, which are highly efficient on modern JavaScript virtual machines. Immutable.js objects are, however, not normal JavaScript objects, so references to JavaScript objects from immutable objects should be avoided. Our teams have had value using this library for tracking mutation and maintaining state, and it is a library we encourage developers to investigate, especially when it's combined with the rest of the Facebook stack.","blip_selector":"immutable-js","name":"Immutable.js","display_name":"Immutable.js","url":"/radar/languages-and-frameworks/immutable-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":952,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"Dapper is a minimal, lightweight ORM of sorts for .NET. Rather than trying to write the SQL queries for you, Dapper maps SQL queries to dynamic objects. Though it's not brand new, Dapper has steadily gained acceptance from ThoughtWorks teams working in .NET. For the C# programmer, it removes some of the drudgery of mapping relational queries to objects while still allowing complete control over the SQL or stored procedures.","blip_selector":"dapper","name":"Dapper","display_name":"Dapper","url":"/radar/languages-and-frameworks/dapper","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":466,"quadrant":"tools","volume_date":"2016-11","description":"Development on Zipkin has continued apace, and since the middle of 2015 it has moved to the openzipkin/zipkin organization at GitHub. There are now bindings for Python, Go, Java, Ruby, Scala and C#; and there are Docker images available for those wanting to get started quickly. We still like this tool. There is an active and growing community around usage of it, and implementation is getting easier. If you need a way of measuring the end-to-end latency of many logical requests, Zipkin continues to be a strong choice. | When building distributed applications to address web-scale or big data requirements, setting up appropriate monitoring tools becomes a non-trivial exercise. Zipkin is a tool that instruments the different components of a service-based system and visualizes the breakdown of logical requests passing through multiple services using a ‘firebug-like’ view. The raw data can be stored in Hadoop for more advanced reporting and data mining.","blip_selector":"zipkin","name":"Zipkin","display_name":"Zipkin","url":"/radar/tools/zipkin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":946,"quadrant":"tools","volume_date":"2016-11","description":"Let's Encrypt first appeared on the Radar last edition, and since December 2015 this project has moved its beta status from private to public, meaning users will no longer be required to have an invitation in order to try it. Let's Encrypt grants access to a simpler mechanism to obtain and manage certificates for a larger set of users who are seeking a way to secure their websites. It also promotes a big step forward in terms of security and privacy. This trend has already begun within ThoughtWorks, and many of our projects now have certificates verified by Let's Encrypt. | Although more sites every day are implementing HTTPS to help protect their own users and improve the integrity of the web as a whole, there are many more sites to go. In addition, we see more and more people using HTTPS within their enterprises, to provide additional security guarantees. One of the main blockers to wider adoption has been the process of getting a certificate in the first place. Aside from the cost, the process itself is far from slick. Let’s Encrypt, a new Certificate Authority, aims to solve all this. First, it provides certificates for free. Second, and arguably more important, it also provides an extremely easy-to-use command-line API, making it easy to fully automate the process of issuing, upgrading and installing certificates. We think that Let’s Encrypt, in beta at the moment, has the chance to be revolutionary in terms of helping more of the web get on to HTTPS, and at the same time showing what good, automatable tools for the security-conscious should look like.","blip_selector":"let-s-encrypt","name":"Let's Encrypt","display_name":"Let's Encrypt","url":"/radar/tools/let-s-encrypt","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1056,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"We are excited that the Redux paradigm has made its way to Swift-land in the form of ReSwift. We’ve found real benefits in the simplicity and readability of codebases once state and state changes are managed in a central place and common idiom. This also helps with building \"offline first\" applications.","blip_selector":"reswift","name":"ReSwift","display_name":"ReSwift","url":"/radar/languages-and-frameworks/reswift","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":833,"quadrant":"techniques","volume_date":"2016-11","description":"We’ve decided to bring consumer-driven contract testing back from the archive for this edition even though we had allowed it to fade in the past. The concept isn’t new, but with the mainstream acceptance of microservices, we need to remind people that consumer-driven contracts are an essential part of a mature microservice testing portfolio, enabling independent service deployments. But in addition, we want to point out that consumer-driven contract testing is a technique and an attitude that requires no special tool to implement. We love frameworks like Pact because they make proper contract tests easier to implement in certain contexts. But we have noticed a tendency for teams to focus on the framework rather than on the general practice. Writing Pact tests is not a guarantee that you are creating consumer-driven contracts; likewise, in many situations you should be creating good consumer-driven contracts even where no pre-built testing tool exists. | When two independently developed services are collaborating, changes to the supplier’s API can cause failures for all its consumers. Consuming services usually cannot test against live suppliers since such tests are slow and brittle, so it’s best to use Test Doubles, leading to the danger that the test doubles get out of sync with the real supplier service. Consumer teams can protect themselves from these failures by using integration contract tests – tests that compare actual service responses with test values. While such contract tests are valuable, they are even more useful when consuming services provide these tests to the supplier, who can then run all their consumers’ contract tests to determine if their changes are likely to cause problems – adopting consumer-driven contracts. Such consumer-driven contract tests are an essential part of a mature microservice testing portfolio.","blip_selector":"consumer-driven-contract-testing","name":"Consumer-driven contract testing","display_name":"Consumer-driven contract testing","url":"/radar/techniques/consumer-driven-contract-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":760,"quadrant":"tools","volume_date":"2016-11","description":"Machine images have become a staple of modern deployment pipelines, and there are a number of tools and techniques to create the images. Because of its comprehensive feature set and the positive experiences we've had with it, we recommend Packer over the alternatives. We also recommend against trying to write custom scripts to do what Packer does out of the box. | We featured 'Machine image as a build artifact' in the last Radar, as an excellent way to implement fast spin-up, immutable servers. The thing holding this technique back was the difficulty in building images, especially when targeting more than one platform. Packer solves this, using your configuration management tool of choice to create images for a number of platforms including AWS, Rackspace, DigitalOcean and even Docker and Vagrant, although we have found the VMWare support to be problematic. | We featured 'Machine image as a build artifact' in the last Radar, as an excellent way to implement fast spin-up, immutable servers. The thing holding this technique back was the difficulty in building images, especially when targeting more than one platform. Packer solves this, using your configuration management tool of choice to create images for a number of platforms including AWS, Rackspace, DigitalOcean and even Docker and Vagrant, although we have found the VMWare support to be problematic.","blip_selector":"packer","name":"Packer","display_name":"Packer","url":"/radar/tools/packer","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1051,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"Rapidoid is a collection of web framework modules, including a fast low-level HTTP server implemented from scratch on top of Java NIO. Clever usage of off-heap input/output buffers, object pools and thread-local data structures provide Rapidoid an edge over other NIO-based servers like Netty. Being a fairly new project, Rapidoid has yet to implement a few features like built-in cache and SSL support; we suggest you check the roadmap for updates.","blip_selector":"rapidoid","name":"Rapidoid","display_name":"Rapidoid","url":"/radar/languages-and-frameworks/rapidoid","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":736,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"We have been prompted to reconsider Elm because of the rapid adoption of Redux framework. Elm—the original inspiration for Redux—offers the view componentization and reactiveness of React.js along with the predictable state of Redux in a compiled, strongly typed functional language. Elm is written in Haskell and has a Haskell-like syntax but compiles down to HTML, CSS and JavaScript for the browser. JavaScript programmers rushing to embrace React.js and Redux might want to also consider Elm as a type-safe alternative for some applications. | Elm is a functional programming language that is used to build web based user interfaces in a functional reactive style. Elm is strongly statically typed and built on the Haskell platform. Elm has a Haskell-like syntax but compiles down to HTML, CSS and JavaScript. While still in its very early days, individuals and teams interested in exploring highly interactive web-based GUIs should look into this interesting little language.","blip_selector":"elm","name":"Elm","display_name":"Elm","url":"/radar/languages-and-frameworks/elm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":714,"quadrant":"platforms","volume_date":"2016-11","description":"We remain excited about Docker as it evolves from a tool to a complex platform of technologies. Development teams love Docker, as the Docker image format makes it easier to achieve parity between development and production, making for reliable deployments. It is a natural fit in a microservices-style application as a packaging mechanism for self-contained services. On the operational front, Docker support in monitoring tools (Sensu, Prometheus, cAdvisor, etc.), orchestration tools (Kubernetes, Marathon, etc.) and deployment-automation tools reflect the growing maturity of the platform and its readiness for production use. A word of caution, though: There is a prevalent view of Docker and Linux containers in general as being \"lightweight virtualization,\" but we would not recommend using Docker as a secure process-isolation mechanism, though we are paying attention to the introduction of user namespaces and seccomp profiles in version 1.10 in this regard. | Since our last radar, Docker has hit 1.0 and has been declared production ready by the authors. During this same period we have seen an explosion of tools based on Docker. We now have PAAS solutions in the form of Deis, cluster management in CoreOS and Kubernetes, and Microsoft, Google, AWS and a host of smaller players are offering or will shortly offer Docker hosting. Microsoft is even looking to support Docker in their next version of Windows Server. Aside from all this change, Docker is being used in anger now by many people, for dev & test and for production loads. We fully expect to see a large pace of change in the Docker ecosystem over the next year, and strongly suggest you take a look at what Docker could offer your own organisation. | Docker continues to gain momentum, and is seeing use on projects although mostly in non-production environments. Docker provides a set of tools to efficiently package and distribute executable machine images, which can then be launched as lightweight containers. A considerable community is growing around the tool. Notable is CoreOS which is an operating system based on ChromeOS built for deploying Docker containers across a cluster with tools for deployment, service discovery and configuration. | The Docker open-source project has generated a great deal of interest within Thoughtworks, and is growing in momentum and maturity. Docker allows applications to be packaged and published as portable lightweight containers that run identically on a laptop or a production cluster. It provides tooling for the creation and management of application containers, and a run-time environment based on LXC (LinuX Containers).","blip_selector":"docker","name":"Docker","display_name":"Docker","url":"/radar/platforms/docker","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":954,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"In the Android application-development world, Robolectric is a unit-testing framework that has been used by multiple teams within our technical community. It offers the best option among those available for writing real unit tests that extend or interact directly with Android components and support JUnit tests. We caution, though, that because it is an implementation of the Android SDK, there might be device-specific issues for some tests that pass in Robolectric. To manually mock all the Android dependencies, ensuring only test of the system-in-test will require a lot of complex code, and this framework addresses this effectively.","blip_selector":"robolectric","name":"Robolectric","display_name":"Robolectric","url":"/radar/languages-and-frameworks/robolectric","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1029,"quadrant":"tools","volume_date":"2016-11","description":"Testing the visual portion of iOS applications can be painful, slow and flakey, which is why we’re happy to include FBSnapshotTestcase in our toolkit. It automates taking, storing and diff-ing snapshots of UI components so you can keep your interfaces pixel-perfect. Since it runs as a unit test (in the simulator), it is faster and more reliable than functional-testing approaches.","blip_selector":"fbsnapshottestcase","name":"FBSnapshotTestcase","display_name":"FBSnapshotTestcase","url":"/radar/tools/fbsnapshottestcase","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":732,"quadrant":"tools","volume_date":"2016-11","description":"We have moved Consul, the service-discovery tool supporting both DNS- and HTTP-based discovery mechanisms, into Adopt. It goes beyond other discovery tools by providing customizable health checks for registered services, ensuring that unhealthy instances are marked accordingly. More tools have emerged to work with Consul to make it even more powerful. Consul Template enables configuration files to be populated with information from Consul, making things like client-side load balancing using mod_proxy much easier. In the world of Docker, registrator can automatically register Docker containers as they appear with Consul with extremely little effort, making it much easier to manage container-based setups. You should still think long and hard about whether you need a tool like this or whether something simpler will do, but if you decide you need service discovery, you won't go far wrong with Consul. | We continue to be impressed with Consul, a service discovery tool supporting both DNS and HTTP-based discovery mechanisms. It goes beyond other discovery tools by providing customizable health-checks for registered services, ensuring that unhealthy instances are marked accordingly. More tools have emerged to work with Consul to make it even more powerful. Consul Template enables configuration files to be populated with information from Consul, making things like client-side load balancing using mod_proxy much easier. In the world of Docker, registrator can automatically register docker containers as they appear with Consul with extremely little effort, making it much easier to manage container-based setups. | Consul makes it simple for services to register themselves and discover other services via DNS or HTTP. It scales automatically, with service look up locally or across data centers. Consul also provides a flexible key/value store for dynamic configuration, with notification of configuration changes. The internal gossip protocol used by Consul is powered by the Serf library, leveraging and building upon the membership and failure detection features.","blip_selector":"consul","name":"Consul","display_name":"Consul","url":"/radar/tools/consul","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":960,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"With the increased need for Android-based applications, Dagger offers a fully static, compile-time dependency-injection framework. Dagger's strictly generated implementation and nonreliance on reflection-based solutions addresses many of the performance and development issues, thereby making it suitable for Android development. With Dagger, there is full traceability with easy debugging because the entire call stack for provision and creation is made available.","blip_selector":"dagger","name":"Dagger","display_name":"Dagger","url":"/radar/languages-and-frameworks/dagger","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":958,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"Aurelia is considered the next-generation JavaScript client framework and was written using a modern version of JavaScript: ECMAScript 2016. Aurelia was created by Rob Eisenberg, the creator of Durandal. He left the Angular 2.0 core team to dedicate his time to this project. The great thing about Aurelia is that it's highly modular, contains simple small libraries and is designed to be customized easily. Aurelia follows the pattern of convention over configuration, which enables easier production and consumption of modules, but there are no strong conventions that you have to adhere to. Aurelia has a large community, and in the project website you can learn more by using the tutorials.","blip_selector":"aurelia","name":"Aurelia","display_name":"Aurelia","url":"/radar/languages-and-frameworks/aurelia","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":959,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"Many iOS developers are using JSPatch to dynamically patch their apps. When a JSPatch-enabled app runs, it loads a chunk of JavaScript (potentially via an insecure HTTP connection) and then bridges to the main Objective-C application code to change behavior, fix bugs, and so on. While convenient, we think monkey-patching live apps is a bad idea and should be avoided. When doing any amount of incremental patching, it's very important that your testing process matches what end users will experience, in order to properly validate functionality. An alternative approach is to use React Native for the app and AppHub and CodePush to push small updates and new features.","blip_selector":"jspatch","name":"JSPatch","display_name":"JSPatch","url":"/radar/languages-and-frameworks/jspatch","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":1024,"quadrant":"tools","volume_date":"2016-11","description":"Babel.js has become the default compiler for writing next-generation JavaScript. Its ecosystem is really taking off, thanks to its restructured plugin system. It allows developers to write ES6 (and even ES7) code that runs in the browser or in the server without sacrificing backward compatibility for older browsers, and with very little configuration. It has first-class support for different build-and-test systems, which makes integration with any current workflow simple. It is a great piece of software that has become the main driver of ES6 (and ES7) adoption and innovation.","blip_selector":"babel","name":"Babel","display_name":"Babel","url":"/radar/tools/babel","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":963,"quadrant":"tools","volume_date":"2016-11","description":"In a world full of libraries and tools that simplify the life of many software developers, deficiencies in their security have become visible and have increased the vulnerability surface in the applications that use them. OWASP Dependency-Check automatically identifies potential security problems in the code, checking if there are any known publicly disclosed vulnerabilities, then using methods to constantly update the database of public vulnerabilities. Dependency-Check has some interfaces and plugins to automate this verification in Java and .NET (which we have used successfully) as well as Ruby, Node.js and Python.","blip_selector":"owasp-dependency-check","name":"OWASP Dependency-Check","display_name":"OWASP Dependency-Check","url":"/radar/tools/owasp-dependency-check","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1065,"quadrant":"tools","volume_date":"2016-11","description":"Pair programming is an essential technique for us, and—given that we’re seeing more and more teams whose members are distributed across multiple locations—we have experimented with a number of tools to support remote pairing. We certainly liked ScreenHero but are concerned about its future. For teams that don’t rely on a graphical IDE, using tmate for pairing has turned out to be a great solution. tmate is a fork of the popular tmux tool, and compared to tmux for remote pairing, the setup is much easier. Compared to graphical screen-sharing solutions, the bandwidth and resource requirements are modest, and it obviously never suffers from blurry screens. Teams can also set up their own server, thus retaining full control of the privacy and integrity of the solution.","blip_selector":"tmate","name":"tmate","display_name":"tmate","url":"/radar/tools/tmate","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":775,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"A lot of work has gone into Spring Boot to reduce complexity and dependencies, which largely alleviates our previous reservations. If you live in a Spring ecosystem and are moving to microservices, Spring Boot is now the obvious choice. For those not in Springland, Dropwizard is also worthy of serious consideration. | Spring Boot allows easy setup of standalone Spring-based applications. It's ideal for pulling up new microservices and easy to deploy. It also makes data access less of a pain, thanks to the JPA mappings through Spring Data. We like that Spring Boot simplifies Java services built with Spring but have learned to be cautious of the many dependencies. Spring still lurks just beneath the surface. If you’re writing microservices with Java, you might also consider using Dropwizard or a microframework like Spark to get the benefits of Spring Boot without the enormous weight of Spring. | Spring Boot allows easy set up of standalone Spring-based applications. It's ideal for pulling up new microservices and easy to deploy. It also makes data access less of a pain due to the hibernate mappings with much less boilerplate code. We like that Spring Boot simplifies Java services built with Spring, but have learned to be cautious of the many dependencies. Spring still lurks just beneath the surface. | Spring boot allows easy set up of standalone Spring-based applications. It's ideal for pulling up new microservices and easy to deploy. It also makes data access less of a pain due to the hibernate mappings with much less boilerplate code. | Spring Boot allows easy set up of standalone Spring-based applications. It's ideal for pulling up new microservices and easy to deploy. It also makes data access less of a pain due to the hibernate mappings with much less boilerplate code.","blip_selector":"spring-boot","name":"Spring Boot","display_name":"Spring Boot","url":"/radar/languages-and-frameworks/spring-boot","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":947,"quadrant":"techniques","volume_date":"2016-11","description":"The use of bug bounties continues to grow in popularity for many organizations, including enterprises and notable government bodies. A bug-bounty program encourages participants to identify potentially damaging vulnerabilities in return for reward or recognition. Companies like HackerOne and Bugcrowd offer services to help organizations manage this process more easily, and we're seeing these services gather adoption. | More and more organizations are starting to use bug bounties to encourage reporting of what are often security-related bugs, and in general help improve the quality of their software. To support these programs, companies like HackerOne and BugCrowd can help organizations manage this process more easily. We have limited experience with these offerings ourselves, but we like the idea of encouraging people to help come forward and highlight what can often be damaging vulnerabilities in an open and transparent way. It's worth noting that there might be some legal issues with encouraging users to find vulnerabilities in your software, so please do check that out first.","blip_selector":"bug-bounties","name":"Bug bounties","display_name":"Bug bounties","url":"/radar/techniques/bug-bounties","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":911,"quadrant":"tools","volume_date":"2016-11","description":"At the top of the testing pyramid for Android application development, our teams are increasingly using Espresso as the functional-testing tool. Its small-core API hides the messy implementation details and helps in writing concise tests, with faster and reliable test execution. With Espresso, you can run automated UI tests simulating user interactions within a single target app on both emulators and real devices across different Android versions. | Espresso is an Android functional-testing tool. Its small-core API hides the messy implementation details and helps in writing concise tests, with faster and reliable test execution.","blip_selector":"espresso","name":"Espresso","display_name":"Espresso","url":"/radar/tools/espresso","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":971,"quadrant":"tools","volume_date":"2016-11","description":"We had our collective minds blown by a little JavaScript command-line refactoring tool called Grasp. Providing a rich set of selectors and operating against the abstract syntax tree, it is leagues ahead of fiddling with sed and grep. A useful addition to the toolkit in our ongoing quest to treat JavaScript as a first-class language.","blip_selector":"grasp","name":"Grasp","display_name":"Grasp","url":"/radar/tools/grasp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1009,"quadrant":"platforms","volume_date":"2016-11","description":"Tarantool is an open source NoSQL solution that combines database and cache into one entity and provides APIs for writing application logic in Lua. Both in-memory and disk-based engines are supported, and users can create multiple indexes (HASH, TREE, RTREE, BITSET) based on their use cases. The data itself is stored in MessagePack format and uses the same protocol to communicate between clients and server. Tarantool supports write-ahead logs, transactions and asynchronous master-master replication. We are happy with the architectural decision of embracing single-writer policy and cooperative multitasking to handle concurrent connections.","blip_selector":"tarantool","name":"Tarantool","display_name":"Tarantool","url":"/radar/platforms/tarantool","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":995,"quadrant":"techniques","volume_date":"2016-11","description":"With the continued rise to domination of the container model led by Docker adoption, we think it's worth calling attention to the continued rapid development in the Unikernel space. Unikernels are single-purpose library operating systems that can be compiled down from high-level languages to run directly on the hypervisors used by commodity cloud platforms. They promise a number of advantages over containers, not least their superfast startup time and very small attack surface area. Many are still at the research-project phase—Drawbridge from Microsoft Research, MirageOS and HaLVM amongst others—but we think the ideas are very interesting and combine nicely with the technique of serverless architecture.","blip_selector":"unikernels","name":"Unikernels","display_name":"Unikernels","url":"/radar/techniques/unikernels","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":961,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"Butterknife is a field and method binding view-injection library. It allows the injection of arbitrary objects, views and listeners, thereby ensuring cleaner code with reduced glue code for Android development. With Butterknife, multiple views can be grouped into a list or array with common actions applied to the views simultaneously, without heavy reliance on XML configurations. Our project teams have used this library and benefited from its simplicity and ease of use.","blip_selector":"butterknife","name":"Butterknife","display_name":"Butterknife","url":"/radar/languages-and-frameworks/butterknife","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":996,"quadrant":"techniques","volume_date":"2016-11","description":"As more development teams incorporate security earlier in the development life cycle, figuring out requirements to limit security risks can seem like a daunting task. Few people have the extensive technical knowledge needed to identify all the risks that an application might face, and teams might struggle just trying to decide where to begin. Relying on frameworks such as OWASP's ASVS (Application Security Verification Standard) can help make this easier. Although somewhat lengthy, it contains a thorough list of requirements categorized by functions such as authentication, access control, and error handling and logging, which can be reviewed as needed. It is also helpful as a resource for testers when it comes time to verify software.","blip_selector":"owasp-asvs","name":"OWASP ASVS","display_name":"OWASP ASVS","url":"/radar/techniques/owasp-asvs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":827,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"In the avalanche of front-end JavaScript frameworks, React.js stands out due to its design around a reactive data flow. Allowing only one-way data binding greatly simplifies the rendering logic and avoids many of the issues that commonly plague applications written with other frameworks. We're seeing the benefits of React.js on a growing number of projects, large and small, while at the same time we continue to be concerned about the state and the future of other popular frameworks like AngularJS. This has led to React.js becoming our default choice for JavaScript frameworks. | One benefit of the ongoing avalanche of front-end JavaScript frameworks is that occasionally a new idea crops up that makes us think. React.js is a UI/view framework in which JavaScript functions generate HTML in a reactive data flow. It differs significantly from frameworks like AngularJS in that it only allows one-way data bindings, greatly simplifying the rendering logic. We have seen several smaller projects achieve success with React.js, and developers are drawn to its clean, composable approach to componentization. | One benefit to the ongoing avalanche of front-end JavaScript frameworks is that occasionally, a new idea crops up that makes us think. React.js is a UI/View framework in which JavaScript functions generate HTML in a reactive data flow. We have seen several smaller projects achieve success with React.js and developers are drawn to its clean, composeable approach to componentization. | One benefit to the ongoing avalanche of front-end JavaScript frameworks is that occasionally, a new idea crops up that makes us think.  React.js is a UI/View framework in which JavaScript functions generate HTML in a reactive data flow.  Although we are wary of mixing code and markup, it results in UI components that are nicely encapsulated and composable.  React.js is getting a lot of developer attention and will benefit from more tools and examples becoming available.","blip_selector":"react-js","name":"React.js","display_name":"React.js","url":"/radar/languages-and-frameworks/react-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":789,"quadrant":"techniques","volume_date":"2016-11","description":"A Data Lake is an immutable data store of largely unprocessed \"raw\" data, acting as a source for data analytics. While the technique can clearly be misused, we have used it successfully at clients, hence motivating its move to trial. We continue to recommend other approaches for operational collaborations, limiting the use of the data lake to reporting, analytics and feeding data into data marts. | A Data Lake is an immutable data store of largely unprocessed 'raw' data, acting as a source for data analytics. Whereas the more familiar Data Warehouse filters and processes the data before storing it, the lake just captures the raw data, leaving it to the users of that data to carry out the particular analysis that they need. Examples include HDFS or HBase within a Hadoop, Spark or Storm processing framework. Usually only a small group of data scientists work on the raw data, developing streams of processed data into lakeshore data marts for most users to query. A Data Lake should only be used for analytics and reporting. For collaboration between operational systems we prefer using services designed for that purpose. | An Enterprise Data Lake is an immutable data store of largely un-processed “raw” data, acting as a source for other processing streams but also made directly available to a significant number of internal, technical consumers using some efficient processing engine. Examples include HDFS or HBase within a Hadoop, Spark or Storm processing framework. We can contrast this with a typical system that collects raw data into some highly restricted space that is only made available to these consumers as the end result of a highly controlled ETL process.\n\nEmbracing the concept of the data lake is about eliminating bottlenecks due to lack of ETL developer staffing or excessive up front data model design. It is about empowering developers to create their own data processing pipelines in an agile fashion when they need it and how they need it—within reasonable limits—and so has much in common with another model that we think highly of, the DevOps model.","blip_selector":"data-lake","name":"Data Lake","display_name":"Data Lake","url":"/radar/techniques/data-lake","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":997,"quadrant":"techniques","volume_date":"2016-11","description":"We are finding Content Security Policies to be a helpful addition to our security toolkit when dealing with websites that pull assets from mixed contexts. The policy defines a set of rules about where assets can come from (and whether to allow inline script tags). The browser then refuses to load or execute JavaScript, CSS or images that violate those rules. When used in conjunction with good practices, such as output encoding, it provides good mitigation for XSS attacks. Interestingly, the optional endpoint for posting JSON reports of violations is how Twitter discovered that ISPs were injecting HTML or JavaScript into their pages.","blip_selector":"content-security-policies","name":"Content Security Policies","display_name":"Content Security Policies","url":"/radar/techniques/content-security-policies","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":836,"quadrant":"techniques","volume_date":"2016-11","description":"We see continued adoption and success of reactive architectures , with reactive language extensions and reactive frameworks being very popular (we added several such blips in this edition of the Radar). User interfaces, in particular, benefit greatly from a reactive style of programming. Our caveats last time still hold true: Architectures based on asynchronous message passing introduce complexity and make the overall system harder to understand—it's no longer possible to simply read the program code and understand what the system does. We recommend assessing the performance and scalability needs of your system before committing to this architectural style. | The techniques of functional reactive programming have steadily gained in popularity over recent years, and we’re seeing increased interest in extending this concept to distributed systems architectures. Partly inspired by “The Reactive Manifesto”, these reactive architectures are based on a one-way, asynchronous flow of immutable events through a network of independent processes (perhaps implemented as microservices). In the right setting, these systems are scalable and resilient and decrease the coupling between individual processing units. However, architectures based entirely on asynchronous message passing introduce complexity and often rely on proprietary frameworks. We recommend assessing the performance and scalability needs of your system before committing to this as a default architectural style.","blip_selector":"reactive-architectures","name":"Reactive architectures","display_name":"Reactive architectures","url":"/radar/techniques/reactive-architectures","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":850,"quadrant":"platforms","volume_date":"2016-11","description":"Many organizations are now looking closely at new data architectures that capture information as immutable sequences of events at scale. Apache Kafka continues to build momentum as an open source messaging framework that provides a solution for publishing ordered event feeds to large numbers of independent, lightweight consumers. Configuring Kafka is nontrivial, but our teams are reporting positive experiences with the framework. | Many recent developments in enterprise software revolve around asynchronous sequences of immutable event sequences as opposed to synchronous, point-to-point requests that modify state. Apache   Kafka is an open-source messaging framework that supports this architectural style by publishing ordered message feeds to many independent, lightweight consumers. Kafka’s unique design allows the number of consumers to scale while maintaining strong ordering on the messages.","blip_selector":"apache-kafka","name":"Apache Kafka","display_name":"Apache Kafka","url":"/radar/platforms/apache-kafka","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":1016,"quadrant":"platforms","volume_date":"2016-11","description":"In the HoloLens, Microsoft has delivered the first truly usable AR headset. Not only is it a beautiful piece of industrial design and an eminently comfortable device to wear, but it also clearly demonstrates the promise of AR for the enterprise via its gorgeous optics and deep Windows 10 integration. We expect HoloLens to be the first AR platform on which we deliver substantial application functionality to our clients in the near term, and we look forward to its evolution as it gains broader traction.","blip_selector":"hololens","name":"HoloLens","display_name":"HoloLens","url":"/radar/platforms/hololens","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":977,"quadrant":"platforms","volume_date":"2016-11","description":"The PaaS space has seen a lot of movement since we last mentioned Cloud Foundry in 2012. While there are various distributions of the open source core, we have been impressed by the offering and ecosystem assembled as Pivotal Cloud Foundry. While we expect continued convergence between the unstructured approach (Docker, Mesos, Kubernetes, etc.) and the more structured and opinionated buildpack style offered by Cloud Foundry and others, we see real benefit for organizations that are willing to accept the constraints and rate of evolution to adopt a PaaS. Of particular interest is the speed of development that comes from the simplification and standardization of the interaction between development teams and platform operations.","blip_selector":"pivotal-cloud-foundry","name":"Pivotal Cloud Foundry","display_name":"Pivotal Cloud Foundry","url":"/radar/platforms/pivotal-cloud-foundry","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":1027,"quadrant":"tools","volume_date":"2016-11","description":"More projects are emitting and consuming information formatted as JSON. Writing tests in Java for JSON can be laborious. JSONassert is a small library to help write smaller tests dealing with JSON by simplifying assertions and providing better error messages.","blip_selector":"jsonassert","name":"JSONassert","display_name":"JSONassert","url":"/radar/tools/jsonassert","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":957,"quadrant":"languages-and-frameworks","volume_date":"2016-11","description":"We've been enjoying how Recharts integrates D3 charts into React.js in a clean and declarative manner.","blip_selector":"recharts","name":"Recharts","display_name":"Recharts","url":"/radar/languages-and-frameworks/recharts","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":976,"quadrant":"tools","volume_date":"2016-11","description":"We know we're in perilous territory here, since we build a competing tool, but we feel we have to address a persistent problem. Continuous Integration tools like CruiseControl and Jenkins are valuable for software development, but as your build process gets more complex it requires something beyond just Continuous Integration: It requires a deployment pipeline. We frequently see people trying to use Jenkins as a Deployment Pipeline with the aid of plugins, but our experience is that these quickly become a tangle. Jenkins 2.0 introduces \"Pipeline as Code\" but continues to model pipelines using plugins and fails to change the core Jenkins product to model pipelines directly. In our experience, tools that are built around a first-class representation of deployment pipelines are much more suitable, and this is what drove us to replace CruiseControl with GoCD. Today we see several products that embrace deployment pipelines, including ConcourseCI, LambdaCD, Spinnaker, Drone and GoCD.","blip_selector":"jenkins-as-a-deployment-pipeline","name":"Jenkins as a deployment pipeline","display_name":"Jenkins as a deployment pipeline","url":"/radar/tools/jenkins-as-a-deployment-pipeline","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":882,"quadrant":"techniques","volume_date":"2016-04","description":"Many services, especially legacy services, are written with the assumption that any request will occur only once. Networks being what they are, this can be difficult to arrange. An idempotency filter is a simple component that merely checks for duplicate requests and ensures that they are sent to the supplier service only once. Such a filter should do only this one task and be used as a decorator over existing service calls.","blip_selector":"idempotency-filter","name":"Idempotency filter","display_name":"Idempotency filter","url":"/radar/techniques/idempotency-filter","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":838,"quadrant":"techniques","volume_date":"2016-04","description":"We’ve long been championing the idea that thinking of software development as a project - something budgeted and delivered during a limited time slot - doesn’t fit the needs of the modern business. Important software efforts need to be an ongoing product that supports and rethinks the business process it is supporting. Such efforts are not complete until the business process, and its software, cease to be useful. Our observation of this products over projects approach, both with our own projects and outside, makes us determine that it is the approach to use for all but exceptional cases. | Most software development efforts are done using the mental model of a project, something that is planned, executed, and delivered within defined time-slots. Agile development challenged much of this model, replacing an up-front determination of requirements with an on-going discovery process that runs concurrently with development. Lean startup techniques, such as A/B testing of observed requirements, further erode this mindset. We consider that most software efforts should follow the lead of Lean Enterprise and consider themselves to be building products that support underlying business processes. Such products do not have a final delivery, rather an on-going process of exploring how best to support and optimize that business process which continues as long as the business is worthwhile. For these reasons we encourage organizations to think in terms of products rather than projects.","blip_selector":"products-over-projects","name":"Products over projects","display_name":"Products over projects","url":"/radar/techniques/products-over-projects","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":936,"quadrant":"languages-and-frameworks","volume_date":"2016-04","description":"We have a number of reservations about the use of HTML5 WebSockets. By allowing the server to initiate actions on the browser, WebSockets departs from the connectionless, request/response model that underpins the World Wide Web today. Security is another big risk with WebSockets. For example, the standard does not impose any cross-origin request policy. However, we do recognize that in certain monitoring or alerting applications, WebSockets can be very useful. If you need to build a .NET WebSockets server, SignalR conveniently implements much of the additional code you need for a robust production application. This includes some recommended security practices such as validating connection tokens and activating SSL when encryption is needed. Although ThoughtWorks teams have been very happy with SignalR, there are still fundamental issues with WebSockets that you should consider before diving in.","blip_selector":"signalr","name":"SignalR","display_name":"SignalR","url":"/radar/languages-and-frameworks/signalr","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":998,"quadrant":"platforms","volume_date":"2016-04","description":"While many deployments of smart devices rely on Wi-Fi connectivity, we have been seeing success with Bluetooth Mesh networks that don't necessitate a hub or gateway. With better energy usage than Wi-Fi and better smartphone adoption than ZigBee, Bluetooth LE deployed as a self-healing mesh provides interesting new approaches for connecting local device-area networks. We are still waiting for the formal approach to emerge from the Bluetooth SIG but have already had successful deployments. We particularly like the lack of infrastructure required to stand up a decentralized network but still retain the option to \"progressively enhance\" the system with the addition of a gateway and cloud services.","blip_selector":"bluetooth-mesh","name":"Bluetooth Mesh","display_name":"Bluetooth Mesh","url":"/radar/platforms/bluetooth-mesh","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":908,"quadrant":"tools","volume_date":"2016-04","description":"Despite the shrinking usage of Internet Explorer, for many products the IE user base is not an insignificant share of the market, and browser compatibility needs to be tested. This is particularly troublesome if you prefer the joys of a UNIX-based system for development. To aid in this dilemma, ievms provides a utility script that brings together Windows-distributed VM images and VirtualBox to automate the setup and testability of various IE versions, from 6 up to Edge.","blip_selector":"ievms","name":"ievms","display_name":"ievms","url":"/radar/tools/ievms","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":941,"quadrant":"languages-and-frameworks","volume_date":"2016-04","description":"OkHttp is a Java HTTP connection library from Square that provides a fluent interface for creating connections, as well as support for the faster HTTP/2 protocol. Even when using HTTP/1.1, OkHttp can provide performance improvements via connection pooling and transparent gzip compression. Supporting both blocking synchronous and nonblocking asynchronous calls, it can also be used as a drop-in replacement for the widely used Apache HttpClient.","blip_selector":"okhttp","name":"OkHttp","display_name":"OkHttp","url":"/radar/languages-and-frameworks/okhttp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":965,"quadrant":"languages-and-frameworks","volume_date":"2016-04","description":"Networking and decoding in iOS applications have been a difficult endeavor for many years. There have been many libraries and attempts to solve this ongoing problem. It looks as though Alamofire is the most robust and developer-friendly library to handle decoding JSON. It was written by the same creator as its Objective-C counterpart (AFNetworking), which was used at great length during the Objective-C days.","blip_selector":"alamofire","name":"Alamofire","display_name":"Alamofire","url":"/radar/languages-and-frameworks/alamofire","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":779,"quadrant":"platforms","volume_date":"2016-04","description":"Password security is still a hotly debated topic with the UK government advocating technical controls that let users remember simpler passwords and Edward Snowden’s password advice being described as only \"borderline secure\". Passwords are generally one of the weakest links in the security chain, so we recommend employing two-factor authentication , which can significantly improve security. Time-based One-Time Password ( TOTP ) is the standard algorithm in this space, with straightforward server-side implementations and free smartphone authenticator apps from Google and Microsoft. | Passwords continue to be a poor mechanism for authenticating users and we’ve recently seen companies such as Yahoo! move to a “no passwords” solution—a one-time code is texted to your phone whenever you need to log in from a new browser. If you are still using passwords we recommend employing two-factor authentication which can significantly improve security. Time-based One-Time Password ( TOTP ) is the standard algorithm in this space, with free smartphone authenticator apps from Google and Microsoft. | Two-factor authentication significantly improves security over simple password-based systems. RFC 6238 -- Time-based One-Time Password Algorithm -- is a standard for two-factor authentication. 'Standard' authenticator apps from Google and Microsoft provide tokens to smartphone users, and there are a number of other client and server implementations readily available. With providers such as Google, Facebook, Dropbox and Evernote using TOTP, there really is no excuse to continue using simple password-based authentication where stronger security would be appropriate. | Two-factor authentication significantly improves security over simple password-based systems. RFC 6238 -- Time-based One-Time Password Algorithm -- is a standard for two-factor authentication. \"Standard\" authenticator apps from Google and Microsoft provide tokens to smartphone users, and there are a number of other client and server implementations readily available. With providers such as Google, Facebook, Dropbox and Evernote using TOTP, there really is no excuse to continue using simple password-based authentication where stronger security would be appropriate.","blip_selector":"totp-two-factor-authentication","name":"TOTP Two-Factor Authentication","display_name":"TOTP Two-Factor Authentication","url":"/radar/platforms/totp-two-factor-authentication","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":830,"quadrant":"languages-and-frameworks","volume_date":"2016-04","description":"Swift is now our default choice for development in the Apple ecosystem. With the release of Swift 2, the language approached a level of maturity that provides the stability and performance required for most projects. A good number of libraries that support iOS development—SwiftyJSON, Quick, etc.—are now migrated over to Swift, which is where the rest of the applications should follow. Swift has now been open sourced, and we are seeing a community of developers dedicated to continuously improving development in iOS. | A year after its public debut, Swift is now our default choice for development in the Apple ecosystem. With the recent release of Swift 2, the language approaches a level of maturity that provides the stability and performance required for most projects. Swift still has issues, especially around tool support, refactoring and testing. However, we feel that these are not substantial enough to warrant avoiding Swift. At the same time, porting large, existing Objective-C codebases is unlikely to pay off. The announcement that Swift will become open source software is a further positive sign. We are hopeful that this will not just be another dumping of internally developed code into a public repository, because Apple has clearly stated that community contributions are encouraged and will be accepted. | With some real-world experience under our belt, Swift still shows a lot of promise. Some of the problems, like long compile times, are being addressed. However, continued language changes cause extra development effort and make building older versions of your own software burdensome. Testing and refactoring also remain painful. On balance, though, you should still consider Swift when starting new development projects for the Apple ecosystem. | Swift, Apple’s new development language, contains many improvements over the perennial Objective-C, including emphasis on functional programming and modern syntax. In most ways, this is an upgrade if you are coding on the Apple platform.","blip_selector":"swift","name":"Swift","display_name":"Swift","url":"/radar/languages-and-frameworks/swift","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":886,"quadrant":"techniques","volume_date":"2016-04","description":"Valuable services support many variations in clients, such as mobile versus web and different forms of web interface. It’s tempting to design a single back-end API to support all clients with a reusable API. But client needs vary, as do constraints such as bandwidth for mobile devices versus the desire for lots of data on fast web connections. Consequently it’s often best to define different back-end services for each kind of front-end client. These back ends should be developed by teams aligned with each front end to ensure that each back end properly meets the needs of its client.","blip_selector":"bff-backend-for-frontends","name":"BFF - Backend for frontends","display_name":"BFF - Backend for frontends","url":"/radar/techniques/bff-backend-for-frontends","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":948,"quadrant":"techniques","volume_date":"2016-04","description":"We firmly believe that long-lived version-control branches harm valuable engineering practices such as continuous integration, and this belief underlies our dislike for Gitflow. We love the flexibility of Git underneath but abhor tools that encourage bad engineering practices. Very short-lived branches hurt less, but most teams we see using Gitflow feel empowered to abuse its branch-heavy workflow, which encourages late integration (therefore discouraging true continuous integration).","blip_selector":"gitflow","name":"Gitflow","display_name":"Gitflow","url":"/radar/techniques/gitflow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":923,"quadrant":"platforms","volume_date":"2016-04","description":"Ceph is a storage platform that can be used as object storage, as block storage, and as a file system, typically running on a cluster of commodity servers. With its first major release having been in July 2012, Ceph is certainly not a new product. We do want to highlight it on this Technology Radar as an important building block for private clouds. It is particularly attractive because its RADOS Gateway component can expose the object store through a RESTful interface that is compatible with Amazon S3 and the OpenStack Swift APIs.","blip_selector":"ceph","name":"Ceph","display_name":"Ceph","url":"/radar/platforms/ceph","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":990,"quadrant":"platforms","volume_date":"2016-04","description":"As Moore's Law predicts, we continue to increase the capacity of computer systems and reduce their cost, and so new processing techniques become possible that only a few years ago would have seemed out of reach. One of these techniques is the in-memory database: Instead of using slow disks or relatively slow SSDs to store data, we can keep it in memory for high performance. One such in-memory database, MemSQL, is making waves because it is horizontally scalable across a cluster and provides a familiar SQL-based query language. MemSQL also connects to Spark for analytics against real-time data, rather than stale data in a warehouse.","blip_selector":"memsql","name":"MemSQL","display_name":"MemSQL","url":"/radar/platforms/memsql","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":992,"quadrant":"platforms","volume_date":"2016-04","description":"Deflect is an open source service protecting NGOs, activist and independent media companies from DDoS attacks. Similar to a commercial CDN, it uses distributed reverse-proxy caching and also hides your server IP addresses and blocks public access to admin URLs. Particular effort is put in to combat the botnets typically used for extrajudicial censoring of independent voices.","blip_selector":"deflect","name":"Deflect","display_name":"Deflect","url":"/radar/platforms/deflect","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":918,"quadrant":"techniques","volume_date":"2016-04","description":"We see many teams run into trouble because they have chosen complex tools, frameworks or architectures because they \"might need to scale\". Companies such as Twitter and Netflix need to be able to support extreme loads and so need these architectures, but they also have extremely skilled development teams able to handle the complexity. Most situations do not require these kinds of engineering feats; teams should keep their  web scale envy in check in favor of simpler solutions that still get the job done.","blip_selector":"high-performance-envy-web-scale-envy","name":"High performance envy/web scale envy","display_name":"High performance envy/web scale envy","url":"/radar/techniques/high-performance-envy-web-scale-envy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":902,"quadrant":"tools","volume_date":"2016-04","description":"Dependency management in iOS and OS X projects used to be either completely manual or completely automatic as part of using CocoaPods. With Carthage , a new middle ground has become available. Carthage manages dependencies - it downloads, builds and updates frameworks - but it leaves the integration of the frameworks into the build of the project to the project. This is in contrast to CocoaPods, which basically takes over the project structure and build setup. It should be noted that Carthage can only deal with dynamic frameworks, which are not available on iOS 7 and below.","blip_selector":"carthage","name":"Carthage","display_name":"Carthage","url":"/radar/tools/carthage","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":955,"quadrant":"languages-and-frameworks","volume_date":"2016-04","description":"The intersection between IoT devices and the JavaScript ecosystem offers interesting possibilities. Cylon.js is a JavaScript library for building interfaces for robotics and the Internet of Things, which has excited our technical community. It offers support for 50+ platform devices, as well as general-purpose input/output support with a shared set of drivers provided by the cylon-gpio module. Control of the devices is then possible through a web browser interface.","blip_selector":"cylon-js","name":"Cylon.js","display_name":"Cylon.js","url":"/radar/languages-and-frameworks/cylon-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":904,"quadrant":"tools","volume_date":"2016-04","description":"We've had rave reviews from a number of ThoughtWorks teams using Browsersync. As the number of devices we deliver web applications to grows, so does the amount of effort that must be devoted to testing across these different devices. Browsersync is a free, open source tool that can dramatically reduce this effort by synchronizing manual browser testing across multiple mobile or desktop browsers. Providing both a CLI and a UI option, the tool is build-pipeline friendly and automates repetitive tasks such as form filling.","blip_selector":"browsersync","name":"Browsersync","display_name":"Browsersync","url":"/radar/tools/browsersync","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":869,"quadrant":"platforms","volume_date":"2016-04","description":"Predictive analytics are used in more and more products, often directly in end user-facing functionality. H2O is an interesting open source package (with a startup behind it) that makes predictive analytics accessible to development teams, offering straightforward use of a wide variety of analytics, great performance and easy integration on JVM-based platforms. At the same time it integrates with the data scientists’ favorite tools, R and Python, as well as Hadoop and Spark. | Predictive analytics are used in more and more products, often directly in end user-facing functionality. H2O is an interesting open source package (with a startup behind it) that makes predictive analytics accessible to development teams, offering straightforward use of a wide variety of analytics, great performance and easy integration on JVM-based platforms. At the same time it integrates with the data scientists’ favorite tools, R and Python, as well as Hadoop and Spark. | Predictive analytics are used in more and more products, often directly in end-user facing functionality. H2O is an interesting new open source package (with a startup behind it) that makes predictive analytics accessible to project teams due to its easy-to-use user interface. At the same time it integrates with the data scientists’ favourite tools, R and Python, as well as Hadoop and Spark. It offers great performance and, in our experience, easy integration at runtime, especially on JVM-based platforms.","blip_selector":"h2o","name":"H2O","display_name":"H2O","url":"/radar/platforms/h2o","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":837,"quadrant":"techniques","volume_date":"2016-04","description":"Flux is an application architecture introduced by Facebook. Usually mentioned in conjunction with React.js , Flux is based on a one-way flow of data up through the rendering pipeline. Flux embraces the modern web landscape of client-side JavaScript applications in a way that avoids the venerable MV* clichés. ThoughtWorks teams are now starting to gain some experience with this architectural style and find that it meshes well with service orientation and solves some of the problems inherent in two-way data binding. | Flux is an application architecture that Facebook has adopted for its web application development. Usually mentioned in conjunction with react.js , Flux is based on a one-way flow of data up through the rendering pipeline triggered by users or other external events modifying data stores. It’s been a while since we’ve seen any alternatives to the venerable model-view-* architectures and Flux embraces the modern web landscape of client-side JavaScript applications talking to multiple back-end services.","blip_selector":"flux","name":"Flux","display_name":"Flux","url":"/radar/techniques/flux","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":912,"quadrant":"tools","volume_date":"2016-04","description":"Gauge is a lightweight cross-platform test-automation tool. Specifications are written in free-form Markdown so test cases can be written in the business language, as opposed to using the more common but restrictive \"given-when-then\" format. Language and IDE support are implemented as plugins to a single core implementation, allowing testers to use the same IDEs as the rest of the team, with powerful capabilities such as autocompletion and refactoring. This tool, open sourced by ThoughtWorks, also supports parallel execution out of the box for all supported platforms. | Gauge is a lightweight cross-platform test-automation tool. Specifications are written in free-form Markdown, so test cases can be written in the business language and can be incorporated into any existing documentation format. Supported languages are implemented as plugins to a single core implementation, which ensures consistency across language implementations. This tool, open sourced by ThoughtWorks, also supports parallel execution out of the box for all supported platforms.","blip_selector":"gauge","name":"Gauge","display_name":"Gauge","url":"/radar/tools/gauge","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":988,"quadrant":"platforms","volume_date":"2016-04","description":"For people who want the benefit of cloud-based collaboration tools but don't want to inadvertently \"become the product\" of a major cloud provider, Sandstorm provides an interesting open source alternative with the potential for self-hosting. Of particular interest is the isolation approach, whereby containerization is applied per document rather than per application, and syscall whitelisting is added to further secure the sandbox.","blip_selector":"sandstorm","name":"Sandstorm","display_name":"Sandstorm","url":"/radar/platforms/sandstorm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":872,"quadrant":"platforms","volume_date":"2016-04","description":"The rise of containers, phoenix servers and continuous delivery has seen a move away from the usual approach to deploying web applications. Traditionally we have built an artifact and then installed that artifact into an application server. The result was long feedback loops for changes, increased build times and the not insignificant overhead of managing these application servers in production. Many of them are a pain to automate too. Most teams we work with favor bundling an embedded http server within your web application. There are plenty of options available: Jetty, SimpleWeb, Webbit and Owin Self-Host amongst others. Easier automation, easier deployment and a reduction in the amount of infrastructure you have to manage lead us to recommend embedded servers over application servers for future projects.","blip_selector":"application-servers","name":"Application Servers","display_name":"Application Servers","url":"/radar/platforms/application-servers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":940,"quadrant":"tools","volume_date":"2016-04","description":"Sleepy Puppy is a delayed cross-site scripting (XSS) payload-management framework recently open sourced by Netflix. It enables you to test vulnerabilities for XSS past the target application when the perpetrator intends to attack a secondary underlying system. With XSS being one of the OWASP Top10, we see this framework assisting with automated security checks for several applications. It simplifies the capturing, managing and tracking of XSS propagation over long periods of time, with customizable payloads. Sleepy puppy also exposes an API that can be integrated with vulnerability tools like ZAP, for automated security checks.","blip_selector":"sleepy-puppy","name":"Sleepy Puppy","display_name":"Sleepy Puppy","url":"/radar/tools/sleepy-puppy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":913,"quadrant":"tools","volume_date":"2016-04","description":"With a growing landscape of services providing RESTful APIs, it is becoming increasingly important to document them. We have previously mentioned Swagger, and in this Technology Radar we’d like to highlight the RESTful API modeling language (RAML). Our teams feel that in comparison to Swagger it is more lightweight and moves the focus from adding documentation to existing APIs to designing APIs.","blip_selector":"raml","name":"RAML","display_name":"RAML","url":"/radar/tools/raml","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":888,"quadrant":"techniques","volume_date":"2016-04","description":"In monitoring, the common approach is to conceive of erroneous conditions and set alerts when these appear. But it’s often difficult to enumerate the myriad failure modes in a software system. Monitoring of invariants is a complementary approach to setting expected normal ranges, often by examining historical behavior, and alerting whenever a system goes outside those bounds.","blip_selector":"monitoring-of-invariants","name":"Monitoring of invariants","display_name":"Monitoring of invariants","url":"/radar/techniques/monitoring-of-invariants","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":924,"quadrant":"platforms","volume_date":"2016-04","description":"The Elastic Container Service (ECS) is AWS’ entry into the multihost Docker space. Although there is a lot of competition in this area, there aren’t many off-premises managed solutions out there yet. Although ECS seems like a good first step, we are worried that it is overly complicated at the moment and lacks a good abstraction layer. If you want to run Docker on AWS, though, this tool should certainly be high on your list. Just don’t expect it to be easy to get started with.","blip_selector":"aws-ecs","name":"AWS ECS","display_name":"AWS ECS","url":"/radar/platforms/aws-ecs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":929,"quadrant":"platforms","volume_date":"2016-04","description":"Presto is an open source distributed SQL query engine designed and optimized for running interactive analytics workloads. Presto's massively parallel processing architecture - combined with advanced code-generation techniques and in-memory processing pipelines - makes it highly scalable. It supports a large subset of ANSI SQL including complex queries, joins, aggregations and window functions. Presto comes with support for a wide range of data sources including Hive , Cassandra , MySQL and PostgreSQL , thereby unifying the interactive analytics interface across data stores of an organization. Applications can connect to Presto using its JDBC interface.","blip_selector":"presto","name":"Presto","display_name":"Presto","url":"/radar/platforms/presto","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":897,"quadrant":"tools","volume_date":"2016-04","description":"Attackers continue to use automated software to crawl public GitHub repositories to find AWS credentials and spin up EC2 instances to mine Bitcoins or for other nefarious purposes. Although adoption of tools like git-crypt and Blackbox to safely store secrets such as passwords and access tokens in code repositories is increasing, it is still all too common that secrets are stored unprotected. It is also not uncommon to see project secrets accidentally checked in to developers' personal repositories. Gitrob can help minimize the damage of exposing secrets. It scans an organization's GitHub repositories, flagging all files that might contain sensitive information that shouldn't have been pushed to the repository. The current release of the tool has some limitations: It can only be used to scan public GitHub organizations and their members, it doesn't inspect the contents of files, it doesn't review the entire commit history, and it fully scans all repositories each time it is run. Despite these limitations, it can be a helpful reactive tool to help alert teams before it is too late. It should be considered a complementary approach to a proactive tool such as Talisman. | Safely storing secrets such as passwords and access tokens in code repositories is now supported by a growing number of tools - for example, git-crypt and Blackbox, which we mentioned in the previous Technology Radar. Despite the availability of these tools, it is still, unfortunately, all too common that secrets are stored unprotected. In fact, it is so common that automated exploit software is used to find AWS credentials and spin up EC2 instances to mine Bitcoins, leaving the attacker with the Bitcoins and the account owner with the bill. Gitrob takes a similar approach and scans an organization’s GitHub repositories, flagging all files that might contain sensitive information that shouldn’t have been pushed to the repository. This is obviously a reactive approach. Gitrob can only alert teams when it is (almost) too late. For this reason, Gitrob can only ever be a complementary tool, to minimize damage.","blip_selector":"gitrob","name":"Gitrob","display_name":"Gitrob","url":"/radar/tools/gitrob","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":887,"quadrant":"techniques","volume_date":"2016-04","description":"Modern web pages tend to contain a plethora of JavaScript widgets and snippets coming from a variety of third-party sources. This can have a negative impact on both security and performance. While we are still waiting for fuller JavaScript isolation with web components, our teams have benefited from using HTML5 iFrames for sandboxing untrusted JavaScript.","blip_selector":"iframes-for-sandboxing","name":"iFrames for sandboxing","display_name":"iFrames for sandboxing","url":"/radar/techniques/iframes-for-sandboxing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":893,"quadrant":"techniques","volume_date":"2016-04","description":"Implementing Continuous Delivery continues to be a challenge for many organizations, and it remains important to highlight useful techniques such as decoupling deployment from release. We recommend strictly using the term Deployment when referring to the act of deploying a change to application components or infrastructure. The term Release should be used when a feature change is released to end users, with a business impact. Using techniques such as feature toggles and dark launches, we can deploy changes to production systems more frequently without releasing features. More-frequent deployments reduce the risk associated with change, while business stakeholders retain control over when features are released to end users.","blip_selector":"decoupling-deployment-from-release","name":"Decoupling deployment from release","display_name":"Decoupling deployment from release","url":"/radar/techniques/decoupling-deployment-from-release","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":900,"quadrant":"techniques","volume_date":"2016-04","description":"The JavaScript world has a plethora of dependency and package-management tools, all of which rely on the Node Package Manager (NPM). Teams are starting to see these extra tools as redundant and are recommending that if you can use solely NPM for package and dependency management, you should. The simplification of using NPM for all the things helps reduce some of the churn in the JavaScript tools space.","blip_selector":"npm-for-all-the-things","name":"NPM for all the things","display_name":"NPM for all the things","url":"/radar/techniques/npm-for-all-the-things","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":892,"quadrant":"languages-and-frameworks","volume_date":"2016-04","description":"Over many years, JavaScript has grown to become probably the most widely used programming language in the world. Nevertheless, the language itself has a few problems that many have attempted to address by using libraries or even by implementing their own languages that run on top of JavaScript (of which we’ve mentioned both CoffeeScript and ClojureScript before). ES6 (aka ECMAScript 6 or ECMAScript 2015), the new version of JavaScript, addresses many of the concerns of the older versions currently in use. Although browser support is scarce, support from mature transpilers like Babel allows you to write ES6 and have it supported in older browsers. For new projects, we strongly suggest starting with ES6 from day one.","blip_selector":"es6","name":"ES6","display_name":"ES6","url":"/radar/languages-and-frameworks/es6","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":834,"quadrant":"techniques","volume_date":"2016-04","description":"The time taken to provision and update environments continues to be a significant bottleneck on many software projects. Phoenix Environments can help with this delay by extending the idea of Phoenix Servers to cover entire environments. We feel this is such a valuable and time-saving technique that you should consider trialing this approach. Using automation, we can create whole environments - including network configuration, load balancing and firewall ports - for example by using CloudFormation in AWS. We can then prove that the process works by tearing the environments down and recreating them from scratch on a regular basis. Phoenix Environments can support provisioning new environments for testing, development, UAT and disaster recovery. As with Phoenix Servers, this pattern is not always applicable, and we need to think carefully about things like state and dependencies. Treating the whole environment as a blue/green deployment can be one approach when environment reconfiguration needs to be done. | The idea of phoenix servers is now well established and has brought many benefits when applied to the right kinds of problems, but what about the environment we deploy these servers into? The concept of Phoenix Environments can help. We can use automation to allow us to create whole environments, including network configuration, load balancing and firewall ports, for example by using CloudFormation in AWS. We can then prove that the process works, by tearing the environments down and recreating them from scratch on a regular basis. Phoenix Environments can support provisioning new environments for testing, development, UAT and so on. They can also simplify the provision of a disaster recovery environment. As with Phoenix Servers this pattern is not always applicable and we need to think about carefully about things like state and dependencies. Treating the whole environment as a green/blue deployment can be one approach when environment reconfiguration needs to be done.","blip_selector":"phoenix-environments","name":"Phoenix Environments","display_name":"Phoenix Environments","url":"/radar/techniques/phoenix-environments","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":987,"quadrant":"platforms","volume_date":"2016-04","description":"Our growing ranks of hardware hackers have been excited by the ESP8266 Wi-Fi microcontroller. Rather than a specific technology innovation, it is the combination of low price point and small form factor that has sparked an inflection point in people's thinking about what is now feasible to achieve with custom hardware devices. Its main characteristics are: Wi-Fi capabilities (it can act as station, access point or a combination of both), low power, open hardware, Arduino SDK programmability, Lua programmability, huge community support and low cost compared with other IoT modules.","blip_selector":"esp8266","name":"ESP8266","display_name":"ESP8266","url":"/radar/platforms/esp8266","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":914,"quadrant":"techniques","volume_date":"2016-04","description":"Many organizations want to leverage distributed or offshore development but have security concerns with their code and other intellectual property sitting outside their control. The result is often to use high-latency remote-desktop solutions for development, adhering to an organization’s security controls but crippling developer productivity. An alternative is to use a Hosted IDE delivered to a browser via VPN. The IDE, code and build environment are hosted within the organization's private cloud, easing security concerns, and the developer experience is significantly improved. Tools in this space include Orion and Che from the Eclipse Foundation, Cloud9 and Code Envy.","blip_selector":"hosted-ide-s","name":"Hosted IDE's","display_name":"Hosted IDE's","url":"/radar/techniques/hosted-ide-s","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":953,"quadrant":"languages-and-frameworks","volume_date":"2016-04","description":"Our teams are moving away from JQuery or raw XHR for remote JavaScript calls and instead are using the new Fetch API and the Fetch polyfill in particular. The semantics remain similar but have cleaner support for promises and CORS support. We are seeing this as the new de-facto approach.","blip_selector":"fetch","name":"Fetch","display_name":"Fetch","url":"/radar/languages-and-frameworks/fetch","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":896,"quadrant":"tools","volume_date":"2016-04","description":"Although SysDig isn’t the newest tool on the Technology Radar, we’re still surprised by how many people haven’t heard of it. A pluggable open source CLI for Linux system troubleshooting, SysDig has some pretty powerful features. One of the key things we like is the ability to generate a system trace on a machine that is experiencing difficulties, which you can then interrogate afterward to find out what was happening. SysDig also contains support for working with containers, something that makes a previously useful tool even more powerful.","blip_selector":"sysdig","name":"SysDig","display_name":"SysDig","url":"/radar/tools/sysdig","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":881,"quadrant":"techniques","volume_date":"2016-04","description":"Traditionally, QA roles have focused on assessing the quality of a software product in a pre-production environment. With the rise of Continuous Delivery, the QA role is shifting to include analyzing software product quality in production. This involves monitoring of the production systems, coming up with alert conditions to detect urgent errors, determining ongoing quality issues and figuring out what measurements you can use in the production environment to make this work. While there is a danger that some organizations will go too far and neglect pre-production QA, our experience shows that QA in production is a valuable tool for organizations that have already progressed to a reasonable degree of Continuous Delivery.","blip_selector":"qa-in-production","name":"QA in production","display_name":"QA in production","url":"/radar/techniques/qa-in-production","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":895,"quadrant":"tools","volume_date":"2016-04","description":"Git can be confusing. Really confusing. And even when it’s used in a simple trunk-based development process, there are still enough nuances to how it works that people can tie themselves in knots from time to time. When this happens, having an understanding of how Git works under the hood is very useful, and GitUp is a Mac-based tool that gives you exactly that. GitUp provides a graphical representation of what is happening as you type normal Git commands into the terminal. You can learn the various Git commands while also understanding what each one does as you use it. GitUp is a useful tool for both people new to Git and those with more Git experience.","blip_selector":"gitup","name":"GitUp","display_name":"GitUp","url":"/radar/tools/gitup","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":903,"quadrant":"languages-and-frameworks","volume_date":"2016-04","description":"Most templating frameworks like Mustache or FreeMarker mix code with markup in a single file to implement complex, dynamic content. Enlive is a Clojure-based templating framework that completely separates programming language from HTML markup and employs CSS selectors to find and replace parts of the document. Enlive demonstrates the power of functional programming to implement complex behavior through a series of simple, composable functions acting on a common abstraction. Our teams working in Clojure have found it to be a very useful and straightforward tool.","blip_selector":"enlive","name":"Enlive","display_name":"Enlive","url":"/radar/languages-and-frameworks/enlive","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":899,"quadrant":"tools","volume_date":"2015-11","description":"Previously, we recommended boot2docker as a way of easily running Docker on your local Windows or OS X machine. Docker Toolbox now replaces boot2docker, adding some tooling as well. Now included is Kitematic for managing your containers, as well as Docker Compose for managing multi-Docker setup (Mac only). It can be used safely as a drop-in replacement for boot2docker, and it will even handle the upgrade for you.","blip_selector":"docker-toolbox","name":"Docker Toolbox","display_name":"Docker Toolbox","url":"/radar/tools/docker-toolbox","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":692,"quadrant":"platforms","volume_date":"2015-11","description":"The SPDY protocol was developed by Google from 2009 as an experiment to provide an alternative protocol to address performance shortcomings of HTTP/1.1. The new HTTP/2 standard protocol includes many of the key performance features of SPDY, and Google has announced it will drop browser SPDY support in early 2016. If your application requires the features of SPDY, we recommend you look instead at HTTP/2. | SPDY is an open networking protocol for low-latency transport of web content proposed for HTTP 2.0 that has seen a rise in modern browser support. SPDY reduces page load time by prioritizing the transfer of subresources so that only one connection is required per client. Transport layer security is used in SPDY implementations with the transmission headers gzip or deflate compressed instead of human-readable text as in HTTP. It is great for high-latency environments.","blip_selector":"spdy","name":"SPDY","display_name":"SPDY","url":"/radar/platforms/spdy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":877,"quadrant":"techniques","volume_date":"2015-11","description":"Offline first web applications provide the ability to design web applications for offline access by employing caching and updating mechanisms. The implementation requires a flag in the DOM to check whether the accessing device is offline or online, accessing local storage when offline, and synchronising data when online. All the major browsers now support an offline mode, which bootstraps the process of downloading and caching the resources such as HTML, CSS, JavaScript, images and other kinds of resources. There are some tools which help simplify offline first implementation such as Hoodie, and CouchDB also offers ability to work with a locally deployed application on a local data storage.","blip_selector":"offline-first-web-applications","name":"Offline first web applications","display_name":"Offline first web applications","url":"/radar/techniques/offline-first-web-applications","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":853,"quadrant":"tools","volume_date":"2015-11","description":"Many many wonderful stories of failure in our industry are caused by the assumption that networks are always reliable and servers respond quickly and correctly all the time. Hamms is an interesting open-source tool which acts as a badly behaved HTTP server, triggering a number of failures including connection failures or slow and/or malformed responses. It may be useful for testing that your software handles failures gracefully.","blip_selector":"hamms","name":"Hamms","display_name":"Hamms","url":"/radar/tools/hamms","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":784,"quadrant":"techniques","volume_date":"2015-11","description":"'Just In Time Design' is an important and useful concept for visual design that the NoPSD movement attempts to capture. You don't need to design the whole application or every UI element up front. Design things as you need them with as lightweight tools as you can use. We have seen a corresponding growth in simpler tools with faster learning curves, such as Sketch, as well as an increasing return to pen-and-paper (especially when paired with an existing robust digital style guide). Because of the limitations of flat mock-ups when you’re designing for screens, creating prototypes of varying fidelity with tools such as Invision, FramerJS and Origami - or simply HTML/CSS and a bit of JavaScript - has also become increasingly common and valuable for communicating design intent. | NoPSD is a movement to integrate design activities into the iterative feedback cycles required to build great software. The name aims to dislodge the PSD as the final canonical design artifact rather than taking a dig at the Adobe software. Instead of signing off on a pixel-perfect design specification at the start of a project, teams are urged to embrace Continuous Design: embedding designers into delivery teams, using lo-fi techniques for prototyping, and collaborating to refine the design in the target UI technology (normally HTML and CSS). This approach speeds responding to real user feedback, allows testing designs across multiple devices and form-factors, and embraces the dynamic nature of both digital products and the product creation process.","blip_selector":"nopsd","name":"NoPSD","display_name":"NoPSD","url":"/radar/techniques/nopsd","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":809,"quadrant":"tools","volume_date":"2015-11","description":"As single page applications and offline-first become more viable and widespread there is a growing need to persist data in the web browser. Local Storage is very easy to use and well supported by the web browsers. For more complex use cases, there is IndexedDB. While it can be a good solution we recommend to only use it when absolutely necessary, due to the increase in complexity and a somewhat clumsy API. We have also had positive experience with the localForage framework that provides an abstraction layer over the various persistence solutions.","blip_selector":"indexeddb","name":"IndexedDB","display_name":"IndexedDB","url":"/radar/tools/indexeddb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":944,"quadrant":"languages-and-frameworks","volume_date":"2015-11","description":"Following many other programming languages, one of the language geeks’ absolute favourites, Haskell, is now also available on the JVM in the form of Frege. This brings a purely functional programming language onto the platform, allowing for easy interoperability with other JVM languages and libraries.","blip_selector":"frege","name":"Frege","display_name":"Frege","url":"/radar/languages-and-frameworks/frege","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":860,"quadrant":"tools","volume_date":"2015-11","description":"REST-assured is a Java domain specific language for testing and validating RESTful services. It simplifies the testing of REST based services built on top of HTTP Builder. REST-assured supports the different REST requests and can be used to validate and verify the responses from the APIs. It also provides a JSON schema validation and can thus be used to verify that the endpoints are returning the right types of expected data.","blip_selector":"rest-assured","name":"REST-assured","display_name":"REST-assured","url":"/radar/tools/rest-assured","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":939,"quadrant":"languages-and-frameworks","volume_date":"2015-11","description":"Traveling Ruby makes it possible to distribute portable, ready-to-run, platform-agnostic Ruby binaries without the need to install an interpreter, packages or additional gems. It decouples running Ruby applications from the development environment they run in.","blip_selector":"traveling-ruby","name":"Traveling Ruby","display_name":"Traveling Ruby","url":"/radar/languages-and-frameworks/traveling-ruby","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":935,"quadrant":"languages-and-frameworks","volume_date":"2015-11","description":"While we still have some reservations about CQRS as a general pattern, the approach can work very well in specific places. In those specific situations, however, a lot of work is left to the developer to properly execute CQRS. Axon is a framework that can help with this on the JVM, and we’ve used it with some success. Although it certainly can’t be considered a perfect solution right now, it continues to evolve and may make much more sense than trying to write everything from scratch.","blip_selector":"axon","name":"Axon","display_name":"Axon","url":"/radar/languages-and-frameworks/axon","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":934,"quadrant":"languages-and-frameworks","volume_date":"2015-11","description":"HyperResource is a Ruby framework for building a RESTful API client. The framework accepts JSON in HAL format and dynamically generates a model object complete with hypermedia links. Although the framework is still in its infancy, we like that it embraces Richardson level 3 REST for better service discoverability and self-documenting protocols.","blip_selector":"hyperresource","name":"HyperResource","display_name":"HyperResource","url":"/radar/languages-and-frameworks/hyperresource","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":921,"quadrant":"platforms","volume_date":"2015-11","description":"In contrast to modern cloud and container solutions based on Linux, even Windows Server Core is large and unwieldy. Microsoft is reacting and has provided the first previews of Nano Server, a further-stripped-down version of Windows Server that drops the GUI stack, 32-bit Win32 support, local logins and remote desktop support, resulting in an on-disk size of about 400MB. The early previews are difficult to work with, and the final solution will be restricted to using the CoreCLR, but for companies that are interested in running .NET-based solutions, Nano Server is definitely worth a look at this stage.","blip_selector":"microsoft-nano-server","name":"Microsoft Nano Server","display_name":"Microsoft Nano Server","url":"/radar/platforms/microsoft-nano-server","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":864,"quadrant":"platforms","volume_date":"2015-11","description":"Particle (formally Spark.io) is a full stack solution for cloud connected devices. Particle Photon is a microcontroller with wifi module. Particle Electron is a variant that connects to a cellular network. Particle OS adds REST API to the devices. This simplifies the entry to IoT and building your own connected devices. | Spark is a full stack solution for cloud connected devices. Spark Photon is a microcontroller with wifi module. Spark Electron is a variant that connects to a cellular network. Spark OS adds REST API to the devices. This simplifies the entry to IoT and building your own connected devices.","blip_selector":"particle-photon-particle-electron","name":"Particle Photon/Particle Electron","display_name":"Particle Photon/Particle Electron","url":"/radar/platforms/particle-photon-particle-electron","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":810,"quadrant":"tools","volume_date":"2015-11","description":"Postman is a Chrome extension that acts as a REST client in your browser, allowing you to create requests and inspect responses.  It is a useful tool when developing an API or implementing a client to call an existing API. Postman supports OAuth1 and OAuth2 tokens allowing addition of them to requests where necessary. The response is available as a prettified JSON or XML. With Postman you are able to retrieve a history of requests performed to quickly edit and test the API response to different data. It offers a suite of extensions that allow you to use it as a full-blown test runner too, although we discourage the record and replay style of testing it promotes. | Postman is a Chrome extension that acts as a REST client in your browser, allowing you to create requests and inspect responses.  It is a useful tool when developing an API or implementing a client to call an existing API. It offers a suite of extensions that allow you to use it as a full-blown test runner too, although we discourage the record and replay style of testing it promotes.","blip_selector":"postman","name":"Postman","display_name":"Postman","url":"/radar/tools/postman","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":846,"quadrant":"tools","volume_date":"2015-11","description":"Security Monkey is another tool in Netflix’s Simian Army, which is a suite of tools designed to ensure that systems are being built in a resilient fashion. As well as providing a (configurable) assessment of any potential security vulnerabilities in your AWS setup, it can also be used to monitor changes on an ongoing basis, alerting different groups as required. It does overlap in some ways with AWS’ own Trusted Advisor Report and CloudTrail service, as it was developed prior to both these services being made generally available, but its capabilities do go beyond these offerings. If either of those services don’t quite meet your requirements, Security Monkey is worth a look.","blip_selector":"security-monkey","name":"Security Monkey","display_name":"Security Monkey","url":"/radar/tools/security-monkey","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":884,"quadrant":"techniques","volume_date":"2015-11","description":"One of the many innovative uses of Docker that we’ve seen on our projects is a technique to manage build-time dependencies. In the past, it was common to run build agents on an OS, augmented with dependencies needed for the target build. But with Docker it is possible to run the compilation step in an isolated environment complete with dependencies without contaminating the build agent. This technique of using Docker for builds  has proven particularly useful for compiling Golang binaries, and the golang-builder container is available for this very purpose.","blip_selector":"docker-for-builds","name":"Docker for builds","display_name":"Docker for builds","url":"/radar/techniques/docker-for-builds","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":943,"quadrant":"languages-and-frameworks","volume_date":"2015-11","description":"Building systems using microservices requires us to think more deeply about failure isolation and testing. *TLA+ *is a formal specification language that can be useful in both these scenarios. For failure isolation, TLA+ can be used to identify invariants in your system that can be monitored directly. An invariant can be the ratio of number of requests to one service to the number of requests to a second service, for example. Any change in this ratio would lead to an alert. TLA+ is also being used to identify subtle design flaws in distributed systems. Amazon, for example, used model-checking based on a formal specification written in TLA+ to identify subtle bugs in Dynamo DB before it was released to the public. For most systems, the investment required to create the formal specification and then perform model checking is probably too great; however, for critical systems - complex ones, or those with many users - we think it’s very valuable to have another tool in our toolbox.","blip_selector":"tla","name":"TLA+","display_name":"TLA+","url":"/radar/languages-and-frameworks/tla","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":938,"quadrant":"languages-and-frameworks","volume_date":"2015-11","description":"Material UI provides reusable components for use in React applications that implement Google's Material Design language. Filling a similar space to Twitter Bootstrap, it gets you up and running quickly but doesn't have the same drawbacks as your application grows. Elemental UI is worth investigating as an alternative.","blip_selector":"material-ui","name":"Material UI","display_name":"Material UI","url":"/radar/languages-and-frameworks/material-ui","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":856,"quadrant":"tools","volume_date":"2015-11","description":"Several of our teams working on .NET projects have recommended Polly as being useful in building microservice-based systems. It encourages the fluent expression of transient exception-handling policies and the Circuit Breaker pattern, including policies such as Retry, Retry Forever and Wait and Retry. Similar libraries already exist in other languages (Hystrix for Java for example), and Polly is a welcome addition from the .NET community. Integrating well with Polly is Brighter. Brighter is another small open source .Net library that provides scaffolding to implement command invocation. Combining the two libraries provides useful circuit-breaking functionality especially in the context of the Ports and Adapters pattern and CQRS. Although they can be used separately, in the wild our teams find they work well together. | Several of our teams working on .Net projects have recommended Polly as being useful when building microservice based systems. It encourages the fluent expression of transient exception handling policies and the circuit breaker pattern including policies such as Retry, Retry Forever and Wait and Retry. Libraries already exist in other languages, Hystrix for Java for example, and Polly is a welcome addition from the .Net community.","blip_selector":"polly","name":"Polly","display_name":"Polly","url":"/radar/tools/polly","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":832,"quadrant":"techniques","volume_date":"2015-11","description":"When we need a diagram that describes the current infrastructure or physical architecture we usually take to our favorite technical diagramming tool. If you are using the cloud or virtualization technologies this no longer makes sense, we can use the provided APIs to interrogate the actual infrastructure and generate a live, automated infrastructure diagram using simple tools like GraphViz or by outputting SVG.","blip_selector":"generated-infrastructure-diagrams","name":"Generated infrastructure diagrams","display_name":"Generated infrastructure diagrams","url":"/radar/techniques/generated-infrastructure-diagrams","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":907,"quadrant":"tools","volume_date":"2015-11","description":"Pageify is a Ruby library for building page objects for UI automation tests, focusing on faster test execution and code readability. It offers simple APIs to dynamically define, operate and assert on the page objects, allowing readable code even when handling elements with complex hierarchies in the DOM. It bundles integration for WebDriver and Capybara.","blip_selector":"pageify","name":"Pageify","display_name":"Pageify","url":"/radar/tools/pageify","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":865,"quadrant":"platforms","volume_date":"2015-11","description":"Heroku, with its 12-factor application model, has changed the way we think about building, deploying, and hosting web applications.  Deis encapsulates the Heroku PaaS model in an open-source framework that deploys onto Docker containers hosted anywhere.  Deis is still evolving, but for applications that fit the 12-factor model it has the potential to greatly simplify deployment and hosting in the environment of your choice.  Deis is yet another example of the rich ecosystem of platforms and tools emerging around Docker.","blip_selector":"deis","name":"Deis","display_name":"Deis","url":"/radar/platforms/deis","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":734,"quadrant":"techniques","volume_date":"2015-11","description":"In the last radar issue we advised against creating a separate DevOps team , as DevOps is about creating a culture of shared responsibility in delivery teams. We recommend embedding operations skills into delivery teams to reduce friction and deliver better outcomes. However where there is a need for significant investment in tooling and automation, we do see a role for a Delivery Engineering team.  Rather than being a helpdesk, these teams build tooling and enable teams to deploy, monitor, and maintain their own production environments. | Some companies with good intentions create a separate DevOps team, which misconstrues the definition of DevOps. Rather than a role, DevOps is a cultural movement encouraging collaboration between operations specialists and developers. Rather than create yet another silo and suffer the consequences of Conway's Law, we advise you to embed these skills into teams, improving feedback loops and communication pathways by removing friction.","blip_selector":"separate-devops-team","name":"Separate DevOps team","display_name":"Separate DevOps team","url":"/radar/techniques/separate-devops-team","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":891,"quadrant":"platforms","volume_date":"2015-11","description":"Fastly, one of a number of CDNs on the market, has a large and growing following on ThoughtWorks projects and is used by many web-scale household names, such as GitHub and Twitter. Its feature set, speed and price point combine to make it a very attractive option when you’re looking for an edge caching solution. We have also seen significant cost savings on projects that move to this platform from another CDN. If you are in the market for a CDN, you could do worse than investigate this one.","blip_selector":"fastly","name":"Fastly","display_name":"Fastly","url":"/radar/platforms/fastly","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":873,"quadrant":"platforms","volume_date":"2015-11","description":"For a while now the Hadoop community has been trying to bring low-latency, interactive SQL capability to the Hadoop platform (better known as SQL-on-Hadoop). This has led to a few open source systems such as Cloudera Impala, Apache Drill, Facebook’s Presto etc being developed actively through 2014. We think the SQL-on-Hadoop trend signals an important shift as it changes Hadoop's proposition from being a batch oriented technology that was complementary to databases into something that could compete with them.  Cloudera Impala was one of the first SQL-on-Hadoop platforms. It is a distributed, massively-parallel, C++ based query engine. The core component of this platform is the Impala daemon that coordinates the execution of the SQL query across one or more nodes of the Impala cluster. Impala is designed to read data from files stored on HDFS in all popular file formats. It leverages Hive's metadata catalog, in order to share databases and tables between the two database platforms. Impala comes with a shell as well as JDBC and ODBC drivers for applications to use.","blip_selector":"cloudera-impala","name":"Cloudera Impala","display_name":"Cloudera Impala","url":"/radar/platforms/cloudera-impala","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":773,"quadrant":"platforms","volume_date":"2015-11","description":"Apache Spark has been steadily gaining ground as a fast and general engine for large-scale data processing. The engine is written in Scala and is well suited for applications that reuse a working set of data across multiple parallel operations. It’s designed to work as a standalone cluster or as part of Hadoop YARN cluster. It can access data from sources such as HDFS, Cassandra, S3 etc. Spark also offers many higher level operators in order to ease the development of data parallel applications. As a generic data processing platform it has enabled development of many higher level tools such as interactive SQL (Spark SQL), real time streaming (Spark Streaming), machine learning library (MLib), R-on-Spark etc. | For iterative processing such as machine learning and interactive analysis, Hadoop map-reduce does not work very well because of its batch-oriented nature. Spark is a fast and general engine for large-scale data processing. It aims to extend map-reduce for iterative algorithms and interactive low latency data mining. It also ships with a machine learning library. | For iterative processing such as machine learning and interactive analysis, Hadoop map-reduce does not work very well because of its batch-oriented nature. Spark is a fast and general engine for large-scale data processing. It aims to extend map-reduce for iterative algorithms and interactive low latency data mining. It also ships with a machine learning library.","blip_selector":"apache-spark","name":"Apache Spark","display_name":"Apache Spark","url":"/radar/platforms/apache-spark","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":787,"quadrant":"techniques","volume_date":"2015-11","description":"Immutable data structures are becoming more popular, with functional languages such as Clojure and Scala providing immutability by default. Immutability allows code to be more easily written, read and reasoned about. Using an accumulate-only data store can confer some of these benefits in the database layer, as well as make audit and historical querying simple. Implementation options vary, from specific accumulative data stores such as Datomic to simply using an “append-don’t-update” approach with a traditional database. Accumulate-only is a design strategy whereby data is removed via retraction rather than update; append-only is an implementation technique. | Immutable data structures are becoming more popular with functional languages such as Clojure providing immutability by default. Immutability allows code to be more easily written, read, and reasoned about. Using an append-only data store can confer some of these benefits in the database layer, as well as making audit and historical querying simple. Implementation options vary, from specific append-only data stores such as Datomic to simply using an “append-don’t-update” approach with a traditional database.","blip_selector":"accumulate-only-data","name":"Accumulate-only data","display_name":"Accumulate-only data","url":"/radar/techniques/accumulate-only-data","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":792,"quadrant":"techniques","volume_date":"2015-11","description":"We still see teams configure their CI and CD tools by directly embedding complex multi-line commands directly into the configuration of the tool. Often these embedded commands also contains steps that would only ever take effect in the build environment including things such as CI specific environment variables, steps that would create/modify files and templates only in the CI environment etc. This makes the build environment a special beast - whose results cannot be duplicated locally on a developer's machine.\n\nThis is extremely problematic because the CI/CD tool, which is supposed to expose problems in your code, itself becomes a complex beast whose behavior is hard to debug and whose results are hard to replicate.\n\nThe way to avoid programming in your CI/CD tool is to extract the complexities of the build process from the guts of the tool and into a simple script which can be invoked by a single command. This script can then be executed on any developer workstation and therefore eliminates the privileged/singular status of the build environment.","blip_selector":"programming-in-your-ci-cd-tool","name":"Programming in your CI/CD tool","display_name":"Programming in your CI/CD tool","url":"/radar/techniques/programming-in-your-ci-cd-tool","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":862,"quadrant":"platforms","volume_date":"2015-11","description":"Apache Kylin is an open source analytics solution from eBay Inc. that enables SQL based multidimensional analysis (OLAP) on very large datasets. Kylin is intended to be a Hadoop based hybrid OLAP (HOLAP) solution that will eventually support both MOLAP and ROLAP style multidimensional analysis. With Kylin you can define cubes using a Cube Designer and initiate an offline process that builds these cubes. The offline process performs a pre-join step to join facts and dimension tables into a flattened out structure. This is followed by a pre-aggregation phase where individual cuboids are built using Map Reduce jobs. The results are stored in HDFS sequence files and are later loaded into HBase. The data requests can originate from SQL submitted using a SQL-based tool. The query engine (based on Apache Calcite ), determines if the target dataset exists in HBase. If so, the engine directly accesses the target data from HBase and returns the result with sub-second latency. If not, the engine routes the queries to Hive (or any other SQL on Hadoop solution enabled on the cluster).","blip_selector":"apache-kylin","name":"Apache Kylin","display_name":"Apache Kylin","url":"/radar/platforms/apache-kylin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":848,"quadrant":"tools","volume_date":"2015-11","description":"Quick is a testing framework for Swift and Objective-C, which comes bundled with Nimble , a matcher framework for tests. Quick helps verify the behavior of Swift and Objective-C programs. Quick has the same syntactic flavour as RSpec and Jasmine and is easy to set up. It is very organized, allows for assertion of types and makes it easy to test asynchronous code.","blip_selector":"quick","name":"Quick","display_name":"Quick","url":"/radar/tools/quick","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":841,"quadrant":"tools","volume_date":"2015-11","description":"Gor is an open-source tool for capturing and replaying live HTTP traffic into a test environment in order to continuously test your system with real data. It can be used to increase confidence in code deployments, configuration changes and infrastructure changes.","blip_selector":"gor","name":"Gor","display_name":"Gor","url":"/radar/tools/gor","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":755,"quadrant":"tools","volume_date":"2015-11","description":"Good testing of components in an enterprise system is critical and with increased emphasis on service-based separation and deployment automation—critical factors for success with microservices—better tooling in this space is needed. The industry term “service virtualization” refers to tools that can emulate specific components in such an environment. We have seen great success with Mountebank, a lightweight tool for stubbing and mocking HTTP, HTTPS, SMTP and TCP. | When testing services, we commonly need to stub out downstream collaborating services. Written by a Thoughtworker, Mountebank is a lightweight service which you can configure via HTTP that is capable of stubbing and mocking HTTP, HTTPS, SMTP and TCP. | When testing services, we commonly need to stub out downstream collaborating services. Written by a Thoughtworker, Mountebank is a lightweight service which you can configure via HTTP that is capable of stubbing and mocking HTTP, HTTPS, SMTP and TCP.","blip_selector":"mountebank","name":"Mountebank","display_name":"Mountebank","url":"/radar/tools/mountebank","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":816,"quadrant":"tools","volume_date":"2015-11","description":"Many organizations are still forcing distributed or offshore development teams to use Citrix remote desktop for development. Although this provides a simple security model – assets supposedly never leave the organization’s servers - using remote desktops for development absolutely cripples developer productivity. There’s not much point paying a cheaper hourly rate for developers if you’re going to impose both the distribution and remote-desktop burdens on them, and we wish more offshore vendors would admit these drawbacks to their clients. It’s much better to use either a 'clean room' secured offshore environment where local development can be done, or a Hosted IDE (e.g. ievms) | For security and compliance reasons, offshore teams are sometimes asked to use Citrix to connect to an onshore virtual desktop, where they do development. While a good tool for some use cases, Citrix provides an extremely poor remote development experience and often cripples an offshore team. There are many better technical solutions, such as the NoMachine remote desktop or Cloud9 IDE, which can provide a more workable experience. An even better solution is to tackle the underlying security and compliance concerns. Since you are trusting the remote team to work on your source code and check in to your code repository, you should try to get to a point where you also trust them to have source code on their machines. They will be much more productive!","blip_selector":"citrix-for-development","name":"Citrix for development","display_name":"Citrix for development","url":"/radar/tools/citrix-for-development","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":857,"quadrant":"tools","volume_date":"2015-11","description":"The ZED Attack Proxy (ZAP) is a project from OWASP which allows you to probe an existing site for security vulnerabilities in an automated fashion. It can be used as part of periodic security testing, or else integrated into a CD pipeline to provide ongoing checks for common vulnerabilities. The use of a tool like ZAP doesn’t replace the need to think carefully about security and do other sorts of more thorough testing, but as another tool to help ensure our systems are more secure it’s a good addition to the toolbox.","blip_selector":"zap","name":"ZAP","display_name":"ZAP","url":"/radar/tools/zap","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":715,"quadrant":"tools","volume_date":"2015-11","description":"Many monitoring tools are built around the concept of the machine or instance. The increasing use of patterns like Phoenix Server and tools like Docker mean this is an increasingly unhelpful way to model infrastructure: Instances are becoming transient while services are the things that persist. Sensu allows an instance to register itself as playing a particular role, and Sensu then monitors it on that basis. Over time, different instances playing that role may come and go. Given these factors and the increasing maturity of the tool, we felt it was time to bring Sensu back on to the radar. | Many monitoring tools are built around the idea of the machine. We monitor what the machine is doing and which software is running on it. When it comes to cloud based infrastructure, especially patterns like Phoenix and Immutable servers this is a problematic approach. Machines come and go, but what is important is that the services remain working. Sensu allows a machine to register itself as playing a particular role and Sensu then monitors it on that basis. When we are finished with the machine we can simply de-register it.","blip_selector":"sensu","name":"Sensu","display_name":"Sensu","url":"/radar/tools/sensu","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":861,"quadrant":"techniques","volume_date":"2015-11","description":"Debugging CSS problems can be painful. How many times have you had to trawl through thousands of overridden styles to work out the source of your problem? This has led many of our teams to introduce various guidelines such as avoiding cascading and overrides, making styles opt-in and emphasizing thoughtful naming. BEM is a simple CSS naming convention (standing for Block, Element, Modifier) that helps give semantic clarity and structure to your CSS. By using BEM, it becomes much easier to understand which CSS rules are influencing the appearance of an element and, more importantly, the intent of those rules. This approach can be seen as moving the OO lesson of favoring composition over inheritance to the world of CSS.","blip_selector":"bem","name":"BEM","display_name":"BEM","url":"/radar/techniques/bem","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":863,"quadrant":"platforms","volume_date":"2015-11","description":"A time series database (TSDB) is a system that is optimized for handling time series data. It allows users to perform CRUD operations on various time series organized as database objects. It also provides the ability to perform statistical calculations on the series as a whole. Although TSDBs are not entirely a new technology we are seeing a renewed interest in the these databases primarily in the realm of IoT applications. This is being facilitated by many open source and commercial platforms (such as  OpenTSDB , InfluxDB , Druid , BlueFloodDB etc.) that have mushroomed recently. Its also worth mentioning that some of these systems use other distributed databases such Cassandra and HBase as their underlying storage engine.","blip_selector":"time-series-databases","name":"Time series databases","display_name":"Time series databases","url":"/radar/platforms/time-series-databases","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":759,"quadrant":"techniques","volume_date":"2015-11","description":"Gartner's Pace-layered Application Strategy approach appears to be creating an unhelpful focus on the idea of layers within an architecture. We find thinking about the pace of change within different business capabilities (which can be made up of several architectural layers) to be a more useful concept. The danger in focusing on layers is that many types of change cut across multiple layers. For example, being able to add new class of stock to a website is not just about having an easy-to-change CMS; you also need to update the database, integration points, warehouse systems, etc. The recognition that some parts of an architecture need to be more maneuverable than others is useful. However, a focus on layers is proving unhelpful. | Gartner's Pace-layered Application Strategy approach to architecture attempts to articulate the fact that decisions about architecture should not be a one-size fits all approach. Instead, it is important to take a balanced view of your technology portfolio. We have qualms about some of the prescriptive recommendations that come with Pace. In particular, we have found some situations where the layering concept is too simplistic. That said, we believe the core idea that different components and systems within an architecture need to change at different paces is sound. | Gartner's Pace-layered Application Strategy approach to architecture attempts to articulate the fact that decisions about architecture shouldn't be a one-size-fits-all approach. Instead, it is important to take a balanced view to your technology portfolio in terms of where to be conservative, and where to take risks. While we have qualms about some of the more prescriptive recommendations that seem to come with Pace, in general we like the concept and many organizations could benefit from adapting similar models.","blip_selector":"pace-layered-application-strategy","name":"Pace-layered Application Strategy","display_name":"Pace-layered Application Strategy","url":"/radar/techniques/pace-layered-application-strategy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":806,"quadrant":"tools","volume_date":"2015-11","description":"Although the idea of dependency management is not new and considered to be a fundamental development practice, it is not widely adopted by the PHP community. Composer is a tool for dependency management in PHP. It is strongly influenced by tools from other technology stacks like Node's npm and Ruby's Bundler. We are now seeing wide adoption across PHP projects and it is fairly mature. You can still have to do some shims for internal libraries, you can use it for most external libraries. | Although the idea of dependency management is not new and considered to be a fundamental development practice, it is not widely adopted by the PHP community. Composer is a tool for dependency management in PHP. It is strongly influenced by tools from other technology stacks like Node's npm and Ruby's Bundler.","blip_selector":"composer","name":"Composer","display_name":"Composer","url":"/radar/tools/composer","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":630,"quadrant":"languages-and-frameworks","volume_date":"2015-11","description":"Since we last talked about Nancy on the technology radar it has become the default choice on our .NET projects. Architectures centred around small, vertical slices and microservices simply require light-weight deployment options and low ceremony tooling. | Nancy continues to gain traction in the Alt.NET community and we have found it particularly useful for deploying low-ceremony, lightweight fakes for testing in a microservices environment. | Nancy is a lightweight, open-source web framework for .NET.  In the spirit of Sinatra for Ruby, Nancy provides just the essentials necessary to implement web applications with minimal extraneous code.  Because the framework is independent of any particular hosting environment, the developer is freed from the IIS and ASP.NET environment.  This makes Nancy an excellent complement to OWIN and compatible with other OWIN modules.  We are really happy to see the emergence of lightweight web frameworks in a number of other languages as well; Spark for Java, Flask for Python, etc..","blip_selector":"nancy","name":"Nancy","display_name":"Nancy","url":"/radar/languages-and-frameworks/nancy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":758,"quadrant":"platforms","volume_date":"2015-05","description":"OSGi (Open Service Gateway initiative) is a specification that aims to remedy the lack of a module system for Java, allowing for dynamic reloading of components. While some projects (notably Eclipse) use OSGi successfully, other uses have exposed the hazards of adding abstractions to platforms never designed for them. Projects that rely on OSGi to define a component system quickly realize that it solves only a small part of the overall problem, and often adds its own accidental complexity to projects such as more complex builds. Most projects now either use old-fashioned JAR files or microservice architectures to manage components, and await the native solution in Java in the Jigsaw module specification.","blip_selector":"osgi","name":"OSGi","display_name":"OSGi","url":"/radar/platforms/osgi","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":798,"quadrant":"platforms","volume_date":"2015-05","description":"After Oracle's acquisition of MySQL, more and more close sourced modules are bundled into its enterprise edition. There are concerns over the future of MySQL. MariaDB is a community-developed GPL-only fork of MySQL intended to remain truly open source, yet fully compatible and competitive with MySQL. High-profile adopters include large-scale internet organizations Google and Wikipedia, as well as key Linux distributors RedHat and SUSE.","blip_selector":"mariadb","name":"MariaDB","display_name":"MariaDB","url":"/radar/platforms/mariadb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":840,"quadrant":"tools","volume_date":"2015-05","description":"The NaCl library (pronounced 'Salt') provides a set of features for encryption, decryption, and signatures designed to make it easier to implement secure network communication or other cryptography requirements. Although these functions exist in other libraries, NaCl promises higher speed and easier to use APIs. Current support is for C and C++ with Python wrappers in progress.","blip_selector":"nacl","name":"NaCl","display_name":"NaCl","url":"/radar/tools/nacl","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":781,"quadrant":"techniques","volume_date":"2015-05","description":"Many projects have external code dependencies, a large amount of which is provided by open source projects. In order to ensure our builds are reproducible, we integrate against known versions of them, but that can mean that it takes a while for us to integrate against newer versions of these libraries leading to a larger merge effort down the line. One approach we have seen to avoid this is to have a nightly Canary Build which tries to pull in the latest version of all dependencies. If the build is green, we know we can change which versions we depend on.","blip_selector":"canary-builds","name":"Canary builds","display_name":"Canary builds","url":"/radar/techniques/canary-builds","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":804,"quadrant":"platforms","volume_date":"2015-05","description":"Securing online accounts is at the same time extremely important and notoriously difficult. Two-factor authentication does greatly increase security and we have recommended TOTP as a good solution. A new entrant in this field is Universal 2nd Factor ( U2F ), a solution based on public key cryptography and inexpensive USB hardware tokens. While developed at Google, it has now become a standard managed by the FIDO Alliance. We do like the promise of better protection against phishing and man-in-the-middle attacks, but are concerned because the standard currently references a specific elliptic curve digital signature algorithm that is considered to be flawed.","blip_selector":"u2f","name":"U2F","display_name":"U2F","url":"/radar/platforms/u2f","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":824,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"While there are lots of fans of Haskell among ThoughtWorks' language devotees, we rarely see it on the kinds of projects we work on—until recently. Several open source projects now marry Hadoop ’s map/reduce jobs to Haskell’s syntax, which some developers and/or data scientists find appealing.","blip_selector":"haskell-hadoop-library","name":"Haskell Hadoop library","display_name":"Haskell Hadoop library","url":"/radar/languages-and-frameworks/haskell-hadoop-library","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":811,"quadrant":"tools","volume_date":"2015-05","description":"Blackbox is a simple tool for encrypting specific files while at rest in your source repository. This is particularly useful if you need to store passwords or private keys. Blackbox works with Git, Mercurial and Subversion and uses GPG for the encryption. Each user has their own key, which makes it easy to revoke access on a granular level. There is a lot happening in this space and a few other players to consider including git-crypt and Trousseau. | Blackbox is a simple tool for encrypting specific files while at rest in your source repository. This is particularly useful if you need to store passwords or private keys. Blackbox works with Git, Mercurial and Subversion and uses GPG for the encryption.  Each user has their own key, which makes it easy to revoke access on a granular level.","blip_selector":"blackbox","name":"Blackbox","display_name":"Blackbox","url":"/radar/tools/blackbox","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":823,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"In the crowded space of JavaScript frameworks, we want to highlight Flight.js as a lightweight framework to build components. Flight gets by without much magic when adding behavior to DOM nodes. Its event-driven and component-based nature promotes writing decoupled code. This makes testing individual components comparatively easy. Care must be taken, however, when components need to interact with each other. There is little support for testing and a real danger to get into event hell. We do like that it uses functional mixins for behavior, like composition instead of inheritance. | In the crowded space of JavaScript frameworks, we want to highlight Flight.js as an alternative to consider. Flight is extremely lightweight and gets by without much magic when adding behavior to DOM nodes. Its event-driven and component-based nature promotes writing decoupled code. This makes testing individual components comparatively easy. Care must be taken, however, when components need to interact with each other.  There is little support for testing and a real danger to get into event hell. We do like that it uses functional mixins for behaviour, like composition instead of inheritance.","blip_selector":"flight-js","name":"Flight.js","display_name":"Flight.js","url":"/radar/languages-and-frameworks/flight-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":783,"quadrant":"techniques","volume_date":"2015-05","description":"When implementing single-page applications, sooner or later the question of offline use will come up. Given how hard it is to get this right when retrofitting an offline mode into an existing application, there is a trend towards implementing single-page applications with an “offline-first” mindset. An important implementation technique that we have used successfully is local storage sync. With this technique, the user facing code never makes requests to the backend. It retrieves data solely from local storage. A background worker synchronises the data in local storage with the backend systems, usually employing calls to some form of REST API.","blip_selector":"local-storage-sync","name":"Local storage sync","display_name":"Local storage sync","url":"/radar/techniques/local-storage-sync","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":828,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"Reagent has emerged as a lightweight minimalist alternative to Om for wrapping React.js in ClojureScript. Whereas Om provides a comprehensive Clojure-idiomatic front-end programming framework, Reagent takes advantage of Clojure’s expressiveness to focus on simple components and a readable DSL for writing HTML. By representing HTML in Clojure data, Reagent retains the performance and understandability of React.js without embedding foreign markup in the code.","blip_selector":"reagent","name":"Reagent","display_name":"Reagent","url":"/radar/languages-and-frameworks/reagent","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":805,"quadrant":"tools","volume_date":"2015-05","description":"Boot2docker is a lightweight linux distribution running Docker, packaged as a VM for OSX and Windows. This is a great way to get started experimenting with Docker. For teams using microservices, it can also be an effective way to run multiple services on a local machine for dev and test purposes, where the overhead of multiple vagrant VMs may be too much.","blip_selector":"boot2docker","name":"Boot2docker","display_name":"Boot2docker","url":"/radar/tools/boot2docker","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":801,"quadrant":"platforms","volume_date":"2015-05","description":"When Oracle ceased development on Sun’s OpenSSO—an open source access management platform—It was picked up by ForgeRock and integrated into their Open Identity Suite. Now named OpenAM, it fills the niche for a scalable, open-source platform that supports OpenID Connect and SAML 2.0. However, OpenAM’s long history has resulted in a sprawling codebase whose documentation can be inscrutable. Hopefully, a slimmed-down alternative with better support for automated deployment and provisioning will emerge soon. | When Oracle ceased development on Sun’s OpenSSO—an open source access management platform—It was picked up by ForgeRock and integrated into their Open Identity Suite.  Now named OpenAM, it fills the niche for a scalable, open-source platform that supports a variety of federated identity standards, including OpenID Connect and SAML 2.0.  These standards are a necessary enabler for secure microservice implementations.","blip_selector":"openam","name":"OpenAM","display_name":"OpenAM","url":"/radar/platforms/openam","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":718,"quadrant":"tools","volume_date":"2015-05","description":"We are excited by the progress made by Xamarin in offering a solid choice for building cross-platform mobile apps. It supports C# and F# as the primary languages with bindings to platform specific SDKs and the Mono runtime environment that works across iOS, Android and Windows Phone. Applications are compiled to native code giving apps a more native look and feel. When using this toolset, it is imperative that the platform specific UI tier be separated from the rest of the tiers to ensure code reuse across different platforms. The recent open-sourcing of the .NET platform should be beneficial for Xamarin both in allowing access to a broader set of .NET tooling and also making development easier on other operating systems. | Among the various choices available for building cross-platform mobile apps, Xamarin offers a fairly unique toolset. It supports C# and F# as the primary language with bindings to platform-specific SDKs and the Mono runtime environment that works across iOS, Android and Windows Phone. Applications are compiled to native code instead of the typical cross-platform approach that renders HTML-based UI in an embedded browser. This gives apps a more native look and feel. When using this toolset, it is imperative that the platform-specific UI tier be separated from the rest of the tiers to ensure code reuse across different platforms. The application binary tends to be a bit bigger due to the runtime environment that is included.","blip_selector":"xamarin","name":"Xamarin","display_name":"Xamarin","url":"/radar/tools/xamarin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":735,"quadrant":"platforms","volume_date":"2015-05","description":"We have been using DigitalOcean for basic compute infrastructure, and the service continues to impress us. If you need developer-friendly cloud infrastructure, it is worth a look. | Although the IaaS space is crowded, there is room for new competitors to enter the market. DigitalOcean has impressed us recently with its cost, speed and simplicity. If all you need is basic compute infrastructure, it is well worth a look.","blip_selector":"digitalocean","name":"DigitalOcean","display_name":"DigitalOcean","url":"/radar/platforms/digitalocean","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":797,"quadrant":"platforms","volume_date":"2015-05","description":"CoreOS is a Linux distribution designed to run large, scalable systems. All applications deployed on a CoreOS instance are run in separate Docker containers, and CoreOS provides a suite of tools to help manage them, including etcd their own distributed configuration store. Newer services, such as fleet, help cluster management by ensuring that a specific number of service instances are always kept running. FastPatch allows atomic CoreOS upgrades using an active-passive root partition scheme and helps with quick rollback in case of problems. These new developments make CoreOS well worth looking into if you are already comfortable with Docker.","blip_selector":"coreos","name":"CoreOS","display_name":"CoreOS","url":"/radar/platforms/coreos","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":800,"quadrant":"platforms","volume_date":"2015-05","description":"Jackrabbit Oak, formerly named Jackrabbit 3, is a scalable and performant implementation of hierarchical content repository for use as the foundation of content management system. In addition to file based storage solution, MongoDB and RDMS storage are also supported, and preferred in large volume use scenarios. Although implemented in Java, it can be easily accessed from various platforms via standards like JCR.","blip_selector":"jackrabbit-oak","name":"Jackrabbit Oak","display_name":"Jackrabbit Oak","url":"/radar/platforms/jackrabbit-oak","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":821,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"Retrofit offers a reliable way to build HTTP clients on Android projects by converting a REST API into a Java interface. Retrofit integrates with OkHttp and allows developers to provide custom error handling for requests. It does JSON parsing automatically using Gson and has a very well supported community.","blip_selector":"retrofit","name":"Retrofit","display_name":"Retrofit","url":"/radar/languages-and-frameworks/retrofit","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":799,"quadrant":"platforms","volume_date":"2015-05","description":"While we are reluctant to recommend wholesale adoption of the Netflix OSS Full Stack unless you happen to be entering the globally distributed video streaming business, the stack is chock full of interesting ideas, complete with open source implementations. Some of the tools, Asgard for example, are highly coupled into a virtually turnkey architecture, making them challenging to use individually. Other tools like Ice and Hystrix, which we featured on the radar previously, can be used stand-alone. We think teams should understand the ideas and approaches encapsulated within the tools even when they choose not to leverage the full stack.","blip_selector":"netflix-oss-full-stack","name":"Netflix OSS Full stack","display_name":"Netflix OSS Full stack","url":"/radar/platforms/netflix-oss-full-stack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":802,"quadrant":"platforms","volume_date":"2015-05","description":"Software Defined Networking ( SDN ) is a broad topic, but is becoming ever more important. The ability to configure our networking devices using software is blurring the lines of where our application deployments end. It encompasses everything from virtual networking appliances like AWS’ Load Balancers or CoreOS’ Flannel, to networking equipment that supports standards like OpenFlow. Where cloud providers have previously focused on compute and storage, we expect the growing array of SDN tools to deliver further efficiencies to how we handle our systems both off and on premise.","blip_selector":"sdn","name":"SDN","display_name":"SDN","url":"/radar/platforms/sdn","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":711,"quadrant":"techniques","volume_date":"2015-05","description":"Treating logs as data gives us greater insight into the operational activity of the systems we build. Structured logging , which is using a consistent, predetermined message format containing semantic information, builds on this technique and enables tools such as Graylog2 and Splunk to yield deeper insights. We recommend adopting structured logging because the benefits outweigh the minimal effort involved and the practice is becoming the default standard. | Treating logs as data gives us greater insight into the operational activity of the systems we build. Structured logging, which is using a consistent, predetermined message format containing semantic information, builds on this technique and enables tools such as Graylog2 and Splunk to yield deeper insights.","blip_selector":"structured-logging","name":"Structured logging","display_name":"Structured logging","url":"/radar/techniques/structured-logging","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":814,"quadrant":"tools","volume_date":"2015-05","description":"As distributed systems become more complex, it can be useful to have tools that help you understand how your system is behaving in production. Packetbeat is an open source tool which uses agents to sniff traffic between nodes, allowing you to see traffic patterns, error rates and other useful information. It requires Elasticsearch and Kibana to work, but if you are already using these tools as part of log aggregation, it could be an easy drop-in to give you more insight into your production system.","blip_selector":"packetbeat","name":"Packetbeat","display_name":"Packetbeat","url":"/radar/tools/packetbeat","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":776,"quadrant":"tools","volume_date":"2015-05","description":"Swagger2.0 is a standard way to describe a RESTful API so that human-readable documentation and client examples can be generated automatically. The update to version 2.0 provides some significant flexibility enhancements and the list of tools for generating documentation continues to expand. There are also several alternatives to Swagger emerging from the vendor community, most significantly RAML and API Blueprint. | Swagger is a standard way to describe a RESTful API so that documentation and client examples can be generated automatically. We think there's a need for some standards in this area and hope that this approach embraces Postel's law and avoids the tight-coupling and inflexibility of standards like WSDL. A number of tools are now available to render documentation and client pages from swagger-compliant descriptions.","blip_selector":"swagger","name":"Swagger","display_name":"Swagger","url":"/radar/tools/swagger","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":820,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"Nashorn is a new JavaScript engine for Java that has been released with Java 8. When the exact same code should be run in the web browser and on the server, which is often the case for validation and data migration logic, it is the tool of choice in the Java world, and that is the case despite some rough edges. We are not convinced that using Nashorn to host entire applications, via Node support or the Avatar project, is a good idea.","blip_selector":"nashorn","name":"Nashorn","display_name":"Nashorn","url":"/radar/languages-and-frameworks/nashorn","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":623,"quadrant":"techniques","volume_date":"2015-05","description":"Traditionally operations groups look to improve the mean time between failures. While avoiding failures is obviously still important, lessons from cloud computing have taught us to expect failure and instead to focus on mean time to recovery. Continuous Delivery automation makes rolling out rapid fixes easier and we are also seeing a growth in monitoring techniques to spot failures quickly through a ‘production immune system’. Teams are also successfully using semantic monitoring and synthetic transactions to exercise production systems in non-destructive ways. This combined focus allows teams to move rapidly with higher confidence, it can also reduce the emphasis on expensive test-execution in pre-production environments and is particularly important in responding to the ever-growing list of security vulnerabilities that are being discovered. | In DevOps-savvy organizations delivery teams often configure production monitoring and respond to incidents themselves. This visibility and access into production environments allows those teams to make changes to their systems to improve their ability to recover quickly when something goes wrong. This focus on mean time to recovery improves quality of service overall, and allows teams to safely deploy more frequently. This can also reduce the emphasis on expensive test execution in non-production environments. Techniques we've used include end-to-end 'semantic monitoring' or reconciliation of real business transactions, and the injection of 'synthetic transactions' which exercise systems in non-destructive ways in production. | In previous radars we recommended arranging automated acceptance tests into longer journeys and, in what we call semantic monitoring, running these tests continuously against a production environment. We still believe that this is an important technique for scenarios the team can anticipate in advance. A variation of this approach, seen especially with startups, is to reduce the number of tests while increasing  monitoring and automatic alarms.  This shifts the focus from avoiding problems that can be anticipated to reducing mean time to recovery for all problems.","blip_selector":"focus-on-mean-time-to-recovery","name":"Focus on mean time to recovery","display_name":"Focus on mean time to recovery","url":"/radar/techniques/focus-on-mean-time-to-recovery","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":788,"quadrant":"techniques","volume_date":"2015-05","description":"While the currency aspect of Bitcoin and other cryptocurrencies gets most of the news, we are equally excited about possibilities for using the Blockchain beyond bitcoin and financial transactions. The Blockchain is a mechanism for verifying the contents of a shared ledger without relying on a centralized service. We already see the Blockchain (either the underlying technology or the public Bitcoin Blockchain) being used at the heart of systems as varied as identity, ownership, record-keeping, voting, cloud storage and even managing networks of smart devices. If you are building systems that require trust over decentralized networks, then the Blockchain is a technology worth assessing.","blip_selector":"blockchain-beyond-bitcoin","name":"Blockchain beyond bitcoin","display_name":"Blockchain beyond bitcoin","url":"/radar/techniques/blockchain-beyond-bitcoin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":844,"quadrant":"tools","volume_date":"2015-05","description":"Developing a software system by first creating a large number of detailed diagrams is an approach that, in our experience, does not compare favourably to the alternatives. However, describing a particularly complex and intricate part of the system with a diagram is usually a good idea, and the UML itself offers a number of useful and commonly understood diagrams. We like PlantUMLfor creating these diagrams because it allows expressing the intent behind the diagrams in a clear textual form, without having to fiddle with overloaded graphical tools. Having a textual form also allows versioning and storage alongside the source code.","blip_selector":"plantuml","name":"PlantUML","display_name":"PlantUML","url":"/radar/tools/plantuml","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":744,"quadrant":"tools","volume_date":"2015-05","description":"Continuously delivering high quality software to production in a rapid and reliable manner requires coordinating many automated steps. GoCD is an open-source tool built by ThoughtWorks to handle exactly this scenario, with the concept of deployment pipelines at its core, it handles complex workflows over many nodes and enables transparent, traceable promotion of trusted artifacts across environments. While it is possible to craft deployment pipelines on top of continuous integration tools, our teams see the benefit derived from a tool purpose built for this job. | Owing to the increasing interest in Continuous Delivery and deployment pipelines, we see many teams trying to extend their Continuous Integration tooling with plugins that provide deployment pipelines at a visual level. GoCD is a tool that was built with the concept of deployment pipelines at its core. GoCD has the ability to sequence workflows both sequentially and in parallel at many levels, to execute specific tasks only on certain machines as well as to deterministically promote and propagate artifacts, which is a key enabler for Continuous Delivery. These are capabilities that most Continuous Integration tools lack, and we recommend that teams who might have otherwise tried to build a deployment pipeline from their Continuous Integration server try GoCD instead. GoCD was built by Thoughtworks, is open-source, and is available for free for all teams. The source code is available under the Apache 2.0 license.","blip_selector":"gocd","name":"GoCD","display_name":"GoCD","url":"/radar/tools/gocd","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":825,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"We don’t know who named Lotus, but we can only assume they are too young to have worked with a certain office collaboration product. Lotus is a new Rack-based MVC framework written in Ruby that can be deployed modularly so that you are free to use only the portions of the framework you need. It is a modern alternative to the monolithic Ruby-on-Rails framework (that turned 10 this year). Lotus has the potential to make full-stack Ruby MVC development as easy as 1-2-3.","blip_selector":"lotus","name":"Lotus","display_name":"Lotus","url":"/radar/languages-and-frameworks/lotus","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":819,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"Ionic framework is an open-source front-end framework that offers a library of mobile-optimized HTML, CSS and JavaScript components and tools for building highly interactive applications. It is built with SASS and optimized for AngularJS. We have seen success in several of our projects employing this framework, with its ease to install and test. We recommend investigating this framework when you are performance obsessed and looking for a seamlessly integrated front-end framework.","blip_selector":"ionic-framework","name":"Ionic Framework","display_name":"Ionic Framework","url":"/radar/languages-and-frameworks/ionic-framework","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":847,"quadrant":"tools","volume_date":"2015-05","description":"pdfmake is a JavaScript library which allows for creation and printing of PDF documents directly in the browser. To use pdfmake you construct a document object that supports structural elements such as tables, columns, and rich styling, then helper methods can create and print or download a PDF without leaving client-side JavaScript.","blip_selector":"pdfmake","name":"pdfmake","display_name":"pdfmake","url":"/radar/tools/pdfmake","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":817,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"The importance of big, visible displays in team areas has been written about many times before, and we certainly value the approach of helping everyone see and understand key pieces of information about how our software or our teams are doing. Dashing is a Ruby-based dashboard system we have been using for many years to create clear, visible displays optimized for large monitors. It is very hackable, allowing you to pull in information from a variety of sources from build systems, ticket or story tracking tools, or production monitoring systems.","blip_selector":"dashing","name":"Dashing","display_name":"Dashing","url":"/radar/languages-and-frameworks/dashing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":794,"quadrant":"techniques","volume_date":"2015-05","description":"Traditional approaches to security have relied on up-front specification followed by validation at the end. This “Security Sandwich” approach is hard to integrate into Agile teams, since much of the design happens throughout the process, and it does not leverage the automation opportunities provided by continuous delivery. Organizations should look at how they can inject security practices throughout the agile development cycle. This includes: evaluating the right level of Threat Modeling to do up-front; when to classify security concerns as their own stories, acceptance criteria, or cross-cutting non-functional requirements; including automatic static and dynamic security testing into your build pipeline; and how to include deeper testing, such as penetration testing, into releases in a continuous delivery model. In much the same way that DevOps has recast how historically adversarial groups can work together, the same is happening for security and development professionals. (But despite our dislike of the Security Sandwich model, it is much better than not considering security at all, which is sadly still a common circumstance.) | Traditional approaches to security have relied on up-front specification followed by validation at the end. This “Security Sandwich” approach is hard to integrate into Agile teams, since much of the design happens throughout the process, and it does not leverage the automation opportunities provided by continuous delivery. Organizations should look at how they can inject security practices throughout the agile development cycle. This includes: evaluating the right level of Threat Modeling to do up-front; when to classify security concerns as their own stories, acceptance criteria, or cross-cutting non-functional requirements; including automatic static and dynamic security testing into your build pipeline; and how to include deeper testing, such as penetration testing, into releases in a continuous delivery model. In much the same way that DevOps has recast how historically adversarial groups can work together, the same is happening for security and development professionals.","blip_selector":"security-sandwich","name":"Security sandwich","display_name":"Security sandwich","url":"/radar/techniques/security-sandwich","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":852,"quadrant":"tools","volume_date":"2015-05","description":"Origami is a free tool for designing user prototypes with a variety of keyboard shortcuts for common functions. It provides the possibility of exporting the prototypes as code snippets to Objective-C for iOS, Java for Android and JavaScript for Web. This tool can be used to rapidly build interactive user facing prototypes and testing user flows. We recommend investigating this tool if the use case fits from the experience we have gathered from several of our teams.","blip_selector":"origami","name":"Origami","display_name":"Origami","url":"/radar/tools/origami","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":808,"quadrant":"tools","volume_date":"2015-05","description":"GitLab is an on-premise Git repository hosting platform that gives proprietary software development teams the familiar and ubiquitous workflow that hosted version control services like GitHub and BitBucket provide OSS developers. While it is available as free community edition software, the commercial enterprise option provides support and deep integration with LDAP servers.","blip_selector":"gitlab","name":"GitLab","display_name":"GitLab","url":"/radar/tools/gitlab","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":807,"quadrant":"tools","volume_date":"2015-05","description":"Cursive is a Clojure IDE that works as a plugin for IntelliJ. While still in early access, we have found it very useful when working with larger Clojure codebases. Cursive provides strong renaming and navigation support, has shown itself to be stable and reliable, and is great for environments with mixed JVM languages. For organizations adopting Clojure, Cursive has helped lower the barrier to entry for existing developers.","blip_selector":"cursive","name":"Cursive","display_name":"Cursive","url":"/radar/tools/cursive","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":835,"quadrant":"techniques","volume_date":"2015-05","description":"These days, most software developers are used to working with Git for source code control and collaboration. But Git can be used as a base mechanism for other circumstances where a group of people need to collaborate on textual documents (that can easily be merged). We’ve seen increasing amounts of projects use Git as the basis for a lightweight CMS , with text-based editing formats. Git has powerful features for tracking changes and exploring alternatives, with a distributed storage model that is fast in use and tolerant of networking issues. The biggest problem with wider adoption is that Git isn’t very easy to learn for non-programmers, but we expect to see more tools that build on top of the core Git plumbing. Such tools simplify the workflow for specific audiences, such as content authors. We would also welcome more tools to support diffing and merging for non-textual documents.","blip_selector":"git-based-cms-git-for-non-code","name":"Git based CMS/Git for non-code","display_name":"Git based CMS/Git for non-code","url":"/radar/techniques/git-based-cms-git-for-non-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":855,"quadrant":"tools","volume_date":"2015-05","description":"Brighter is an open source library for .Net that provides scaffolding to implement Command Invocation. We have had good feedback from teams using it, especially in conjunction with the ports and adaptors pattern and CQRS. They especially like that it integrates well with Polly to provide circuit breaking functionality.","blip_selector":"brighter","name":"Brighter","display_name":"Brighter","url":"/radar/tools/brighter","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":818,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"We have used the Django REST framework, which is a flexible and customizable framework that makes it easy to build web APIs, in several of our projects. It allows you to build RESTful APIs in Python with Django, exposing API endpoints which are accessible from a consumer front-end. Django REST gives a browsable web API that allows developers to visualize data being transferred through the API and returns response examples, which the consumer application will receive. It provides a number of authentication schemes out of the box, and allows implementation of custom schemes.","blip_selector":"django-rest","name":"Django REST","display_name":"Django REST","url":"/radar/languages-and-frameworks/django-rest","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":756,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"We have seen continued interest in Om , a ClojureScript wrapper around Facebook's ReactJS front-end programming framework. Om leverages the inherent immutability of ClojureScript, allowing automatic features like snapshots of UI state and undo. And due to the efficiency of ClojureScript's data structures, some Om applications run faster than identical ones based on the raw underlying React framework. The ecosystem of components and applications around Om is growing and our teams are starting to pick it up. | Adopting the entire Clojure stack (the Clojure and ClojureScript languages, and optionally the Datomic database) offers some advantages like immutable data structures from user interface to backend. Several frameworks have appeared in the Clojure space to leverage its unique features, but the most promising so far is Om. Om is a ClojureScript wrapper around Facebook's React JavaScript reactive programming framework. Yet Om leverages the inherent immutability of ClojureScript, allowing automatic features like snapshots of UI state and undo. And due to the efficiency of ClojureScript's data structures, some Om applications run faster than identical ones based on the raw underlying React framework. We expect significant evolution and innovation to continue around Om.","blip_selector":"om","name":"Om","display_name":"Om","url":"/radar/languages-and-frameworks/om","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":699,"quadrant":"techniques","volume_date":"2015-05","description":"The term Datensparsamkeit is taken from German privacy legislation and describes the idea to only store as much personal information as is absolutely required for the business or applicable laws. Customer privacy continues to be a hot topic. Companies such as Uber are apparently collecting highly personal customer data, as well as being quite lax with security. This is a disaster waiting to happen. Following datensparsamkeit or using de-identification techniques even in jurisdictions where it is not legally mandated, can allow you to reduce the information you store. If you never store the information, you do not need to worry about someone stealing it. | In our desire to support ever-changing business models, learn from past behavior and provide the best experience for every individual visitor, we are tempted to record as much data as possible. At the same time hackers are more ferocious than ever, with one spectacular security breach after another, and we now know of unprecedented mass-surveillance by government agencies. The term Datensparsamkeit is taken from German privacy legislation and describes the idea to only store as much personal information as is absolutely required for the business or applicable laws. Some examples are instead of storing a customer's full IP address in access logs, just using the first two or three octets and instead of logging transit journeys with a username using an anonymous token. If you never store the information, you do not need to worry about someone stealing it.","blip_selector":"datensparsamkeit","name":"Datensparsamkeit","display_name":"Datensparsamkeit","url":"/radar/techniques/datensparsamkeit","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":803,"quadrant":"platforms","volume_date":"2015-05","description":"Text-it-as-a-service / Rapidpro offers ability to easily set up or modify complex short message service application for business without extensive need of a developer. With the lower costs of text messages compared to USSD sessions, this provides a more affordable way to build scalable applications targeting feature phones and we have seen success in our projects. Flows are very simple to build and actions can be triggered at any point such as sending an sms, email or even calling an external api.","blip_selector":"text-it-as-a-service-rapidpro-io","name":"Text it as a service / Rapidpro.io","display_name":"Text it as a service / Rapidpro.io","url":"/radar/platforms/text-it-as-a-service-rapidpro-io","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":683,"quadrant":"languages-and-frameworks","volume_date":"2015-05","description":"We continue to see teams run into trouble using JSF - JavaServer Faces - and are recommending you avoid this technology. Teams seem to choose JSF because it is a Java EE standard without really evaluating whether the programming model suits them. We think JSF is flawed because its programming model encourages use of its own abstractions rather than fully embracing the underlying web model. JSF, like ASP.NET webforms, attempts to create stateful component trees on top HTML markup and the stateless HTTP protocol. The improvements in JSF 2.0 and 2.2, such as the introduction of stateless views and the promotion of GET, are steps in the right direction, maybe even an acknowledgement that the original model was flawed, but we feel this is a too little too late. Rather than dealing with the complexity of JSF we recommend teams use simple frameworks and work closely with web technologies including HTTP, HTML and CSS. | We continue to see teams run into trouble using JSF - JavaServer Faces - and are recommending you avoid this technology. Teams seem to choose JSF because it is a Java EE standard without really evaluating whether the programming model suits them. We think JSF is flawed because its programming model encourages use of its own abstractions rather than fully embracing the underlying web model. JSF, like ASP.NET webforms, attempts to create stateful component trees on top HTML markup and the stateless HTTP protocol. The improvements in JSF 2.0 and 2.2, such as the introduction of stateless views and the promotion of GET, are steps in the right direction, maybe even an acknowledgement that the original model was flawed, but we feel this is a too little too late. Rather than dealing with the complexity of JSF we recommend teams use simple frameworks and work closely with web technologies including HTTP, HTML and CSS.","blip_selector":"jsf","name":"JSF","display_name":"JSF","url":"/radar/languages-and-frameworks/jsf","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":733,"quadrant":"languages-and-frameworks","volume_date":"2015-01","description":"The Clojure core.async library allows asynchronous communication using channels, with similar syntax and capabilities to Google's Go language. The core.async library solves many common problems in an elegant way, cleaning up event callback setup and adding simple concurrency primitives. It also highlights one of the advantages of the Lisp nature of Clojure: channels add operators that are consistent with existing Clojure operators, seamlessly weaving new functionality into the language core. In addition, core.async is supported in both Clojure and ClojureScript (despite JavaScript's lack of threads), utilizing underlying platform abstractions to provide a consistent interface to both languages. | The Clojure core.async library allows asynchronous communication using channels, with similar syntax and capabilities to Google's Go language. The core.async library solves many common problems in an elegant way, cleaning up event callback setup and adding simple concurrency primitives. It also highlights one of the advantages of the Lisp nature of Clojure: channels add operators that are consistent with existing Clojure operators, seamlessly weaving new functionality into the language core. In addition, core.async is supported in both Clojure and ClojureScript (despite JavaScript's lack of threads), utilizing underlying platform abstractions to provide a consistent interface to both languages.","blip_selector":"core-async","name":"Core Async","url":"/radar/languages-and-frameworks/core-async","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":743,"quadrant":"tools","volume_date":"2015-01","description":"Device fragmentation in the Android world is often cited as a problem because it can be difficult to understand how your applications will behave on a large number of disparate platforms. GenyMotion is an emulator which can mimic the characteristics of a number of different Android devices. Our teams have found this very effective in giving fast feedback for our Android applications. | Device fragmentation in the Android world is often cited as a problem because it can be difficult to understand how your applications will behave on a large number of disparate platforms. GenyMotion is an emulator which can mimic the characteristics of a number of different Android devices. Our teams have found this very effective in giving fast feedback for our Android applications.","blip_selector":"genymotion","name":"GenyMotion","url":"/radar/tools/genymotion","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":646,"quadrant":"tools","volume_date":"2015-01","description":"With increasing scrutiny over the privacy of data, more companies are concerned about sharing web analytics with third parties. Snowplow Analytics and Piwik are examples of open-source analytics platforms that can be self-hosted and provide a promising feature set and roadmap. | With increasing scrutiny over the privacy of data, more companies are concerned about sharing web analytics with third parties. Snowplow Analytics and Piwik are examples of open-source analytics platforms that can be self-hosted and provide a promising feature set and roadmap. | We see great promise in Snowplow Analytics, an open source web analytics platform that derives intelligent information from regular web analytics, based on open data principles and cloud Storage.","blip_selector":"snowplow-analytics-piwik","name":"Snowplow Analytics & Piwik","url":"/radar/tools/snowplow-analytics-piwik","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":746,"quadrant":"languages-and-frameworks","volume_date":"2015-01","description":"We see lots of teams creating RESTful interfaces without paying any attention to hypermedia. HAL is a simple format for incorporating hyperlinks into JSON representations which is easy to implement and consume. HAL is well supported by libraries for parsing and representing JSON, and there are HAL-aware REST client libraries such as Hyperclient which make it easy to navigate resources by following links. | We see lots of teams creating RESTful interfaces without paying any attention to hypermedia. HAL is a simple format for incorporating hyperlinks into JSON representations which is easy to implement and consume. HAL is well supported by libraries for parsing and representing JSON, and there are HAL-aware REST client libraries such as Hyperclient which make it easy to navigate resources by following links.","blip_selector":"hal","name":"HAL","url":"/radar/languages-and-frameworks/hal","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":731,"quadrant":"platforms","volume_date":"2015-01","description":"CoAP is an open standards communication protocol for the Internet of Things (IoT). While there is currently a proliferation of competing standards in the IoT space, we particularly like CoAP. It is specifically designed for resource-constrained devices and local radio networks. It uses UDP for transport, but is semantically compatible with HTTP. CoAP uses a web-based model of devices with their own URLs and a request-response paradigm that supports RESTful and decentralized approaches. | CoAP is an open standards communication protocol for the Internet of Things (IoT). While there is currently a proliferation of competing standards in the IoT space, we particularly like CoAP. It is specifically designed for resource-constrained devices and local radio networks. It uses UDP for transport, but is semantically compatible with HTTP. CoAP uses a web-based model of devices with their own URLs and a request-response paradigm that supports RESTful and decentralized approaches.","blip_selector":"coap","name":"CoAP","url":"/radar/platforms/coap","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":705,"quadrant":"techniques","volume_date":"2015-01","description":"We continue to see teams and organizations equating velocity with productivity. When properly used, velocity allows the incorporation of \\”yesterday's weather\\” into a team’s internal iteration planning process. The key here is that velocity is an internal measure for a team, it is just a capacity estimate for that given team at that given time. Organizations and managers who equate internal velocity with external productivity start to set targets for velocity, forgetting that what actually matters is working software in production. Treating velocity as productivity leads to unproductive team behaviors that optimize this metric at the expense of actual working software. | Of all the approaches that we might disagree with, equating velocity with productivity has become so prevalent that we felt it important to call it out in our hold ring. When properly used, velocity allows the incorporation of \"yesterday's weather\" into the iteration planning process. Velocity is simply a capacity estimate for a given team at a given time. It can improve as a team gels or by fixing problems like technical debt or a flaky build server. However, like all metrics, it can be misused. For example, over-zealous project managers attempt to insist on continual improvement of velocity. Treating velocity as productivity leads to unproductive team behaviors that optimize the metric at the expense of actual working software.","blip_selector":"velocity-as-productivity","name":"Velocity as productivity","url":"/radar/techniques/velocity-as-productivity","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":729,"quadrant":"platforms","volume_date":"2015-01","description":"AMD recently released an 8-core ARM SoC (System on a Chip) designed for servers and has committed to releasing an ARM SoC with integrated graphics in 2015. ARM-based servers are an interesting alternative to x86 because they are significantly more energy efficient. For some workloads, building an ARM-powered Cloud is preferable. | AMD recently released an 8-core ARM SoC (System on a Chip) designed for servers and has committed to releasing an ARM SoC with integrated graphics in 2015. ARM-based servers are an interesting alternative to x86 because they are significantly more energy efficient. For some workloads, building an ARM-powered Cloud is preferable.","blip_selector":"arm-server-soc","name":"ARM Server SoC","url":"/radar/platforms/arm-server-soc","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":782,"quadrant":"techniques","volume_date":"2015-01","description":"There has been a lot of recent attention to the use of ATOM-style event feeds over HTTP as a method of integration.  Instead of maintaining a live service to expose those feeds, it is often acceptable to use old-style scheduled batch processing to create and publish feed files.  When combined with cloud technology like Amazon's S3 file storage and hypermedia linking, this can create a highly available, yet simple and testable solution.  Our teams have started to call this old-meets-new approach 'Hipster batch'.","blip_selector":"hipster-batch","name":"Hipster batch","url":"/radar/techniques/hipster-batch","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":769,"quadrant":"techniques","volume_date":"2015-01","description":"In the last radar we talked about Capturing Explicit Domain Events, putting emphasis on recording the business-meaningful events that have triggered state transitions instead of just CRUD'ing entities. REST interfaces commonly use PUT to update resource state, however it's often better to POST to record a new event resource which captures intent. REST without PUT has a side-benefit of separating command and query interfaces and forces consumers to allow for eventual consistency. | In the last radar we talked about Capturing Explicit Domain Events, putting emphasis on recording the business-meaningful events that have triggered state transitions instead of just CRUD'ing entities. REST interfaces commonly use PUT to update resource state, however it's often better to POST to record a new event resource which captures intent. REST without PUT has a side-benefit of separating command and query interfaces and forces consumers to allow for eventual consistency.","blip_selector":"rest-without-put","name":"REST without PUT","url":"/radar/techniques/rest-without-put","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":764,"quadrant":"techniques","volume_date":"2015-01","description":"Forward Secrecy (sometimes known as 'Perfect Forward Secrecy' or PFS) is a cryptographic technique that protects previous communications sessions even if a server’s master keys are later compromised. Despite being simple to enable for HTTPS connections, many servers are not configured this way, and we recommend enabling forward secrecy to improve security. Note that we don't generally like the word 'perfect' when used to describe cryptographic protocols -- even the best protocol can be broken by a flaw in implementation, random number generator, or by advances in cryptanalytic techniques. Even so, it is important to enable the best security available, while keeping informed of new attacks and protocol improvements. | Forward Secrecy (sometimes known as \"Perfect Forward Secrecy\" or PFS) is a cryptographic technique that protects previous communications sessions even if a server’s master keys are later compromised. Despite being simple to enable for HTTPS connections, many servers are not configured this way, and we recommend enabling forward secrecy to improve security. Note that we don't generally like the word \"perfect\" when used to describe cryptographic protocols - even the best protocol can be broken by a flaw in implementation, random number generator, or by advances in cryptanalytic techniques. Even so, it's important to enable the best security available, whilst keeping informed of new attacks and protocol improvements.","blip_selector":"forward-secrecy","name":"Forward Secrecy","url":"/radar/techniques/forward-secrecy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":785,"quadrant":"techniques","volume_date":"2015-01","description":"Many of our customers have made DevOps a reality in their organization with delivery teams that build, deploy, and support their own applications and services.  Unfortunately, a regular roadblock on that journey is allowing teams to have superuser privileges in production environments.  In most organizations, the production environment is shared, and therefore risky to provide access widely.  It is effective when we can partition infrastructure along team bounds, so that those teams can have safe isolated access to do their work, without risking impact to other systems.  Where cloud environments are used, this is much easier to implement, aligning account structures to team boundaries.","blip_selector":"partition-infrastructure-along-team-bounds","name":"Partition infrastructure along team bounds","url":"/radar/techniques/partition-infrastructure-along-team-bounds","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":739,"quadrant":"platforms","volume_date":"2015-01","description":"Given the popularity of event sourcing, it is no surprise that tools in this space are maturing. EventStore is an open source functional database for storing immutable events and performing complex event processing on the event streams. Unlike other tools in this space, EventStore exposes event streams as Atom collections which therefore require no special infrastructure such as message buses or highly specialized clients to use. | Given the popularity of event sourcing, it is no surprise that tools in this space are maturing. EventStore is an open source functional database for storing immutable events and performing complex event processing on the event streams. Unlike other tools in this space, EventStore exposes event streams as Atom collections which therefore require no special infrastructure such as message buses or highly specialized clients to use.","blip_selector":"eventstore","name":"EventStore","url":"/radar/platforms/eventstore","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":770,"quadrant":"tools","volume_date":"2015-01","description":"Roslyn, a .NET compiler platform under the Apache License 2.0, is a next-generation set of compilers for C# and VB.NET written entirely as managed code. It provides access to the compiler as a service and includes code analysis APIs allowing developers to access information from the compiler that was previously treated as a black box, for example syntactic and semantic models. The most immediate impact should be seen in enhancements to .NET IDEs through refactoring and code generation tools. We also expect to see improved code diagnostics and static analysis, although it will be interesting to see what the community comes up with. Meanwhile Xamarin has a Mono-compatible copy of Roslyn source code hosted on GitHub and plans to bundle Roslyn’s compilers with Mono as it stabilizes, in addition to integrating the best parts into their code base. | Roslyn, a .NET compiler platform under the Apache License 2.0, is a next-generation set of compilers for C# and VB.NET written entirely as managed code. It provides access to the compiler as a service and includes code analysis APIs allowing developers to access information from the compiler that was previously treated as a black box, for example syntactic and semantic models. The most immediate impact should be seen in enhancements to .NET IDEs through refactoring and code generation tools. We also expect to see improved code diagnostics and static analysis, although it will be interesting to see what the community comes up with. Meanwhile Xamarin has a Mono-compatible copy of Roslyn source code hosted on GitHub and plans to bundle Roslyn’s compilers with Mono as it stabilizes, in addition to integrating the best parts into their code base.","blip_selector":"roslyn","name":"Roslyn","url":"/radar/tools/roslyn","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":741,"quadrant":"tools","volume_date":"2015-01","description":"With techniques such as continuous delivery becoming more mainstream, automated database migrations are a baseline capability for many software teams. While there are many tools in this space, we continue to recommend Flyway for its low-friction approach. Flyway has a vibrant open-source community behind it, and support for both traditional and cloud-based databases such as Amazon Redshift and Google Cloud SQL. | Automated database migrations are common on agile projects, and we are happy to see advances in the tools for this space. Flyway makes it as painless as possible to automate changes to databases. While not as feature-rich as some competing tools, we have used it on multiple projects and appreciate its low friction.","blip_selector":"flyway","name":"Flyway","url":"/radar/tools/flyway","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":748,"quadrant":"platforms","volume_date":"2015-01","description":"iBeacons are the Apple implementation of the broader category of beacons, which are small devices that use low energy Bluetooth (BLE) to provide fine-grained proximity information for mobile phones and other devices. Despite the hype surrounding iBeacons and the limitations to the accuracy and reliability of the information they provide, we do feel that they open interesting opportunities as trigger points for interacting with your users in a contextually relevant manner. | iBeacons are the Apple implementation of the broader category of beacons, which are small devices that use low energy Bluetooth (BLE) to provide fine-grained proximity information for mobile phones and other devices. Despite the hype surrounding iBeacons and the limitations to the accuracy and reliability of the information they provide, we do feel that they open interesting opportunities as trigger points for interacting with your users in a contextually relevant manner.","blip_selector":"ibeacon","name":"iBeacon","url":"/radar/platforms/ibeacon","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":752,"quadrant":"techniques","volume_date":"2015-01","description":"A living CSS style guide is a page on your site that uses your current CSS styles and acts as a reference for all the currently available visual elements and design patterns. This helps to tightly integrate design into your delivery process by promoting co-ownership of the UI and avoids duplication of styling across your application. Styling changes are visible in the guide immediately and changes propagate across your site from a central location. A sensible way to do this is with a well organized SASS/LESS file structure with semantically named elements that separates structure, aesthetics, and interaction. | A living CSS style guide is a page on your site that uses your current CSS styles and acts as a reference for all the currently available visual elements and design patterns. This helps to tightly integrate design into your delivery process by promoting co-ownership of the UI and avoids duplication of styling across your application. Styling changes are visible in the guide immediately and changes propagate across your site from a central location. A sensible way to do this is with a well organized SASS/LESS file structure with semantically named elements that separates structure, aesthetics, and interaction.","blip_selector":"living-css-style-guides","name":"Living CSS Style Guides","url":"/radar/techniques/living-css-style-guides","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":747,"quadrant":"techniques","volume_date":"2015-01","description":"A Microservice architecture by its very nature increases significantly the number of applications, services, and interactions in your deployed environments. Our projects are showing renewed focus on building Humane Registries which aggregate information about running services from the live environment and present it in a form for humans to comprehend. These registries favor up-to-date information from running services instead of human-curated documentation. | A Microservice architecture by its very nature increases significantly the number of applications, services, and interactions in your deployed environments. Our projects are showing renewed focus on building Humane Registries which aggregate information about running services from the live environment and present it in a form for humans to comprehend. These registries favor up-to-date information from running services instead of human-curated documentation.","blip_selector":"humane-registry","name":"Humane registry","url":"/radar/techniques/humane-registry","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":813,"quadrant":"tools","volume_date":"2015-01","description":"GorillaREPL is a tool for creating nicely-rendered documents consisting of text, live Clojure code, and plots.  In some ways similar to iPython notebooks, GorillaREPL should be particularly useful for data analysts or code tutorials.  But beyond that, GorillaREPL is fun!.  It is a creative way to demonstrate the power of Clojure’s simple abstractions over immutable values.","blip_selector":"gorilla-repl","name":"Gorilla REPL","url":"/radar/tools/gorilla-repl","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":761,"quadrant":"tools","volume_date":"2015-01","description":"Consumer-Driven Contracts are a testing approach to help service interfaces evolve with confidence without unknowingly breaking consumers. The similarly named Pact and Pacto are two new open-source tools which allow testing interactions between service providers and consumers in isolation against a contract. Both have grown out of projects which are building RESTful microservices and show great promise. | Consumer-Driven Contracts are a testing approach to help service interfaces evolve with confidence without unknowingly breaking consumers. The similarly named Pact and Pacto are two new open-source tools which allow testing interactions between service providers and consumers in isolation against a contract. Both have grown out of projects which are building RESTful microservices and show great promise.","blip_selector":"pact-pacto","name":"Pact & Pacto","url":"/radar/tools/pact-pacto","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":767,"quadrant":"languages-and-frameworks","volume_date":"2015-01","description":"Q is a fully Promises/A+ compliant implementation in JavaScript that lets users compose promises arbitrarily deeply without the need for the deeply nested callbacks that obscure control flow. Q takes care of threading fulfilled values and rejected promises through the appropriate code paths. The space of Promises/A+ compliant libraries is currently very active with alternatives like Bluebird also rapidly gaining mindshare. | Q is a fully Promises/A+ compliant implementation in JavaScript that lets users compose promises arbitrarily deeply without the need for the deeply nested callbacks that obscure control flow. Q takes care of threading fulfilled values and rejected promises through the appropriate code paths. The space of Promises/A+ compliant libraries is currently very active with alternatives like Bluebird also rapidly gaining mindshare.","blip_selector":"q-bluebird","name":"Q & Bluebird","url":"/radar/languages-and-frameworks/q-bluebird","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":749,"quadrant":"techniques","volume_date":"2015-01","description":"Conway's Law asserts that organizations are constrained to produce application designs which are copies of their communication structures. This often leads to unintended friction points. The 'Inverse Conway Maneuver' recommends evolving your team and organizational structure to promote your desired architecture. Ideally your technology architecture will display isomorphism with your business architecture. | Conway's Law asserts that organizations are constrained to produce application designs which are copies of their communication structures. This often leads to unintended friction points. The 'Inverse Conway Maneuver' recommends evolving your team and organizational structure to promote your desired architecture. Ideally your technology architecture will display isomorphism with your business architecture.","blip_selector":"inverse-conway-maneuver","name":"Inverse Conway Maneuver","url":"/radar/techniques/inverse-conway-maneuver","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":728,"quadrant":"tools","volume_date":"2015-01","description":"Mobile test automation is becoming increasingly important. Appium is a test automation framework that can test mobile web, mobile native and mobile hybrid applications on iOS and Android. We have used this in multiple projects and have seen significant gains. At the core, Appium is a webserver that exposes a REST API, receiving connections from a client, listening for commands, executing those commands on a mobile device and responding with an HTTP response representing the result of the command execution. It allows tests to be written against iOS and Android using the same API. Appium is open source with easy setup using npm. | Mobile test automation is becoming increasingly important. Appium is a test automation framework which can be used to test mobile web, mobile native and mobile hybrid applications on iOS and Android. At the core, Appium is a webserver that exposes a REST API, receiving connections from a client, listening for commands, executing those commands on a mobile device and responding with an HTTP response representing the result of the command execution. It allows tests to be written against multiple platforms (iOS, Android) using the same API. Appium is open source with easy set up using npm.","blip_selector":"appium","name":"Appium","url":"/radar/tools/appium","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":751,"quadrant":"tools","volume_date":"2015-01","description":"Leaflet.js is a JavaScript library for mobile-friendly interactive maps. The library places a huge emphasis on performance, usability and simplicity, and as such works efficiently across mobile platforms and desktop browsers. It is a viable library to consider when building interactive maps for mobiles. | Leaflet.js is a JavaScript library for mobile-friendly interactive maps. The library places a huge emphasis on performance, usability and simplicity, and as such works efficiently across mobile platforms and desktop browsers. It is a viable library to consider when building interactive maps for mobiles.","blip_selector":"leaflet-js","name":"leaflet.js","url":"/radar/tools/leaflet-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":757,"quadrant":"platforms","volume_date":"2015-01","description":"OpenID Connect is a standard protocol for federated identity built on OAuth 2.0. It addresses a long-standing need for a simple, web-based protocol to exchange trusted authentication and authorization information. Previous standards like SAML or generic OAuth 2.0 have proven too broad and complex to ensure universal compatibility. Our hope is that OpenID Connect can provide a useful basis for secure access to RESTful microservices with authenticated end-user identity. | OpenID Connect is a standard protocol for federated identity built on OAuth 2.0. It addresses a long-standing need for a simple, web-based protocol to exchange trusted authentication and authorization information. Previous standards like SAML or generic OAuth 2.0 have proven too broad and complex to ensure universal compatibility. Our hope is that OpenID Connect can provide a useful basis for secure access to RESTful microservices with authenticated end-user identity.","blip_selector":"openid-connect","name":"OpenID Connect","url":"/radar/platforms/openid-connect","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":726,"quadrant":"tools","volume_date":"2015-01","description":"We mentioned Thoughtworks' SnapCI -- a hosted service that provides deployment pipelines -- on the last edition of the Radar. Since then, we have seen many teams successfully use SnapCI on their projects. If you need a simple continuous delivery solution in the cloud, SnapCI can provide it with one click. No hardware, no hassle. | We mentioned Thoughtworks' SnapCI - a hosted service that provides deployment pipelines - on the last edition of the Radar. Since then, we have seen many teams successfully use SnapCI on their projects. If you need a simple continuous delivery solution in the cloud, SnapCI can provide it with one click. No hardware, no hassle.","blip_selector":"snapci","name":"SnapCI","url":"/radar/tools/snapci","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":768,"quadrant":"languages-and-frameworks","volume_date":"2015-01","description":"R is traditionally used as stand alone analysis tool by research teams. With improvements in packages like Rook and RJSONIO, it has become trivial to wrap the computational logic and expose it as an API. Thoughtworks teams are using R as Compute platform to crunch large datasets in real time, using in-memory storage integrated with enterprise systems. | R is traditionally used as stand alone analysis tool by research teams. With improvements in packages like Rook and RJSONIO, it has become trivial to wrap the computational logic and expose it as an API. Thoughtworks teams are using R as Compute platform to crunch large datasets in real time, using in-memory storage integrated with enterprise systems.","blip_selector":"r-as-compute-platform","name":"R as Compute Platform","url":"/radar/languages-and-frameworks/r-as-compute-platform","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":762,"quadrant":"tools","volume_date":"2015-01","description":"In the previous radar, we mentioned the log aggregation service Papertrail as a way to collect and analyze logs from a variety of sources including web servers, routers, databases and PaaS services. Our subsequent experiences using it and the integrations from PaaS providers such as Heroku nudge it into something we would happily recommend as a convenient and expedient option, notwithstanding our concerns about widespread adoption of services that centralize large quantities of data aggregated from multiple parties. | Papertrail is a log aggregation service that aggregates data from a variety of sources including web-servers, routers, databases and PaaS services. In addition to aggregation it provides search, filtering, and alerts and notifications out of the box. While undeniably convenient and expedient in many cases, we remain concerned about widespread adoption of services that centralize large quantities of data aggregated from multiple parties.","blip_selector":"papertrail","name":"Papertrail","url":"/radar/tools/papertrail","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":750,"quadrant":"languages-and-frameworks","volume_date":"2015-01","description":"The team behind Java 8 had to fight two battles: the community forces encouraging forever backwards compatibility (a long hallmark of Java) and the technical challenge of making a deep language change mesh with existing libraries and features. They succeeded on both fronts, breathing new life into the Java Language and placing it on par with other mainstream languages in terms of functional programming features. In particular, Java 8 has excellent syntactic magic that allows seamless interoperability between Lambda blocks, the new higher-order function feature, and SAM (Single Abstract Method) interfaces, the traditional way of passing behavior. | The team behind Java 8 had to fight two battles: the community forces encouraging forever backwards compatibility (a long hallmark of Java) and the technical challenge of making a deep language change mesh with existing libraries and features. They succeeded on both fronts, breathing new life into the Java Language and placing it on par with other mainstream languages in terms of functional programming features. In particular, Java 8 has excellent syntactic magic that allows seamless interoperability between Lambda blocks, the new higher-order function feature, and SAM (Single Abstract Method) interfaces, the traditional way of passing behavior.","blip_selector":"java-8","name":"Java 8","url":"/radar/languages-and-frameworks/java-8","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":745,"quadrant":"tools","volume_date":"2015-01","description":"In the last radar we called out Gulp as a strong competitor to Grunt, with a clean API and fast builds thanks to its streaming approach. While we still like it as a tool, we are moving it out from trial back to assess since Grunt has both a broader usage model and better industry adoption and tooling. We do see some teams successfully using Gulp inside Grunt, when the speed of intermediate result caching is required, but we are not recommending it as the default JavaScript build tool. | Gulp is an alternative to Grunt. It is a command-line task automation tool that helps developers with SaaS compilation, autoprefixing, minification, concatenation and so on. Gulp's central idea is the use of streams, and its plugins are designed to do only one task.","blip_selector":"gulp","name":"Gulp","url":"/radar/tools/gulp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":786,"quadrant":"techniques","volume_date":"2015-01","description":"Static site generators like Middleman or Jekyll have become popular for creating simple websites or blogs, but we are increasingly seeing their use as part of more complex application stacks. The default assumption that all content delivered over HTTP has to be dynamically created on request is shifting, with more teams looking to use static pre-generated content.","blip_selector":"static-site-generators","name":"Static site generators","url":"/radar/techniques/static-site-generators","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":684,"quadrant":"languages-and-frameworks","volume_date":"2015-01","description":"The Go language gradually changed status from \"Just Another Language\" to a valuable tool in many projects. While steadfastly single paradigm in a world of increasingly complex languages, it seems to keep a nice balance between expressiveness, power, and simplicity. | The Go language gradually changed status from \"Just Another Language\" to a valuable tool in many projects. While steadfastly single paradigm in a world of increasingly complex languages, it seems to keep a nice balance between expressiveness, power, and simplicity. | The Go language was originally developed by Google as a system programming language to replace C & C++. Four years out, Go is gaining traction in other areas. The combination of very small, statically linked binaries combined with an excellent HTTP library means Go has been popular with organizations making use of finer-grained, microservice architectures.","blip_selector":"go-language","name":"Go language","url":"/radar/languages-and-frameworks/go-language","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":754,"quadrant":"platforms","volume_date":"2015-01","description":"Mapbox is an open mapping platform we have used on several projects. It allows a developer to quickly add a map to an application and to style the map. Mapbox can serve as an alternative to conventional mapping platforms, and it also allows for mobile friendly maps. | Mapbox is an open mapping platform we have used on several projects. It allows a developer to quickly add a map to an application and to style the map. Mapbox can serve as an alternative to conventional mapping platforms, and it also allows for mobile friendly maps.","blip_selector":"mapbox","name":"Mapbox","url":"/radar/platforms/mapbox","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":723,"quadrant":"tools","volume_date":"2015-01","description":"Grunt is rapidly becoming the de facto JavaScript build tool with high adoption and a growing ecosystem. While slower than newer alternatives, such as Gulp, in terms of file processing, Grunt covers a broader set of build-related activities, has a proliferation of plugins and makes it easy to author and publish self-written plugins to npm. | We have seen a growth in the Grunt ecosystem and it is currently being used in several of our projects. With the proliferation of plugins and the ease to author and publish self-written plugins to npm, automation using Grunt can be done with little effort. We suggest choosing a task runner that best meets the needs of the project and Grunt is one of the task runners you should consider. | Several of our Thoughtworks teams developing Node.js apps are using Grunt to automate most of the development activities like minification, compilation, and linting. Many of the common tasks are available as Grunt plugins. You can even programmatically generate the configuration if necessary.","blip_selector":"grunt-js","name":"Grunt.js","url":"/radar/tools/grunt-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":742,"quadrant":"tools","volume_date":"2015-01","description":"The big Cloud providers have clearly raised the bar for provisioning, monitoring, and configuration, simplifying these tasks dramatically through powerful tools. Organizations that want to keep their compute and storage resources in-house are looking for similar solutions that work within their organizational context. Foreman has worked really well for us, and it is open-source software, too. | The big Cloud providers have clearly raised the bar for provisioning, monitoring, and configuration, simplifying these tasks dramatically through powerful tools. Organizations that want to keep their compute and storage resources in-house are looking for similar solutions that work within their organizational context. Foreman has worked really well for us, and it is open-source software, too.","blip_selector":"foreman","name":"Foreman","url":"/radar/tools/foreman","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":753,"quadrant":"techniques","volume_date":"2015-01","description":"Many deployments requires machine images for different server roles like applications and services, databases, and reverse proxies. Because building a machine image from scratch, using an operating system ISO and provisioning scripts, can take a considerable amount of time it can be useful to create a build pipeline for machine images. The first stage in the pipeline sets up a base image according to general standards in the organization. Subsequent stages can then enhance the base image for different purposes. If several applications or services have similar requirements, an application server for example, the pipeline can be extended by an intermediate stage, which takes the base image and provides an image with an application server but no application/service. These pipelines are not linear, they are trees that are branching out from the base image. | Many deployments require machine images for different server roles like applications and services, databases, and reverse proxies. Because building a machine image from scratch, using an operating system ISO and provisioning scripts, can take a considerable amount of time it can be useful to create a build pipeline for machine images. The first stage in the pipeline sets up a base image according to general standards in the organization. Subsequent stages can then enhance the base image for different purposes. If several applications or services have similar requirements, an application server for example, the pipeline can be extended by an intermediate stage, which takes the base image and provides an image with an application server but no application/service. These pipelines are not linear, they are trees that are branching out from the base image.","blip_selector":"machine-image-pipelines","name":"Machine image pipelines","url":"/radar/techniques/machine-image-pipelines","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":778,"quadrant":"techniques","volume_date":"2015-01","description":"We continue to see organizations create separate Development and QA teams. Fast feedback is a core tenet of Agile and critical to the success of a project. Using a separate QA team slows down this feedback, creates an \"us and them\" mentality and makes it more difficult to build quality into the software. Testing should be a tightly integrated activity and isn't something the team can outsource. We recommend integrated teams where testers work closely with developers instead of having testing as a separate organization. | We continue to see organizations create separate Development and QA teams. Fast feedback is a core tenet of Agile and critical to the success of a project. Using a separate QA team slows down this feedback, creates an \"us and them\" mentality and makes it more difficult to build quality into the software. Testing should be a tightly integrated activity and isn't something the team can outsource. We recommend integrated teams where testers work closely with developers instead of having testing as a separate organization.","blip_selector":"testing-as-a-separate-organization","name":"Testing as a separate organization","url":"/radar/techniques/testing-as-a-separate-organization","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":812,"quadrant":"tools","volume_date":"2015-01","description":"We have recommended D3.js before and in this radar we want to extend our recommendation to Dc.js, a charting library based on D3 for exploring large multi-dimensional datasets. With D3, it shares the ease with which beautiful interactive graphs can be created. It is different in that it trades the flexibility to create almost any kind of data visualization for a simpler programming model to create common chart types.","blip_selector":"dc-js","name":"Dc.js","url":"/radar/tools/dc-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":763,"quadrant":"techniques","volume_date":"2015-01","description":"With the proliferation of complex JavaScript websites and applications, we are finding that browser-side issues such as slow network calls, poor render times, and JavaScript errors can have a big impact on user experience. Server-side monitoring clearly does not help in these scenarios and often these types of issues are being missed. Front end instrumentation extends the types of monitoring we are used to with server-side code to the browser, allowing for early warning of potential issues and far easier diagnosis should production issues occur. | With the proliferation of single-page JavaScript applications, we have found that slow Ajax calls, excessive DOM manipulation, and unexpected JavaScript errors in the browser can have a big impact on perceived website responsiveness. It is very useful to collect and aggregate this profiling information from real end-users' browsers. Real user monitoring provides early warning and diagnosis of production issues, and helps pinpoint them to a specific locality.","blip_selector":"front-end-instrumentation","name":"Front end instrumentation","url":"/radar/techniques/front-end-instrumentation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":738,"quadrant":"techniques","volume_date":"2014-07","description":"Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not only can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes. Complementary to the capture of business meaningful events, the technique has positive implications for analytics in driving greater customer insight. | Event sourcing is an approach to thinking about persistent data where the primary record is a log of all events that make updates. A traditional representation of database state can be entirely recreated by reprocessing this event log. Event sourcing’s benefits include strong auditing, creation of historic state, and replaying of events for debugging and analysis. Event sourcing has been around for a while, but we think it is used much less than it should be.","blip_selector":"event-sourcing","name":"Event Sourcing","url":"/radar/techniques/event-sourcing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":570,"quadrant":"platforms","volume_date":"2014-07","description":"In the last technology radar, we spoke about Vumi as a platform for using USSD as a UI for feature phones. Vumi has become very stable and its open source nature gives it appeal. In our projects, we have been able to integrate with telecommunication networks seamlessly and rapidly due to the simplicity of configuration. The platform is also readily available and scalable. | Vumi is a scalable open source messaging engine driving conversations through frugal methods on mobile devices. Vumi facilitates SMS, IM and USSD interactions between companies and their clients, health services and their patients, governments and citzens, and more. Vumi integrates with telcos and allows you to build apps on top of it easily. You only have to pay for carrier charges.","blip_selector":"vumi","name":"Vumi","url":"/radar/platforms/vumi","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":693,"quadrant":"platforms","volume_date":"2014-07","description":"Heterogeneous and overwhelmingly large amounts of data is not the only theme of big data. In certain circumstances, speed of processing can be as important as the volume. Storm is a distributed realtime computation system. It has similar scalability to Hadoop, with throughput as fast as a million tuples per second. It enables for real time processing what Hadoop does for batch.","blip_selector":"storm","name":"Storm","url":"/radar/platforms/storm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":724,"quadrant":"tools","volume_date":"2014-07","description":"Testing HTTP-based micro-services can be painful and tricky. Particularly in two scenarios, the consumption of a group of micro-services from front-end, and the communication between micro-services. To deal with these, Moco can be handy. It is a lightweight stub framework for testing HTTP-based endpoints. You can have an embedded stubbed service up and running with 2 lines of Java or Groovy code, or a standalone one with few lines of JSON to describe the required behavior.","blip_selector":"moco","name":"Moco","url":"/radar/tools/moco","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":643,"quadrant":"platforms","volume_date":"2014-07","description":"PostgreSQL is expanding to become the NoSQL choice of SQL databases. Version 9.2 includes the ability to store JSON data with full querying capabilities on the content of the JSON document. Other extensions let the user store and query data in the form of key/value pairs. This lets you take advantage of the underlying storage and transactional capabilities of a time-tested database without being tied to a relational data model. This is ideal for those who want both SQL and NoSQL applications but prefer a single reliable infrastructure that they already know how to support.","blip_selector":"postgresql-for-nosql","name":"PostgreSQL for NoSQL","url":"/radar/platforms/postgresql-for-nosql","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":257,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"Scala is a large language that is popular because of its approachability for new developers. This banquet of features is a problem because many aspects of Scala, like implicit conversions and dynamics, can get you into trouble. To successfully use Scala, you need to research the language and have a very strong opinion on which parts are right for you, creating your own definition of Scala, the good parts. You can disable the parts you do not want using a system called feature flags. | In the previous radar we had two JVMbased functional programming languages, Clojure and Scala, in our Assess category. We had expressed a slight preference for Clojure because it is the smaller and more focused language. Since the last radar we have realized that the wider applicability of Scala makes it more approachable for enterprise developers, and we have witnessed great successes in the adoption of Scala. Consequently we have moved Scala into our Trial category. Pay careful attention to the idiomatic use of Scala if it is introduced to a new team to avoid \"Java without semicolons\" or Perl styles. | The functional languages F#, Clojure and Scala still reside in the assess ring of the radar. Interest in functional languages continues to grow. Two characteristics of functional languages in particular are driving this interest, immutability with its implications for parallelism and functions as first class objects. While the introduction of closures to C# brings some of the latter capability, functional languages are almost synonymous with immutability. The placement of these languages within the assess ring indicates our view of their relative maturity and appropriateness. F#, based on OCaml, is fully supported within the Visual Studio toolset. F# includes support for objects and imperative constructs in addition to functional language constructs in a natural way. Scala, like F#, combines the object and functional paradigms, although the syntax of Scala is more Java-like. Clojure began as a JVM language and is now available on the .NET CLR. Clojure does allow for mutable state although it has an extensive set of immutable persistent data structures, all supporting multi-threaded applications. There are many similarities between these three languages, but at the moment we believe F# and Clojure to be better suited to most organizations for assessing than Scala. More work clearly needs to be done to validate this assertion. | In the previous radar, we lumped functional languages together in a group. For this version, we’ve exploded that group and started calling out the ones interesting to us. Of the current crop of functional languages, the one we like the most is Clojure: a simple, elegant implementation of Lisp on the JVM. The other two that we fi nd interesting are Scala (a re-thinking of Java in functional form) and F#, the OCaml derivative from Microsoft that now appears “in the box” in Visual Studio 2010.","blip_selector":"scala-the-good-parts","name":"Scala, the good parts","url":"/radar/languages-and-frameworks/scala-the-good-parts","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":712,"quadrant":"tools","volume_date":"2014-07","description":"Using Dependency management tools for JavaScript has helped our delivery teams handle large amounts of JavaScript by structuring their code and loading the dependencies at runtime. Though this simplified the effort in most cases, lazy loading complicates supporting offline mode. Different dependency management tools have different strengths, so choose based on your context.","blip_selector":"dependency-management-for-javascript","name":"Dependency management for JavaScript","url":"/radar/tools/dependency-management-for-javascript","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":690,"quadrant":"platforms","volume_date":"2014-07","description":"With the cost of industrial robots dropping and their safety and ease of use increasing, the world of useful, commercial robotics is opening up. Robots like Rethink Robotics' Baxter or Universal Robotics' U5, make it feasible for small to medium-sized businesses to automate repetitive tasks previously performed by humans. Increasingly, enterprise software will have to integrate with low-cost robotics as another participant in the value stream. The challenge lies in making the experience easy and productive for the human co-workers as well.","blip_selector":"low-cost-robotics","name":"Low-cost robotics","url":"/radar/platforms/low-cost-robotics","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":694,"quadrant":"platforms","volume_date":"2014-07","description":"In the previous radar we cautioned against the use of traditional web component frameworks that provide a component model on the server side. The Web Components standard that originated at Google is something quite different. It provides an easier way to create recyclable widgets by helping with encapsulation of HTML, CSS and JavaScript, so they do not interfere with the rest of the page and the page does not interfere with them. Developers can use as much or as little of the framework as needed. Early support is provided by the Polymer Project.","blip_selector":"web-components-standard","name":"Web Components standard","url":"/radar/platforms/web-components-standard","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":716,"quadrant":"tools","volume_date":"2014-07","description":"All development for iOS must be carried out on OS X. Due to technical and licensing restrictions running server farms with OS X is neither easy nor common. In spite of these difficulties, Travis CI, with support from Sauce Labs, now provides cloud-based continuous integration services for iOS and OS X projects.","blip_selector":"hosted-solutions-for-testing-ios","name":"Hosted solutions for testing iOS","url":"/radar/tools/hosted-solutions-for-testing-ios","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":701,"quadrant":"techniques","volume_date":"2014-07","description":"As the lines between hardware and software continue to blur, we see traditional computing increasingly embedded in everyday objects. Although connected devices are now ubiquitous in retail spaces, automobiles, homes, and workplaces, we still do not understand how to blend them into a useful computing experience that goes beyond a simple glass screen. Tangible interaction is a discipline that blends software and hardware technology, architecture, user experience, and industrial design. The goal is to provide natural environments made up of physical objects where humans can manipulate and understand digital data.","blip_selector":"tangible-interaction","name":"Tangible interaction","url":"/radar/techniques/tangible-interaction","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":766,"quadrant":"tools","volume_date":"2014-07","description":"Protractor is a testing framework based on Jasmine that wraps WebDriverJS with functionality specifically designed to execute end-to-end tests for Angular.JS applications. We've found it to be a standout in the rapidly evolving space of JavaScript testing frameworks. Despite being designed to run end-to-end tests with a real backend, Protractor tests can also be made to work with a stubbed HTTP gateway to run purely client side tests.","blip_selector":"protractor-for-angularjs","name":"Protractor for AngularJS","url":"/radar/tools/protractor-for-angularjs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":307,"quadrant":"tools","volume_date":"2014-07","description":"We continue to see teams run into productivity problems attempting to use TFS as a version control system. Teams that want to practice frequent code check-ins, a core part of continuous integration, have found its heavyweight approach significantly drains productivity. This often leads to teams checking in less frequently, causing more problematic merges. We recommend tools such as Git, Perforce, and Subversion instead.","blip_selector":"tfs","name":"TFS","url":"/radar/tools/tfs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":685,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"Hive is a data warehouse built on top of Hadoop which provides a SQL-like query and data definition language. Hive converts queries into MapReduce jobs that can be run across the entire Hadoop cluster. Like all useful abstractions, Hive does not try to deny the existence of the underlying mechanics of Hadoop and supports custom map-reduce operations as a powerful extension mechanism. Despite the superficial similarities to SQL, Hive does not try to be a replacement for low-latency, real-time query engines found on relational database systems. We strongly advise against using Hive for online ad-hoc querying purposes.","blip_selector":"hive","name":"Hive","url":"/radar/languages-and-frameworks/hive","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":730,"quadrant":"tools","volume_date":"2014-07","description":"CartoDB is an open-source GIS tool built on PostGIS and PostgreSQL. It allows for storage and searching of geospatial data using SQL. It also provides a handy JavaScript library, CartoDB.js, for map styling and data visualization.","blip_selector":"cartodb","name":"CartoDB","url":"/radar/tools/cartodb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":703,"quadrant":"techniques","volume_date":"2014-07","description":"Barely a week goes by without the IT industry being embarrassed by yet another high profile loss of data, leak of passwords, or breach of a supposedly secure system. There are good resources to help with making sure security gets treated as a first-class concern during software development and we need to stop ignoring them; the OWASP Top 10 is a good place to start.","blip_selector":"ignoring-owasp-top-10","name":"Ignoring OWASP Top 10","url":"/radar/techniques/ignoring-owasp-top-10","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":594,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"Along with JavaScript and HTML, CSS is a core technology for creating websites. Unfortunately, the language itself lacks key features, which leads to a high level of duplication and a lack of meaningful abstractions. While CSS3 aims to rectify some of these issues, it will be years before the modules that make up CSS3 will be properly supported in most browsers. Fortunately, there is a solution today using CSS preprocessors like SASS and LESS. Due to their quality and support, we believe that the days of handwritten CSS, for anything apart from trivial work, are over.","blip_selector":"handwritten-css","name":"Handwritten CSS","url":"/radar/languages-and-frameworks/handwritten-css","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":709,"quadrant":"techniques","volume_date":"2014-07","description":"Chef and Puppet servers are a central place to store recipes/manifests that propagate configuration changes to managed machines. They are also a central database of node information and provide access control for manifests/recipes. The disadvantage of having these servers is that they become a bottleneck when multiple clients simultaneously connect to them. They are a single point of failure and take effort to be robust and reliable. In light of this, we recommend chef-solo or standalone puppet in conjunction with a version control system when the server is primarily used to store recipes/manifests. Teams can always introduce the servers as the need arises or if they find themselves reinventing solutions to the problems the servers have already solved.","blip_selector":"masterless-chef-puppet","name":"Masterless Chef/Puppet","url":"/radar/techniques/masterless-chef-puppet","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":720,"quadrant":"tools","volume_date":"2014-07","description":"Since first featuring Ansible in the last radar, we continue to be impressed with its capabilities and ease of use compared to other offerings in this space. Based on our experiences over the last year we have no hesitation in recommending Ansible as a great option for automated control of your infrastructure. | In the category of DevOps orchestration engines, Ansible has nearly universal acclaim within Thoughtworks projects. It has useful tools and abstractions at a useful level of granularity.","blip_selector":"ansible","name":"Ansible","url":"/radar/tools/ansible","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":725,"quadrant":"tools","volume_date":"2014-07","description":"We have long favored the use of hand-drawn, low fidelity prototypes to illustrate user interactions without getting caught up in the nitty-gritty of the graphic design. Prototype On Paper is a tool that allows individual mockups drawn on paper to be captured via camera on iOS or Android and linked together to allow for testing of user interaction. This bridges the gap nicely between the static, lo-fi paper prototypes and more hi-fi prototyping techniques.","blip_selector":"prototype-on-paper","name":"Prototype On Paper","url":"/radar/tools/prototype-on-paper","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":765,"quadrant":"techniques","volume_date":"2014-07","description":"We value unit testing on projects and we like techniques such as property-based unit testing which augment it. This is a practice of using data generators to create randomized inputs within defined ranges. It allows a quick check for boundary conditions and other unanticipated failure modes and has burgeoning support on multiple platforms.","blip_selector":"property-based-unit-testing","name":"Property-based unit testing","url":"/radar/techniques/property-based-unit-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":600,"quadrant":"platforms","volume_date":"2014-07","description":"The gap between what \"enterprise-class\" commercial packages provide and what is actually needed is widening. This is especially true for internet facing applications. Innovative solutions that really scale and easily support modern techniques such as continuous delivery are written by practitioners for practitioners. They originate with many internet scale companies and are refined as open source software. Big enterprise solutions often obstruct effective delivery due to their accumulated bloat, cumbersome licensing restrictions, and feature sets that are driven by check-lists and imaginary requirements far removed from the realities of most development teams.","blip_selector":"big-enterprise-solutions","name":"Big enterprise solutions","url":"/radar/platforms/big-enterprise-solutions","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":691,"quadrant":"platforms","volume_date":"2014-07","description":"The number and maturity of on-premise private cloud options continue to increase. From OpenStack-based options like Rackspace's private cloud to PAAS options like CloudFoundry, for those organizations seeking to make use of existing infrastructure or for whom an increased level of control is needed over off-premise cloud, then these solutions are well worth a look. | The need for physically storing data within nations or organizations has increased significantly in recent years. There is concern around sensitivity of information hosted in cloud environments. Organizations are looking into private cloud as an alternative when data that needs to be housed in close proximity with control over access and distribution. Private cloud offers cloud infrastructure provisioned for exclusive use by a single organization with the following characteristics; on-demand self-service, broad network access, resource pooling, rapid elasticity and measured service. | Because of concerns over privacy and security, or a need to repurpose existing hardware investments, many businesses are choosing to implement their own private cloud. There are are a variety of products, both open source and commercial for this purpose, but it should be noted that compute, storage, and network management are only the starting points for a useful private cloud. There are many services and processes that must be custom implemented to provide a cloud facility that rivals the public offerings from Amazon, Rackspace, or others.","blip_selector":"private-clouds","name":"Private Clouds","url":"/radar/platforms/private-clouds","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":722,"quadrant":"tools","volume_date":"2014-07","description":"Following our recommendation in the last radar to consider a focus on reducing mean time to recovery, we want to highlight Chaos Monkey from Netflix's Simian Army suite. It is a tool that randomly disables instances in the production environment during normal operation. When run with comprehensive monitoring and a team on stand by, it helps to uncover unexpected weaknesses in the system, which in turn allows the development team to build automatic recovery mechanisms ahead of time, rather than struggling to respond to an outage that caught everyone by surprise.","blip_selector":"chaos-monkey","name":"Chaos Monkey","url":"/radar/tools/chaos-monkey","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":710,"quadrant":"techniques","volume_date":"2014-07","description":"Virtualization and Cloud Computing have made it easy to procure and provision hardware and virtual servers. But with this flexibility comes scale and complexity, and managing our virtual estates has become increasingly difficult. Using techniques more familiar in the software development world such as TDD, BDD and CI offers an approach to managing this complexity and gives us the confidence to make changes to our infrastructure in a safe, repeatable and automatable manner. Provisioning testing tools, like rspec-puppet, Test Kitchen and serverspec, are available for most platforms.","blip_selector":"provisioning-testing","name":"Provisioning testing","url":"/radar/techniques/provisioning-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":625,"quadrant":"techniques","volume_date":"2014-07","description":"Many of our teams are getting great benefit from publishing virtual machine images as a build artifact during their automated build processes. These machine images are published with the application and all dependencies, often in an immutable state. With minimal additional configuration the image can be used to create identical virtual machines in all environments eliminating many common sources of error and waste. Tools are emerging to make this approach simpler, for example Packer in the tools section of the Radar. This approach is working well in companies that take a mature approach to cloud and virtualization, and where delivery teams have responsibility and access right through to production. | Most virtualization technologies provide a way to launch a machine from an image. By creating a machine image as a build artifact early in your build pipeline and promoting it through the pipeline as it passes further suites of tests, you can reliably deploy the exact machine that passed the tests into production. This technique eliminates most causes of the snowflake server anti-pattern.","blip_selector":"machine-image-as-a-build-artifact","name":"Machine image as a build artifact","url":"/radar/techniques/machine-image-as-a-build-artifact","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":677,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"Julia is a dynamic, procedural and homoiconic programming language designed to address the needs of high performance scientific computing. The implementation of the language is organized around the concept of generic functions and dynamic method dispatch. Julia programs are largely functions that can contain multiple definitions for different combinations of argument types. The combination of these language features and the LLVM based just-in-time compiler help Julia achieve a high level of performance. Julia also supports a multiprocessing environment based on message passing to allow programs to run on multiple processes. This enables programmers to create distributed programs based on any of the models for parallel programming.","blip_selector":"julia","name":"Julia","url":"/radar/languages-and-frameworks/julia","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":574,"quadrant":"techniques","volume_date":"2014-07","description":"Technology trends have broken down the garden walls that used to surround corporate IT networks and lead to a perimeterless enterprise. Employees frequently use their own consumer devices to access corporate data through cloud services and web APIs, often without the organization's knowledge. As devices continue to proliferate and more applications move to the cloud, businesses are being forced to rethink fundamental assumptions about data access and network security.","blip_selector":"perimeterless-enterprise","name":"Perimeterless enterprise","url":"/radar/techniques/perimeterless-enterprise","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":678,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"PowerShell remains a widely used option for doing low-level automation on Windows machines. Pester is a testing library that makes it possible to execute and validate PowerShell commands. Pester simplifies testing of scripts during development with a powerful mocking system that makes it possible to setup stubs and doubles in tests. Pester tests can also be integrated into a continuous integration system to prevent regression defects.","blip_selector":"pester","name":"Pester","url":"/radar/languages-and-frameworks/pester","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":695,"quadrant":"platforms","volume_date":"2014-07","description":"While centralized integration of data for analysis and reporting remains a good strategy, traditional Enterprise Data Warehouse (EDW) initiatives have a higher than 50% failure rate. Big up-front data modeling results in overbuilt warehouses that take years to deliver and are expensive to maintain. We are placing these old-style EDWs and techniques on hold in this edition of the radar. Instead, we advocate evolving towards an EDW. Test and learn by building small, valuable increments that are frequently released to production. Nontraditional tools and techniques can help, for example using a Data Vault schema design or even a NoSQL document store such as HDFS.","blip_selector":"enterprise-data-warehouse","name":"Enterprise Data Warehouse","url":"/radar/platforms/enterprise-data-warehouse","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":700,"quadrant":"techniques","volume_date":"2014-07","description":"The reduction in cost, size, power consumption and simplicity of physical devices has led to an explosion in devices that open physical domains to software. These devices often contain little more than a sensor and a communication component like Bluetooth Low Energy or WiFi. As software engineers, we need to expand our thinking to include bridging physical and digital worlds with simple hardware. We are already seeing this in the car, the home, the human body, agriculture and other physical environments. The cost and time required to prototype such devices is shrinking to match the fast iterations possible in software.","blip_selector":"bridging-physical-and-digital-worlds-with-simple-hardware","name":"Bridging physical and digital worlds with simple hardware","url":"/radar/techniques/bridging-physical-and-digital-worlds-with-simple-hardware","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":519,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"Dropwizard is an opinionated combination of several lightweight Java tools and frameworks, many of which would merit mention in their own right. The package embodies many of our favorite techniques, including an embedded HTTP server, support for RESTful endpoints, built-in operational metrics and health-checks, and straightforward deployments. Dropwizard makes it easy to do the right thing, allowing you to concentrate on the essential complexity of a problem rather than the plumbing.","blip_selector":"dropwizard","name":"Dropwizard","url":"/radar/languages-and-frameworks/dropwizard","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":740,"quadrant":"tools","volume_date":"2014-07","description":"When using techniques such as 'instrument all the things' and semantic logging, you may end up with huge amount of log data. Collecting, aggregating and moving this data can be problematic. Flume is a distributed system for exactly this purpose. It has a flexible architecture based on streaming data flows. With built-in support for HDFS, Flume can easily move multi-terabyte log data from many different sources to a centralized data store for further processing.","blip_selector":"flume","name":"Flume","url":"/radar/tools/flume","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":680,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"After some delays, mainly caused by patent claims from Apple, the W3C has now finalized the Touch Events recommendation. However, in the meantime, Pointer Events, a newer, broader, and richer standard, is picking up momentum. We recommend considering Pointer Events for HTML interfaces that must work across different input methods.","blip_selector":"pointer-events","name":"Pointer Events","url":"/radar/languages-and-frameworks/pointer-events","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":674,"quadrant":"platforms","volume_date":"2014-07","description":"Hadoop's initial architecture was based on the paradigm of scaling data horizontally and metadata vertically. While data storage and processing were handled by the slave nodes reasonably well, the masters that managed metadata were a single point of failure and limiting for web scale usage. Hadoop 2.0 has significantly re-architected both HDFS and the Map Reduce framework to address these issues. The HDFS namespace can be federated now using multiple name nodes on the same cluster and deployed in a HA mode. MapReduce has been replaced with YARN, which decouples cluster resource management from job state management and eliminates the scale/performance issues with the JobTracker. Most importantly, this change encourages deploying new distributed programming paradigms in addition to MapReduce on Hadoop clusters.","blip_selector":"hadoop-2-0","name":"Hadoop 2.0","url":"/radar/platforms/hadoop-2-0","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":698,"quadrant":"techniques","volume_date":"2014-07","description":"As client-side JavaScript applications grow in sophistication, we see an increased need for engineering sophistication to match. A common architectural flaw is unfettered access to the DOM from across the codebase - mixing DOM manipulation with application logic and AJAX calls. This makes the code difficult to understand and extend. Thinking about separation of concerns is a useful antidote, aggressively restricting all DOM access (usually jQuery usage) to a thin 'segregation layer'. One pleasant side-effect of this approach is that everything outside of the segregated DOM layer can be tested rapidly in isolation from the browser using a lean JavaScript engine such as node.js.","blip_selector":"segregated-dom-plus-node-for-js-testing","name":"Segregated DOM plus node for JS Testing","url":"/radar/techniques/segregated-dom-plus-node-for-js-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":774,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"Spray/akka-http is a suite of lightweight Scala libraries providing client/server RESTful support on top of Akka. It fully embraces the Actor-, Future-, and Stream-based programming models used by the underlying platform. This lets you work on RESTful applications with idiomatic Scala code without worrying about wrapping around other Java libraries.","blip_selector":"spray-akka-http","name":"Spray/akka-http","url":"/radar/languages-and-frameworks/spray-akka-http","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":737,"quadrant":"platforms","volume_date":"2014-07","description":"Espruino is a microcontroller that natively executes JavaScript and thus lets the large number of JavaScript programmers get started very quickly. Using an event-based model similar to Node.js, Espurino devices can be very power efficient while still being responsive. Less powerful than a Raspberry Pi and slightly slower than an Arduino, Espruino makes an interesting alternative in low-power environments that need responsive behavior but can sacrifice some of the raw high level features and execution speed of those platforms.","blip_selector":"espruino","name":"Espruino","url":"/radar/platforms/espruino","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":706,"quadrant":"techniques","volume_date":"2014-07","description":"When using techniques such as \"instrument all the things\" and semantic logging, it can be very useful to capture domain events explicitly. You can avoid having to infer user intent behind state transitions by modeling these transitions as first-class concerns. One method of achieving this outcome is to use an event sourced architecture with application events being mapped to business meaningful events.","blip_selector":"capture-domain-events-explicitly","name":"Capture domain events explicitly","url":"/radar/techniques/capture-domain-events-explicitly","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":719,"quadrant":"tools","volume_date":"2014-07","description":"We continue to see teams expend significant effort on un-maintainable Ant and Nant build scripts. These are hard to understand and extend due to the inherent lack of expressiveness and clean modularity provided by the tools. Alternatives like Gradle, Buildr, and PSake have clearly demonstrated superior maintainability and productivity.","blip_selector":"ant","name":"Ant","url":"/radar/tools/ant","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":669,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"The Play Framework 2 blip has generated many internal discussions. We had competing suggestions to move it to adopt and hold. These differences relate primarily to the specific applications for which it is used, how it is used, and what expectations people have for it. While none of these issues are unique for Play, Play has generated far more controversy than is typical in the standard library versus framework debate. We reiterate the cautions stated in the previous radar, and we will monitor how Play continues to mature to support its sweet spot. | The recent release of Play Framework 2.1.1 with support for controller dependency injection, asynchronous, non-blocking I/O, a code-reload workflow, database migrations, asset pipelining, and flexible deployment options has made it more attractive to developers. For this reason Play re-appears on the radar as something for teams to seriously consider when building web applications and services on the JVM. A word of caution however, Play embraces a functional programming style which, when working with the Java language, still translates into a plethora of static methods that may be difficult to unit test outside a running server.","blip_selector":"play-framework-2","name":"Play Framework 2","url":"/radar/languages-and-frameworks/play-framework-2","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":780,"quadrant":"languages-and-frameworks","volume_date":"2014-07","description":"We are intrigued by the possibilities offered by the Wolfram language. Building on the symbolic approaches of the Mathematica language it also has access to a vast array of algorithms and data from the Wolfram Alpha project, which means that very succinct programs can analyze and visualize powerful combinations of real-world data.","blip_selector":"wolfram-language","name":"Wolfram Language","url":"/radar/languages-and-frameworks/wolfram-language","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":633,"quadrant":"techniques","volume_date":"2014-07","description":"Development environments in the cloud allow you to entirely outsource development infrastructure, leaving your team with nothing more than laptops and an internet connection. By using a combination of best-of-breed services such as private GitHub repositories and Snap CI's continuous integration in the cloud, your teams may never need to bother in-house IT for infrastructure again.","blip_selector":"development-environments-in-the-cloud","name":"Development environments in the cloud","url":"/radar/techniques/development-environments-in-the-cloud","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":516,"quadrant":"languages-and-frameworks","volume_date":"2014-01","description":"Micro-frameworks are emerging as a way to handle increasing complexity in applications both on client- and server-side. Sinatra was one of the first examples of that trend in the server-side space, exposing a lightweight DSL to build fast services that can be easily composed. Similar offerings are available for other languages, including Spark for Java, Flask for Python, Sclatra for Scala, Compojure for Clojure and Nancy for .NET. | Micro-frameworks are emerging as a way to handle increasing complexity in applications both on client- and server-side. Sinatra was one of the early precursors of that trend in server-side space, exposing a lightweight DSL to build fast services that can be easily composed. Flask, Scalatra and Compojure are similar offerings for Python, Scala and Clojure respectively.","blip_selector":"sinatra","name":"Sinatra","url":"/radar/languages-and-frameworks/sinatra","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":687,"quadrant":"languages-and-frameworks","volume_date":"2014-01","description":"Until recently, Microsoft's Web API was the least-worst option for building a RESTful service using ASP.NET. Web API 2 fixes a number of rough edges with better support for flexible routing, sub-resources, media types and improved testability. It continues to be our preferred library for building .NET REST APIs.","blip_selector":"web-api","name":"Web API","url":"/radar/languages-and-frameworks/web-api","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":721,"quadrant":"tools","volume_date":"2014-01","description":"On mobile projects, we have been impressed with the functionality and gradually evolving capabilities and maturity of Calabash. It is an automated acceptance test tool for both Android and iOS applications that supports common ecosystem tools like Cucumber. It is an attractive choice on heterogeneous projects.","blip_selector":"calabash","name":"Calabash","url":"/radar/tools/calabash","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":508,"quadrant":"platforms","volume_date":"2014-01","description":"For problems that fit the document database model, MongoDB is now the most popular choice. In addition to ease of use and a solid technical implementation, the community and ecosystem contributed to this success. We are aware of problems where teams were tempted by the popularity of MongoDB when a document database was not a good fit or they did not understand the inherent complexity. When used appropriately, however, MongoDB has proven itself on many projects. | For problems that fit the document databases model, MongoDB provides easy programmability, a query interface, high availability with automated failover, and automated sharding capabilities. It allows for a smooth transition to NoSQL data stores from the RDBMS model, with the inclusion of familiar concepts, such as the ability to define indexes. | Document-oriented databases treat each record as a document with the ability to add any number of fields of arbitrary size. A relatively large amount of the attention that has been directed at document databases has landed on mongoDB, a highly scalable option with support for querying, indexing, replication and sharding. Beyond its enterprise feature set, its popularity is aided by its driver support for Java, Ruby, PHP, C#, Python and a number of other languages.","blip_selector":"mongodb","name":"MongoDB","url":"/radar/platforms/mongodb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":697,"quadrant":"platforms","volume_date":"2014-01","description":"We observe organizations that have piloted Hadoop successfully starting to consolidate their Hadoop infrastructure services into a centralized, managed platform before rolling it out across the enterprise.These Hadoop as a service platforms are characterized by the control tier that interfaces with and coordinates among different core Hadoop infrastructure components. The capabilities of the platform are usually exposed via higher-level abstractions to the enterprise. Such a managed platform gives organizations the ability to deploy processes, infrastructure and datasets in a fairly consistent way across the organization. These services are built in private data centers and public cloud infrastructure.","blip_selector":"hadoop-as-a-service","name":"Hadoop as a service","url":"/radar/platforms/hadoop-as-a-service","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":579,"quadrant":"tools","volume_date":"2014-01","description":"Many organizations that have moved to more agile ways of working continue to use heavyweight testing tools. These tools have problems that make them unsuitable for fast moving software delivery. Large complex tools have high learning curves and require specialist skills and training, making it hard for the team themselves to test. Often this results in an unnecessary overhead for every release as other teams get involved. Expensive and limiting software licenses makes this problem even worse. Some heavyweight tools use a 'model driven' approach where an attempt is made to accurately model the usage patterns of the application, which leads to costly test script maintenance and development time being lost to 'false positives'. We have seen few situations where simple open source solutions cannot give the required level of confidence for much less time, effort and money.","blip_selector":"heavyweight-test-tools","name":"Heavyweight test tools","url":"/radar/tools/heavyweight-test-tools","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":258,"quadrant":"languages-and-frameworks","volume_date":"2014-01","description":"Clojure is a dynamic, functional language that runs on the JVM. Although its roots are in Lisp, one of the oldest computer languages, it also embodies many modern programming concepts, including lazy evaluation and advanced concurrency abstractions. Clojure has spawned a vibrant community of programmers who are contributing a rich set of frameworks and tools. One example of these is Midje, an innovative spin on unit testing and mocking frameworks. | The functional languages F#, Clojure and Scala still reside in the assess ring of the radar. Interest in functional languages continues to grow. Two characteristics of functional languages in particular are driving this interest, immutability with its implications for parallelism and functions as first class objects. While the introduction of closures to C# brings some of the latter capability, functional languages are almost synonymous with immutability. The placement of these languages within the assess ring indicates our view of their relative maturity and appropriateness. F#, based on OCaml, is fully supported within the Visual Studio toolset. F# includes support for objects and imperative constructs in addition to functional language constructs in a natural way. Scala, like F#, combines the object and functional paradigms, although the syntax of Scala is more Java-like. Clojure began as a JVM language and is now available on the .NET CLR. Clojure does allow for mutable state although it has an extensive set of immutable persistent data structures, all supporting multi-threaded applications. There are many similarities between these three languages, but at the moment we believe F# and Clojure to be better suited to most organizations for assessing than Scala. More work clearly needs to be done to validate this assertion. | In the previous radar, we lumped functional languages together in a group. For this version, we’ve exploded that group and started calling out the ones interesting to us. Of the current crop of functional languages, the one we like the most is Clojure: a simple, elegant implementation of Lisp on the JVM. The other two that we fi nd interesting are Scala (a re-thinking of Java in functional form) and F#, the OCaml derivative from Microsoft that now appears “in the box” in Visual Studio 2010.","blip_selector":"clojure","name":"Clojure","url":"/radar/languages-and-frameworks/clojure","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":689,"quadrant":"platforms","volume_date":"2014-01","description":"The recent explosion of mobile-focused products, coupled with widespread adoption of 'Lean Start-up' approaches that put a premium on time-to-market for new ideas, has spawned an ecosystem of Backend as a service (BaaS) offerings that enable developers to focus on the client application while offloading backend concerns. Assess adding these services to your toolkit where fast and low-cost proving of a new product idea is important. Our usual advice on build/buy/borrow decisions still applies: be clear on which functional areas are strategic to your business and which are commodities. For potentially strategic areas be sure to plan a migration path that will allow you to use the BaaS provider to get started quickly, while avoiding friction when your architecture evolves and you need to migrate to owning this functionality and customizing it as a differentiator.","blip_selector":"backend-as-a-service","name":"Backend as a service","url":"/radar/platforms/backend-as-a-service","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":628,"quadrant":"techniques","volume_date":"2014-01","description":"For years, teams and organizations have seen the dangers of siloing expertise around technical disciplines. While we value input from experts on advanced applications, developers should have basic knowledge of user interfaces, databases, and data science, the newest industry darling. While advanced applications requires deep expertise, we are pushing for collaborative analytics and data science, where all developers use basic statistical analysis and tools to make better decisions, and work closely with experts when things get complicated.","blip_selector":"collaborative-analytics-and-data-science","name":"Collaborative analytics and data science","url":"/radar/techniques/collaborative-analytics-and-data-science","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":562,"quadrant":"platforms","volume_date":"2014-01","description":"58%  of all phones sold last year globally were feature phones. In many developing countries, this is an even larger majority. If your market requires you to develop for these areas, you need to develop with this constraint in mind. These phones use SMS and USSD as a user interface. SMS is a long standing technique for sending messages, and USSD allows you to send SMS like messages in a secure session.  You should look at USSD and SMS as another UI and UX platform and treat them as first-class citizens.","blip_selector":"sms-and-ussd-as-a-ui","name":"SMS and USSD as a UI","url":"/radar/platforms/sms-and-ussd-as-a-ui","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":554,"quadrant":"tools","volume_date":"2014-01","description":"Managing the web of dependencies in a distributed system is complicated, and is a problem more people are facing with the move to finer-grained microservices. Hystrix is a library for the JVM from Netflix that implements patterns for dealing with downstream failure, offers real-time monitoring of connections, and caching and batching mechanisms to make inter-service dependencies more efficient. In combination with hystrix-dashboard and Turbine, this tool can be used to build more resilient systems and provide near-real time data on throughput, latency and fault tolerance. | Managing dependencies in distributed systems can become complicated, and is a problem more people are facing with the move to finer-grained micro services. Hystrix is a library for  the JVM from Netflix that implements patterns for dealing with downstream failure, offers real-time monitoring of connections,and caching and batching mechanisms to make inter-servicedependencies more efficient.","blip_selector":"hystrix","name":"Hystrix","url":"/radar/tools/hystrix","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":560,"quadrant":"tools","volume_date":"2014-01","description":"Windows infrastructure automation should be adopted, however it still remains more difficult than automation on a Unix platform. Tools like Chef and Puppet are increasing their support, but there are also Windows specific solutions being developed like Octopus. Octopus allows automated deployment of your ASP.NET applications and Windows services and decreases dependency on PowerShell. It can be used with both NuGet using Octopak and TeamCity to create a full build, package, and deployment pipeline.","blip_selector":"octopus","name":"Octopus","url":"/radar/tools/octopus","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":667,"quadrant":"platforms","volume_date":"2014-01","description":"Over the past year we have seen a gradual uptake in the adoption of Elastic Search as an open source search platform. It is an extensible, multi-tenanted, and horizontally scalable search solution based on Apache Lucene. It allows complex data structures to be indexed and retrieved through a JSON based REST API. It provides an elegant model of operation with automatic discovery of peers in a cluster, failover, and replication. Elastic Search can be extended with a plugin system that allows adding new functionality and changing existing behavior. The community around this tool is quite vibrant as illustrated by the number of client libraries available in languages like Java, C#, Ruby, and JavaScript.","blip_selector":"elastic-search","name":"Elastic Search","url":"/radar/platforms/elastic-search","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":637,"quadrant":"languages-and-frameworks","volume_date":"2014-01","description":"One thing that has slowed the evolution of a rich, open source web development ecosystem on the .NET platform has been over-dependence on IIS and the ASP.NET framework.  OWIN specifies an open HTTP handling interface that decouples web server from application much like Rack has done for the Ruby community.  We are excited about OWIN because it opens up the possibility of new .NET web development tools composed of simple, independently-developed modules.  Nancy is the perfect example of this.  We also hope it will increase the practice of deploying web applications as standalone, self-hosted services on the .NET platform.","blip_selector":"owin","name":"OWIN","url":"/radar/languages-and-frameworks/owin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":688,"quadrant":"platforms","volume_date":"2014-01","description":"Akka is a toolkit and runtime for building highly concurrent, distributed, and fault tolerant event-driven applications on the JVM. It offers very lightweight event-driven processes with approximately 2.7 million actors per GB RAM and a 'let-it-crash'model of fault-tolerance designed to work in a distributed environment. Akka can be used as a library for web-apps or as a stand-alone kernel to drop an application into.","blip_selector":"akka","name":"Akka","url":"/radar/platforms/akka","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":622,"quadrant":"tools","volume_date":"2014-01","description":"With the rise of devices with multiple form factors and pixel densities, the issue of presenting high quality icons at all scales has become important. Icon fonts solve this problem by using browser support for WebFonts and SVG instead of scaled images or maintaining different icon sets. As always, when making extensive use of SVG, pay attention to power consumption on mobile devices and performance on older devices.","blip_selector":"icon-fonts","name":"Icon fonts","url":"/radar/tools/icon-fonts","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":534,"quadrant":"platforms","volume_date":"2014-01","description":"Redis has proven a useful tool on multiple Thoughtworks projects, used as both structured cache and data store distributed across multiple countries.","blip_selector":"redis","name":"Redis","url":"/radar/platforms/redis","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":500,"quadrant":"tools","volume_date":"2014-01","description":"D3 continues to gain traction as a library for creating rich visualisations in the browser. Previously, it was somewhat low-level, requiring more work for the creation of commonly used visualisations than less sophisticated, more targeted libraries. Since the last radar, libraries like Rickshaw for charting and Crossfilter for in-browser dataset exploration have helped make D3 even more accessible than before. | D3 is a JavaScript library for binding datasets into the DOM, and then declaratively transforming the document to create rich visualizations - ranging from graphs to heatmaps. With support for HTML, CSS and SVG, and an extensible plug-in model, we like the fact that this library allows us to deliver information in more intuitive ways.","blip_selector":"d3","name":"D3","url":"/radar/tools/d3","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":638,"quadrant":"tools","volume_date":"2014-01","description":"As the systems we build involve more fine-grained services spread across more machines than ever before, the challenge of how to get information aggregated to allow for easy problem identification and resolution is more pressing than ever. Logstash has emerged as an easy way to parse and filter logs at source, and then forward them to a single aggregation point. Although Logstash provides some searching and filtering, Graylog2 is often used in conjunction to provide for more fully-featured querying and reporting.","blip_selector":"logstash-graylog2","name":"Logstash & Graylog2","url":"/radar/tools/logstash-graylog2","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":682,"quadrant":"languages-and-frameworks","volume_date":"2014-01","description":"Yeoman attempts to make web application developers more productive by simplifying activities like scaffold, build and package management. It is a collection of the tools Yo, Grunt and Bower that work well as a set.","blip_selector":"yeoman","name":"Yeoman","url":"/radar/languages-and-frameworks/yeoman","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":708,"quadrant":"techniques","volume_date":"2014-01","description":"You cannot act on important business events unless you monitor them. The principle, instrument all the things, encourages us to think proactively about how we achieve this at the start of our software development.This allows us to expose key metrics, monitor them, and report on them to improve operational effectiveness.","blip_selector":"instrument-all-the-things","name":"Instrument all the things","url":"/radar/techniques/instrument-all-the-things","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":707,"quadrant":"techniques","volume_date":"2014-01","description":"Increasingly, HTML is rendered not only on the server but also on the client, in the web browser. In many cases this split rendering will remain a necessity but with the growing maturity of JavaScript templating libraries an interesting approach has become viable: client and server rendering with same code.","blip_selector":"client-and-server-rendering-with-same-code","name":"Client and server rendering with same code","url":"/radar/techniques/client-and-server-rendering-with-same-code","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":545,"quadrant":"techniques","volume_date":"2014-01","description":"Capturing client-side JavaScript errors has helped our delivery teams identify issues specific to a browser or plug-in configuration that impact user experience. Over the past year a number of service providers have started to surface in support of this requirement. Other than storing these errors in application data stores, web applications can log this data to web analytics or existing monitoring tools such as New Relic to offload storage requirements.","blip_selector":"capturing-client-side-javascript-errors","name":"Capturing client-side JavaScript errors","url":"/radar/techniques/capturing-client-side-javascript-errors","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":557,"quadrant":"tools","volume_date":"2014-01","description":"Both Puppet and Chef have had to deal with sharing community-contributed modules and manifests for commonly used services and tasks. Both the Puppet Forge and Chef’s Cookbook repository have helped, but people ended up copying and pasting these recipes into their own codebases, preventing them from taking advantage of later bugfixes and improvements. Librarian-puppet and Librarian-Chef attempt to solve this by making it easy to declare your module dependencies, including pulling in known versions of code from these community sites.","blip_selector":"librarian-puppet-and-librarian-chef","name":"Librarian-puppet and Librarian-Chef","url":"/radar/tools/librarian-puppet-and-librarian-chef","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":460,"quadrant":"platforms","volume_date":"2014-01","description":"Graph databases store information as arbitrarily interconnected nodes linked by named relations, rather than as tables and joins. Schema-less and highly extensible, they are an excellent choice for modelling semi-structured data in complex domains. Neo4j is the front-runner in the space both its REST API and its Cypher query language support simple and fast storage and traversal of graphs. | Graph databases store information as interconnected nodes with arbitrary relations rather than tables and nameless relations. Graph databases are an excellent choice for complex domains with semi-structured data since they’re schema-less and highly extensible. Neo4j is the front-runner in the graph database space being an embedded Java component, which supports fast storage and search of graphs for Java solutions (including server applications). The Neo4j community is highly active and now has a basic REST API enabling it as more general purpose database engine. Neo4j moving into the trial category is representative of our experience trialling it in real-world scenarios and the early successes we’ve achieved.","blip_selector":"neo4j","name":"Neo4J","url":"/radar/platforms/neo4j","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":713,"quadrant":"tools","volume_date":"2014-01","description":"Cloud-init is a simple but powerful technique for carrying out actions on a cloud instance at boot time. It is particularly useful when used with instance metadata to allow a newly booted instance to pull the configuration, dependencies and software needed to perform a particular role. When used together with the Immutable or Phoenix server pattern, this can create a very responsive and light-weight mechanism for managing deployments in the cloud.","blip_selector":"cloud-init","name":"Cloud-init","url":"/radar/tools/cloud-init","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":704,"quadrant":"techniques","volume_date":"2014-01","description":"As more businesses move online we have noted a tendency to end up with siloed metrics. Specific tools are implemented to gather and display specific metrics: one tool for page-views and browser behavior, another for operational data and another to consolidate log messages. This leads to data silos and the need to swivel-chair integrate between the tools in order to gather business intelligence that is crucial to running the business. This is a tool-led split in the analytics domain that hurts the team’s ability to make decisions. A much better solution is to have a consolidated view of near-real time analytics using integrated dashboards displaying time-sensitive domain and team relevant information.","blip_selector":"siloed-metrics","name":"Siloed metrics","url":"/radar/techniques/siloed-metrics","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":655,"quadrant":"tools","volume_date":"2014-01","description":"Gatling is another newer player in the automated performance testing space. It is similar to Locust and is much lighter weight than the older options such as JMeter and Grinder. Built on Scala, the DSL provides extensive functionality out of the box including easily configured data feeds and response assertions. In cases where customization is needed, it is easy to drop into Scala to provide extensions. The default generation of numerous dynamic views of the data via Highcharts adds to its appeal.","blip_selector":"gatling","name":"Gatling","url":"/radar/tools/gatling","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":280,"quadrant":"platforms","volume_date":"2014-01","description":"Node.js is a lightweight web container that is a strong option for development of micro services and as a server to mobile and single-page web applications. Due to the asynchronous nature of node.js, developers are turning to promise libraries to simplify their application code. As the use of promises mature within the node.js community, we expect to see more applications developed for node.js. For those teams that are reluctant to try node.js in production, it is still worthwhile to consider node.js for development tasks like running JavaScript tests outside of the browser or generating static web content from tools like CoffeeScript, SASS, and LESS. | For many years JavaScript was predominantly used as a client side Web programming language, but a lightweight language such as JavaScript can easily be embedded in different environments, including the server side. Node.js allows developers to write applications in JavaScript on both client and server sides. Since most servers spend the majority of their time waiting for I/O operations, Node.js’ event driven non-blocking architecture is very efficient. Unlike threadbased solutions, Node.js does not need to wait for I/O operations to complete while processing incoming requests, making it a good choice when implementing high performance services.","blip_selector":"node-js","name":"Node.js","url":"/radar/platforms/node-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":604,"quadrant":"platforms","volume_date":"2014-01","description":"The open source OpenStack project is gathering steam, and in recent months is becoming a more viable platform for deploying your own private clouds. Many issues which made OpenStack hard to get up and running have been addressed, and new features are being added all the time. It is clear that the OpenStack consortium and its members like Rackspace, Redhat, and HP are committed to the project as the basis for their own OpenStack-based cloud services. | OpenStack is a new cloud operating system that promises a complete open-source solution. OpenStack is a fabric cloud controller which leverages existing virtualization technologies such as KVM and will integrate with other virtualisation tools such as Xen and OpenVZ. Currently under heavy development, OpenStack is expected to provide a stable production-ready solution by the end of Q2 2011.","blip_selector":"openstack","name":"OpenStack","url":"/radar/platforms/openstack","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":395,"quadrant":"techniques","volume_date":"2014-01","description":"We are seeing an uptick in adoption of microservices as a technique for distributed system design, both in Thoughtworks and in the wider community. Frameworks such as Dropwizard and practices like declarative provisioning point to a maturing of the technologies and tools. Avoiding the usual monolithic approach and being sympathetic to the need to replace parts of systems individually has important positive implications for the total cost of ownership of systems. We see this as having greatest impact in the mid-to-long term, specifically with respect to the two-to-five year rewrite cycle. | Microservices, often deployed out-of-container or using an embedded HTTP server, are a move away from traditional large technical services. This approach trades benefits such as maintainability for additional operational complexity. These drawbacks are typically addressed using infrastructure automation and continuous deployment techniques. On balance, microservices are an effective way of managing technical debt and handling different scaling characteristics especially when deployed in a service oriented architecture built around business capabilities.","blip_selector":"microservices","name":"Microservices","url":"/radar/techniques/microservices","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":532,"quadrant":"platforms","volume_date":"2014-01","description":"PhoneGap, now renamed as Apache Cordova, is a platform that lets you develop cross-platform mobile applications using HTML, CSS and JavaScript. It abstracts away platform specific native code through a set JavaScript APIs that remain consistent across different mobile platforms. Cordova is available for a wide array of platforms including iOS, Android, Blackberry, Windows Phone, and WebOS.","blip_selector":"phonegap-apache-cordova","name":"PhoneGap/Apache Cordova","url":"/radar/platforms/phonegap-apache-cordova","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":660,"quadrant":"techniques","volume_date":"2014-01","description":"HTML5 storage, also known as local storage or web storage, is a mechanism for storing client side data in modern browsers, including iOS and Android mobile browsers. We recommend using HTML5 storage instead of cookies in almost all cases. HTML5 Storage can accommodate up to 5MB of data while cookies are limited to 4KB. Cookie data is transmitted in every request, which slows down your application and potentially exposes data over insecure HTTP connections. In contrast, HTML5 storage data remains securely in the browser. Cookies should be reserved for storing small simple pieces of data like a session ID.","blip_selector":"html5-storage-instead-of-cookies","name":"HTML5 storage instead of cookies","url":"/radar/techniques/html5-storage-instead-of-cookies","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":401,"quadrant":"techniques","volume_date":"2014-01","description":"Previously, support for Windows in tools like Chef and Puppet was lacking, leading to large amounts of Powershell scripting to achieve simple infrastructure automation tasks. Achieving the same level of automation for Windows was more challenging than for Unix. In the last 12 months however, both Chef and Puppet support for Windows has improved drastically.  That support, combined with the inherent power of Powershell makes Windows infrastructure automation extremely viable. | Mature tools such as PowerShell, together with newer options such as Chef and Puppet, lead us to highlight Windows infrastructure automation on this edition of the technology radar. Manual configuration using a mouse and menu options is slow and leads to misconfiguration and “snowflake” machines in an unknown state. We recommend command-line tools for their clarity and scriptability.","blip_selector":"windows-infrastructure-automation","name":"Windows infrastructure automation","url":"/radar/techniques/windows-infrastructure-automation","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":583,"quadrant":"tools","volume_date":"2014-01","description":"We see interest on Thoughtworks projects around PhantomJS, a headless web testing tool that allows functional testing against a realistic target.","blip_selector":"phantomjs","name":"PhantomJS","url":"/radar/tools/phantomjs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":526,"quadrant":"techniques","volume_date":"2014-01","description":"We increasingly see mobile applications that work really well during development and testing, but run into trouble when they are deployed in the real world. Mobile testing on mobile networks reveals how your app performs under a variety of conditions. You might test using 3G or LTE or deliberately use a poor WiFi network with overloaded access points. Measure network performance for your target environment, then simulate the conditions using latency and packet-loss inducing tools. In addition, it is sometimes necessary to examine exactly how your device and software are using the network with a tool such as Wireshark.","blip_selector":"mobile-testing-on-mobile-networks","name":"Mobile testing on mobile networks","url":"/radar/techniques/mobile-testing-on-mobile-networks","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":528,"quadrant":"techniques","volume_date":"2014-01","description":"Since the last radar a few advances have made continuous delivery for native apps on mobile devices less painful. Xctool, the recently open-sourced 'better xcodebuild' improves iOS build automation and unit testing. The arrival of automatic updates in iOS7 reduces the friction of regular releases. Travis-CI now supports OS X agents, removing another hurdle in seamless CD pipelines for mobile platforms. Our advice from the last radar on the value of hybrid approaches and the importance of test automation for mobile still applies. | With HTML5 blurring the line between traditional native apps and web apps, we are beginning to experiment with continuous delivery for mobile devices. Services such as TestFlight allow you to deploy native apps to real devices multiple times per day. With a wholly or partially HTML5-based application changes can be deployed without submitting a new app to an app store. If your organization has an enterprise app store, you may be able to easily push builds to it. While the techniques for implementing CD to mobile devices are improving, we note that testing practices are lagging behind. To be successful you will need to increase your focus on automated testing to ensure that everything actually works once it gets to the device.","blip_selector":"continuous-delivery-for-mobile-devices","name":"Continuous delivery for mobile devices","url":"/radar/techniques/continuous-delivery-for-mobile-devices","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":648,"quadrant":"languages-and-frameworks","volume_date":"2014-01","description":"The expansion of single-page and mobile browser-based applications into mainstream use, along with continued growth of node.js for server-side applications, have led to increased adoption of CoffeeScript to simplify JavaScript codebases. As a language that compiles into JavaScript code for runtime execution, many concerns have been raised about the difficulty of debugging applications written in CoffeeScript. The introduction of Source Maps in CoffeeScript 1.6.1 is helping producers of development tools address this concern.  We expect this will lead to further adoption of the language following the lead of highly visible technology firms such as Dropbox. | JavaScript is a powerful, ubiquitous programming language with tricky and error prone syntax. Coffeescript fixes many of the warts of JavaScript in a clean, simple syntax that generates readable JavaScript. For example, creating true private variables in JavaScript is a syntactic nightmare; CoffeeScript generates the technically correct but hideous syntax. Some readers may be confused by our advocacy of Coffeescript given our general dislike for GWT, because on the surface they seem similar: tools that generate JavaScript. However, it is the level of abstraction that differs. GWT has an elaborate component model, which tries to hide details about the underlying language (JavaScript) and platform (the web). Coffeescript tries to make it easier to write proper JavaScript, avoiding pathological but default “features” of JavaScript, and does not build a layer that tries to insulate you from the platform.","blip_selector":"coffeescript","name":"CoffeeScript","url":"/radar/languages-and-frameworks/coffeescript","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":470,"quadrant":"tools","volume_date":"2013-05","description":"Riemann is an open source server that aggregates and relays events in real time. Written in Clojure, and based on Netty, it is capable of handling thousands of concurrent connections per node. Riemann uses a simple Protobuf protocol for events, which allows it to aggregate everything from CPU and memory use to orders placed to error rates. It forwards to systems like Graphite, triggers email alerts, and provides a dashboard for monitoring these metrics. Riemann is an important part of the movement towards handling data as generic streams of events in real-time, as opposed to using specialized systems for different types of data.","blip_selector":"riemann","name":"Riemann","url":"/radar/tools/riemann","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":471,"quadrant":"platforms","volume_date":"2013-05","description":"A fundamental rethinking of how databases work, Datomic is an immutable database server with fascinating transactional and deployment characteristics. One of the common headaches on agile projects is managing database migrations, especially restoring previous states. Datomic makes the need for migrations go away - every version of the data (and schema) is preserved by the database. While still evolving, we appreciate Datomic’s boldness of vision.","blip_selector":"datomic","name":"Datomic","url":"/radar/platforms/datomic","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":605,"quadrant":"platforms","volume_date":"2013-05","description":"While AWS continues to add more features, Rackspace Cloud has become a viable competition in the storage and compute space. Some users may value the more thorough customer support available for Rackspace, as well as the ability to mix in more traditional hosting models. We are not excited about this just because Rackspace is a client of ours and we have had the pleasure developing the platform. We have successfully used Rackspace Cloud with several other clients, and would look forward to it being offered in more geographical locations.","blip_selector":"rackspace-cloud","name":"Rackspace Cloud","url":"/radar/platforms/rackspace-cloud","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":304,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"Backbone.js is a great example of an abstraction pushed too far. While we initially liked the ease of wire-up, in practice it suffers from the same issues as all such databound frameworks from WebForms to client/server tools. We find that it blurs the framework and model too much, forcing either bad architectural decisions or elaborate framework hackery in order to preserve sanity. | Even though JavaScript increasingly plays a more important role in today’s world of software development, it is still troublesome to organize in a clean structure. Backbone.js is a library which provides an MVC (model view controller) framework for JavaScript heavy applications. It allows developers to write JavaScript code in a more manageable and testable way.","blip_selector":"backbone-js","name":"Backbone.js","url":"/radar/languages-and-frameworks/backbone-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":406,"quadrant":"tools","volume_date":"2013-05","description":"PSake (pronounced ‘sake’ like the Japanese rice wine) is a build automation tool implemented in PowerShell. PSake provides a tidy syntax for declaring build tasks and dependencies without programming in XML. You also have access to all the features of PowerShell and the .NET framework from within your build scripts.","blip_selector":"psake","name":"PSake","url":"/radar/tools/psake","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":407,"quadrant":"tools","volume_date":"2013-05","description":"Frank is an open source library that allows functional tests for iOS written in Cucumber and executed on a remote device. This fills an important niche where acceptance test-driven development was previously cumbersome and awkward.","blip_selector":"frank","name":"Frank","url":"/radar/tools/frank","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":518,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"Our continued use of node.js on new production applications has re-enforced our need for reliable packaging of JavaScript code and libraries. The Node Package Manager (npm) is an important part of the node.js ecosystem and a useful tool for packaging node.js applications. Developers of browser applications with large amounts of JavaScript or CoffeeScript should consider Require.js to help with structuring their code and loading dependencies at run time","blip_selector":"require-js-npm","name":"Require.js & NPM","url":"/radar/languages-and-frameworks/require-js-npm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":629,"quadrant":"techniques","volume_date":"2013-05","description":"Application configuration can be a source of pain when getting started with a new tool, managing deployments to different environments, or trying to understand why applications behave differently in different places. We are a big fan of minimizing application configuration, trying to ensure that applications work sensibly out of the box with the bare minimum of Configuration.","blip_selector":"minimizing-application-configuration","name":"Minimizing application configuration","url":"/radar/techniques/minimizing-application-configuration","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":419,"quadrant":"tools","volume_date":"2013-05","description":"Language-based build tools like Gradle and Rake continue to offer finer-grained abstractions and more flexibility long term than XML and plug-in based tools like Ant and Maven. This allows them to grow gracefully as projects become more complex. | Maven has long been a staple of build automation in the Java space. However, given its lack of flexibility and support for automation best practices, especially in the Continuous Delivery domain, the use of alternatives such as Gradle should be considered.","blip_selector":"maven","name":"Maven","url":"/radar/tools/maven","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":456,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"We have long thought of JavaScript as a first class language, and have been keenly following the development of testing tools in that space. The cream of the crop for out-of-browser testing is currently Jasmine. Jasmine paired with Node.js is the go-to choice for robust testing of both client- and serverside JavaScript.","blip_selector":"jasmine-paired-with-node-js","name":"Jasmine paired with Node.js","url":"/radar/languages-and-frameworks/jasmine-paired-with-node-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":520,"quadrant":"tools","volume_date":"2013-05","description":"Jekyll represents the “microization” of frameworks in the web publishing space. While the focus is maintained on doing one thing - sites that feature blogs - as transparently as possible, it also shows the path to a more lightweight future. One example of this that we like is that it is now trivially easy to publish useful documentation for your software project.","blip_selector":"jekyll","name":"Jekyll","url":"/radar/tools/jekyll","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":658,"quadrant":"techniques","volume_date":"2013-05","description":"Blue-green deployment is a pattern for performing software upgrades. By setting up the latest version of your application on an identical clone of your production application stack, traffic can be switched, near instantaneously, from the current production stack to the new one as soon as the test suite and the business determine it is appropriate. Though this is an old technique, infrastructure automation and resources in the cloud make it worth reconsidering.","blip_selector":"blue-green-deployment","name":"Blue-green deployment","url":"/radar/techniques/blue-green-deployment","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":481,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"This language/framework was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | Gremlin is an imperative graph traversal language supported by multiple graph databases. Its concise constructs can be used in place of the native language of the database, leading to faster development times and, in some cases, faster execution. We recommend its use as a good alternative in simple scenarios.","blip_selector":"gremlin","name":"Gremlin","url":"/radar/languages-and-frameworks/gremlin","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":617,"quadrant":"techniques","volume_date":"2013-05","description":"The use of promises for asynchronous programming is an old technique that is also known as futures. It is gaining renewed interest in light of the extensive use of JavaScript on both the client and server side. This technique eliminates the use of deeply nested callbacks, flags and pollers and has first-class support from libraries such as jQuery. Teams developing JavaScript codebases of significant complexity should take advantage of this.","blip_selector":"promises-for-asynchronous-programming","name":"Promises for asynchronous programming","url":"/radar/techniques/promises-for-asynchronous-programming","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":478,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"ClojureScript illustrates just how cross-platform the core of Clojure really is: they ported the primary parts to run on JavaScript. It is missing some of the whizzbang features of Clojure on the JVM and CLR, like software transactional memory, but has a surprisingly high fidelity with its more sophisticated cousins. One interesting option afforded by ClojureScript is the ability to send data structures à la JSON using ClojureScript as the data structure. Because Clojure is a Lisp, this means that you can also send “real” code.","blip_selector":"clojurescript","name":"ClojureScript","url":"/radar/languages-and-frameworks/clojurescript","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":505,"quadrant":"tools","volume_date":"2013-05","description":"We are strong believers in in-line automated performance testing, although open source tools in this space have been somewhat limited to date. Locust is a firm favorite that provides the ability to write tests in Python, with good support for running multiple injectors, basic statistics generation, and a useful web dashboard. Its approach to web load testing focuses more on the simulation of users than just generating hits per second. We would typically recommend Locust over and above older tools such as JMeter or Grinder.","blip_selector":"locust","name":"Locust","url":"/radar/tools/locust","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":446,"quadrant":"techniques","volume_date":"2013-05","description":"Bringing users in to a controlled environment for formal testing can be a slow and expensive proposition. Much useful, qualitative feedback can be gathered quickly and cheaply through guerrilla user testing - by going out into the world and testing with small samples of the general public. Another alternative is remote usability testing, where you can send out everything from wireframes to final applications for testing by people all over the world. Usabila, Loop11 and Treejack all provide tools where you can ask users to carry out specific tasks, and capture everything from the time taken to complete a task, to the user’s thoughts and feelings while doing so.","blip_selector":"guerrilla-user-testing","name":"Guerrilla user testing","url":"/radar/techniques/guerrilla-user-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":394,"quadrant":"tools","volume_date":"2013-05","description":"In previous radars we have talked about embedded servlet containers, and these are now widely adopted on our projects. Tools such as SimpleWeb and Webbit take the simple, embedded approach further and offer raw HTTP server functionality without implementing the Java Servlet specification. At the same time, Tomcat, the most popular Java application server, is increasingly used in embedded setups and Microsoft provides self-hosted servers for the .NET framework, lending further weight to this Trend. | We have talked much already about embedded servlet containers - and these are now widely adopted on our projects. Tools such as SimpleWeb and Webbit take the simple, embedded approach further and offer raw HTTP server functionality without implementing the Java Servlet specification. We are pleased to see a corresponding reduction in the complexity of test code that takes advantage of this. | Embedding a servlet container, such as Jetty, inside a Java application has many advantages over running the application inside a container. Testing is relatively painless because of the simple startup, and the development environment is closer to production. Nasty surprises like mismatched versions of libraries or drivers are eliminated by not sharing across multiple applications. While you will have to manage and monitor multiple Java Virtual Machines in production using this model, we feel the advantages offered by the simplicity and isolation are significant.","blip_selector":"embedded-servlet-containers","name":"Embedded servlet containers","url":"/radar/tools/embedded-servlet-containers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":315,"quadrant":"tools","volume_date":"2013-05","description":"Language-based build tools like Gradle and Rake continue to offer finer-grained abstractions and more flexibility long term than XML and plug-in based tools like Ant and Maven. This allows them to grow gracefully as projects become more complex. | Two things have caused fatigue with XML-based build tools like Ant and Maven: too many angry pointy braces and the coarseness of plug-in architectures. While syntax issues can be dealt with through generation, plug-in architectures severely limit the ability for build tools to grow gracefully as projects become more complex. We have come to feel that plug-ins are the wrong level of abstraction, and prefer language-based tools like Gradle and Rake instead, because they offer finer-grained abstractions and more flexibility long term. | This tool was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | Gradle is an attempt to bring sanity to the enterprise build space by marrying best-of-breed tools with cutting edge techniques. Gradle allows you to interact with your existing Maven repositories, but adds scriptability to your builds with a clean domain specific language.","blip_selector":"gradle","name":"Gradle","url":"/radar/tools/gradle","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":521,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"There is a tendency to equate the need for offline functionality with the need to build an app. Despite the slow standardization process, most HTML5 features have now been implemented across all major browsers. Its local storage capabilities, comprehensively supported across mobile and tablet browsers - makes HTML5 for offline applications a very suitable option. | HTML5 includes features that allow control and storage of offline data within the browser using client side JavaScript. These features allows creation of offline mobile web applications in a cross platform way that would have previously required installed applications. For instance an application that can download articles for reading later or a data capture application that can work offline and upload when you are online. While the standard is not finalized yet, support for these offline features is available and ready for use in the WebKit based browsers found on iOS, Android and newer Blackberry phones.","blip_selector":"html5-for-offline-applications","name":"HTML5 for offline applications","url":"/radar/languages-and-frameworks/html5-for-offline-applications","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":314,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"It is startling to us that we continue to find new systems in 2011 that implement significant business logic in stored procedures. Programming languages commonly used to implement stored procedures lack expressiveness, are difficult to test, and discourage clean modular design. You should only consider stored procedures executing within the database engine in exceptional circumstances, where there is a proven performance issue.","blip_selector":"logic-in-stored-procedures","name":"Logic in stored procedures","url":"/radar/languages-and-frameworks/logic-in-stored-procedures","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":663,"quadrant":"tools","volume_date":"2013-05","description":"We see several JavaScript frameworks embrace browser-based templating, moving more layout work to the client. While this approach is useful in many cases, it does introduce operational issues involving caching, performance, and search. We believe these tools should be assessed carefully to ensure suitability for the target deployment environment.","blip_selector":"browser-based-templating","name":"Browser-based templating","url":"/radar/tools/browser-based-templating","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":449,"quadrant":"techniques","volume_date":"2013-05","description":"Acceptance tests generally exercise the system from the ‘outside’, traversing an entire network stack for the security of exercising the complete application. In-process acceptance testing challenges the notion that test code and application under- test must run in different processes in order to achieve these benefits. When using an embedded container, it is easy to set up the system, run the tests over HTTP and to verify the final state without the setup costs associated with deploying to and communicating with a separate container.","blip_selector":"in-process-acceptance-testing","name":"In-process acceptance testing","url":"/radar/techniques/in-process-acceptance-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":514,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"We think it is essential to inspire the next generation of technologists. Scratch, Alice, and Kodu are programming languages that rely on visual environments and building blocks as teaching devices. They offer exciting possibilities for educational programs and organizations intending to foster programming knowledge in environments beyond academia.","blip_selector":"scratch-alice-and-kodu","name":"Scratch, Alice, and Kodu","url":"/radar/languages-and-frameworks/scratch-alice-and-kodu","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":488,"quadrant":"techniques","volume_date":"2013-05","description":"Log files generated by web servers, databases, networking infrastructure, and back-end systems are a valuable source of operational and behavioral data for a business. In the past, these files were mostly viewed as a source of diagnostic information in the case of failure, but with lowered cost of storage, and availability of tools such as Splunk for indexing and retrieving millions of events, they can also be a source of customer insights. Treating logs as data and storing complete logs instead of just collecting predefined metrics provides a means to answer novel questions that a business could not have previously anticipated.","blip_selector":"logs-as-data","name":"Logs as data","url":"/radar/techniques/logs-as-data","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":416,"quadrant":"tools","volume_date":"2013-05","description":"Package systems for third-party library management continue to gain acceptance and features across all platforms. We called out NuGet as a recent entry, and the addition of Chocolatey NuGet exemplifies the advances and capabilities springing up around this essential agile engineering practice. | Package management systems are a widely accepted practice for incorporating third party libraries. Tools such as RubyGems, Maven, APT, are available at both language and system level. NuGet is such a system for .Net platform. It consists of a Visual Studio IDE extension and a PowerShell module that opens the possibility for further improving build automation on the .Net platform.","blip_selector":"nuget","name":"NuGet","url":"/radar/tools/nuget","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":475,"quadrant":"platforms","volume_date":"2013-05","description":"The amount of data that even a relatively low volume website can generate is huge. Once you add in analytics, business metrics, demographics, user profiles and multiple devices, it can become overwhelming. Many organizations use data warehouses as a repository with data being sucked in from all parts of the organization. The challenge here is that these often turn into “Data Fortresses.” Even getting timely business metrics becomes a challenge, let alone running exploratory queries across the entire data set. Technologies like the cloud based BigQuery help. The pay-as-you-go model and the ability to do ad hoc queries lets you gain insight without buying specialist hardware and software. A data-driven business should put data in the hands of the decision makers, not hidden behind technological barriers and bureaucracy. | Google’s BigQuery brings data analytics to the cloud. Rather than loading data into an expensive data-warehouse with predefined indexes, BigQuery allows you to upload and investigate a data set through ad-hoc SQL-like queries. This is a great way to create a cheap proof-of-concept or even a complete application, as processing of hundreds of gigabytes of data by thousands of servers happens in seconds.","blip_selector":"bigquery","name":"BigQuery","url":"/radar/platforms/bigquery","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":494,"quadrant":"techniques","volume_date":"2013-05","description":"Development teams typically produce tests that specify and validate application behavior, but stop running them once the application goes into production. This is a missed opportunity. Semantic monitoring uses your tests to continuously evaluate your application, combining test-execution and realtime monitoring. With microservices, and similar fine-grained architectural approaches, it is increasingly important to test their interaction at run-time. Incorporating the validation of consumer-driven contracts into a monitoring facility is one way to approach this. While still evolving, we see great promise in the merging of two separate but important verification schemes.","blip_selector":"semantic-monitoring","name":"Semantic monitoring","url":"/radar/techniques/semantic-monitoring","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":490,"quadrant":"techniques","volume_date":"2013-05","description":"We are rapidly heading towards a world where the majority of consumer interactions are from mobile devices. Mobile first embraces this trend by designing user interfaces and server interactions that target mobile devices in the first instance. The mobile first strategy contrasts with approaches that assume a highly capable client device connected to a fast and reliable network and then degrade the experience to fit the limitations of the device.","blip_selector":"mobile-first","name":"Mobile first","url":"/radar/techniques/mobile-first","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":564,"quadrant":"techniques","volume_date":"2013-05","description":"Increasing quality and range of choices for inexpensive or free video conferencing is leading to a new way of working for distributed teams. Always-on video connections can help create a sense of co-location by telepresence, even when the team is distributed geographically. This is becoming the defacto standard in some of our offshore delivery centers. We are also seeing increased use of screen-sharing tools like ScreenHero for remote pairing. We would caution those looking for a silver bullet to eliminate the need for physical co-location. There is no substitute for the understanding and empathy created by faceto- face communication.","blip_selector":"co-location-by-telepresence","name":"Co-location by telepresence","url":"/radar/techniques/co-location-by-telepresence","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":549,"quadrant":"tools","volume_date":"2013-05","description":"Several Thoughtworks teams called out the usefulness of Faraday, a Ruby HTTP client library that provides a common interface over a variety of adapters and integrates nicely with Rack middleware.","blip_selector":"faraday","name":"Faraday","url":"/radar/tools/faraday","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":541,"quadrant":"tools","volume_date":"2013-05","description":"By putting IObservables and IObservers on an equal footing with IEnumerables and IEnumerators, Rx for .NET allows developers to use their existing knowledge of LINQ (Language Integrated Query) operators to query and compose asynchronous operations and event-based code using a common underlying abstraction of observable event streams. Microsoft has also released RxJS to bring the benefits of reactive programming to JavaScript. They open sourced the entire Rx framework, making it useful for Windows rich client applications and single-page JavaScript applications.","blip_selector":"reactive-extensions-for-net","name":"Reactive Extensions for .Net","url":"/radar/tools/reactive-extensions-for-net","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":489,"quadrant":"techniques","volume_date":"2013-05","description":"One such technique for achieving this is responsive web design. Starting with a basic presentation of content - and typically keeping the essential information constant - the experience is enhanced to suit the features detected on more capable browsers. This commonly takes the form of layout and format changes based on screen size.","blip_selector":"responsive-web-design","name":"Responsive web design","url":"/radar/techniques/responsive-web-design","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":479,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"An unlikely contender in the programming languages space, Lua has seen massive adoption across a variety of industries. It is used as a scripting platform in game development and music composition; embedded in point-of-sale appliances and network devices; and in extending NoSQL databases with safe execution semantics. We expect further growth in time to come.","blip_selector":"lua","name":"Lua","url":"/radar/languages-and-frameworks/lua","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":632,"quadrant":"techniques","volume_date":"2013-05","description":"NoSQL data stores continue to become mainstream, and teams should acknowledge the need for database migrations for NoSQL. Especially with an implicit or dynamic schema you are likely to want to reconfigure data over time. There are several approaches such as running an explicit migration when deploying a new build of your application, or using dynamic migrations in code as documents are loaded and processed.","blip_selector":"database-migrations-for-nosql","name":"Database migrations for NoSQL","url":"/radar/techniques/database-migrations-for-nosql","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":284,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"This language/framework was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"css-frameworks","name":"CSS frameworks","url":"/radar/languages-and-frameworks/css-frameworks","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":458,"quadrant":"tools","volume_date":"2013-05","description":"While many tools exist for displaying graphs for system monitoring purposes, Graphite has emerged recently as the clear leader in this space. Capable of charting metrics in realtime, it features a round-robin database that is able to store long periods of historic data, while still providing more recent information at a higher fidelity. Numerous configuration options exist on the dashboard, and the resulting graphs can then be embedded in webpages to increase visibility.","blip_selector":"graphite","name":"Graphite","url":"/radar/tools/graphite","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":482,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"JavaScript is moving outside of the browser, emerging as an important technology for cross-platform development. It is front-and-center in the approach to code reuse taken by Node.js, Meteor.js and mobile frameworks like Calatrava. Along with the recent proliferation of other languages that compile to JavaScript, this makes us wonder if we should start to consider JavaScript as a platform and not just a language.","blip_selector":"javascript-as-a-platform","name":"JavaScript as a platform","url":"/radar/languages-and-frameworks/javascript-as-a-platform","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":576,"quadrant":"techniques","volume_date":"2013-05","description":"Failing tests reveal bugs in production code. However, analyzing test runs for other properties can reveal interesting information. A simple example would be to monitor which tests fail frequently and run them earlier in your build pipeline to get fast feedback. Similarly, tracking other properties such as test execution times and ratios of long-running tests to fast-tests can provide actionable metrics.","blip_selector":"analyzing-test-runs","name":"Analyzing test runs","url":"/radar/techniques/analyzing-test-runs","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":418,"quadrant":"platforms","volume_date":"2013-05","description":"Riak is a distributed key-value store that is schema-less and data-type agnostic. It can be put to good use in write-heavy projects to store data such as sessions, shopping carts and streaming logs - whilst it retains the ability to perform complex queries in a full-text search. The distributed cluster can self-recover without a single master, has tuneable consistency and availability settings and can do collision detection and resolution if needed - all of which can be particularly helpful in high availability environments. | Riak is a distributed key-value store that is schemaless and data-type agnostic. It can be put to good use in write heavy projects to store data such as sessions, shopping carts and streaming logs. The ability of the distributed cluster to self recover, distribute data across the cluster with tunable consistency and availability settings, do collision detection and resolve those if needed can be helpful in high availability environments and provide interesting solutions in the architecture.","blip_selector":"riak","name":"Riak","url":"/radar/platforms/riak","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":510,"quadrant":"platforms","volume_date":"2013-05","description":"Continuous integration in the cloud is one of those obvious-in-hindsight infrastructure offerings that supports agile development. With no local software and minimal configuration, it just works. With mature offerings now in place, serious developers are left with no excuse for avoiding this important practice.","blip_selector":"continuous-integration-in-the-cloud","name":"Continuous integration in the cloud","url":"/radar/platforms/continuous-integration-in-the-cloud","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":477,"quadrant":"platforms","volume_date":"2013-05","description":"We have previously been skeptical of claims of reusable code working across platforms. Our experience with many tools in the market has been mixed and we advise caution to our clients who are looking at these types of solutions. Taking an approach that carefully navigates these dangerous waters, we feel Calatrava is worth evaluating for mobile application development. The framework neatly follows the separation of business and presentation logic, maximising reuse where there is commonality, and providing native access where speed or device-specific idioms are to be followed.","blip_selector":"calatrava","name":"Calatrava","url":"/radar/platforms/calatrava","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":666,"quadrant":"tools","volume_date":"2013-05","description":"Both TestFlight and HockeyApp allow you to manage the deployment of mobile applications without an app store, makinguser testing easier. They offer crash reporting and analytic capabilities to gather data in the field. HockeyApp supports IOS, Android, & Windows Phone, while TestFlight supports iOS and Android. We have used both tools successfully to help deliver mobile applications. This is clearly a fast evolving space.","blip_selector":"testflight-hockeyapp","name":"TestFlight & HockeyApp","url":"/radar/tools/testflight-hockeyapp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":486,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"As the industry shifted from desktop GUI development to the web, it seemed natural to port the most successful patterns and designs to the new paradigm. After 15 years of trying, we feel that there are still no component-based frameworks that have successfully achieved this. We recommend not attempting to make web development into something that it fundamentally is not. It is time to accept the page and request-based nature of the web, and focus on the frameworks that support - rather than work against - these concepts.","blip_selector":"component-based-frameworks","name":"Component-based frameworks","url":"/radar/languages-and-frameworks/component-based-frameworks","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":390,"quadrant":"techniques","volume_date":"2013-05","description":"While unit and acceptance testing are widely embraced as standard development practices, this trend has not continued into the realm of performance testing. Currently, the common tooling drives testers towards creating throw away code and a click-and-script mentality. Treating performance testing as a first-class citizen enables the creation of better tests that cover more functionality, leading to better tooling to create and run performance tests, resulting in a test suite that is maintainable and can itself be tested.","blip_selector":"performance-testing-as-a-first-class-citizen","name":"Performance testing as a first-class citizen","url":"/radar/techniques/performance-testing-as-a-first-class-citizen","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":480,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"Introducing a Ruby compiler and toolchain for developing iOS applications, RubyMotion has unsurprisingly caused quite a stir in the Thoughtworks development community. There continues to be a need to understand the underlying iOS APIs and some Objective-C when building applications, but there are clear benefits for those who find working with the Ruby language and tools more comfortable.","blip_selector":"rubymotion","name":"RubyMotion","url":"/radar/languages-and-frameworks/rubymotion","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":452,"quadrant":"techniques","volume_date":"2013-05","description":"When designing a domain model, the aggregate pattern helps to add structure and modularity. Mapped to a relational database the aggregate is not visible in the table structure. Document databases, like MongoDB, allow you to model aggregates as documents. This 1:1 mapping means that the aggregate root should be the object that is loaded from the collection.","blip_selector":"aggregates-as-documents","name":"Aggregates as documents","url":"/radar/techniques/aggregates-as-documents","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":469,"quadrant":"tools","volume_date":"2013-05","description":"Like most good software developers, we choose our tools with care. We are especially keen on interesting departures from the norm, which is why we helped back the Light Table Kickstarter project. While still very early in development, the promised interactivity rivals the best of the Smalltalk world, with a modern twist; we are anxious to see what will come of this ambitious project.","blip_selector":"light-table","name":"Light Table","url":"/radar/tools/light-table","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":668,"quadrant":"platforms","volume_date":"2013-05","description":"Zepto.js is a lightweight JavaScript library that is largely based on JQuery. The API is identical to JQuery although it does not offer full compatibility with it. With a vastly compressed file size, Zepto is a compelling option when building responsive web Applications.","blip_selector":"zepto-js","name":"Zepto.js","url":"/radar/platforms/zepto-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":234,"quadrant":"platforms","volume_date":"2013-05","description":"We are reiterating our advice that given the progress and acceptance of simpler web-as-platform techniques such as REST and OAuth and the known issues with WS-*, it should only be used cautiously. | Previously our advice has been to tread carefully when using the WS-* stack beyond the basic profile. Given the progress and acceptance of simpler web-as-platform techniques such as REST and OAuth and the known issues with the WS-*, it should only be used cautiously. | Web services are now widely used as an enabler for service oriented architectures as well as for the integration of existing applications. We see mature tools and largely interoperable implementations for web service standards covered by WS-I Basic Profile, but we remain skeptical about the proliferation and value of WS-* standards beyond Basic Profile.","blip_selector":"ws","name":"WS-*","url":"/radar/platforms/ws","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":495,"quadrant":"techniques","volume_date":"2013-05","description":"Breaking up monolithic applications and building systems from microservices requires a solid strategy to integrate output from disparate systems into a coherent experience for the end-user. Integrating at the presentation layer using Edge Side Includes (ESI) for page composition is a practical and elegant solution. This can occur within your environment using a reverse proxy like Varnish or closer to the user in a Content Delivery Network (CDN).","blip_selector":"edge-side-includes-for-page-composition","name":"Edge Side Includes for page composition","url":"/radar/techniques/edge-side-includes-for-page-composition","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":515,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"With JavaScript development on the rise, there is a growing need for reusable, extensible UI tooling. Twitter Bootstrap builds on the best offerings in the space, to provide a powerful set of patterns and components that help developers create responsive and adaptive applications with pleasant aesthetics.","blip_selector":"twitter-bootstrap","name":"Twitter Bootstrap","url":"/radar/languages-and-frameworks/twitter-bootstrap","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":448,"quadrant":"techniques","volume_date":"2013-05","description":"The adoption of Continuous Delivery means many teams are creating an automated deployment pipeline that carries their code all the way to production. Pipelines allow the visualization of otherwise complex chains of build and deployment activities. Further, they provide the ability to reliably trace build artifacts as they progress through each stage on their path to production. Several vendors are now building CI servers that support the pipeline as a first-class feature and not just a visual element. We recommend teams look closely at these products to avoid wasting time trying to shoehorn a pipeline into a tool without adequate support.","blip_selector":"automated-deployment-pipeline","name":"Automated deployment pipeline","url":"/radar/techniques/automated-deployment-pipeline","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":524,"quadrant":"tools","volume_date":"2013-05","description":"UIAutomator looks like the most promising tool for testing Android user interfaces by allowing fine-grained control over components during test and facilitating testing on multiple Devices.","blip_selector":"uiautomator","name":"UIAutomator","url":"/radar/tools/uiautomator","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":502,"quadrant":"tools","volume_date":"2013-05","description":"Hadoop continues to be the most popular framework to develop distributed data-processing applications. Although programming Hadoop applications in Java is not particularly difficult, designing efficient MapReduce pipelines does require a good amount of experience. Apache Pig simplifies Hadoop development by offering a high level language, called Pig Latin, and an execution runtime. Pig Latin is procedural and provides a SQL-like interface to work with large datasets. The execution infrastructure compiles Pig Latin into an optimized sequence of MapReduce programs that run on the cluster. Pig Latin is extensible through user-defined functions in different languages such as Ruby, JavaScript, Python and Java.","blip_selector":"apache-pig","name":"Apache Pig","url":"/radar/tools/apache-pig","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":483,"quadrant":"techniques","volume_date":"2013-05","description":"We have previously spoken about executing automated tests at the appropriate layer of your application. In this radar, we want to be very specific - we recommend against exhaustive browser based testing. Web browser automation tools like Selenium have encouraged widespread automated testing through the browser. While these tests continue to have their place in a test portfolio, most teams find that executing the bulk of tests through the browser creates a slow and fragile test suite.","blip_selector":"exhaustive-browser-based-testing","name":"Exhaustive browser based testing","url":"/radar/techniques/exhaustive-browser-based-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":457,"quadrant":"tools","volume_date":"2013-05","description":"Precedents set by cloud providers are now changing expectations within the corporate datacenter. In the cloud, many systems scale automatically, either to provide additional availability or in response to increased demand. Crucial to managing a growing estate, immutable servers, or ‘phoenix servers’, are a sensible approach for enterprises looking at IaaS and PaaS. In contrast, custom-configured ‘snowflake servers’ increase the load on the operations group and encourage a “works on my machine” mentality. Being able to re-provision machines - hard or virtual - from scratch using tools such as Chef or Puppet can drastically reduce the complexity of managing large server farms. Coupled with software that is designed to withstand failure, this will lead to more scalable and reliable systems.","blip_selector":"immutable-servers","name":"Immutable servers","url":"/radar/tools/immutable-servers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":436,"quadrant":"platforms","volume_date":"2013-05","description":"Sometimes, architectural decisions lead you to incorporate infrastructure items that you can only afford one of, such as mainframes or search appliances. This is a terrible idea. It severely restricts testing and deployment flexibility. We strongly favor infrastructure you can easily set up and tear down. Singleton infrastructure belongs to misguided vendor-driven architectures of the past.","blip_selector":"singleton-infrastructure","name":"Singleton infrastructure","url":"/radar/platforms/singleton-infrastructure","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":517,"quadrant":"languages-and-frameworks","volume_date":"2013-05","description":"Single-page web application development continues to flourish along with the frameworks supporting data binding, client-side templates, validation, and other capabilities. The JavaScript MV* frameworks in active use on Thoughtworks projects include AngularJS, Knockout, and Ember.js. Each has advocates and a few detractors. We expect continuing innovative churn in this vibrant space.","blip_selector":"javascript-mv-frameworks","name":"JavaScript MV* frameworks","url":"/radar/languages-and-frameworks/javascript-mv-frameworks","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9282,"quadrant":"tools","volume_date":"2012-10","description":"The tool Vagrant makes it simple for teams to distribute virtualized development environments constructed using version-control friendly descriptors. Vagrant helps eliminate environmental differences in development and reduce troubleshooting waste.","blip_selector":"vagrant","name":"Vagrant","url":"/radar/tools/vagrant","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9068,"quadrant":"techniques","volume_date":"2012-10","description":"For those organizations that are required to integrate systems, many continue to use a common database, sharing data between applications through the database tier. In many cases this has become an established and accepted architectural pattern: database based integration. The side affect of such an approach is greater coupling of database schemas, release schedules, performance and quality of service across applications.","blip_selector":"database-based-integration","name":"Database based integration","url":"/radar/techniques/database-based-integration","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9126,"quadrant":"techniques","volume_date":"2012-10","description":"We have found adding simple health check pages to applications is incredibly useful. This allows people to quickly understand the health of an individual node. We often extend them to add metrics like the number of orders placed, error rates, or similar information. Using simple embedded web servers, even non-web based services can easily expose internal information over HTTP. By using microformats, these web pages can easily be scraped by other monitoring tools to become part of holistic monitoring.","blip_selector":"health-check-pages","name":"Health check pages","url":"/radar/techniques/health-check-pages","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9062,"quadrant":"tools","volume_date":"2012-10","description":"There are a couple of usability testing tools that match our preferred ‘guerrilla’ approach. Eye-tracking has long been a useful technique when designing compelling user interfaces, however the equipment and software associated with it is expensive and typically requires the use of specialist firms. Crazy Egg is a cheaper, software-only solution that produces heat maps based on mouse movement. This movement has a strong correlation with gaze, and can be used as a reasonable approximation. Silverback captures not only the screen during a test, but also records the face and voice of the user. This can be invaluable in sharing rich test experiences with the wider development team.","blip_selector":"crazy-egg","name":"Crazy Egg","url":"/radar/tools/crazy-egg","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9074,"quadrant":"tools","volume_date":"2012-10","description":"We strongly favor code-base visualization techniques. In particular, Dependency Structure Matrices (DSM) have proven to be extremely useful, especially in support of an evolutionary architecture and emergent design. Tools support for DSM is widespread.","blip_selector":"dependency-structure-matrices","name":"Dependency Structure Matrices","url":"/radar/tools/dependency-structure-matrices","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9143,"quadrant":"techniques","volume_date":"2012-10","description":"Automation is one of the core practices of Continuous Delivery. While companies are getting better at automating the management of infrastructure and environments, one commonly forgotten aspect is infrastructure automation of development workstations. This leads to huge gains in productivity by avoiding manually building specific environments and allows a seamless pairing environment. As with other parts of the environment, tools like Puppet and Chef can be used though they are not entirely necessary as the judicious use of platform packaging and language build tools can be sufficient.","blip_selector":"infrastructure-automation-of-development-workstations","name":"Infrastructure automation of development workstations","url":"/radar/techniques/infrastructure-automation-of-development-workstations","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9300,"quadrant":"tools","volume_date":"2012-10","description":"Zucchini is a testing framework that provides Cucumber-style BDD testing for iOS apps. It uses CoffeeScript for feature definitions, takes screenshots as tests are run, and we’ve been very happy with it.","blip_selector":"zucchini","name":"Zucchini","url":"/radar/tools/zucchini","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9205,"quadrant":"techniques","volume_date":"2012-10","description":"With the popularity of embedded HTTP servers increasing, so has the technique of out-of-container functional testing. That is writing tests at the boundary of the system, using a mock container to provide both fast feedback and high coverage. Servers such as Jetty and tools like Plasma for the .Net platform can provide a significant reduction in the time it takes to run your test suite.","blip_selector":"out-of-container-functional-testing","name":"Out-of-container functional testing","url":"/radar/techniques/out-of-container-functional-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9148,"quadrant":"tools","volume_date":"2012-10","description":"This tool was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This tool was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"jade","name":"Jade","url":"/radar/tools/jade","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9272,"quadrant":"techniques","volume_date":"2012-10","description":"Test recorders seem invaluable as they provide a quick way to capture navigation through an application. However, we strongly advise against their regular use, as it tends to result in brittle tests which break with small changes to the UI. The test code they produce tends to be relatively poor and riddled with unnecessary duplication. Most importantly, test recorders tend to cut channels of communication between the test automation and development teams. When faced with an application that is difficult to test through the user interface, the solution is to have a critical conversation between the teams to build a more testable UI.","blip_selector":"test-recorders","name":"Test recorders","url":"/radar/techniques/test-recorders","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9038,"quadrant":"languages-and-frameworks","volume_date":"2012-10","description":"The industry is experiencing something of a renaissance in programming languages. Thoughtworks thinks it is time to start assessing which other languages will help your organization while taking stock of the useful lifetime remaining for your current choices. You need to care about languages. Traditionally structured organizations with separate support teams may find skills constrain choice, DevOps offers a path forwards here.","blip_selector":"care-about-languages","name":"Care about languages","url":"/radar/languages-and-frameworks/care-about-languages","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9284,"quadrant":"platforms","volume_date":"2012-10","description":"Representing yet another evolution away from traditional, free-standing application containers, Vert.x is an application framework that bridges synchronous and asynchronous programming styles. This gives the programmer the option to trade off scalability and performance for simplicity. Unlike Node.js, Vert.x is a library that can be called from a variety of languages supported on the JVM, including Java, Ruby and JavaScript.","blip_selector":"vert-x","name":"Vert.x","url":"/radar/platforms/vert-x","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9088,"quadrant":"tools","volume_date":"2012-10","description":"IT departments are increasingly striving to liberate data from disparate systems. A broad set of approaches have been promoted under the generic term Service Oriented Architecture (SOA). This has led to confusion about what the term and approach actually means. We believe businesses do not need the complex enterprise service bus products advocated by vendors. ESBs actively undermine the reasons for choosing the bus approach: low latency, loose coupling, and transparency.","blip_selector":"esb","name":"ESB","url":"/radar/tools/esb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9002,"quadrant":"techniques","volume_date":"2012-10","description":"Machine learning, semantic analysis, text mining, quantitative analytics, and other advanced analytics techniques have steadily matured over the past 15 years. They offer incredible potential for prediction, forecasting, identifying repeatable patterns, and providing insight into unstructured data. Historically, our ability to store and rapidly analyze large amounts of audio, video and image data has been severely limited. This placed constraints on sample size, as well as the time it would take to validate analytical models and put them into production. Now, using a spectrum of new technologies like NoSQL, data harvesters, MapReduce frameworks, and clusters of shared-nothing commodity servers, we have the power necessary to make truly effective use of these techniques. Combined with the massive increase in global data available from sensors, mobile devices and social media and we see this as a field with tremendous opportunity.","blip_selector":"advanced-analytics","name":"Advanced analytics","url":"/radar/techniques/advanced-analytics","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9260,"quadrant":"platforms","volume_date":"2012-10","description":"Node.js is just one example of a class of single threaded servers with asynchronous I/O that are seeing increased popularity. A traditional web or application server associates each incoming request with a thread until all the processing tasks associated with that request are complete and the response has been sent back. If any of those tasks involve I/O, the thread blocks while that I/O takes place. This approach can waste finite resources such as file descriptors and memory since each connection occupies a thread whether or not that thread is actually consuming CPU cycles. An alternative architecture is starting to emerge in implementations like Node.js (a JavaScript server running on Google V8), Nginx (an open source web server and proxy), and Webbit (a Java application server), that uses a single thread to serve many connections, processing all I/O asynchronously. These servers support orders of magnitude more simultaneous connections because each one consumes far fewer resources.","blip_selector":"single-threaded-servers-with-asynchronous-i-o","name":"Single threaded servers with asynchronous I/O","url":"/radar/platforms/single-threaded-servers-with-asynchronous-i-o","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9129,"quadrant":"tools","volume_date":"2012-10","description":"Increasingly performant JavaScript engines, combined with widespread support for embedded SVG documents in HTML, has lead to pure JavaScript-based client-side graphing and visualization solutions gaining a lot of traction. Highcharts is one of the best ones we have come across, with out-of-the-box support for multiple highly-configurable interactive chart types, and the ability to easily render large data sets.","blip_selector":"highcharts","name":"Highcharts","url":"/radar/tools/highcharts","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9151,"quadrant":"platforms","volume_date":"2012-10","description":"A continuing cause of delivery problems lies in the use of Java Portal Server packages. These problems occur in both open source and commercial portal platforms. The promised productivity of these platforms is hindered by their complex and unwieldy programming models and difficulty in automating deployment, data migration, and tests. Although product demos are compelling, the base features of portal products are often a poor fit for real web applications, while the extra advertised features such as single sign-on or search are usually already served by existing, targeted, enterprise assets.","blip_selector":"java-portal-servers","name":"Java portal servers","url":"/radar/platforms/java-portal-servers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9004,"quadrant":"techniques","volume_date":"2012-10","description":"Applying agile methods to data warehousing, business intelligence and agile analytics provides better return and improved business responsiveness. This is done by applying lightweight technologies like REST services to move data around in near real-time instead of batch updates. This allows information about customer behavior and application usage to be derived and responses incorporated within the applications for better user experience and data visualization.","blip_selector":"agile-analytics","name":"Agile analytics","url":"/radar/techniques/agile-analytics","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9181,"quadrant":"platforms","volume_date":"2012-10","description":"Meteor.js is a client- and server-side JavaScript application framework, run inside a web browser, or in a Node.js container, and backed by MongoDB for persistence. It uses “Smart Packages” - little bundles of code that can run in the browser or as part of a cloud service. It allows hot code deploys and live in-browser updates. We think the idea is great, even if the framework is not yet ready for primetime.","blip_selector":"meteor-js","name":"Meteor.js","url":"/radar/platforms/meteor-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9166,"quadrant":"platforms","volume_date":"2012-10","description":"One style of virtualization that is particularly attractive for SaaS and PaaS implementations is the virtual container. Linux containers such as OpenVZ provide the isolation and management benefits of a virtual machine without the overhead usually associated with general-purpose virtualization. In the container model, the guest OS is limited to being the same as the underlying host OS, but that is not a serious limitation for many cloud applications.","blip_selector":"linux-containers","name":"Linux containers","url":"/radar/platforms/linux-containers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9298,"quadrant":"platforms","volume_date":"2012-10","description":"There are a number of enterprise software packages on the market that purport to offer flexible functionality with zero coding. This is certainly an appealing notion – that a non-technical business user could configure software to the unique requirements of any business without learning a programming language or hiring a professional software developer. However, it should be kept in mind that any change that affects the behavior of software in production, whether it is code, configuration, data or environments, could cause defects or failures in the business system. Writing code is only one step in a professional software production lifecycle. The need for repeatable analysis, testing, build, and deployment does not go away because the system is modified via a dragand- drop interface instead of a high-level programming language. When evaluating a zero-code package, ensure that the the product supports these processes and that you have the necessary IT support structures in place to implement them.","blip_selector":"zero-code-packages","name":"Zero-code packages","url":"/radar/platforms/zero-code-packages","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9080,"quadrant":"platforms","volume_date":"2012-10","description":"We find that many businesses are starting to build their own internal cloud deployment environments that can be easily replicated for development and testing environments. In many cases, provisioning is selfservice, and with a single keystroke, developers can create a set of hosts that implement core enterprise assets and collaborating systems. In a sense, this is a domain-specific PaaS offered to internal customers.","blip_selector":"domain-specific-paas","name":"Domain-specific PaaS","url":"/radar/platforms/domain-specific-paas","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9154,"quadrant":"tools","volume_date":"2012-10","description":"With mobile applications on the rise, JavaScript size and performance is even more critical. JavaScript micro frameworks have emerged as a direct response to ‘bloat’ in some of the larger libraries. These small libraries do exactly one thing, such as DOM selection or MVC, and can be under one kilobyte in size. By combining a number of micro frameworks, developers can get exactly the functionality they need without the overhead of a larger library. Microjs.com hosts a collection of these micro frameworks, as well as a tool that can bundle them into a single library.","blip_selector":"javascript-micro-frameworks","name":"JavaScript micro frameworks","url":"/radar/tools/javascript-micro-frameworks","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9227,"quadrant":"tools","volume_date":"2012-10","description":"Of all the build tools and languages we use across our projects, the one we keep coming back to is Rake. Rake is a beautiful, simple and powerful build language implemented as an internal Domain-Specific Language on Ruby. Ruby’s ability to run across several virtual-machine platforms means that Rake is equally available - while leaving open the option to utilize more language-specific tools for some tasks. Finding a similar combination of elegance and flexibility is difficult regardless of your platform, so we recommend trying Rake for Java and .Net projects.","blip_selector":"rake-for-java-net","name":"Rake for Java & .NET","url":"/radar/tools/rake-for-java-net","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9008,"quadrant":"languages-and-frameworks","volume_date":"2012-10","description":"We are seeing a common pattern of creating single-page web applications. Rather than requiring full page refresh, these request smaller sets of data from the server, and change the displayed content of their page through modifying the DOM. To make this more manageable, JavaScript MV* frameworks have been developed that support data binding, client-side templates, and validation. While lightweight applications may not need a framework, for more complex scenarios, AngularJS and Knockout should be considered as the current front-runners in this field.","blip_selector":"angularjs-and-knockout","name":"AngularJS and Knockout","url":"/radar/languages-and-frameworks/angularjs-and-knockout","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9097,"quadrant":"languages-and-frameworks","volume_date":"2012-10","description":"Microsoft’s F# continues to evolve, with the recent release of F# 3.0 beta. F# is excellent at concisely expressing business and domain logic. Developers trying to achieve explicit business logic within an application may opt to express their domain in F# with the majority of plumbing code in C#. | The functional languages F#, Clojure and Scala still reside in the assess ring of the radar. Interest in functional languages continues to grow. Two characteristics of functional languages in particular are driving this interest, immutability with its implications for parallelism and functions as first class objects. While the introduction of closures to C# brings some of the latter capability, functional languages are almost synonymous with immutability. The placement of these languages within the assess ring indicates our view of their relative maturity and appropriateness. F#, based on OCaml, is fully supported within the Visual Studio toolset. F# includes support for objects and imperative constructs in addition to functional language constructs in a natural way. Scala, like F#, combines the object and functional paradigms, although the syntax of Scala is more Java-like. Clojure began as a JVM language and is now available on the .NET CLR. Clojure does allow for mutable state although it has an extensive set of immutable persistent data structures, all supporting multi-threaded applications. There are many similarities between these three languages, but at the moment we believe F# and Clojure to be better suited to most organizations for assessing than Scala. More work clearly needs to be done to validate this assertion. | In the previous radar, we lumped functional languages together in a group. For this version, we’ve exploded that group and started calling out the ones interesting to us. Of the current crop of functional languages, the one we like the most is Clojure: a simple, elegant implementation of Lisp on the JVM. The other two that we fi nd interesting are Scala (a re-thinking of Java in functional form) and F#, the OCaml derivative from Microsoft that now appears “in the box” in Visual Studio 2010.","blip_selector":"f","name":"F#","url":"/radar/languages-and-frameworks/f","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9134,"quadrant":"platforms","volume_date":"2012-10","description":"Hybrid clouds combine the best features of public clouds and private data centers. They allow applications to run in a private data center during normal periods, and then use rented space in a public cloud for overflow capacity during peak traffic periods. There are now a number of infrastructure solutions that allow automatic and consistent deployment across a hybrid cloud, such as Palette and RightScale. With robust offerings from Amazon, Rackspace and others, we are moving hybrid clouds to “Trial” on this edition of the radar. | Hybrid clouds describe a set of patterns that combine the best features of public clouds and private data centers. They allow applications to run in a private data center during normal periods then use rented space in a public cloud for overflow capacity during peak traffic periods. Another way to combine public and private clouds in an agile way is to use the elasticity and malleability of public clouds for developing and understanding an application’s production characteristics, then moving it into permanent infrastructure in a private data center when it is stable.","blip_selector":"hybrid-clouds","name":"Hybrid clouds","url":"/radar/platforms/hybrid-clouds","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9214,"quadrant":"techniques","volume_date":"2012-10","description":"Polyglot persistence is the technique of storing data in various data stores based on efficiency and how that data is going to be used. Do not just use the default database, often an RDBMS, for all the needs of the application. Instead, ask questions like: Does session management data belong in the database or does it belong in a key-value store? Do relationships between customers and products belong in a graph database? Using NoSQL databases like MongoDB, Riak and Neo4J allows us to reconsider how data is treated, even with-in a single application.","blip_selector":"polyglot-persistence","name":"Polyglot Persistence","url":"/radar/techniques/polyglot-persistence","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9202,"quadrant":"platforms","volume_date":"2012-10","description":"Selecting the right cloud provider from an almost bewildering array of options continues to be difficult. One strategy is to adopt an open source IaaS platform such as OpenStack or CloudStack. This allows you to run a private cloud that is consistent with a public cloud, and to migrate from one cloud provider to another should the need arise. Going one step further, Apache’s Deltacloud abstracts away from specific provider APIs to give a consistent experience across cloud platforms.","blip_selector":"open-source-iaas","name":"Open source IaaS","url":"/radar/platforms/open-source-iaas","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9294,"quadrant":"techniques","volume_date":"2012-10","description":"It might sound odd for us to mention this, given how mainstream Agile development has become, but we are noticing teams rediscover and embrace work-in-progress limits. Methods such as Kanban limit the amount of in-flight work, forcing better workflow into the team and more visibility into bottlenecks.","blip_selector":"work-in-progress-limits","name":"Work-in-Progress limits","url":"/radar/techniques/work-in-progress-limits","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9235,"quadrant":"languages-and-frameworks","volume_date":"2012-10","description":"As adoption continues to expand, so does the size of many JavaScript codebases. To improve modularity of code and help manage this, we are seeing teams embrace libraries such as Require.js. Using the Asynchronous Module Definition (AMD) format, code is split into modules, easing development and maintenance, and an optimization tool then combines and minifies scripts for production deployment.","blip_selector":"require-js","name":"Require.js","url":"/radar/languages-and-frameworks/require-js","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9079,"quadrant":"languages-and-frameworks","volume_date":"2012-10","description":"Domain-Specific Languages is an old technique that we think is significantly under-used. We hope that the publication of Martin Fowler’s latest book will encourage more people to utilize them. | A significant amount of innovation occurred in the JavaScript space thanks to the Ruby on Rails community. This same community has helped to move both internal and external DSLs forward as a means for more closely mapping business requirements in code. Ruby’s syntax lends itself easily to the creation of easily readable DSLs, while language tools such as ANTLR help to make the creation of new domain specific languages more accessible to interested developers.","blip_selector":"domain-specific-languages","name":"Domain-Specific Languages","url":"/radar/languages-and-frameworks/domain-specific-languages","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9283,"quadrant":"tools","volume_date":"2012-10","description":"Some tools seek to enable and facilitate different ways of working. Unfortunately other tools are created using a different premise, one of low trust in users and the need to enforce a predefined process. ClearCase and TFS do this. This makes version control systems with “implicit workflow” unsuitable tools for modern agile software development. Project methodologies and the best ways of working on a project need to emerge. Tools that enforce high ceremony around things like check in just get in the way and kill productivity.","blip_selector":"vcs-with-implicit-workflow","name":"VCS with “implicit workflow”","url":"/radar/tools/vcs-with-implicit-workflow","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9109,"quadrant":"tools","volume_date":"2012-10","description":"In a mixed Ruby/Java application, running on the JVM, there are differences in package format and dependency resolution that need to be dealt with. By providing an Ivy compatible proxy that packages RubyGems as JARs and uses Ivy to resolve Gem dependencies, GemJars consolidates and simplifies the building of truly polyglot codebases.","blip_selector":"gemjars","name":"GemJars","url":"/radar/tools/gemjars","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9114,"quadrant":"languages-and-frameworks","volume_date":"2012-10","description":"Dart is Google’s attempt at creating a programming language to replace JavaScript due to JavaScript’s perceived flaws and inherent performance issues. Dart, in line with previous Google languages, provides Java-like syntax and semantics that are intended to be more appealing than JavaScript’s prototype-based nature. Reception within the browser-development community has been understandably cool and it remains to be seen if the language will become more widely accepted, though Chrome’s continued rise and the search for alternatives like CoffeeScript may yet shift that balance.","blip_selector":"google-dart","name":"Google Dart","url":"/radar/languages-and-frameworks/google-dart","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9018,"quadrant":"platforms","volume_date":"2012-10","description":"One of the foundational technologies of the Web as platform, Atom is an extensible data syndication format with broad tool support in almost all languages. In conjunction with the Atom Publication Protocol, Atom comprises a lightweight platform for publishing and consuming data with high quality-of-service guarantees. Atom-based solutions trade scalability for latency, however, making Atom often inappropriate for very low-latency scenarios.","blip_selector":"atom","name":"ATOM","url":"/radar/platforms/atom","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9185,"quadrant":"platforms","volume_date":"2012-10","description":"Despite apparent resistance in the Global North, mobile payment systems such as Kenya’s M-Pesa are providing secure cashless monetary transactions. With the service rolling out across Africa, the system opens up the market for the millions of people with mobile phones but lacking access to traditional banking outlets. Providers such as Square are slowly improving the situation, but the North continues to lag.","blip_selector":"mobile-payment-systems","name":"Mobile payment systems","url":"/radar/platforms/mobile-payment-systems","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9072,"quadrant":"techniques","volume_date":"2012-10","description":"Tools such as Pallet offer a compelling approach to environment creation and management through declarative provisioning. Usually, this is accomplished by declaring your environment topology - a number of instances, OS, network configuration and applications - using a DSL, and then creating the entire environment automatically via a commandline tool. This approach differs in the decoupling of instance creation and application provision, and in the addition of the ability to declare dependencies between domain-specific application-level services over multiple boxes.","blip_selector":"declarative-provisioning","name":"Declarative provisioning","url":"/radar/techniques/declarative-provisioning","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9056,"quadrant":"techniques","volume_date":"2012-10","description":"Application deployments often suffer from an excess of environment-specific configuration settings, including the hostnames of dependent services. Configuration in DNS is a valuable technique to reduce this complexity by using standard hostnames like ‘mail’ or ‘db’ and have DNS resolve to the correct host for that environment. This can be achieved in multiple ways, including split-horizon DNS or configuring search subdomains. Collaboration between development teams and IT operations is essential to achieve this, but that is unfortunately still difficult in some organizations.","blip_selector":"configuration-in-dns","name":"Configuration in DNS","url":"/radar/techniques/configuration-in-dns","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9100,"quadrant":"techniques","volume_date":"2012-10","description":"Disappointingly, we continue to see development teams embrace the practice of feature branching to isolate work and defer integration. Feature branches commonly cause significant pain and unpredictability during late merges, but more importantly prevent the continual design improvement necessary to maintain high quality software. We recommend continuous integration and branch by abstraction as an alternative to feature branching.","blip_selector":"feature-branching","name":"Feature branching","url":"/radar/techniques/feature-branching","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9292,"quadrant":"platforms","volume_date":"2012-10","description":"Despite a promising start to Windows Phone, a well thought-out user interface, and probably the best development experience of any mobile platform, we have seen several stumbles in the execution of the platform strategy by Microsoft and its partners. This makes us less optimistic about the future of the platform than we were in the last radar. | Windows Phone 7 has surprised even some of the long time critics of Windows platforms. After many failed attempts, Microsoft has managed not only to produce a mobile operating system that provides a user experience on par with the other major contenders in the space but also the development support to go with it. Microsoft is making Windows Phone 7 a viable competitor and another choice for a more integrated experience in the corporate arena. Whether it will be able to change adoption trends remains to be seen.","blip_selector":"windows-phone","name":"Windows Phone","url":"/radar/platforms/windows-phone","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9256,"quadrant":"tools","volume_date":"2012-10","description":"There are a couple of usability testing tools that match our preferred ‘guerrilla’ approach. Eye-tracking has long been a useful technique when designing compelling user interfaces, however the equipment and software associated with it is expensive and typically requires the use of specialist firms. Crazy Egg is a cheaper, software-only solution that produces heat maps based on mouse movement. This movement has a strong correlation with gaze, and can be used as a reasonable approximation. Silverback captures not only the screen during a test, but also records the face and voice of the user. This can be invaluable in sharing rich test experiences with the wider development team.","blip_selector":"silverback","name":"Silverback","url":"/radar/tools/silverback","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9246,"quadrant":"tools","volume_date":"2012-10","description":"Rather than wrestling with licenses and setting up clusters of machines for performance testing, we’re seeing a rise in SaaS performance testing tools such as Blitz.io and Tealeaf. These services make it easy to run performance tests with a huge number of geographically diverse clients, without investing heavily in infrastructure.","blip_selector":"saas-performance-testing-tools","name":"SaaS performance testing tools","url":"/radar/tools/saas-performance-testing-tools","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9037,"quadrant":"platforms","volume_date":"2012-10","description":"This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"care-about-hardware","name":"Care about hardware","url":"/radar/platforms/care-about-hardware","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9075,"quadrant":"techniques","volume_date":"2012-10","description":"With deployment automation tools maturing, including PowerShell on Windows, scripts are increasingly sophisticated and contain a lot of logic. We recommend deployment and scripting test tools, such as Pester for PowerShell and TOFT for Chef and Puppet. It is critical to have good test coverage around the most important aspects of your deployment automation.","blip_selector":"deployment-and-scripting-test-tools","name":"Deployment and scripting test tools","url":"/radar/techniques/deployment-and-scripting-test-tools","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9171,"quadrant":"tools","volume_date":"2012-10","description":"While MVC has been a staple of web development for the past few years, most libraries and frameworks fail to adhere to one of its most important principles: keeping logic out of the view layer. The consequences of not having logicfree markup include complex dependencies, difficulty testing and inability to reuse code. Recent DSLs like Mustache are available for many languages and platforms and have started to turn the trend. They allow editing in any tool, without extra requirements for language support and provide huge gains for UI development and overall application design.","blip_selector":"logic-free-markup","name":"Logic-free markup","url":"/radar/tools/logic-free-markup","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9247,"quadrant":"languages-and-frameworks","volume_date":"2012-10","description":"CSS has been an extremely popular and effective DSL for styling web pages. It does, however, have some annoying limitations which have led to a number of languages that build on CSS to make it easier to write and modify. We’ve had good experiences with SASS, SCSS, and LESS.","blip_selector":"sass-scss-less-and-stylus","name":"SASS, SCSS, LESS, and Stylus","url":"/radar/languages-and-frameworks/sass-scss-less-and-stylus","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9013,"quadrant":"tools","volume_date":"2012-10","description":"Apple’s mobile devices are going strong and native apps are a cornerstone of their success. Writing these native apps has become much more pleasant and productive since JetBrains launched AppCode, an IDE for iOS and OS X development that replicates the strengths of their IDEs for other platforms.","blip_selector":"appcode","name":"AppCode","url":"/radar/tools/appcode","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9234,"quadrant":"techniques","volume_date":"2012-10","description":"Bringing users in to a controlled environment for formal testing can be a slow and expensive proposition. Much useful, qualitative feedback can be gathered quickly and cheaply through guerrilla user testing - by going out into the world and testing with small samples of the general public. Another alternative is remote usability testing, where you can send out everything from wireframes to final applications for testing by people all over the world. Usabila, Loop11 and Treejack all provide tools where you can ask users to carry out specific tasks, and capture everything from the time taken to complete a task, to the user’s thoughts and feelings while doing so.","blip_selector":"remote-usability-testing","name":"Remote usability testing","url":"/radar/techniques/remote-usability-testing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9169,"quadrant":"tools","volume_date":"2012-03","description":"This tool was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"log-aggregation-indexing","name":"Log aggregation & indexing","url":"/radar/tools/log-aggregation-indexing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9014,"quadrant":"platforms","volume_date":"2012-03","description":"AppHarbor is a Platform as a Service (PaaS) offering for the .NET Platform using the same pricing model and structure pioneered by Heroku. It is a promising take on the deployment of .NET applications as it abstracts away most of the underlying configuration needs that come with the platform. It is maturing quickly and we expect it will see growing interest in time to come.","blip_selector":"appharbor","name":"AppHarbor","url":"/radar/platforms/appharbor","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9279,"quadrant":"platforms","volume_date":"2012-03","description":"Ubiquitous computing is tricky term as it covers many different ideas. What we find interesting and exciting at the moment is that both consumer and specialist mobile devices are increasingly based on commodity operating systems such as Android or iOS. This means that in many cases, software can be developed by organizations themselves, opening the door to innovative new applications without requiring expensive niche skills. Lower price points for the hardware also make this area more accessible, especially with peripherals like payment card readers, PIN key pads and high quality bar code scanners becoming available for both Android and iOS devices. When combined with features already available on these consumer devices, whole new ways of working open up.","blip_selector":"ubiquitous-computing","name":"Ubiquitous computing","url":"/radar/platforms/ubiquitous-computing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9042,"quadrant":"tools","volume_date":"2012-03","description":"This tool was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"client-side-mvc","name":"Client-side MVC","url":"/radar/tools/client-side-mvc","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9052,"quadrant":"platforms","volume_date":"2012-03","description":"One of the principal mechanisms that allows agile software development to work is feedback loops. One common yet expensive broken feedback loop we have observed is the lack of communication between those responsible for hardware and software. The end result creates cost but not worth. You must view architecture holistically; neither hardware nor software has a full enough perspective to be successful alone.","blip_selector":"communication-between-those-responsible-for-hardware-and-software","name":"Communication between those responsible for hardware and software","url":"/radar/platforms/communication-between-those-responsible-for-hardware-and-software","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9048,"quadrant":"tools","volume_date":"2012-03","description":"Many organizations try to minimize change in production IT environments. This frequently leads to behavioral anti-patterns. One example of this is over use of code in configuration to affect the behavior of production systems. Changes that really belong in code end up in configuration files which don’t necessarily pass through the same levels of testing as the application. Streamlining the path to production and focusing on quality simplifies rather than complicate things.","blip_selector":"code-in-configuration","name":"Code in configuration","url":"/radar/tools/code-in-configuration","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9239,"quadrant":"platforms","volume_date":"2012-03","description":"We have long been less than enthusiastic about RIA technologies such as Flash and Silverlight because of vendor lock in potential, anemic support for agile engineering practices, and potential for overuse. It seems even the large vendors are starting to agree with us. Now that modern versions of HTML handle most of the common cases that formerly required RIA, we feel that new projects must have enormous justification and careful strategic thought before using any of these technologies. | Rich Internet Applications (RIA) are a popular topic, driven by the effort and marketing of big vendors pushing their offerings. RIA is useful for complex visualizations but ill-suited for other programming tasks because it doesn’t fully support the engineering hygiene we require for our tools: testing is difficult and application partitioning is cumbersome. These frameworks also don’t support common elements we take for granted in applications hosted in a browser: bookmarking, addressability, browser controls, and other aspects. We’re not entirely critical of these tools, but think that their sweet spot is rich visualizations, not building traditional data entry CRUD applications. | Our position on Rich Internet Applications has changed over the past year. Experience has shown that platforms such as Silverlight, Flex and JavaFX may be useful for rich visualizations of data but provide few benefits over simpler web applications. Given that these toolsets have limited support for automated testing, it would suggest that a more traditional web application stack provides greater value for enterprise development. We recommend only using RIA platforms for rich visualizations incorporated into web applications, not as comprehensive development targets.","blip_selector":"ria","name":"RIA","url":"/radar/platforms/ria","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":521521,"quadrant":"languages-and-frameworks","volume_date":"2012-03","description":"While HTML5 is an evolving standard, many elements have reached the stage where they can be safely used in production to create both on and offline mobile web applications. Based on our projects we think HTML5 is ready to be adopted for mobile web applications. As the standard continues to evolve we expect HTML5 will become an increasingly viable alternative to native applications with the distinct advantage of being inherently cross platform. | HTML 5 continues to be the preferred choice for developing complex Web-based applications, with features including improved integration of rich audio and video content, clientside storage, better document structure, Web sockets and offline use. Safari, Chrome, Firefox and Opera each support significant subsets of the proposed standards, with support coming in Internet Explorer 9. HTML 5 is likely to remain in draft for some time to come, however; early adopters may wish to reflect on the bleakly comedic saga of two separate groups attempting to drive its evolution. | HTML 5 offers a large number of improvements over HTML 4 and XHTML 1.0. Many of these improvements are focused on providing support for developing complex web applications, and improving integration of rich content such as audio and video in standard ways. Features such as client-side storage, web sockets and offline use will further establish the position of the web browser as a viable enterprise application platform. | This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"html-5","name":"HTML 5","url":"/radar/languages-and-frameworks/html-5","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9161,"quadrant":"platforms","volume_date":"2012-03","description":"This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"kvm","name":"KVM","url":"/radar/platforms/kvm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9275,"quadrant":"techniques","volume_date":"2012-03","description":"All too often caching is an afterthought used to address performance problems with a blanket approach and common cache lifetime. This leads to issues and workarounds. The “time value” of information is inherently linked to the business purpose and hence needs to be captured at the same time as other requirements. We believe thoughtful caching should be addressed early in the project and not just treated as a last minute performance fix.","blip_selector":"thoughtful-caching","name":"Thoughtful caching","url":"/radar/techniques/thoughtful-caching","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9265,"quadrant":"tools","volume_date":"2012-03","description":"Measuring software internal quality is still a mystery, even though many source code metrics have been around for years. The problem with those metrics is they usually only capture one aspect of quality. We must consult many metrics to come to a conclusion about the overall quality of our code. Sonar is an integrated tool for checking, tracking and visualizing those metrics. It not only combines metrics together, but also mixes them with historical measures, giving us a better insight into the internal quality of the codebase.","blip_selector":"sonar","name":"Sonar","url":"/radar/tools/sonar","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9128,"quadrant":"platforms","volume_date":"2012-03","description":"Heroku is a beautifully simple Platform as a Service (PaaS). Although Heroku began as a Ruby on Rails platform, it is evolving to support a variety of languages and web frameworks, most recently Clojure. Heroku uses a standard stack and deploys applications with a simple Git push. Heroku’s recent acquisition by Salesforce.com has not diminished its service quality. | Heroku is a beautifully simple Platform as a Service (PaaS) for Rack-compatible frameworks such as Ruby on Rails. In contrast to similar offerings for other languages, which often limit development to a programming model specific to the service platform, Heroku uses the standard Rails stack and even allows deployment with a plain Git push. Heroku was recently acquired by Salesforce.com and so has significant backing behind their service.","blip_selector":"heroku","name":"Heroku","url":"/radar/platforms/heroku","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9217,"quadrant":"tools","volume_date":"2012-03","description":"Powershell is as important tool for managing Windows servers and applications. Built into Windows 2008 and Windows 7, Powershell allows Unix-like scripting and automation across a server farm. Scripts can be executed on remote machines, and a single command can manage hundreds of machines at once. Powershell scripts can deploy and configure applications and operating system components, and can be extended by writing .NET “commandlets.”","blip_selector":"powershell","name":"PowerShell","url":"/radar/tools/powershell","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9277,"quadrant":"platforms","volume_date":"2012-03","description":"While virtualization is on the rise, some organizations are treating virtual machines like physical infrastructure. We frown on doing a full operating system install for each VM or using VMs for load testing. Virtual machines can be cloned, snapshotted, and manipulated in ways physical machines cannot, and also have vastly different performance characteristics than physical hardware. VMs should be used with full understanding of their benefits and limitations, otherwise you can really get into trouble with them.","blip_selector":"treating-vms-like-physical-infrastructure","name":"Treating VMs like physical infrastructure","url":"/radar/platforms/treating-vms-like-physical-infrastructure","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9116,"quadrant":"platforms","volume_date":"2012-03","description":"The use of GPUs for computing offers efficiencies and performance for certain classes of problems that would be prohibitively expensive for more traditional hardware. Problems that fit Single Instruction Multiple Data (SIMD) processing models can gain significant advantages at the cost of difficult learning curves using specialized APIs. OpenCL, CUDA from NVidia and DirectCompute from Microsoft offer developers access to General-purpose computing on graphics processing units (GPGPU).","blip_selector":"gpgpu","name":"GPGPU","url":"/radar/platforms/gpgpu","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9001,"quadrant":"techniques","volume_date":"2012-03","description":"Many teams focus acceptance testing at the story level, but this can lead to a large number of hard to maintain tests, and a focus on completing individual stories instead of coherent functionality needed to go live. A more holistic approach is to group user stories into journeys for which we create acceptance tests. Journeys through a system are a set of user interactions that provide value for both users and the business. At the outset a journey acceptance test will cover only one step, but as stories are completed the journey is expanded to encompass each stage in the user’s progress. Once the acceptance test of journeys passes, this tells us we have delivered real value.","blip_selector":"acceptance-test-of-journeys","name":"Acceptance test of journeys","url":"/radar/techniques/acceptance-test-of-journeys","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9203,"quadrant":"platforms","volume_date":"2012-03","description":"OpenSocial is a specification that provides a standard way to share content between semi-trusted applications. While initially proposed for public facing social networking sites, it has possibly more potential within the corporate firewall, where the benefits of being able to share data and content between applications in a standard manner frequently outweigh the requirements of scale and security.","blip_selector":"opensocial","name":"OpenSocial","url":"/radar/platforms/opensocial","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9259,"quadrant":"techniques","volume_date":"2012-03","description":"A key step in the Continuous Delivery process is the ability to release software arbitrarily close to when the business wants it. The ability to do single command deploy relies on a complete set of activities that fall under the umbrella of Continuous Delivery including extensive automation of everything from build/test to scripted environment provisioning and deployment. We have found that adopting this as a goal tends to drive the automation and testing pre-requisites upstream into the rest of your organization.","blip_selector":"single-command-deploy","name":"Single command deploy","url":"/radar/techniques/single-command-deploy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9064,"quadrant":"tools","volume_date":"2012-03","description":"With very few exceptions, tools that claimed to create seamless user experiences across Windows, Linux and OSX did not deliver. We ended up with compromised experiences on one or more of the operating systems. Mobile adds complexity to this problem with different hardware form factors and conventions for user interactions. We have made several attempts to use cross platform mobile toolkits on our projects with varying degrees of success. We saw issues like having to create a project for each platform or invoking specific native UI widgets to get things working. For these reasons we have put cross platform mobile toolkits in hold. While this may change in the future, we remain skeptical especially given past experiences on hardware that was far more homogeneous.","blip_selector":"cross-platform-mobile-toolkits","name":"Cross-platform mobile toolkits","url":"/radar/tools/cross-platform-mobile-toolkits","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9271,"quadrant":"techniques","volume_date":"2012-03","description":"The advent of behavior-driven design (BDD) testing frameworks like Cucumber, combined with browser automation tools like Selenium, has encouraged widespread use of acceptance testing at the browser level. This unfortunately encouraged doing the bulk of testing where the cost to run the tests is the greatest. Instead, we should test at the appropriate level, as close to the code as possible, so that tests can be run with maximum efficiency. Browser-level tests should be the icing on the cake, supported by acceptance and unit tests executed at appropriate layers.","blip_selector":"test-at-the-appropriate-level","name":"Test at the appropriate level","url":"/radar/techniques/test-at-the-appropriate-level","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9096,"quadrant":"techniques","volume_date":"2012-03","description":"Experience Design (XD) is an example of ways in which agility must evolve to accommodate real-world constraints. We are always interested in finding innovative ways to incorporate what have traditionally been up-front exercises into practices like Continuous Delivery. XD is a ripe field for study.","blip_selector":"experience-design-xd","name":"Experience Design (XD)","url":"/radar/techniques/experience-design-xd","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9030,"quadrant":"techniques","volume_date":"2012-03","description":"Building your own technology radar helps you decide, normalize, and publicize consensus technology views for all interested parties. Thoughtworks produces a technology radar for clients and friends, telling the world our opinions about upcoming technology trends. You should do this for your own company as well. Too many decisions in large companies happen in a vacuum, with no input from the technologists who have to live with them every day.","blip_selector":"build-your-own-radar","name":"Build your own radar","url":"/radar/techniques/build-your-own-radar","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9257,"quadrant":"techniques","volume_date":"2012-03","description":"Starting performance tests late in a project is risky and costly. Very simple performance tests that exercise key parts of the system, run on a regular basis, are good enough to track trends, so we can react if we see a change in performance. Run these tests with your build or as an overnight job and graph the results to create simple performance trending. Complex performance tests in a truly representative environment are still useful, but don’t wait for them to start understanding how the performance of your code is changing.","blip_selector":"simple-performance-trending","name":"Simple performance trending","url":"/radar/techniques/simple-performance-trending","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9103,"quadrant":"tools","volume_date":"2012-03","description":"There are many advantages to using OS-native packages to deploy components and dependencies, however the tools which build native packages for Linux are not trivial. FPM is a useful tool which makes it easy to create RPM, DEB, or Solaris packages with a minimum of fuss.","blip_selector":"fpm","name":"FPM","url":"/radar/tools/fpm","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9254,"quadrant":"techniques","volume_date":"2012-03","description":"A decade ago when memory was at a premium, application servers made a lot of sense. They were popular and useful as a mechanism to run and manage multiple applications on a shared server or cluster. These days applications are more often run on separate physical or virtual servers and the need for an application server is reduced. Consider evaluating server / application container end-of-life within your organization, and only use one if you benefit from the added complexity.","blip_selector":"server-application-container-end-of-life","name":"Server / application container end-of-life","url":"/radar/techniques/server-application-container-end-of-life","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9156,"quadrant":"tools","volume_date":"2012-03","description":"JavaScript is now established as a powerful, mainstream language that can be used in a variety environments both on client and server sides. As JavaScript codebases expand, more JavaScript tooling support becomes necessary, especially in the continuous integration and testing spaces. Tools like Backbone.js, SpineJS, JavaScriptMVC, Jasmine, JSTestDriver and HRcov are coming to the forefront. They are created by a vibrant community that continues to grow.","blip_selector":"javascript-tooling","name":"JavaScript tooling","url":"/radar/tools/javascript-tooling","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9067,"quadrant":"techniques","volume_date":"2012-03","description":"We have long advocated for both static and dynamic code analysis tools to help glean information about your code base. As the focus of software development broadens because of the Continuous Delivery movement, data visualizations of development and operations with effective, actionable profiling and monitoring should be part of your technical stack as well.","blip_selector":"data-visualizations-of-development-and-operations","name":"Data visualizations of development and operations","url":"/radar/techniques/data-visualizations-of-development-and-operations","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9111,"quadrant":"tools","volume_date":"2012-03","description":"Starting from a challenge posed to the Linux community to stop using commercial version control, Git has proved itself. Git embodies a well architected, high performance implementation of distributed version control. Git is powerful, so it should be used with respect, but that power enables agile engineering workflows that simply cannot exist with other tools. Git’s popularity is supported by the existence of GitHub. GitHub combines public and private Git repositories, social networking, and a host of other innovative tools and approaches. | Subversion moves back into the Adopt section of the radar because it is a solid version control tool suitable for most teams. We consider Subversion’s features to be the basic standard for a modern version control tool. Thoughtworkers continue to embrace and recommend Distributed Version Control tools such as Git and Mercurial, but we caution that these systems often require deeper understanding to get the most out of them. New to the radar is GitHub, a “social coding” tool supporting both source code hosting and social networking. GitHub is arguably one of the main reasons Git has become the leading DVCS tool, and GitHub’s collaboration features are often used by enterprises that need to support distributed teams.","blip_selector":"github","name":"GitHub","url":"/radar/tools/github","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9093,"quadrant":"techniques","volume_date":"2012-03","description":"If the rate at which business is changing is an indicator of change in requirements, then the days of doing upfront database design are gone. Instead, projects should follow evolutionary database techniques and continue to change their database schemas as new requirements are implemented over the course of the project. Deployment of database changes should also be automated so that the application release that relies on those changes does not have to wait for manual deployment of the database changes. Automated database deployment ensures that application and database changes can be deployed automatically. Evolutionary database and automated database deployments ensure highly productive teams a path to continuous delivery. | The industry has seen significant changes to the way we use and store data over the past few years. Agile development practices have lead to greater emphasis on evolutionary database design, requiring new tools that support migration of schemas in line with changes to the domain model of an application. As storage space consistently becomes cheaper and data access speeds increase, many organizations are investigating the use of multiple schemas to hold data for different purposes, e.g. transactional and analysis schemas. Incremental data warehousing is becoming increasingly popular as the cost of moving data between a transactional data store and an analysis environment is less than the value of having access to near real-time reporting of critical business data.","blip_selector":"evolutionary-database","name":"Evolutionary database","url":"/radar/techniques/evolutionary-database","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9092,"quadrant":"techniques","volume_date":"2012-03","description":"We recommend adopting evolutionary architecture as an alternative to traditional up-front, heavy-weight enterprise architectural designs. | In contrast to traditional up-front, heavy-weight enterprise architectural designs, we recommend adopting evolutionary architecture. It provides the benefits of enterprise architecture without the problems caused by trying to accurately predict the future. Instead of guessing how components will be re-used, evolutionary architecture supports adaptability, using proper abstractions, database migrations, test suites, continuous integration and refactoring to harvest re-use as it occurs within a system. The driving technical requirements for a system should be identified early to ensure they are properly handled in subsequent designs and implementations. We advocate delaying decisions to the latest responsible moment, which might in fact be up-front for some decisions. | One principle of agile software development is the notion of the last responsible moment. This notion applied to architectural considerations is controversial among traditional architects. We believe that, given properly articulated principles and appropriate test suites, architectures can evolve to meet the changing needs of a system, allowing for architectural decisions to be made at the last responsible moment without compromising the integrity of the system. We call this approach evolutionary architecture, in that we allow the architecture to evolve over time, always respecting the architectural guiding principles. | We assist many of our clients in adapting enterprise software architecture practices to fit within an Agile software delivery approach. In the past year we have seen increased interest in evolutionary enterprise architecture and how service oriented architectures shape the boundaries between enterprise units. The value of an evolutionary approach to enterprise architecture is the creation of lighter weight systems that ease integration between disparate parts. By embracing this approach and the notion of the web as an enterprise application platform, we have reduced overall complexity of application architectures, increased quality and scalability, and reduced development costs.","blip_selector":"evolutionary-architecture","name":"Evolutionary architecture","url":"/radar/techniques/evolutionary-architecture","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9046,"quadrant":"platforms","volume_date":"2012-03","description":"Cloud Foundry is an open source Platform as a Service that can be deployed in your own data center or hosted by VMWare. At present Cloud Foundry supports Java/ Spring applications, Rails, Sinatra, Grails and Node.js. Additional services include MongoDB, MySQL and Redis. The platform seems to be enjoying active development with the recent addition of Scala and Lift support. Cloud Foundry is an interesting addition to the growing list of PaaS solutions. It is not clear what the relationship between vFabric and Cloud Foundry will be going forward.","blip_selector":"cloud-foundry","name":"Cloud Foundry","url":"/radar/platforms/cloud-foundry","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9220,"quadrant":"techniques","volume_date":"2012-03","description":"Continuous Delivery techniques are shortening the “last mile” to get changes into production, allowing more frequent feature releases. A production immune system tracks changes as they are put into production, and automatically rolls back changes that have a negative effect on key metrics, such as revenue. Solid metrics, as well as automated A/B deployment, are required for this kind of aggressive rollback to be successful.","blip_selector":"production-immune-system","name":"Production immune system","url":"/radar/techniques/production-immune-system","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9077,"quadrant":"techniques","volume_date":"2012-03","description":"Improving the interactions and relationship between development and IT operations gives us more effective delivery and production systems that are more stable and maintainable. Creating a DevOps culture requires attention to team organization, work practices, reporting lines, and incentives - leading to joint responsibility for faster and safer delivery. We recommend adopting DevOps because we cannot see any situation where attention in this area will not have a positive benefit. | The DevOps movement continues to gain traction as people pay more attention to the often-broken relationship between development and operations. DevOps promotes closer collaboration and joint responsibility between development and operations. DevOps applies agile practices to operations processes such as provisioning, change management and production monitoring and also brings productionlike thinking, tools and environments to development. DevOps is a key underpinning for organizations wanting to achieve continuous delivery of application releases into production. | DevOps is a new movement seeking to achieve the business need for rapid delivery of software products while maintaining the stability of live environments. It uses two approaches: first, promoting closer collaboration between development and operations; second, applying practices shared with agile (collaboration, automation, simplicity, etc) to operations processes such as provisioning, change management, and production monitoring. It encompasses culture, processes, and tools - all supporting better communication, faster feedback and delivery, and more predictable outcomes.","blip_selector":"devops","name":"DevOps","url":"/radar/techniques/devops","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9158,"quadrant":"tools","volume_date":"2012-03","description":"Thoughtworks has used jQuery Mobile on two projects with mobile websites and had mixed experiences. One project found the library very useful for dealing with device differences and graceful degradation on older browsers. On this project we were working in a way that fit with the jQuery Mobile approach. Our other project found the tool less useful and felt to some extent it was trying to force them to work a particular way that did not fit their application well. For these reasons we have decided to leave this tool in assess. If you are doing mobile web it is definitely worth spiking but it may not fit every application.","blip_selector":"jquery-mobile","name":"jQuery Mobile","url":"/radar/tools/jquery-mobile","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9031,"quadrant":"platforms","volume_date":"2012-03","description":"Many teams encounter problems that are caused by their test environment missing an expensive hardware component that is only present in production. While a pre-production environment in many cases cannot approach the scale of a production environment, all of its components should be present. We recommend not buying solutions you can only afford one of, such as SAN, firewalls or load balancers, as this prevents realistic testing anywhere but in production.","blip_selector":"buying-solutions-you-can-only-afford-one-of","name":"Buying solutions you can only afford one of","url":"/radar/platforms/buying-solutions-you-can-only-afford-one-of","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9187,"quadrant":"platforms","volume_date":"2012-03","description":"Mobile web was in our trial ring on previous radars, but we’ve moved it into adopt in recognition of the fact we have created many mobile web applications. We believe this is the right way to create web content for mobile devices. | Mobile Web was in our Assess category on previous radars, but we’ve moved it into Trial in recognition of the fact that the Web is increasingly consumed through iPhone, Android and tablet devices. Many devices can render a fair approximation of a desktop browser experience, but to present the user with a truly optimal experience we recommend adapting a site to the screen size, display and control behaviors particular to the device. Techniques such as progressive enhancement can allow a single site to adapt to both desktop and mobile browsers. Large format mobile devices, such as the Apple iPad and Amazon Kindle, provide a new model of ubiquitous computing. Their long battery life, simple interfaces and easy connectivity have the potential to change the way we interact with computers. Apple’s new user interfaces discard the familiar desktop metaphors of files and folders that have been standard since the introduction of the Macintosh in 1984.","blip_selector":"mobile-web","name":"Mobile web","url":"/radar/platforms/mobile-web","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9178,"quadrant":"techniques","volume_date":"2012-03","description":"There is a worrying trend that developers are becoming too distant from the hardware on which their code runs. Increasing virtualization and separation between development and operations makes this worse. In stark contrast some teams are writing code that leverages mechanical sympathy to get incredibly high performance from their software. The LMAX Disruptor is an open-source example in Java. For high performance cases like finance and Big data, getting closer to the metal can yield big returns.","blip_selector":"mechanical-sympathy","name":"Mechanical sympathy","url":"/radar/techniques/mechanical-sympathy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9201,"quadrant":"tools","volume_date":"2012-03","description":"Open source BI tools such as Pentaho, JasperSoft, CloverETL, Talend, BIRT and SpagoBI are matching features with the proprietary tools and allowing for easy entry into the BI space. We recommend that you assess them.","blip_selector":"open-source-bi-etl-tools","name":"Open source BI/ETL tools","url":"/radar/tools/open-source-bi-etl-tools","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9122,"quadrant":"platforms","volume_date":"2012-03","description":"GWT is a reasonable implementation of a poor architectural choice. GWT attempts to hide many of the details of the web as a platform by creating desktop metaphors in Java and generating JavaScript code to implement them. First, in many ways, JavaScript is more powerful and expressive than Java, so we suspect that the generation is going in the wrong direction. Secondly, it is impossible to hide a complex abstraction difference like that from event-driven desktop to stateless-web without leaky abstraction headaches eventually popping up. Third, it suffers from the same shortcomings of many elaborate frameworks, where building simple, aligned applications is quick and easy, building more sophisticated but not supported functionality is possible but difficult, and building the level of sophistication required by any non-trivial application becomes either impossible or so difficult it isn’t reasonable. | In the last radar we placed the Google Web Toolkit (GWT) on hold and tried to provide a few reasons for that decision. As it turned out the conciseness of the text didn’t allow us to adequately make our points so that they were not misunderstood. We are interested in a discussion but our opinion about the suitability and usability of GWT has still not changed. | Google Web Toolkit (GWT) offers an interesting premise: write Swing-like Java code and generate unit testable JavaScript widgets and user interfaces. From a practical standpoint this doesn’t work well. First, using code-gen to produce the artifacts is time consuming, artificially extending build times and requiring manual changes to obtain optimal package layout. Second, if the JavaScript doesn’t behave exactly as you want you will have to hack the generated code. Third, using Java to generate JavaScript means that you can’t take direct advantage of the powerful features of JavaScript or numerous libraries such as JQuery. Finally, the JUnit support is quite limited, for example code using reflection cannot be tested.","blip_selector":"gwt","name":"GWT","url":"/radar/platforms/gwt","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9105,"quadrant":"languages-and-frameworks","volume_date":"2012-03","description":"Functional programming continues its slow but steady ascent into developer mind share and, increasingly, code bases. New languages like Clojure, Scala, and F# offer great new features. Now libraries such as Functional Java, TotallyLazy and LambdaJ are back porting some functional language capabilities, particularly around higher-order functions and collections, into Java. We like this trend because it previews common capabilities of future languages yet allows developers to stay in their comfort zone.","blip_selector":"functional-java","name":"Functional Java","url":"/radar/languages-and-frameworks/functional-java","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9176,"quadrant":"techniques","volume_date":"2012-03","description":"Despite advances in automation, many people fall back on manual infrastructure management. We often see problems caused by manual configuration of firewalls and load balancers, and especially by DBAs cutting and pasting SQL scripts to run against production databases. All of these activities, if not fully automated, should at least be scripted and repeatable across environments.","blip_selector":"manual-infrastructure-management","name":"Manual infrastructure management","url":"/radar/techniques/manual-infrastructure-management","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9019,"quadrant":"techniques","volume_date":"2012-03","description":"If the rate at which business is changing is an indicator of change in requirements, then the days of doing upfront database design are gone. Instead, projects should follow evolutionary database techniques and continue to change their database schemas as new requirements are implemented over the course of the project. Deployment of database changes should also be automated so that the application release that relies on those changes does not have to wait for manual deployment of the database changes. Automated database deployment ensures that application and database changes can be deployed automatically. Evolutionary database and automated database deployments ensure highly productive teams a path to continuous delivery. | When moving to continuous delivery, deployment of database changes should also be automated so that the application release that relies on those changes does not have to wait for manual deployment of the database changes. Automated database deployment ensures that the full cycle of deploying application and database changes is automated.","blip_selector":"automate-database-deployment","name":"Automate database deployment","url":"/radar/techniques/automate-database-deployment","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9152,"quadrant":"languages-and-frameworks","volume_date":"2012-03","description":"Rich experiences delivered via the web to desktops, tablets and mobile devices rely heavily on JavaScript, and we continue to recommend treating JavaScript as a “first class” language within your application. Developers should carefully consider how they structure, test, refactor and maintain JavaScript code, applying the same rigor as they would with any other programming language. | The maintainability, testability and readability of JavaScript is a very significant contributor to the productivity of teams producing Web-based applications and sites. Thoughtworks believes JavaScript deserves to be treated as a first class language, viewing it as second class citizen has become an excuse for a whole series of bad practice we would not tolerate in Java or C#. We need to use the same kind of tools (e.g. unit testing) and approaches (e.g. refactoring) as we’d use for any other production language. V8 and other JavaScript engines are raising the bar on performance, while Flash & Silverlight seem to be losing momentum to HTML5 + JavaScript in areas where a rich client-like experience is required. This is good news for all interested in open standards on the Web.","blip_selector":"javascript-as-a-first-class-language","name":"JavaScript as a first class language","url":"/radar/languages-and-frameworks/javascript-as-a-first-class-language","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9057,"quadrant":"techniques","volume_date":"2012-03","description":"If you are wondering “What comes after agile?,” you should look towards continuous delivery. While your development processes may be fully optimized, it still might take your organization weeks or months to get a single change into production. Continuous delivery focuses on maximizing automation including infrastructure as code, environment management and deployment automation to ensure your system is always ready for production. It is about tightening your feedback loops and not putting off anything until the end. Continuous delivery is not the same as continuous deployment, which means deploying every change to production. Continuous delivery is not a cowboy show. It puts you in charge of your production environment. The business can pick and choose what and when to deploy. If you think you’ve nailed agile development, but aren’t considering how to achieve continuous delivery, you really haven’t even started.","blip_selector":"continuous-delivery-cd","name":"Continuous Delivery (CD)","url":"/radar/techniques/continuous-delivery-cd","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9285,"quadrant":"platforms","volume_date":"2012-03","description":"vFabric is a new Platform as a Service (PaaS) offering from VMWare. Based on enhanced versions of opensource web and messaging platforms Tomcat, Apache, and RabbitMQ, vFabric aims to deliver a Java based PaaS on a variety of cloud platforms. Currently supported platforms include VMForce, a collaboration between VMWare and force.com, Google App Engine and Amazon EC2. The addition of the GemFire in-memory distributed data management platform and Hyperic monitoring and management tool make vFabric an interesting set of technologies for Java developers looking to move to the cloud.","blip_selector":"vfabric","name":"vFabric","url":"/radar/platforms/vfabric","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9086,"quadrant":"techniques","volume_date":"2012-03","description":"Emergent design is one of the more advanced aspects of agile engineering practices, and therefore an area of active research & development. Such architectures should be driven by the underlying technical requirements of the system, rather than speculative planning for a future that may change. We have identified at least two facets of emergent design: the Lean software principle of last responsible moment, which mostly applies to greenfield projects, and finding & harvesting idiomatic patterns, which is more applicable to existing projects. | As Agile practices move further toward mainstream adoption, we see significant benefits from the adoption of Lean software development practices as well. These practices have their roots in the Toyota Production System and complement much of our understanding of Agile software development to date. One topic that Lean has also given us greater insight into is that of set-based design. Set-based design leads us to implement similar solutions at the same time while the cost of doing so is constrained. This leads us into the area of emergent design and the ability to let experience shape our design decisions and defer key decisions until the last responsible moment.","blip_selector":"emergent-design","name":"Emergent design","url":"/radar/techniques/emergent-design","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9145,"quadrant":"platforms","volume_date":"2012-03","description":"Tablet devices provide a new model of computing. The next generation of tablets show the potential for new interaction paradigms, and we expect interest and innovation to continue to escalate. | The iPhone changed the face of the mobile phone. The iPad has the potential to radically alter the way users interact with and consume Web-based resources and applications and will spawn a plethora of similar tablet devices. The addition of wireless application distribution in IOS4 allows organizations to securely host and distribute in-house applications without using the App Store, overcoming one of the main barriers to corporate adoption. IOS4’s introduction of multitasking with applications running in the background has opened up new possibilities for enterprise applications, at the cost of extra battery usage.","blip_selector":"tablet","name":"Tablet","url":"/radar/platforms/tablet","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9107,"quadrant":"languages-and-frameworks","volume_date":"2012-03","description":"This language/framework was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This language/framework was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"future-of-java","name":"Future of Java","url":"/radar/languages-and-frameworks/future-of-java","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9039,"quadrant":"techniques","volume_date":"2012-03","description":"Technical debt is a powerful and useful metaphor for dealing with the compromises we make when building software. Unfortunately it has become a catch-all term for many different kinds of issues and problems, leading to confusion and “devaluation” of the term. A very useful approach for dealing with this is catagorization of technical debt, assigning value and prioritizing debt payback in an analogous way to user stories. This helps the team focus on the most important areas and keeps issues transparent and measurable.","blip_selector":"categorization-prioritization-of-technical-debt","name":"Categorization & prioritization of technical debt","url":"/radar/techniques/categorization-prioritization-of-technical-debt","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9049,"quadrant":"techniques","volume_date":"2012-03","description":"This technique was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This technique was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This technique was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This technique was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"coding-architects","name":"Coding architects","url":"/radar/techniques/coding-architects","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9110,"quadrant":"tools","volume_date":"2012-03","description":"Starting from a challenge posed to the Linux community to stop using commercial version control, Git has proved itself. Git embodies a well architected, high performance implementation of distributed version control. Git is powerful, so it should be used with respect, but that power enables agile engineering workflows that simply cannot exist with other tools. Git’s popularity is supported by the existence of GitHub. GitHub combines public and private Git repositories, social networking, and a host of other innovative tools and approaches. | In previous radars we recommended Distributed Version Control (DVCS) tools in general while mentioning Git and Mercurial in particular. In this edition we narrow our recommendation to only Mercurial and Git as these two have become the clear front-runners. Due to its success and the associated network effect GitHub remains the recommended option for enterprises that want to interact with the open source community.","blip_selector":"git","name":"Git","url":"/radar/tools/git","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9251,"quadrant":"techniques","volume_date":"2012-03","description":"Scrum was one of the founding approaches to Agile software development, and continues to provide a worthwhile core for the management side of software development. Scrum Certification schemes have proven counterproductive, granting only a veneer of competence, which often misleads teams into a distorted experience of agility.","blip_selector":"scrum-certification","name":"Scrum certification","url":"/radar/techniques/scrum-certification","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9180,"quadrant":"tools","volume_date":"2012-03","description":"In contrast we have seen considerable success with Simple Message Buses where the integration problems are solved at the end points, rather than inside a vendor ESB system. The most well known Simple Message Bus approach is one based on the principles of REST and leveraging the proven scalability of the web. However organizations that have already invested in ESB infrastructure can leverage the useful parts of that infrastructure (reliable messaging etc) while still using a Simple Message Bus approach and performing integrations at the edges of the system.","blip_selector":"message-buses-without-smarts","name":"Message buses without smarts","url":"/radar/tools/message-buses-without-smarts","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9089,"quadrant":"techniques","volume_date":"2012-03","description":"RESTful APIs have become standard in our industry. A good REST API provides a simple, lightweight means of building customizations and integrations. However, many of the quick, high value integrations we’d like to build require knowing when something happened. Consider building an event API, which, when used in conjunction with a REST API, facilitates simple workflow, notification, and synchronization integrations. These integrations often require no more than 20 or 30 lines of code. Often event APIs take the form of a “web hook” or callback mechanism, but don’t be afraid of using a poll-based Atom style either. An Atom event API scales cheaply and gives the client the power to guarantee delivery.","blip_selector":"event-api-s","name":"Event API’s","url":"/radar/techniques/event-api-s","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9192,"quadrant":"tools","volume_date":"2011-07","description":"The Ruby language community is responsible for a number of innovations in the area of testing. The next generation of testing tools such as rspec and Cucumber are two such tools that have come out of this community. These tools, along with Thoughtworks’ Twist, provide a way to express tests in a more natural language syntax that captures the intent of the system in a way that end users can quickly grasp.","blip_selector":"next-generation-test-tools","name":"Next-generation test tools","url":"/radar/tools/next-generation-test-tools","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9244,"quadrant":"platforms","volume_date":"2011-07","description":"Charles Nutter and the JRuby team continue to improve JRuby at a frantic pace. It is fast and they place massive importance on keeping their ecosystem up-to-date, including DB adapters, gem management, and modern Rails deployment. Rails 3 + JRuby is an awesome platform. There really is no reason to not be using Ruby, one of our favorite languages, in the enterprise. | Functional languages have a wide range of practical uses, including simulation, computational fi nance, computational science, large scale data processing and parsing. These fields benefit from functional programming techniques that simplify concurrent execution and the expression of complex mathematical functions concisely. Functional programming requires a shift in thinking for enterprise developers experienced in object oriented development. Moving to an often terse syntax for solving complex problems may initially be intimidating to many. As with all forms of programming languages, syntax is just one aspect of the language itself. In functional programming another significant aspect is the use of common idioms. These idioms speed code comprehension and increase overall maintainability. This might not be news to all, but it is worth noting that dynamic languages are long ready for adoption and trial. Ruby, particularly when deployed on JRuby, is ready for adoption. Thoughtworks uses Ruby and JRuby extensively in both its Services and Product work. Groovy is ready for trial and could prove more accessible than Ruby/JRuby in a Java shop. For the right type of applications, Ruby, JRuby, and Groovy prove far more effective, expressive, and productive than Java and C#.","blip_selector":"jruby","name":"JRuby","url":"/radar/platforms/jruby","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9221,"quadrant":"techniques","volume_date":"2011-07","description":"Recent use of progressive enhancement with mobile applications has been very effective and demonstrates the universal nature of this web design strategy. We encourage people to adopt this strategy to keep their code clean and give each user the optimal experience for their device.","blip_selector":"progressive-enhancement","name":"Progressive Enhancement","url":"/radar/techniques/progressive-enhancement","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9195,"quadrant":"tools","volume_date":"2011-07","description":"NoSQL technologies are maturing daily, allowing for innovative solutions as businesses need to scale massively or ask intelligent questions of existing data. Technologies like MongoDB, Riak, Neo4J, Cassandra and many others are helping power the NoSQL space. | NoSQL is about scale, massive datasets, cloud data, social network data, data in buckets, data in graphs i.e. a range of use cases for which “traditional” SQL databases may not be the optimal choice. Unravelling NoSQL and trying to explain what it is and why you should or should not be interested in it is difficult as the term covers a wide range of technologies, data architectures and priorities and represents as much a movement or a school of thought as it does any particular technology. Types of NoSQL technologies include key-value, column and object stores as well as document, graph and XML databases.","blip_selector":"nosql","name":"NoSQL","url":"/radar/tools/nosql","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9266,"quadrant":"tools","volume_date":"2011-07","description":"Application logs are both a blessing and a curse. They are comforting to have when a production issue arises, but actually digging out the data we need usually requires cobbling together scripts written in tools such as AWK and sed. Splunk is an elegant solution that quickly analyzes many standard log file formats like IIS, Log4J and syslog, and is extensible to custom formats. It indexes files, statically or in real time, to generate canned or custom reports. If the raw log fields do not provide what you need, simply use a regular expression, either inline or to define a new field, to get the desired level of detail. Splunk’s full power is difficult to describe, so we recommend downloading and trying it.","blip_selector":"splunk","name":"Splunk","url":"/radar/tools/splunk","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9033,"quadrant":"tools","volume_date":"2011-07","description":"Application designs that incorporate caching reverse proxies as first class design elements are simpler and more resilient to infrastructure failures. Placing a caching reverse proxy between an application and a web service it consumes reduces the risk of service failures affecting the application while improving overall system performance.","blip_selector":"caching-reverse-proxies","name":"Caching reverse proxies","url":"/radar/tools/caching-reverse-proxies","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9262,"quadrant":"techniques","volume_date":"2011-07","description":"Smart Phones with a GPS, cameras and a screen are but one example of smart systems which are proliferating around us, fusing the real and the digital world. Augmented reality apps like Google Goggles, geolocation services & smart grids are just some of the possible applications.","blip_selector":"smart-systems","name":"Smart systems","url":"/radar/techniques/smart-systems","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9090,"quadrant":"techniques","volume_date":"2011-07","description":"This technique was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"event-driven-business-intelligence","name":"Event driven business intelligence","url":"/radar/techniques/event-driven-business-intelligence","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9286,"quadrant":"techniques","volume_date":"2011-07","description":"Data visualizations have been effective in business and IT decision making. Organizations are making effective use of real time data through visualizations. These visualizations include point in time data as well as trends plotted over time. We are seeing increased adoption of these techniques in optimizing operations and software development. | Evolutionary and emergent design of enterprise systems requires significant vigilance by development and architecture teams. Collecting metrics to capture development trends is a key part of understanding the stress points for a system under development. Assessing this information in its raw form is even more difficult than taking stock of a system at the source code level. To address this concern we have found a number of visualization tools and techniques to get what we refer to as the 1000ft view of the system and its internal quality. This 1000ft view allows us to identify visual patterns that help find and address issues more quickly.","blip_selector":"visualization-and-metrics","name":"Visualization and metrics","url":"/radar/techniques/visualization-and-metrics","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9035,"quadrant":"techniques","volume_date":"2011-07","description":"Initiatives that span multiple projects require shared understanding of the business context, operating model, and strategic goals of an organization, as well as any existing technical, organizational and process constraints impinging on planning and design activities. As part of our evolutionary approach to enterprise architecture, we use business capability modelling to create lightweight hierarchical models of the business functions that are an essential part of an organization’s needs and goals. Capabilities describe an organization’s operating model in terms of goals and competencies (what is to be done), rather than implementation specifics (how things are done). Whereas business architecture models based on people, process or technology are contingent, volatile and often short lived, and therefore ill-suited to the long-term planning needs of the organization, capability models provide a description of the business context that is stable enough to serve as a basis for identifying and prioritising technology and process initiatives.","blip_selector":"capability-modeling","name":"Capability modeling","url":"/radar/techniques/capability-modeling","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9141,"quadrant":"techniques","volume_date":"2011-07","description":"Like iterative software development, there is lot of value to be gained by delivering data warehousing projects using iterative techniques. Iterative data warehousing techniques allow the end users of the data warehouse to determine what reports they want and the ETL developers and data modelers to deliver those features without wasting time with data modeling and ETL jobs that do not provide immediate value to the business. | The industry has seen significant changes to the way we use and store data over the past few years. Agile development practices have lead to greater emphasis on evolutionary database design, requiring new tools that support migration of schemas in line with changes to the domain model of an application. As storage space consistently becomes cheaper and data access speeds increase, many organizations are investigating the use of multiple schemas to hold data for different purposes, e.g. transactional and analysis schemas. Incremental data warehousing is becoming increasingly popular as the cost of moving data between a transactional data store and an analysis environment is less than the value of having access to near real-time reporting of critical business data.","blip_selector":"iterative-data-warehousing","name":"Iterative data warehousing","url":"/radar/techniques/iterative-data-warehousing","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9211,"quadrant":"techniques","volume_date":"2011-07","description":"Almost every enterprise has “legacy systems” that are expensive to operate and upgrade. Often a system will become legacy over the course of several years, through neglect or atrophy. We recommend using platform roadmaps to maximize the value of a systems portfolio and plan for the upgrade and eventual retirement of systems.","blip_selector":"platform-roadmaps","name":"Platform roadmaps","url":"/radar/techniques/platform-roadmaps","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9059,"quadrant":"techniques","volume_date":"2011-07","description":"If you are wondering “What comes after agile?,” you should look towards continuous delivery. While your development processes may be fully optimized, it still might take your organization weeks or months to get a single change into production. Continuous delivery focuses on maximizing automation including infrastructure as code, environment management and deployment automation to ensure your system is always ready for production. It is about tightening your feedback loops and not putting off anything until the end. Continuous delivery is not the same as continuous deployment, which means deploying every change to production. Continuous delivery is not a cowboy show. It puts you in charge of your production environment. The business can pick and choose what and when to deploy. If you think you’ve nailed agile development, but aren’t considering how to achieve continuous delivery, you really haven’t even started. | The past 2 years or more has seen a proliferation of continuous integration tools and platforms leading to substantial innovation in the build and release space. Distribution of builds is one such innovation and yet another is the way in which builds are now structured to make greater use of automation in various stages of the build. Build pipelines help to provide greater insight into the quality of each build and the environments to which they have been deployed. A natural expansion of the build pipeline meme is the adoption of continuous deployment techniques, where the intention is to extend the build pipeline into the production environment. This relies on automated deployment techniques and authorization mechanisms built into the continuous integration toolset. One of the key benefits is the ability to move new functionality into production rapidly and reliably.","blip_selector":"continuous-deployment","name":"Continuous deployment","url":"/radar/techniques/continuous-deployment","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9268,"quadrant":"tools","volume_date":"2011-07","description":"Subversion moves back into the Adopt section of the radar because it is a solid version control tool suitable for most teams. We consider Subversion’s features to be the basic standard for a modern version control tool. Thoughtworkers continue to embrace and recommend Distributed Version Control tools such as Git and Mercurial, but we caution that these systems often require deeper understanding to get the most out of them. New to the radar is GitHub, a “social coding” tool supporting both source code hosting and social networking. GitHub is arguably one of the main reasons Git has become the leading DVCS tool, and GitHub’s collaboration features are often used by enterprises that need to support distributed teams. | Distributed version control systems such as Git and Mercurial have had significant exposure in the past year or more as open source projects move to this toolset en masse. The social networking aspect that GitHub and Bitbucket have brought to distributed version control has helped to propel these tools forward and into enterprises looking for ways to develop across multiple geographies. The move for many to a distributed version control system has resulted in a move away from tools such as Subversion and other centralized version control systems. As organizations assess and choose between these two different toolsets, we suggest that you evaluate both in relation to your team’s specific needs. While we have seen widespread adoption of distributed version control tools within Thoughtworks and beyond, we still advocate the use of continuous integration and limits to the amount of time that code is spent outside of the main branch.","blip_selector":"subversion","name":"Subversion","url":"/radar/tools/subversion","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9054,"quadrant":"techniques","volume_date":"2011-07","description":"Concurrency is a difficult problem and increasingly difficult to avoid. Hardware isn’t getting faster but multicore platforms are becoming the norm, with even mobile phones containing two or more processors. Concurrency abstractions and patterns -- which are not new, but less widely known -- are helping address many of the challenges seen in this space. In particular the models seen in Clojure, Erlang, Retlang and Event Patterns offer a more testable and reliable approach than the better known threads, locks and semaphores.","blip_selector":"concurrency-abstractions-and-patterns","name":"Concurrency abstractions and patterns","url":"/radar/techniques/concurrency-abstractions-and-patterns","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9124,"quadrant":"languages-and-frameworks","volume_date":"2011-07","description":"HAML is a language that allows you to use indentation to lay out the structure of HTML. While not a general replacement for HTML, it is effective for focusing on the hierarchical structure of tags.","blip_selector":"haml","name":"HAML","url":"/radar/languages-and-frameworks/haml","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9219,"quadrant":"techniques","volume_date":"2011-07","description":"One of the goals of SOA has been to decouple services by exchanging human-readable business documents instead of programming parameters. However, in implementing SOA, many businesses have simply used web services to expose the underlying programming models of back-end systems. Procedure oriented integration is nothing more than remote procedure calls implemented via a different protocol. The consequences of this are additional layers of complexity with no improvement in business flexibility. To avoid this, implementers of SOA should first understand the business meaning of their services, then implement human-readable contracts that are independent of legacy system implementation.","blip_selector":"procedure-oriented-integration","name":"Procedure oriented integration","url":"/radar/techniques/procedure-oriented-integration","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9073,"quadrant":"tools","volume_date":"2011-07","description":"Every Infrastructure as a Service (IaaS) cloud offering provides their own API for performing common tasks. Deltacloud aims to abstract those APIs and provide a RESTful interface for performing common cloud management functions, making it possible to migrate virtual infrastructure between clouds.","blip_selector":"deltacloud","name":"Deltacloud","url":"/radar/tools/deltacloud","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9252,"quadrant":"tools","volume_date":"2011-07","description":"When building mobile web applications we can now use Selenium 2 mobile tests to run the same acceptance tests on iOS, Android and Blackberry. This works on emulators, simulators and physical devices. We have successfully used this approach on production software for all 3 platforms. While the Blackberry driver is still in beta, we found it stable enough for use. The key challenge is the different ways to install the driver and start the browser, but this only needs to be solved once. We suggest that companies doing mobile web for these devices try this approach. We see no reason why this approach cannot be extended to Windows Phone in the future.","blip_selector":"selenium-2-testing-of-mobile-websites","name":"Selenium 2 testing of mobile websites","url":"/radar/tools/selenium-2-testing-of-mobile-websites","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9197,"quadrant":"platforms","volume_date":"2011-07","description":"OAuth is a web-friendly, lightweight standard for authorization that allows a user to share private resources between internet services, e.g., allowing your favorite social networking site to access your photos from your favorite photo sharing site. OAuth is simple, avoids password proliferation, and allows a service to grant bare minimum privileges. If you are exposing your application’s data in a lightweight, web-friendly manner you should strongly consider using OAuth as your standard for authorization. | OAuth is a Web-based authorization protocol that allows applications to access a user’s secured resources in another application without the user having to share their private security credentials. Now an RFC, OAuth represents a significant standards-based attempt to improve privacy and security for Web browser and machine-based access to distributed Web resources. Library support is patchy and adopters can expect to spend some time wrangling their code to achieve true interoperability. OAuth 2.0 is due towards the end of 2010, with specific flows for Web applications, desktop applications, mobile phones, and household devices. Because OAuth 2.0 is not backwardly compatible with version 1 and the implementation challenges around the current version, OAuth is still in the assess ring. | The Web is a global data structure that enables us to share information. However not all data is meant to be shared by everyone and it’s important to be able to share information on the Web in a disciplined and governable manner without requiring massive centralized infrastructure. OAuth provides a way of sharing resources on the Web responsibly and securely. It is a Web protocol (for Web browsers or machine-to-machine interactions), which allows federated authorization of access to Web resources. What’s interesting is that OAuth is a simple protocol to implement and utilize and yet its design goals match many common enterprise authorization problems. OAuth remains in the assessment category, however, because it has fragmented, and the IETF has not yet drawn the community back together under an Internet RFC.","blip_selector":"oauth","name":"OAuth","url":"/radar/platforms/oauth","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9276,"quadrant":"tools","volume_date":"2011-07","description":"If your test suites are growing slower and you have already verified that it is not a serious problem with your application, first make your tests faster, then look at parallelization. The Test Load Balancer (TLB) project is a big development in the world of parallel test execution. It removes the inefficiencies of manual work distribution using smart algorithms and historical test execution data to optimize workload distribution and minimize elapsed time. Further, it orders the tests in intelligent ways like executing the test that failed in the previous execution first to get quicker feedback. Parallel execution can occur across a grid of machines or across multiple processes on a single machine. JUnit, RSpec, Test::Unit, Twist and Cucumber are currently supported and NUnit is under development.","blip_selector":"tlb","name":"TLB","url":"/radar/tools/tlb","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9011,"quadrant":"tools","volume_date":"2011-07","description":"Exposing APIs to a large developer community creates nonfunctional requirements that are often the same from one business to another. Key management, authentication, access control, traffic management, caching, tracking and reporting are often implemented as commodity functions that can be reused across applications and businesses without modification. Some service providers have spotted this trend and are offering API management via software as a service. Prominent vendors in this space include Mashery and Apigee, who both offer the option of hosting the service on a customer’s own infrastructure. API management services may also be interesting to enterprise customers who are using Web as platform techniques for their internal SOA, providing a lighter weight alternative to traditional SOA management tools.","blip_selector":"api-management-services","name":"API management services","url":"/radar/tools/api-management-services","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9191,"quadrant":"tools","volume_date":"2011-07","description":"We have regularly used New Relic hosted performance monitoring with Ruby on Rails systems in development and production. The combination of fast setup and comprehensive reporting has proven extremely valuable in troubleshooting performance. We are now seeing good results from the New Relic monitoring services for Java and .NET systems.","blip_selector":"new-relic-beyond-rails","name":"New Relic beyond Rails","url":"/radar/tools/new-relic-beyond-rails","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9071,"quadrant":"techniques","volume_date":"2011-07","description":"Traditional approaches to implementing data warehouses and business intelligence work from the bottom up in horizontal tiers, assembling and cleansing data sources from across the enterprise then aggregating them into a comprehensive data mart before reports can be generated. Some people are now employing an alternative approach that starts with the real outcome--a business decision--and pulls work items through the process as needed to support that decision. Decision driven business intelligence allows a more incremental approach to BI and facilitates rapid feedback to the decision makers who are the ultimate consumers of business intelligence.","blip_selector":"decision-driven-bi","name":"Decision driven BI","url":"/radar/techniques/decision-driven-bi","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9179,"quadrant":"tools","volume_date":"2011-07","description":"In previous radars we recommended Distributed Version Control (DVCS) tools in general while mentioning Git and Mercurial in particular. In this edition we narrow our recommendation to only Mercurial and Git as these two have become the clear front-runners. Due to its success and the associated network effect GitHub remains the recommended option for enterprises that want to interact with the open source community.","blip_selector":"mercurial","name":"Mercurial","url":"/radar/tools/mercurial","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9012,"quadrant":"platforms","volume_date":"2011-01","description":"This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"app-containers","name":"App containers","url":"/radar/platforms/app-containers","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":92281,"quadrant":"tools","volume_date":"2011-01","description":"While we are bullish on RDFa, we remain highly guarded on native RDF triple stores as a persistence mechanism. The leading available triple stores vary greatly in their capabilities, capacity, and performance characteristics. If you are exploring the use of a triple store, you must do extensive testing to make sure the triple store fits your needs.","blip_selector":"rdf-triple-stores","name":"RDF triple stores","url":"/radar/tools/rdf-triple-stores","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9120,"quadrant":"languages-and-frameworks","volume_date":"2011-01","description":"Functional languages have a wide range of practical uses, including simulation, computational fi nance, computational science, large scale data processing and parsing. These fields benefit from functional programming techniques that simplify concurrent execution and the expression of complex mathematical functions concisely. Functional programming requires a shift in thinking for enterprise developers experienced in object oriented development. Moving to an often terse syntax for solving complex problems may initially be intimidating to many. As with all forms of programming languages, syntax is just one aspect of the language itself. In functional programming another significant aspect is the use of common idioms. These idioms speed code comprehension and increase overall maintainability. This might not be news to all, but it is worth noting that dynamic languages are long ready for adoption and trial. Ruby, particularly when deployed on JRuby, is ready for adoption. Thoughtworks uses Ruby and JRuby extensively in both its Services and Product work. Groovy is ready for trial and could prove more accessible than Ruby/JRuby in a Java shop. For the right type of applications, Ruby, JRuby, and Groovy prove far more effective, expressive, and productive than Java and C#.","blip_selector":"groovy","name":"Groovy","url":"/radar/languages-and-frameworks/groovy","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9150,"quadrant":"languages-and-frameworks","volume_date":"2011-01","description":"The purchase of Sun, and thus their Java assets, by Oracle introduced uncertainty regarding the future of Java. This uncertainty continues despite the recent announcements of Oracle’s Java roadmap, which had both encouraging and worrying aspects. As a result we continue to highlight the issue. We recommend monitoring the situation rather than any immediate actions to move off the platform. | As we have discussed previously, the Java language appears to be moving slowly as the Java community waits for Java 7. Having waited for new language features to surface for almost 3 years, the Java community has begun to innovate in new languages that run on the Java Virtual Machine, languages such as Groovy, JRuby, Scala and Clojure. With the increase in number of languages available on the JVM, we expect enterprises to begin to assess the suitability of reducing the amount of Java specific code developed in their enterprise applications in favor of these newer languages. This is not to say that enterprises should outright abandon Java as a programming language, we do however suggest that you look for alternatives that may be more fi t for purpose in the area that new development is taking place. | As C# continues to surge ahead, the Java language appears to be moving slowly as the Java community waits for Java 7. Having waited for new language features to surface for almost 3 years, the Java community has begun to innovate in new languages that run on the Java Virtual Machine, languages such as Groovy, JRuby, Scala and Clojure. With the increase in number of languages available on the JVM, we expect enterprises to begin to assess the suitability of reducing the amount of Java specific code developed in their enterprise applications in favor of these newer languages.","blip_selector":"java-language-end-of-life","name":"Java language end of life","url":"/radar/languages-and-frameworks/java-language-end-of-life","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9267,"quadrant":"tools","volume_date":"2011-01","description":"This tool was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This tool was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This tool was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"squid","name":"Squid","url":"/radar/tools/squid","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9082,"quadrant":"platforms","volume_date":"2011-01","description":"AWS is the most mature and broadest of the current cloud offerings providing scalable services for computation (EC2), storage (S3 & SBS), databases (SimpleDB & RDS), messaging (SQS & SNS), etc. The list of services provided by AWS continues to expand rapidly with new services being added on an almost monthly basis, (http://bit.ly/90887v). While existing applications can be deployed on AWS through the use of Amazon Machine Images the full benefits of this platform will come from applications that are developed to take advantage of AWS. The usage based billing model adopted by AWS allows organizations to scale applications without large upfront investment and avoid the overhead cost of under utilized hardware. | The Cloud continues to be of interest to us, with Software as a Service the most mature cloud component. Platform and Infrastructure as service offerings have reached different levels of maturity, and we reflect that in our placement of EC2, Google App Engine and Azure.","blip_selector":"ec2-s3","name":"EC2 & S3","url":"/radar/platforms/ec2-s3","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9063,"quadrant":"tools","volume_date":"2011-01","description":"While there has been much publicity around Apple’s squashing cross-platform development options for the iPhone and iPad, there are still perfectly valid options. PhoneGap and Appcelerator Titanium’s approach is to provide a native compatibility layer for all the major mobile platforms to your Web standard HTML+CSS+JS application.","blip_selector":"cross-mobile-platforms","name":"Cross mobile platforms","url":"/radar/tools/cross-mobile-platforms","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9021,"quadrant":"techniques","volume_date":"2011-01","description":"We strongly believe that all software delivery organizations need to be making use of automated technical tests. This sort of test spans failover testing, performance testing and soak testing among others; these activities can start early in a project’s life-cycle and continue through to maintenance. The common practice of waiting until near the end of a project is fraught with risk with little time available to find and fix problems. For example the requirement for a comprehensive production-like environment before the start of performance testing is a dangerous fallacy, we can discover bottlenecks, track performance trends and test our performance tests, without waiting for a perfect environment. | Significant advances in the tools for automating functional testing haven’t been replicated in the technical testing space. Data management for performance, load and soak testing is a particular issue. However, the tools are improving and increased visibility for these tools supports the early and often technical testing that we advocate.","blip_selector":"automation-of-technical-tests","name":"Automation of technical tests","url":"/radar/techniques/automation-of-technical-tests","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9290,"quadrant":"techniques","volume_date":"2011-01","description":"Our understanding of the Web has matured to the point where we believe it is a viable platform for building distributed systems. RESTful techniques have advanced past pretty URIs + JSON towards hypermedia systems that project business protocols over the Internet and support seamless business process and service composition. The Web provides a powerful capability for scale, resiliency, and ease of implementation with commodity infrastructure like caches and Web servers with commodity protocols (like HTTP, AtomPub, and OAuth). Moving from trial to adopt is indicative of our position that the Web is ready for primetime, not just for Internet-facing systems but as a practical base for enterprise systems delivery. | We assist many of our clients in adapting enterprise software architecture practices to fit within an Agile software delivery approach. In the past year we have seen increased interest in evolutionary enterprise architecture and how service oriented architectures shape the boundaries between enterprise units. The value of an evolutionary approach to enterprise architecture is the creation of lighter weight systems that ease integration between disparate parts. By embracing this approach and the notion of the web as an enterprise application platform, we have reduced overall complexity of application architectures, increased quality and scalability, and reduced development costs.","blip_selector":"web-as-platform","name":"Web as platform","url":"/radar/techniques/web-as-platform","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9009,"quadrant":"tools","volume_date":"2011-01","description":"In today’s connected systems environments almost all new development needs to integrate with existing applications and services. In conjunction with our adoption of simple message buses and integration techniques at the edges of a system, we have successfully used small libraries such as Apache Camel to perform the protocol bridging, message transformation and message routing tasks common to such integrations. Camel’s fluent Java interface, unit testing support and connectors for many different transports and message formats provide for an effective anti-corruption layer when implementing distributed applications.","blip_selector":"apache-camel","name":"Apache camel","url":"/radar/tools/apache-camel","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9232,"quadrant":"techniques","volume_date":"2011-01","description":"Business are running 24x7, but the information needed to make business decisions is being provided by outdated methods using ETL jobs in batch mode. The batch window is shrinking as businesses move to global markets and are open for longer durations. The data provided by these jobs is out of date by the time the business needs to make a decision. There is substantial value in taking the event as it happens in the transactional system and feeding it to the data warehouse so that the business can get near real-time business intelligence.","blip_selector":"real-time-business-intelligence","name":"Real-time business intelligence","url":"/radar/techniques/real-time-business-intelligence","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9243,"quadrant":"languages-and-frameworks","volume_date":"2011-01","description":"This language/framework was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This language/framework was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"ruby","name":"Ruby","url":"/radar/languages-and-frameworks/ruby","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9015,"quadrant":"platforms","volume_date":"2011-01","description":"This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"application-appliances","name":"Application appliances","url":"/radar/platforms/application-appliances","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9007,"quadrant":"platforms","volume_date":"2011-01","description":"The iPhone and android operating systems have rapidly become key players in the mobile platform marketplace. Apple’s app store and Google’s open source operating system have helped both companies leapfrog the competition in capturing developer mindshare.","blip_selector":"android","name":"Android","url":"/radar/platforms/android","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9098,"quadrant":"platforms","volume_date":"2011-01","description":"Facebook has become popular in part due to its rich API and explosion of third-party applications. Thoughtworks is now starting to see our clients consider Facebook as a business platform. In addition to having a Facebook presence, businesses are building Facebook applications that are tightly integrated with their own services and offer useful functionality to Facebook users.","blip_selector":"facebook-as-business-platform","name":"Facebook as business platform","url":"/radar/platforms/facebook-as-business-platform","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9289,"quadrant":"tools","volume_date":"2011-01","description":"Microsoft’s WCF HTTP API, though currently in the early stages of development, is evolving rapidly, and we’re already impressed by its support for HTTP primitives as well as idioms such as content negotiation and conditional requests. The API encourages the development of highly testable solutions with a clear separation of concerns. What is of particular interest to us is the way in which the project is being developed in Codeplex’s open source community. The ability for the community to steer the development of this part of the .NET platform merits this project’s early inclusion in the radar. While the license does allow for using the library in production today, given the current volatility of the API, we caution against taking a dependency at this early stage; many of the features we’ve admired in recent releases, such as its use of an attributelight programming model based on convention over configuration, may not make it into the first version.","blip_selector":"wcf-http","name":"WCF HTTP","url":"/radar/tools/wcf-http","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9255,"quadrant":"techniques","volume_date":"2011-01","description":"Integrated business processes now routinely span multiple systems and even enterprises. This raises the question of how these processes should be coordinated. In our experience centralized orchestration solutions often fail to deliver the promised benefits. They are costly to implement, and because they maintain application state on behalf of many consumers, they are often difficult to scale. This has lead us to prefer service choreography, where independently distributed participants collaborate according to an application protocol. Using the Web as platform, hypermedia-driven application protocols allow us to implement integrated business processes that are easy to evolve and easy to scale.","blip_selector":"service-choreography","name":"Service choreography","url":"/radar/techniques/service-choreography","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9032,"quadrant":"languages-and-frameworks","volume_date":"2011-01","description":"When C# first appeared, many saw it as a direct competitor to the Java language’s dominance in enterprise application development. This was often attributed to the syntactical similarities that the two languages shared. Since its introduction, however, C# has continued to move forward with the adoption of language features such as lambda expressions, extension methods, object initializers and automatic property setters and getters, all of which are available in the 3.5 release of the language. With the 4.0 release of C#, we will see the introduction of a dynamic keyword and named and optional parameters, which will continue to bring C# more in line with languages such as Ruby and well ahead of the Java language.","blip_selector":"c-4-0","name":"C# 4.0","url":"/radar/languages-and-frameworks/c-4-0","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9228,"quadrant":"platforms","volume_date":"2011-01","description":"RDFa, a mechanism for attaching meaningful vocabularies to HTML content that is being quickly and widely adopted by content providers, is the first mainstream success to arise from the Semantic Web stack. RDFa enables tools ranging from custom point integrations to Google spiders to more richly understand your Web content. If you would like to quickly open up your content to a multitude of integration possibilities in a simple, cheap, standards-based fashion, we recommend you try RDFa. | Semantic Web W3C standards, and the tools implementing them, are at last worthy of real attention. RDF and RDFa allow anyone to say anything about anything in a sharable, structured format. This proves a much more powerful means of linking and structuring data from disparate sources than the strictness of RDBMS, or the mess that is unstructured Web data. Correspondingly SPARQL is the query standard that allows information to be mined from RDF marked-up data. | The semantic web and its underlying technologies, including RDF & SPARQL, have been around for 8 years or more. Broader uptake of the Cloud and non-relational databases such Neo4j have helped move the semantic web into the reach of enterprise developers. Outside of the semantic web, nonrelational databases are being adopted as alternatives to relational databases in a number of situations. Leveraging these technologies will require new approaches to architecture and development that suggest widespread adoption will only occur over a number of years.","blip_selector":"rdfa","name":"RDFa","url":"/radar/platforms/rdfa","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9238,"quadrant":"tools","volume_date":"2011-01","description":"The hypermedia constraint from REST is now understood as critical in sharing business protocols over the Web. Unfortunately many frameworks for building computer to- computer systems on the Web are ignorant of this constraint and tend towards simple CRUD systems. Restfulie is the first of a new generation of frameworks that natively support hypermedia, for Ruby, Java, and .NET. In Restfulie, business protocols are implemented using DSLs and exposed across the Web through hypermedia representations; clients drive those protocols through a similar declarative mechanism, consuming server-generated representations as they work towards a business goal. As the fi rst framework of its kind, Restfulie is opinionated and provides strict “training wheels” in order to bootstrap newcomers. However, it is an empirical proof that the Web and hypermedia can be used to orchestrate complex business activities.","blip_selector":"restfulie","name":"Restfulie","url":"/radar/tools/restfulie","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9029,"quadrant":"techniques","volume_date":"2011-01","description":"The past 2 years or more has seen a proliferation of continuous integration tools and platforms leading to substantial innovation in the build and release space. Distribution of builds is one such innovation and yet another is the way in which builds are now structured to make greater use of automation in various stages of the build. Build pipelines help to provide greater insight into the quality of each build and the environments to which they have been deployed. A natural expansion of the build pipeline meme is the adoption of continuous deployment techniques, where the intention is to extend the build pipeline into the production environment. This relies on automated deployment techniques and authorization mechanisms built into the continuous integration toolset. One of the key benefits is the ability to move new functionality into production rapidly and reliably.","blip_selector":"build-pipelines","name":"Build pipelines","url":"/radar/techniques/build-pipelines","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9016,"quadrant":"tools","volume_date":"2010-08","description":"We have been tracking ASP.NET MVC since its early release candidates. This is an exciting development in the .NET space from Microsoft, both in the programming model and in the open source license under which Microsoft has released the library. ASP.NET MVC is similar to MVC frameworks on the Java platform and is a move away from the ASP.NET Web Forms approach to one that supports greater levels of automated testing.","blip_selector":"asp","name":"ASP","url":"/radar/tools/asp","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9005,"quadrant":"platforms","volume_date":"2010-08","description":"While .NET has proven itself as a solid platform, many practitioners are dissatisfied with many of the default Microsoft tools and practices. This has led to the growth of the Alt.NET community, which champions techniques that we find more effective along with (usually opensource) tools that better support them.","blip_selector":"alt-net","name":"ALT.NET","url":"/radar/platforms/alt-net","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9144,"quadrant":"tools","volume_date":"2010-08","description":"Thoughtworks has been working with Intentional Software for the past several years, and we are thrilled at the recent limited availability and production use of the Intentional Domain Workbench. We believe this technology represents a radical departure from the traditional software development approach. We place this technology in the assess ring, since we believe that it is time to begin exploring the application of Intentional’s technology in proofs of concept.","blip_selector":"intentional-software","name":"Intentional Software","url":"/radar/tools/intentional-software","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9113,"quadrant":"platforms","volume_date":"2010-08","description":"At the start of October 2009, Thoughtworks became a customer of Google Apps. Although we have heard a wide range of opinions about the user experience offered by Google Mail, Calendar and Documents, the general consensus is that our largely consultant workforce is happy with the move. The next step that we as a company are looking to embrace is Google as a corporate platform beyond the standard Google Apps; in particular we are evaluating the use of Google App Engine for a number of internal systems initiatives. Google App Engine, Amazon EC2 and Salesforce.com all claim to be Cloud providers, yet each of their offerings differ. The Cloud fits into a broad categorization of service offerings split out into Infrastructure as a Service (e.g. Amazon EC2 and Rackspace), Platform as a Service (e.g. App Engine) and Software as a Service (e.g. Salesforce.com). In some cases, providers may span multiple service categories, further diluting the Cloud as a label. Regardless, the value of infrastructure, platform and software in the cloud is difficult to question and although many offerings have hit bumps in the road, they certainly have earned their position on the radar.","blip_selector":"google-as-corporate-platform","name":"Google as corporate platform","url":"/radar/platforms/google-as-corporate-platform","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9146,"quadrant":"platforms","volume_date":"2010-08","description":"The iPhone changed the face of the mobile phone. The iPad has the potential to radically alter the way users interact with and consume Web-based resources and applications and will spawn a plethora of similar tablet devices. The addition of wireless application distribution in IOS4 allows organizations to securely host and distribute in-house applications without using the App Store, overcoming one of the main barriers to corporate adoption. IOS4’s introduction of multitasking with applications running in the background has opened up new possibilities for enterprise applications, at the cost of extra battery usage. | The iPhone and android operating systems have rapidly become key players in the mobile platform marketplace. Apple’s app store and Google’s open source operating system have helped both companies leapfrog the competition in capturing developer mindshare.","blip_selector":"iphone","name":"iPhone","url":"/radar/platforms/iphone","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9138,"quadrant":"platforms","volume_date":"2010-08","description":"Internet browsers such as Google Chrome, Safari, Opera and Firefox, have made serious inroads in the implementation of the HTML 5 specification. With these advances it is now possible to experience many of the improvements that HTML brings. Unfortunately so far Microsoft has lagged on implementing these new standards. We recommend that organizations favor standards compliant browsers over IE8. | Web browsers continue to evolve as they strive to keep pace with new specifications in HTML, CSS and JavaScript. Alas, many enterprises have yet to embrace the end of life for IE6 and move to a newer and more standards compliant option. Of the browsers available today, Firefox and Opera provide support for the widest range of platforms. The Google browser, Chrome, brings new innovation to the browser space by splitting browser tabs into separate processes while providing a new implementation of JavaScript. These changes appear to give Chrome a significant performance boost over other browsers and have influenced the creation of a netbook OS called Chrome OS. While enterprises may look to move off IE6 and onto Microsoft’s IE8, we remain concerned about IE8’s current level of compliance to web standards.","blip_selector":"ie8","name":"IE8","url":"/radar/platforms/ie8","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9160,"quadrant":"platforms","volume_date":"2010-08","description":"While the radar has called out the possibility of the Java language nearing its end of life, the JVM is demonstrating its resilience as a general-purpose virtual machine for other languages such as Ruby, Groovy, Scala and Clojure.","blip_selector":"jvm-as-platform","name":"JVM as platform","url":"/radar/platforms/jvm-as-platform","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9137,"quadrant":"platforms","volume_date":"2010-08","description":"IE6 is a significantly deficient browser with many documented security holes and should be phased out as soon as possible. Browsers such as Firefox and Chrome can be installed alongside IE, allowing the user to choose which one to use. We recommend that organizations with intranet applications that require IE6 consider using it only for those specific applications, and install one of these alongside for general use. | Web browsers continue to evolve as they strive to keep pace with new specifications in HTML, CSS and JavaScript. Alas, many enterprises have yet to embrace the end of life for IE6 and move to a newer and more standards compliant option. Of the browsers available today, Firefox and Opera provide support for the widest range of platforms. The Google browser, Chrome, brings new innovation to the browser space by splitting browser tabs into separate processes while providing a new implementation of JavaScript. These changes appear to give Chrome a significant performance boost over other browsers and have influenced the creation of a netbook OS called Chrome OS. While enterprises may look to move off IE6 and onto Microsoft’s IE8, we remain concerned about IE8’s current level of compliance to web standards.","blip_selector":"ie6-end-of-life","name":"IE6 End of Life","url":"/radar/platforms/ie6-end-of-life","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9112,"quadrant":"platforms","volume_date":"2010-08","description":"This platform was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view. | The Cloud continues to be of interest to us, with Software as a Service the most mature cloud component. Platform and Infrastructure as service offerings have reached different levels of maturity, and we reflect that in our placement of EC2, Google App Engine and Azure.","blip_selector":"google-app-engine","name":"Google App Engine","url":"/radar/platforms/google-app-engine","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9167,"quadrant":"platforms","volume_date":"2010-08","description":"Google maps has led the way in bringing mapping mainstream. But businesses, governments and non-profit organizations are now learning to use these location based services to communicate more effectively with customers. With other mapping services providers getting into the act, there is going to be a proliferation of applications built around mapping targeted at customers who are now much more map-savvy.","blip_selector":"location-based-services","name":"Location based services","url":"/radar/platforms/location-based-services","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9224,"quadrant":"tools","volume_date":"2010-08","description":"Puppet is a free, open source data center automation tool for managing changes to your production and production-like environments. Using Puppet, you can keep the configuration of your environments in version control and push changes out to your systems in a controlled, automated fashion. Infrastructure automation tools like puppet have the benefits of reducing manual effort allowing ops to focus on higher priorities, providing consistency and repeatability by reducing waste eliminating environmental differences between test and production environments.","blip_selector":"puppet","name":"Puppet","url":"/radar/tools/puppet","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9287,"quadrant":"tools","volume_date":"2010-04","description":"This tool was included in this edition of the Radar for visibility. We felt that there wasn't anything substantial to add to the discourse around it, but that it was important to keep this in view.","blip_selector":"visualizations-for-business-data","name":"Visualizations for business data","url":"/radar/tools/visualizations-for-business-data","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9078,"quadrant":"tools","volume_date":"2010-04","description":"Subversion moves back into the Adopt section of the radar because it is a solid version control tool suitable for most teams. We consider Subversion’s features to be the basic standard for a modern version control tool. Thoughtworkers continue to embrace and recommend Distributed Version Control tools such as Git and Mercurial, but we caution that these systems often require deeper understanding to get the most out of them. New to the radar is GitHub, a “social coding” tool supporting both source code hosting and social networking. GitHub is arguably one of the main reasons Git has become the leading DVCS tool, and GitHub’s collaboration features are often used by enterprises that need to support distributed teams. | Distributed version control systems such as Git and Mercurial have had significant exposure in the past year or more as open source projects move to this toolset en masse. The social networking aspect that GitHub and Bitbucket have brought to distributed version control has helped to propel these tools forward and into enterprises looking for ways to develop across multiple geographies. The move for many to a distributed version control system has resulted in a move away from tools such as Subversion and other centralized version control systems. As organizations assess and choose between these two different toolsets, we suggest that you evaluate both in relation to your team’s specific needs. While we have seen widespread adoption of distributed version control tools within Thoughtworks and beyond, we still advocate the use of continuous integration and limits to the amount of time that code is spent outside of the main branch.","blip_selector":"distributed-version-control","name":"Distributed version control","url":"/radar/tools/distributed-version-control","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9162,"quadrant":"tools","volume_date":"2010-04","description":"It is likely that test languages will continue to evolve with the assistance of language workbenches, tools that assist in the creation of domain specific languages. Tools such as Jetbrains’ MPS and Intentional Software’s offering are leading the industry in this area. Both provide ways of creating new languages to map business software more closely to the end user’s domain language.","blip_selector":"language-workbenches","name":"Language workbenches","url":"/radar/tools/language-workbenches","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9164,"quadrant":"techniques","volume_date":"2010-04","description":"As Agile practices move further toward mainstream adoption, we see significant benefits from the adoption of Lean software development practices as well. These practices have their roots in the Toyota Production System and complement much of our understanding of Agile software development to date. One topic that Lean has also given us greater insight into is that of set-based design. Set-based design leads us to implement similar solutions at the same time while the cost of doing so is constrained. This leads us into the area of emergent design and the ability to let experience shape our design decisions and defer key decisions until the last responsible moment.","blip_selector":"lean-software-development","name":"Lean software development","url":"/radar/techniques/lean-software-development","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9045,"quadrant":"platforms","volume_date":"2010-04","description":"The Cloud continues to be of interest to us, with Software as a Service the most mature cloud component. Platform and Infrastructure as service offerings have reached different levels of maturity, and we reflect that in our placement of EC2, Google App Engine and Azure.","blip_selector":"cloud","name":"Cloud","url":"/radar/platforms/cloud","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9163,"quadrant":"platforms","volume_date":"2010-04","description":"Large format mobile devices, such as the Apple iPad and Amazon Kindle, provide a new model of ubiquitous computing. Their long battery life, simple interfaces and easy connectivity have the potential to change the way we interact with computers. Apple’s new user interfaces discard the familiar desktop metaphors of files and folders that have been standard since the introduction of the Macintosh in 1984.","blip_selector":"large-format-mobile-devices","name":"Large format mobile devices","url":"/radar/platforms/large-format-mobile-devices","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9041,"quadrant":"platforms","volume_date":"2010-01","description":"Web browsers continue to evolve as they strive to keep pace with new specifications in HTML, CSS and JavaScript. Alas, many enterprises have yet to embrace the end of life for IE6 and move to a newer and more standards compliant option. Of the browsers available today, Firefox and Opera provide support for the widest range of platforms. The Google browser, Chrome, brings new innovation to the browser space by splitting browser tabs into separate processes while providing a new implementation of JavaScript. These changes appear to give Chrome a significant performance boost over other browsers and have influenced the creation of a netbook OS called Chrome OS. While enterprises may look to move off IE6 and onto Microsoft’s IE8, we remain concerned about IE8’s current level of compliance to web standards.","blip_selector":"chrome-os","name":"Chrome OS","url":"/radar/platforms/chrome-os","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"trial","blip_id":9194,"quadrant":"platforms","volume_date":"2010-01","description":"The semantic web and its underlying technologies, including RDF & SPARQL, have been around for 8 years or more. Broader uptake of the Cloud and non-relational databases such Neo4j have helped move the semantic web into the reach of enterprise developers. Outside of the semantic web, nonrelational databases are being adopted as alternatives to relational databases in a number of situations. Leveraging these technologies will require new approaches to architecture and development that suggest widespread adoption will only occur over a number of years.","blip_selector":"non-relational-databases","name":"Non-relational databases","url":"/radar/platforms/non-relational-databases","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9115,"quadrant":"tools","volume_date":"2010-01","description":"Google Wave has sprung up over the past few months and looks to be a promising platform for collaboration over the Internet. The platform is still in early beta and suffers from some stability issues. Some early developers have integrated with the Google Wave platform but commercial releases of software that utilize Google Wave will likely wait until the beta tag has been lifted from the product.","blip_selector":"google-wave","name":"Google Wave","url":"/radar/tools/google-wave","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9040,"quadrant":"platforms","volume_date":"2010-01","description":"Web browsers continue to evolve as they strive to keep pace with new specifications in HTML, CSS and JavaScript. Alas, many enterprises have yet to embrace the end of life for IE6 and move to a newer and more standards compliant option. Of the browsers available today, Firefox and Opera provide support for the widest range of platforms. The Google browser, Chrome, brings new innovation to the browser space by splitting browser tabs into separate processes while providing a new implementation of JavaScript. These changes appear to give Chrome a significant performance boost over other browsers and have influenced the creation of a netbook OS called Chrome OS. While enterprises may look to move off IE6 and onto Microsoft’s IE8, we remain concerned about IE8’s current level of compliance to web standards.","blip_selector":"chrome","name":"Chrome","url":"/radar/platforms/chrome","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9101,"quadrant":"platforms","volume_date":"2010-01","description":"Web browsers continue to evolve as they strive to keep pace with new specifications in HTML, CSS and JavaScript. Alas, many enterprises have yet to embrace the end of life for IE6 and move to a newer and more standards compliant option. Of the browsers available today, Firefox and Opera provide support for the widest range of platforms. The Google browser, Chrome, brings new innovation to the browser space by splitting browser tabs into separate processes while providing a new implementation of JavaScript. These changes appear to give Chrome a significant performance boost over other browsers and have influenced the creation of a netbook OS called Chrome OS. While enterprises may look to move off IE6 and onto Microsoft’s IE8, we remain concerned about IE8’s current level of compliance to web standards.","blip_selector":"firefox","name":"Firefox","url":"/radar/platforms/firefox","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9106,"quadrant":"languages-and-frameworks","volume_date":"2010-01","description":"The remaining two language types included on the radar are often grouped together. While functional and concurrent languages may be adopted in similar environments, their approaches are different. Functional programming focuses on expressing code in the form of mathematical functions that avoid maintaining state across multiple invocations. While functional languages such as Haskell have been around for a number of years, new functional (themed) languages such as Scala, F# and Clojure have sparked some interest in this paradigm. Due to the way in which functional languages manage state, interest in these languages has increased by many developers seeking to make the most out of multi-core processors. Many concurrent languages are also functional languages. The distinction lies in the emphasis on running operations in parallel. A number of such languages exist; Erlang is currently the most popular of these languages. Concurrent languages commonly provide some means for handling concurrency by using messages to communicate across multiple threads.","blip_selector":"functional-languages","name":"Functional languages","url":"/radar/languages-and-frameworks/functional-languages","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"assess","blip_id":9055,"quadrant":"languages-and-frameworks","volume_date":"2010-01","description":"The remaining two language types included on the radar are often grouped together. While functional and concurrent languages may be adopted in similar environments, their approaches are different. Functional programming focuses on expressing code in the form of mathematical functions that avoid maintaining state across multiple invocations. While functional languages such as Haskell have been around for a number of years, new functional (themed) languages such as Scala, F# and Clojure have sparked some interest in this paradigm. Due to the way in which functional languages manage state, interest in these languages has increased by many developers seeking to make the most out of multi-core processors. Many concurrent languages are also functional languages. The distinction lies in the emphasis on running operations in parallel. A number of such languages exist; Erlang is currently the most popular of these languages. Concurrent languages commonly provide some means for handling concurrency by using messages to communicate across multiple threads.","blip_selector":"concurrent-languages","name":"Concurrent languages","url":"/radar/languages-and-frameworks/concurrent-languages","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"hold","blip_id":9213,"quadrant":"tools","volume_date":"2010-01","description":"Polyglot programming continues to gain widespread acceptance across the industry reflecting the reality that software developers have many languages and tools at their disposal. One area that we have yet to see take off is the creation of polyglot development environments, capable of satisfying multiple language needs of development teams. While Eclipse, IntelliJ, Visual Studio and others have some cross-language capabilities, their support for a wide range of languages is limited at best.","blip_selector":"polyglot-development-environments","name":"Polyglot development environments","url":"/radar/tools/polyglot-development-environments","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"},{"ring":"adopt","blip_id":9281,"quadrant":"techniques","volume_date":"2010-01","description":"The benefits of user-centered design are often understated. Gaining a broader understanding of data flows and users’ goals simplify the overall architecture of a system while optimizing user interaction. In the past year we have seen a greater uptake of user-centered design in Agile software development practices as experts in both fields have established new ways of working together.","blip_selector":"user-centered-design","name":"User centered design","url":"/radar/techniques/user-centered-design","isCurrentEdition":false,"isRecentEdition":false,"last_modified_date":"2023-08-01T10:34:30.917Z"}]